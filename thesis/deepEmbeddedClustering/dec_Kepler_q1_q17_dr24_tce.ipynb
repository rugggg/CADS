{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kepler_data = pd.read_csv(\"kepler_small_sample.csv\", header=159)\n",
    "kepler_data = pd.read_csv(\"kepler_data_full.csv\", header=159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rowid', 'kepid', 'tce_plnt_num', 'tce_rogue_flag',\n",
       "       'tce_delivname', 'rowupdate', 'tce_datalink_dvs',\n",
       "       'tce_datalink_dvr', 'tce_period', 'tce_period_err', 'tce_time0bk',\n",
       "       'tce_time0bk_err', 'tce_time0', 'tce_time0_err', 'tce_ror',\n",
       "       'tce_ror_err', 'tce_dor', 'tce_dor_err', 'tce_incl',\n",
       "       'tce_incl_err', 'tce_impact', 'tce_impact_err', 'tce_duration',\n",
       "       'tce_duration_err', 'tce_ingress', 'tce_ingress_err', 'tce_depth',\n",
       "       'tce_depth_err', 'tce_eccen', 'tce_eccen_err', 'tce_longp',\n",
       "       'tce_longp_err', 'tce_limbdark_mod', 'tce_ldm_coeff1',\n",
       "       'tce_ldm_coeff2', 'tce_ldm_coeff3', 'tce_ldm_coeff4',\n",
       "       'tce_num_transits', 'tce_trans_mod', 'tce_full_conv',\n",
       "       'tce_model_snr', 'tce_model_chisq', 'tce_model_dof', 'tce_robstat',\n",
       "       'tce_dof1', 'tce_dof2', 'tce_chisq1', 'tce_chisq2',\n",
       "       'tce_chisqgofdof', 'tce_chisqgof', 'tce_prad', 'tce_prad_err',\n",
       "       'tce_sma', 'tce_sma_err', 'tce_eqt', 'tce_eqt_err', 'tce_insol',\n",
       "       'tce_insol_err', 'tce_nkoi', 'tce_ioflag', 'tce_quarters',\n",
       "       'tce_steff', 'tce_steff_err', 'tce_slogg', 'tce_slogg_err',\n",
       "       'tce_smet', 'tce_smet_err', 'tce_sradius', 'tce_sradius_err',\n",
       "       'tce_steff_prov', 'tce_slogg_prov', 'tce_smet_prov',\n",
       "       'tce_sradius_prov', 'tcet_period', 'tcet_period_err',\n",
       "       'tcet_time0bk', 'tcet_time0bk_err', 'tcet_time0', 'tcet_time0_err',\n",
       "       'tcet_duration', 'tcet_duration_err', 'tcet_ingress',\n",
       "       'tcet_ingress_err', 'tcet_depth', 'tcet_depth_err',\n",
       "       'tcet_full_conv', 'tcet_model_chisq', 'tcet_model_dof',\n",
       "       'wst_robstat', 'wst_depth', 'tce_mesmedian', 'tce_mesmad',\n",
       "       'tce_maxmes', 'tce_minmes', 'tce_maxmesd', 'tce_minmesd',\n",
       "       'tce_max_sngle_ev', 'tce_max_mult_ev', 'tce_bin_oedp_stat',\n",
       "       'tce_rmesmad', 'tce_rsnrmes', 'tce_rminmes', 'tce_albedo',\n",
       "       'tce_albedo_err', 'tce_ptemp', 'tce_ptemp_err', 'tce_albedo_stat',\n",
       "       'tce_ptemp_stat', 'av_vf_pc', 'av_vf_pc_err', 'av_vf_afp',\n",
       "       'av_vf_afp_err', 'av_vf_ntp', 'av_vf_ntp_err', 'av_pp_pc',\n",
       "       'av_pp_afp', 'av_pp_ntp', 'av_training_set', 'av_pred_class',\n",
       "       'boot_fap', 'boot_mesthresh', 'boot_mesmean', 'boot_messtd',\n",
       "       'tce_cap_stat', 'tce_hap_stat', 'tce_rb_tpdur', 'tce_rb_tcount0',\n",
       "       'tce_rb_tcount1', 'tce_rb_tcount2', 'tce_rb_tcount3',\n",
       "       'tce_rb_tcount4', 'tce_fwm_stat', 'tce_fwm_sra', 'tce_fwm_sra_err',\n",
       "       'tce_fwm_sdec', 'tce_fwm_sdec_err', 'tce_fwm_srao',\n",
       "       'tce_fwm_srao_err', 'tce_fwm_sdeco', 'tce_fwm_sdeco_err',\n",
       "       'tce_fwm_prao', 'tce_fwm_prao_err', 'tce_fwm_pdeco',\n",
       "       'tce_fwm_pdeco_err', 'tce_dicco_mra', 'tce_dicco_mra_err',\n",
       "       'tce_dicco_mdec', 'tce_dicco_mdec_err', 'tce_dicco_msky',\n",
       "       'tce_dicco_msky_err', 'tce_dikco_mra', 'tce_dikco_mra_err',\n",
       "       'tce_dikco_mdec', 'tce_dikco_mdec_err', 'tce_dikco_msky',\n",
       "       'tce_dikco_msky_err'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kepler_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kepler_data.columns.values.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowid</th>\n",
       "      <th>kepid</th>\n",
       "      <th>tce_plnt_num</th>\n",
       "      <th>tce_rogue_flag</th>\n",
       "      <th>tce_delivname</th>\n",
       "      <th>rowupdate</th>\n",
       "      <th>tce_datalink_dvs</th>\n",
       "      <th>tce_datalink_dvr</th>\n",
       "      <th>tce_period</th>\n",
       "      <th>tce_period_err</th>\n",
       "      <th>...</th>\n",
       "      <th>tce_dicco_mdec</th>\n",
       "      <th>tce_dicco_mdec_err</th>\n",
       "      <th>tce_dicco_msky</th>\n",
       "      <th>tce_dicco_msky_err</th>\n",
       "      <th>tce_dikco_mra</th>\n",
       "      <th>tce_dikco_mra_err</th>\n",
       "      <th>tce_dikco_mdec</th>\n",
       "      <th>tce_dikco_mdec_err</th>\n",
       "      <th>tce_dikco_msky</th>\n",
       "      <th>tce_dikco_msky_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1162345</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001162/001162345/dv/kplr001162345-002-2014...</td>\n",
       "      <td>001/001162/001162345/dv/kplr001162345-20141002...</td>\n",
       "      <td>0.831850</td>\n",
       "      <td>6.693070e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504060</td>\n",
       "      <td>1.2900</td>\n",
       "      <td>0.507070</td>\n",
       "      <td>1.1200</td>\n",
       "      <td>-0.183490</td>\n",
       "      <td>1.2100</td>\n",
       "      <td>0.424890</td>\n",
       "      <td>1.3000</td>\n",
       "      <td>0.462820</td>\n",
       "      <td>1.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1292087</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001292/001292087/dv/kplr001292087-002-2014...</td>\n",
       "      <td>001/001292/001292087/dv/kplr001292087-20141002...</td>\n",
       "      <td>1.095240</td>\n",
       "      <td>1.009340e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075798</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.097899</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>0.206520</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>-0.077214</td>\n",
       "      <td>0.2380</td>\n",
       "      <td>0.220490</td>\n",
       "      <td>0.2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1293031</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001293/001293031/dv/kplr001293031-002-2014...</td>\n",
       "      <td>001/001293/001293031/dv/kplr001293031-20141002...</td>\n",
       "      <td>0.719273</td>\n",
       "      <td>1.388330e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337380</td>\n",
       "      <td>0.2890</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>-0.308090</td>\n",
       "      <td>0.1910</td>\n",
       "      <td>0.283570</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.418720</td>\n",
       "      <td>0.2070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1162345</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001162/001162345/dv/kplr001162345-003-2014...</td>\n",
       "      <td>001/001162/001162345/dv/kplr001162345-20141002...</td>\n",
       "      <td>0.831833</td>\n",
       "      <td>9.431020e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167200</td>\n",
       "      <td>0.1830</td>\n",
       "      <td>0.253410</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.087066</td>\n",
       "      <td>0.2510</td>\n",
       "      <td>0.106320</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.137420</td>\n",
       "      <td>0.1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1164109</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001164/001164109/dv/kplr001164109-001-2014...</td>\n",
       "      <td>001/001164/001164109/dv/kplr001164109-20141002...</td>\n",
       "      <td>622.408000</td>\n",
       "      <td>8.547390e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>10.358000</td>\n",
       "      <td>0.0894</td>\n",
       "      <td>15.177000</td>\n",
       "      <td>0.0893</td>\n",
       "      <td>-11.060000</td>\n",
       "      <td>0.0892</td>\n",
       "      <td>10.437000</td>\n",
       "      <td>0.0894</td>\n",
       "      <td>15.207000</td>\n",
       "      <td>0.0893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>757450</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>000/000757/000757450/dv/kplr000757450-001-2014...</td>\n",
       "      <td>000/000757/000757450/dv/kplr000757450-20141002...</td>\n",
       "      <td>8.884920</td>\n",
       "      <td>1.186150e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072221</td>\n",
       "      <td>0.0697</td>\n",
       "      <td>0.072362</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.069581</td>\n",
       "      <td>0.0719</td>\n",
       "      <td>-0.103820</td>\n",
       "      <td>0.0782</td>\n",
       "      <td>0.124980</td>\n",
       "      <td>0.0763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>892667</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>000/000892/000892667/dv/kplr000892667-001-2014...</td>\n",
       "      <td>000/000892/000892667/dv/kplr000892667-20141002...</td>\n",
       "      <td>2.262110</td>\n",
       "      <td>2.726600e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303870</td>\n",
       "      <td>0.1920</td>\n",
       "      <td>0.411080</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.139510</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.115410</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.181060</td>\n",
       "      <td>0.1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>892772</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>000/000892/000892772/dv/kplr000892772-001-2014...</td>\n",
       "      <td>000/000892/000892772/dv/kplr000892772-20141002...</td>\n",
       "      <td>5.092600</td>\n",
       "      <td>3.144550e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.296300</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>4.920200</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>-4.962200</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>-1.555100</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>5.200200</td>\n",
       "      <td>0.1650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1026032</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001026/001026032/dv/kplr001026032-001-2014...</td>\n",
       "      <td>001/001026/001026032/dv/kplr001026032-20141002...</td>\n",
       "      <td>8.460440</td>\n",
       "      <td>3.822250e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051352</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.078983</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>-0.004233</td>\n",
       "      <td>0.0737</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.064587</td>\n",
       "      <td>0.0770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1026032</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001026/001026032/dv/kplr001026032-002-2014...</td>\n",
       "      <td>001/001026/001026032/dv/kplr001026032-20141002...</td>\n",
       "      <td>4.230220</td>\n",
       "      <td>7.327150e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003027</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>0.051184</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>-0.014758</td>\n",
       "      <td>0.0716</td>\n",
       "      <td>-0.137870</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.138650</td>\n",
       "      <td>0.0840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1026133</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001026/001026133/dv/kplr001026133-002-2014...</td>\n",
       "      <td>001/001026/001026133/dv/kplr001026133-20141002...</td>\n",
       "      <td>2.691910</td>\n",
       "      <td>2.128260e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010105</td>\n",
       "      <td>0.2830</td>\n",
       "      <td>0.154550</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.042965</td>\n",
       "      <td>0.2510</td>\n",
       "      <td>-0.076421</td>\n",
       "      <td>0.2620</td>\n",
       "      <td>0.087671</td>\n",
       "      <td>0.2490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1026133</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001026/001026133/dv/kplr001026133-001-2014...</td>\n",
       "      <td>001/001026/001026133/dv/kplr001026133-20141002...</td>\n",
       "      <td>1.346290</td>\n",
       "      <td>2.253040e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.747430</td>\n",
       "      <td>0.6710</td>\n",
       "      <td>1.426300</td>\n",
       "      <td>0.5660</td>\n",
       "      <td>1.213400</td>\n",
       "      <td>0.6350</td>\n",
       "      <td>-0.785060</td>\n",
       "      <td>0.6510</td>\n",
       "      <td>1.445200</td>\n",
       "      <td>0.5700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1028018</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001028/001028018/dv/kplr001028018-001-2014...</td>\n",
       "      <td>001/001028/001028018/dv/kplr001028018-20141002...</td>\n",
       "      <td>0.614378</td>\n",
       "      <td>9.733760e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049380</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.052617</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>0.075046</td>\n",
       "      <td>0.0701</td>\n",
       "      <td>0.173300</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.188850</td>\n",
       "      <td>0.1310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1026957</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001026/001026957/dv/kplr001026957-001-2014...</td>\n",
       "      <td>001/001026/001026957/dv/kplr001026957-20141002...</td>\n",
       "      <td>21.761300</td>\n",
       "      <td>1.002540e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149110</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>0.155260</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>-0.078708</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>-0.282140</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.292920</td>\n",
       "      <td>0.1580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1160891</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001160/001160891/dv/kplr001160891-001-2014...</td>\n",
       "      <td>001/001160/001160891/dv/kplr001160891-20141002...</td>\n",
       "      <td>0.940463</td>\n",
       "      <td>2.619800e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104310</td>\n",
       "      <td>0.3170</td>\n",
       "      <td>0.255800</td>\n",
       "      <td>0.3960</td>\n",
       "      <td>-0.144430</td>\n",
       "      <td>0.3160</td>\n",
       "      <td>0.097590</td>\n",
       "      <td>0.3160</td>\n",
       "      <td>0.174310</td>\n",
       "      <td>0.3830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1160891</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001160/001160891/dv/kplr001160891-002-2014...</td>\n",
       "      <td>001/001160/001160891/dv/kplr001160891-20141002...</td>\n",
       "      <td>0.940463</td>\n",
       "      <td>8.053180e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223580</td>\n",
       "      <td>0.2560</td>\n",
       "      <td>0.223670</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.063502</td>\n",
       "      <td>0.2340</td>\n",
       "      <td>0.267290</td>\n",
       "      <td>0.2620</td>\n",
       "      <td>0.274730</td>\n",
       "      <td>0.2910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1160891</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001160/001160891/dv/kplr001160891-003-2014...</td>\n",
       "      <td>001/001160/001160891/dv/kplr001160891-20141002...</td>\n",
       "      <td>0.940520</td>\n",
       "      <td>7.876050e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073798</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>0.191460</td>\n",
       "      <td>0.2360</td>\n",
       "      <td>-0.057165</td>\n",
       "      <td>0.2190</td>\n",
       "      <td>0.071770</td>\n",
       "      <td>0.1710</td>\n",
       "      <td>0.091754</td>\n",
       "      <td>0.2330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1162150</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001162/001162150/dv/kplr001162150-002-2014...</td>\n",
       "      <td>001/001162/001162150/dv/kplr001162150-20141002...</td>\n",
       "      <td>0.778568</td>\n",
       "      <td>9.102830e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731240</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>1.624200</td>\n",
       "      <td>0.9160</td>\n",
       "      <td>1.851500</td>\n",
       "      <td>1.0500</td>\n",
       "      <td>-1.255200</td>\n",
       "      <td>0.6740</td>\n",
       "      <td>2.236900</td>\n",
       "      <td>0.9280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1162150</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001162/001162150/dv/kplr001162150-001-2014...</td>\n",
       "      <td>001/001162/001162150/dv/kplr001162150-20141002...</td>\n",
       "      <td>0.778574</td>\n",
       "      <td>7.102600e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.204600</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>1.911200</td>\n",
       "      <td>0.1740</td>\n",
       "      <td>1.749900</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>-1.540000</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>2.331000</td>\n",
       "      <td>0.1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1161345</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001161/001161345/dv/kplr001161345-001-2014...</td>\n",
       "      <td>001/001161/001161345/dv/kplr001161345-20141002...</td>\n",
       "      <td>4.287580</td>\n",
       "      <td>1.116950e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.448280</td>\n",
       "      <td>0.2040</td>\n",
       "      <td>0.755810</td>\n",
       "      <td>0.2680</td>\n",
       "      <td>-0.745250</td>\n",
       "      <td>0.3280</td>\n",
       "      <td>-0.323000</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.812240</td>\n",
       "      <td>0.3050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1162150</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001162/001162150/dv/kplr001162150-003-2014...</td>\n",
       "      <td>001/001162/001162150/dv/kplr001162150-20141002...</td>\n",
       "      <td>1.284530</td>\n",
       "      <td>2.585230e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.586720</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>1.745700</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>2.052400</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>-1.051400</td>\n",
       "      <td>0.2530</td>\n",
       "      <td>2.306100</td>\n",
       "      <td>0.3940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1293031</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001293/001293031/dv/kplr001293031-001-2014...</td>\n",
       "      <td>001/001293/001293031/dv/kplr001293031-20141002...</td>\n",
       "      <td>1.078610</td>\n",
       "      <td>1.272980e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208310</td>\n",
       "      <td>0.5190</td>\n",
       "      <td>0.395070</td>\n",
       "      <td>0.3390</td>\n",
       "      <td>0.355910</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.171660</td>\n",
       "      <td>0.5540</td>\n",
       "      <td>0.395150</td>\n",
       "      <td>0.3150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1162345</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001162/001162345/dv/kplr001162345-001-2014...</td>\n",
       "      <td>001/001162/001162345/dv/kplr001162345-20141002...</td>\n",
       "      <td>0.831777</td>\n",
       "      <td>1.136930e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628050</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.629640</td>\n",
       "      <td>0.2530</td>\n",
       "      <td>-0.166720</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>0.570610</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.594470</td>\n",
       "      <td>0.2260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1292087</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001292/001292087/dv/kplr001292087-001-2014...</td>\n",
       "      <td>001/001292/001292087/dv/kplr001292087-20141002...</td>\n",
       "      <td>0.547626</td>\n",
       "      <td>2.340840e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.159040</td>\n",
       "      <td>0.1540</td>\n",
       "      <td>0.205660</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.089230</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.224180</td>\n",
       "      <td>0.1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1573174</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001573/001573174/dv/kplr001573174-002-2014...</td>\n",
       "      <td>001/001573/001573174/dv/kplr001573174-20141002...</td>\n",
       "      <td>0.748347</td>\n",
       "      <td>7.786200e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102230</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.129850</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.0850</td>\n",
       "      <td>0.034881</td>\n",
       "      <td>0.1560</td>\n",
       "      <td>0.037720</td>\n",
       "      <td>0.1550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1574792</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001574/001574792/dv/kplr001574792-001-2014...</td>\n",
       "      <td>001/001574/001574792/dv/kplr001574792-20141002...</td>\n",
       "      <td>1.391750</td>\n",
       "      <td>1.488460e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270060</td>\n",
       "      <td>0.3060</td>\n",
       "      <td>0.275020</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>0.118790</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.296830</td>\n",
       "      <td>0.3060</td>\n",
       "      <td>0.319720</td>\n",
       "      <td>0.3490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1575690</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001575/001575690/dv/kplr001575690-001-2014...</td>\n",
       "      <td>001/001575/001575690/dv/kplr001575690-20141002...</td>\n",
       "      <td>1.126200</td>\n",
       "      <td>1.569320e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012700</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>1.619800</td>\n",
       "      <td>0.7430</td>\n",
       "      <td>0.037418</td>\n",
       "      <td>0.0701</td>\n",
       "      <td>-0.162080</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>0.166340</td>\n",
       "      <td>0.0704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1575873</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001575/001575873/dv/kplr001575873-001-2014...</td>\n",
       "      <td>001/001575/001575873/dv/kplr001575873-20141002...</td>\n",
       "      <td>0.629008</td>\n",
       "      <td>7.191270e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>10.172000</td>\n",
       "      <td>0.3450</td>\n",
       "      <td>10.349000</td>\n",
       "      <td>0.3240</td>\n",
       "      <td>1.821800</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>10.058000</td>\n",
       "      <td>0.3630</td>\n",
       "      <td>10.221000</td>\n",
       "      <td>0.3410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1865567</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001865/001865567/dv/kplr001865567-001-2014...</td>\n",
       "      <td>001/001865/001865567/dv/kplr001865567-20141002...</td>\n",
       "      <td>0.674634</td>\n",
       "      <td>1.044420e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160620</td>\n",
       "      <td>0.2830</td>\n",
       "      <td>0.181040</td>\n",
       "      <td>0.2710</td>\n",
       "      <td>0.191810</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.246790</td>\n",
       "      <td>0.3090</td>\n",
       "      <td>0.312560</td>\n",
       "      <td>0.1320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1865567</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>001/001865/001865567/dv/kplr001865567-002-2014...</td>\n",
       "      <td>001/001865/001865567/dv/kplr001865567-20141002...</td>\n",
       "      <td>1.349420</td>\n",
       "      <td>8.020030e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.566710</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.572150</td>\n",
       "      <td>0.1710</td>\n",
       "      <td>0.248410</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>-0.470940</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.532440</td>\n",
       "      <td>0.1680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20337</th>\n",
       "      <td>20338</td>\n",
       "      <td>12508604</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>012/012508/012508604/dv/kplr012508604-002-2014...</td>\n",
       "      <td>012/012508/012508604/dv/kplr012508604-20141002...</td>\n",
       "      <td>0.557521</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20338</th>\n",
       "      <td>20339</td>\n",
       "      <td>12508604</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>012/012508/012508604/dv/kplr012508604-003-2014...</td>\n",
       "      <td>012/012508/012508604/dv/kplr012508604-20141002...</td>\n",
       "      <td>1.114970</td>\n",
       "      <td>1.145120e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109260</td>\n",
       "      <td>0.0940</td>\n",
       "      <td>0.110940</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>-0.064962</td>\n",
       "      <td>0.0783</td>\n",
       "      <td>-0.167340</td>\n",
       "      <td>0.0945</td>\n",
       "      <td>0.179510</td>\n",
       "      <td>0.0942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20339</th>\n",
       "      <td>20340</td>\n",
       "      <td>12509148</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>012/012509/012509148/dv/kplr012509148-001-2014...</td>\n",
       "      <td>012/012509/012509148/dv/kplr012509148-20141002...</td>\n",
       "      <td>1.556300</td>\n",
       "      <td>1.155740e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812390</td>\n",
       "      <td>0.6960</td>\n",
       "      <td>0.862270</td>\n",
       "      <td>0.6620</td>\n",
       "      <td>0.261980</td>\n",
       "      <td>0.3180</td>\n",
       "      <td>0.864440</td>\n",
       "      <td>0.8690</td>\n",
       "      <td>0.903270</td>\n",
       "      <td>0.8320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20340</th>\n",
       "      <td>20341</td>\n",
       "      <td>11671226</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011671/011671226/dv/kplr011671226-002-2014...</td>\n",
       "      <td>011/011671/011671226/dv/kplr011671226-20141002...</td>\n",
       "      <td>2.957120</td>\n",
       "      <td>9.978780e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.203360</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.329760</td>\n",
       "      <td>0.3970</td>\n",
       "      <td>-0.496030</td>\n",
       "      <td>0.3310</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>0.663090</td>\n",
       "      <td>0.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20341</th>\n",
       "      <td>20342</td>\n",
       "      <td>11671226</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011671/011671226/dv/kplr011671226-003-2014...</td>\n",
       "      <td>011/011671/011671226/dv/kplr011671226-20141002...</td>\n",
       "      <td>2.957150</td>\n",
       "      <td>1.009300e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281590</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.323840</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>-0.064916</td>\n",
       "      <td>0.4040</td>\n",
       "      <td>0.037741</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.075090</td>\n",
       "      <td>0.3970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20342</th>\n",
       "      <td>20343</td>\n",
       "      <td>11671226</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011671/011671226/dv/kplr011671226-004-2014...</td>\n",
       "      <td>011/011671/011671226/dv/kplr011671226-20141002...</td>\n",
       "      <td>2.957100</td>\n",
       "      <td>1.386450e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.355560</td>\n",
       "      <td>0.2760</td>\n",
       "      <td>0.391290</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>-0.404980</td>\n",
       "      <td>0.3430</td>\n",
       "      <td>-0.560460</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.691470</td>\n",
       "      <td>0.4370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20343</th>\n",
       "      <td>20344</td>\n",
       "      <td>11671226</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011671/011671226/dv/kplr011671226-005-2014...</td>\n",
       "      <td>011/011671/011671226/dv/kplr011671226-20141002...</td>\n",
       "      <td>2.957170</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20344</th>\n",
       "      <td>20345</td>\n",
       "      <td>11671429</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011671/011671429/dv/kplr011671429-001-2014...</td>\n",
       "      <td>011/011671/011671429/dv/kplr011671429-20141002...</td>\n",
       "      <td>112.464000</td>\n",
       "      <td>3.037760e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217970</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.220220</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.308590</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.282250</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.2580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20345</th>\n",
       "      <td>20346</td>\n",
       "      <td>11717240</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011717/011717240/dv/kplr011717240-001-2014...</td>\n",
       "      <td>011/011717/011717240/dv/kplr011717240-20141002...</td>\n",
       "      <td>1.035500</td>\n",
       "      <td>1.434610e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064943</td>\n",
       "      <td>1.6000</td>\n",
       "      <td>1.241700</td>\n",
       "      <td>1.6000</td>\n",
       "      <td>1.106500</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>-0.006294</td>\n",
       "      <td>1.4700</td>\n",
       "      <td>1.106500</td>\n",
       "      <td>1.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20346</th>\n",
       "      <td>20347</td>\n",
       "      <td>11717240</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011717/011717240/dv/kplr011717240-002-2014...</td>\n",
       "      <td>011/011717/011717240/dv/kplr011717240-20141002...</td>\n",
       "      <td>1.872050</td>\n",
       "      <td>2.638920e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.816600</td>\n",
       "      <td>0.9370</td>\n",
       "      <td>2.494700</td>\n",
       "      <td>0.6340</td>\n",
       "      <td>-1.873700</td>\n",
       "      <td>0.8510</td>\n",
       "      <td>0.805420</td>\n",
       "      <td>0.7360</td>\n",
       "      <td>2.039500</td>\n",
       "      <td>0.7640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20347</th>\n",
       "      <td>20348</td>\n",
       "      <td>11718053</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011718/011718053/dv/kplr011718053-001-2014...</td>\n",
       "      <td>011/011718/011718053/dv/kplr011718053-20141002...</td>\n",
       "      <td>1.316180</td>\n",
       "      <td>1.237290e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.907540</td>\n",
       "      <td>0.7570</td>\n",
       "      <td>0.909970</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>-0.097671</td>\n",
       "      <td>0.4160</td>\n",
       "      <td>-0.882480</td>\n",
       "      <td>0.8180</td>\n",
       "      <td>0.887870</td>\n",
       "      <td>0.5660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20348</th>\n",
       "      <td>20349</td>\n",
       "      <td>11718144</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011718/011718144/dv/kplr011718144-001-2014...</td>\n",
       "      <td>011/011718/011718144/dv/kplr011718144-20141002...</td>\n",
       "      <td>16.458100</td>\n",
       "      <td>4.851470e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384610</td>\n",
       "      <td>0.4630</td>\n",
       "      <td>0.399770</td>\n",
       "      <td>0.5350</td>\n",
       "      <td>-0.082912</td>\n",
       "      <td>0.3510</td>\n",
       "      <td>0.463960</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>0.471310</td>\n",
       "      <td>0.5160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20349</th>\n",
       "      <td>20350</td>\n",
       "      <td>11764462</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011764/011764462/dv/kplr011764462-001-2014...</td>\n",
       "      <td>011/011764/011764462/dv/kplr011764462-20141002...</td>\n",
       "      <td>5.699200</td>\n",
       "      <td>1.056740e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000521</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.041172</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.031105</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.257480</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.259350</td>\n",
       "      <td>0.2440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20350</th>\n",
       "      <td>20351</td>\n",
       "      <td>11766996</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011766/011766996/dv/kplr011766996-001-2014...</td>\n",
       "      <td>011/011766/011766996/dv/kplr011766996-20141002...</td>\n",
       "      <td>2.570150</td>\n",
       "      <td>3.561260e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107490</td>\n",
       "      <td>0.6820</td>\n",
       "      <td>0.146050</td>\n",
       "      <td>0.7950</td>\n",
       "      <td>0.061271</td>\n",
       "      <td>0.4770</td>\n",
       "      <td>0.153500</td>\n",
       "      <td>0.6340</td>\n",
       "      <td>0.165280</td>\n",
       "      <td>0.7560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20351</th>\n",
       "      <td>20352</td>\n",
       "      <td>11766996</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011766/011766996/dv/kplr011766996-002-2014...</td>\n",
       "      <td>011/011766/011766996/dv/kplr011766996-20141002...</td>\n",
       "      <td>2.569230</td>\n",
       "      <td>2.162220e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.2770</td>\n",
       "      <td>0.165280</td>\n",
       "      <td>0.2690</td>\n",
       "      <td>0.045429</td>\n",
       "      <td>0.2420</td>\n",
       "      <td>0.186720</td>\n",
       "      <td>0.2690</td>\n",
       "      <td>0.192160</td>\n",
       "      <td>0.2670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20352</th>\n",
       "      <td>20353</td>\n",
       "      <td>11766996</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011766/011766996/dv/kplr011766996-003-2014...</td>\n",
       "      <td>011/011766/011766996/dv/kplr011766996-20141002...</td>\n",
       "      <td>2.569240</td>\n",
       "      <td>2.707320e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927950</td>\n",
       "      <td>0.6180</td>\n",
       "      <td>0.998970</td>\n",
       "      <td>0.7860</td>\n",
       "      <td>0.330870</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>0.973240</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>1.027900</td>\n",
       "      <td>0.7060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20353</th>\n",
       "      <td>20354</td>\n",
       "      <td>11809145</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011809/011809145/dv/kplr011809145-001-2014...</td>\n",
       "      <td>011/011809/011809145/dv/kplr011809145-20141002...</td>\n",
       "      <td>1.067020</td>\n",
       "      <td>1.023600e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.266600</td>\n",
       "      <td>0.8420</td>\n",
       "      <td>1.417400</td>\n",
       "      <td>0.8890</td>\n",
       "      <td>0.915380</td>\n",
       "      <td>0.5980</td>\n",
       "      <td>-1.707600</td>\n",
       "      <td>0.8290</td>\n",
       "      <td>1.937400</td>\n",
       "      <td>0.8810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20354</th>\n",
       "      <td>20355</td>\n",
       "      <td>11809179</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011809/011809179/dv/kplr011809179-001-2014...</td>\n",
       "      <td>011/011809/011809179/dv/kplr011809179-20141002...</td>\n",
       "      <td>0.972957</td>\n",
       "      <td>1.654570e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184260</td>\n",
       "      <td>0.3260</td>\n",
       "      <td>0.394530</td>\n",
       "      <td>0.3390</td>\n",
       "      <td>0.369440</td>\n",
       "      <td>0.2710</td>\n",
       "      <td>0.117130</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.387570</td>\n",
       "      <td>0.3250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20355</th>\n",
       "      <td>20356</td>\n",
       "      <td>11809179</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011809/011809179/dv/kplr011809179-002-2014...</td>\n",
       "      <td>011/011809/011809179/dv/kplr011809179-20141002...</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>6.689970e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510440</td>\n",
       "      <td>0.5830</td>\n",
       "      <td>0.531790</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>-0.118650</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>0.445010</td>\n",
       "      <td>0.6130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20356</th>\n",
       "      <td>20357</td>\n",
       "      <td>11809346</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011809/011809346/dv/kplr011809346-001-2014...</td>\n",
       "      <td>011/011809/011809346/dv/kplr011809346-20141002...</td>\n",
       "      <td>0.624575</td>\n",
       "      <td>1.570140e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>0.0850</td>\n",
       "      <td>0.014216</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.087154</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>-0.023840</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.090355</td>\n",
       "      <td>0.0786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20357</th>\n",
       "      <td>20358</td>\n",
       "      <td>11826440</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011826/011826440/dv/kplr011826440-002-2014...</td>\n",
       "      <td>011/011826/011826440/dv/kplr011826440-20141002...</td>\n",
       "      <td>0.967130</td>\n",
       "      <td>3.891750e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.369430</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>0.372220</td>\n",
       "      <td>0.8260</td>\n",
       "      <td>-0.198650</td>\n",
       "      <td>0.8620</td>\n",
       "      <td>-0.549660</td>\n",
       "      <td>0.3720</td>\n",
       "      <td>0.584460</td>\n",
       "      <td>0.7550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20358</th>\n",
       "      <td>20359</td>\n",
       "      <td>11852982</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011852/011852982/dv/kplr011852982-001-2014...</td>\n",
       "      <td>011/011852/011852982/dv/kplr011852982-20141002...</td>\n",
       "      <td>13.815000</td>\n",
       "      <td>1.980510e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062709</td>\n",
       "      <td>1.1500</td>\n",
       "      <td>0.071575</td>\n",
       "      <td>1.1700</td>\n",
       "      <td>-0.122460</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>-0.244840</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.273760</td>\n",
       "      <td>0.1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20359</th>\n",
       "      <td>20360</td>\n",
       "      <td>11853130</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011853/011853130/dv/kplr011853130-001-2014...</td>\n",
       "      <td>011/011853/011853130/dv/kplr011853130-20141002...</td>\n",
       "      <td>76.879400</td>\n",
       "      <td>4.380300e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012056</td>\n",
       "      <td>0.0738</td>\n",
       "      <td>0.020771</td>\n",
       "      <td>0.0761</td>\n",
       "      <td>-0.137600</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>-0.070176</td>\n",
       "      <td>0.0766</td>\n",
       "      <td>0.154460</td>\n",
       "      <td>0.0954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20360</th>\n",
       "      <td>20361</td>\n",
       "      <td>11905761</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011905/011905761/dv/kplr011905761-001-2014...</td>\n",
       "      <td>011/011905/011905761/dv/kplr011905761-20141002...</td>\n",
       "      <td>3.580240</td>\n",
       "      <td>6.820920e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104300</td>\n",
       "      <td>0.5680</td>\n",
       "      <td>0.178590</td>\n",
       "      <td>0.7920</td>\n",
       "      <td>0.122860</td>\n",
       "      <td>0.6040</td>\n",
       "      <td>-0.158370</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>0.200430</td>\n",
       "      <td>0.8170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20361</th>\n",
       "      <td>20362</td>\n",
       "      <td>11905761</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011905/011905761/dv/kplr011905761-002-2014...</td>\n",
       "      <td>011/011905/011905761/dv/kplr011905761-20141002...</td>\n",
       "      <td>3.580330</td>\n",
       "      <td>6.525820e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080262</td>\n",
       "      <td>0.2080</td>\n",
       "      <td>0.089655</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>-0.058048</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>-0.084028</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.102130</td>\n",
       "      <td>0.2310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20362</th>\n",
       "      <td>20363</td>\n",
       "      <td>11906217</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011906/011906217/dv/kplr011906217-001-2014...</td>\n",
       "      <td>011/011906/011906217/dv/kplr011906217-20141002...</td>\n",
       "      <td>37.910200</td>\n",
       "      <td>1.999770e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043427</td>\n",
       "      <td>0.0668</td>\n",
       "      <td>0.048061</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>-0.164470</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>-0.027880</td>\n",
       "      <td>0.0773</td>\n",
       "      <td>0.166820</td>\n",
       "      <td>0.0691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20363</th>\n",
       "      <td>20364</td>\n",
       "      <td>11908559</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011908/011908559/dv/kplr011908559-001-2014...</td>\n",
       "      <td>011/011908/011908559/dv/kplr011908559-20141002...</td>\n",
       "      <td>0.716006</td>\n",
       "      <td>1.242640e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102790</td>\n",
       "      <td>0.4170</td>\n",
       "      <td>0.206710</td>\n",
       "      <td>0.6510</td>\n",
       "      <td>-0.239690</td>\n",
       "      <td>0.6460</td>\n",
       "      <td>0.157510</td>\n",
       "      <td>0.3990</td>\n",
       "      <td>0.286810</td>\n",
       "      <td>0.6470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20364</th>\n",
       "      <td>20365</td>\n",
       "      <td>11923562</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011923/011923562/dv/kplr011923562-002-2014...</td>\n",
       "      <td>011/011923/011923562/dv/kplr011923562-20141002...</td>\n",
       "      <td>0.739472</td>\n",
       "      <td>1.068170e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147810</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>-0.140490</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>-0.163850</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.215830</td>\n",
       "      <td>0.1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20365</th>\n",
       "      <td>20366</td>\n",
       "      <td>11923819</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011923/011923819/dv/kplr011923819-001-2014...</td>\n",
       "      <td>011/011923/011923819/dv/kplr011923819-20141002...</td>\n",
       "      <td>33.159500</td>\n",
       "      <td>9.532960e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181860</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>0.188820</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>-0.092241</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>-0.289220</td>\n",
       "      <td>0.0671</td>\n",
       "      <td>0.303570</td>\n",
       "      <td>0.0672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20366</th>\n",
       "      <td>20367</td>\n",
       "      <td>11923819</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>q1_q17_dr24_tce</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>011/011923/011923819/dv/kplr011923819-002-2014...</td>\n",
       "      <td>011/011923/011923819/dv/kplr011923819-20141002...</td>\n",
       "      <td>33.159200</td>\n",
       "      <td>9.280150e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181260</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>0.185860</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>-0.087781</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>-0.287540</td>\n",
       "      <td>0.0670</td>\n",
       "      <td>0.300640</td>\n",
       "      <td>0.0672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20367 rows  156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rowid     kepid  tce_plnt_num  tce_rogue_flag    tce_delivname  \\\n",
       "0          1   1162345             2             NaN  q1_q17_dr24_tce   \n",
       "1          2   1292087             2             NaN  q1_q17_dr24_tce   \n",
       "2          3   1293031             2             NaN  q1_q17_dr24_tce   \n",
       "3          4   1162345             3             NaN  q1_q17_dr24_tce   \n",
       "4          5   1164109             1             NaN  q1_q17_dr24_tce   \n",
       "5          6    757450             1             NaN  q1_q17_dr24_tce   \n",
       "6          7    892667             1             NaN  q1_q17_dr24_tce   \n",
       "7          8    892772             1             NaN  q1_q17_dr24_tce   \n",
       "8          9   1026032             1             NaN  q1_q17_dr24_tce   \n",
       "9         10   1026032             2             NaN  q1_q17_dr24_tce   \n",
       "10        11   1026133             2             NaN  q1_q17_dr24_tce   \n",
       "11        12   1026133             1             NaN  q1_q17_dr24_tce   \n",
       "12        13   1028018             1             NaN  q1_q17_dr24_tce   \n",
       "13        14   1026957             1             NaN  q1_q17_dr24_tce   \n",
       "14        15   1160891             1             NaN  q1_q17_dr24_tce   \n",
       "15        16   1160891             2             NaN  q1_q17_dr24_tce   \n",
       "16        17   1160891             3             NaN  q1_q17_dr24_tce   \n",
       "17        18   1162150             2             NaN  q1_q17_dr24_tce   \n",
       "18        19   1162150             1             NaN  q1_q17_dr24_tce   \n",
       "19        20   1161345             1             NaN  q1_q17_dr24_tce   \n",
       "20        21   1162150             3             NaN  q1_q17_dr24_tce   \n",
       "21        22   1293031             1             NaN  q1_q17_dr24_tce   \n",
       "22        23   1162345             1             NaN  q1_q17_dr24_tce   \n",
       "23        24   1292087             1             NaN  q1_q17_dr24_tce   \n",
       "24        25   1573174             2             NaN  q1_q17_dr24_tce   \n",
       "25        26   1574792             1             NaN  q1_q17_dr24_tce   \n",
       "26        27   1575690             1             NaN  q1_q17_dr24_tce   \n",
       "27        28   1575873             1             NaN  q1_q17_dr24_tce   \n",
       "28        29   1865567             1             NaN  q1_q17_dr24_tce   \n",
       "29        30   1865567             2             NaN  q1_q17_dr24_tce   \n",
       "...      ...       ...           ...             ...              ...   \n",
       "20337  20338  12508604             2             NaN  q1_q17_dr24_tce   \n",
       "20338  20339  12508604             3             NaN  q1_q17_dr24_tce   \n",
       "20339  20340  12509148             1             NaN  q1_q17_dr24_tce   \n",
       "20340  20341  11671226             2             NaN  q1_q17_dr24_tce   \n",
       "20341  20342  11671226             3             NaN  q1_q17_dr24_tce   \n",
       "20342  20343  11671226             4             NaN  q1_q17_dr24_tce   \n",
       "20343  20344  11671226             5             NaN  q1_q17_dr24_tce   \n",
       "20344  20345  11671429             1             NaN  q1_q17_dr24_tce   \n",
       "20345  20346  11717240             1             NaN  q1_q17_dr24_tce   \n",
       "20346  20347  11717240             2             NaN  q1_q17_dr24_tce   \n",
       "20347  20348  11718053             1             NaN  q1_q17_dr24_tce   \n",
       "20348  20349  11718144             1             NaN  q1_q17_dr24_tce   \n",
       "20349  20350  11764462             1             NaN  q1_q17_dr24_tce   \n",
       "20350  20351  11766996             1             NaN  q1_q17_dr24_tce   \n",
       "20351  20352  11766996             2             NaN  q1_q17_dr24_tce   \n",
       "20352  20353  11766996             3             NaN  q1_q17_dr24_tce   \n",
       "20353  20354  11809145             1             NaN  q1_q17_dr24_tce   \n",
       "20354  20355  11809179             1             NaN  q1_q17_dr24_tce   \n",
       "20355  20356  11809179             2             NaN  q1_q17_dr24_tce   \n",
       "20356  20357  11809346             1             NaN  q1_q17_dr24_tce   \n",
       "20357  20358  11826440             2             NaN  q1_q17_dr24_tce   \n",
       "20358  20359  11852982             1             NaN  q1_q17_dr24_tce   \n",
       "20359  20360  11853130             1             NaN  q1_q17_dr24_tce   \n",
       "20360  20361  11905761             1             NaN  q1_q17_dr24_tce   \n",
       "20361  20362  11905761             2             NaN  q1_q17_dr24_tce   \n",
       "20362  20363  11906217             1             NaN  q1_q17_dr24_tce   \n",
       "20363  20364  11908559             1             NaN  q1_q17_dr24_tce   \n",
       "20364  20365  11923562             2             NaN  q1_q17_dr24_tce   \n",
       "20365  20366  11923819             1             NaN  q1_q17_dr24_tce   \n",
       "20366  20367  11923819             2             NaN  q1_q17_dr24_tce   \n",
       "\n",
       "        rowupdate                                   tce_datalink_dvs  \\\n",
       "0      2015-01-15  001/001162/001162345/dv/kplr001162345-002-2014...   \n",
       "1      2015-01-15  001/001292/001292087/dv/kplr001292087-002-2014...   \n",
       "2      2015-01-15  001/001293/001293031/dv/kplr001293031-002-2014...   \n",
       "3      2015-01-15  001/001162/001162345/dv/kplr001162345-003-2014...   \n",
       "4      2015-01-15  001/001164/001164109/dv/kplr001164109-001-2014...   \n",
       "5      2015-01-15  000/000757/000757450/dv/kplr000757450-001-2014...   \n",
       "6      2015-01-15  000/000892/000892667/dv/kplr000892667-001-2014...   \n",
       "7      2015-01-15  000/000892/000892772/dv/kplr000892772-001-2014...   \n",
       "8      2015-01-15  001/001026/001026032/dv/kplr001026032-001-2014...   \n",
       "9      2015-01-15  001/001026/001026032/dv/kplr001026032-002-2014...   \n",
       "10     2015-01-15  001/001026/001026133/dv/kplr001026133-002-2014...   \n",
       "11     2015-01-15  001/001026/001026133/dv/kplr001026133-001-2014...   \n",
       "12     2015-01-15  001/001028/001028018/dv/kplr001028018-001-2014...   \n",
       "13     2015-01-15  001/001026/001026957/dv/kplr001026957-001-2014...   \n",
       "14     2015-01-15  001/001160/001160891/dv/kplr001160891-001-2014...   \n",
       "15     2015-01-15  001/001160/001160891/dv/kplr001160891-002-2014...   \n",
       "16     2015-01-15  001/001160/001160891/dv/kplr001160891-003-2014...   \n",
       "17     2015-01-15  001/001162/001162150/dv/kplr001162150-002-2014...   \n",
       "18     2015-01-15  001/001162/001162150/dv/kplr001162150-001-2014...   \n",
       "19     2015-01-15  001/001161/001161345/dv/kplr001161345-001-2014...   \n",
       "20     2015-01-15  001/001162/001162150/dv/kplr001162150-003-2014...   \n",
       "21     2015-01-15  001/001293/001293031/dv/kplr001293031-001-2014...   \n",
       "22     2015-01-15  001/001162/001162345/dv/kplr001162345-001-2014...   \n",
       "23     2015-01-15  001/001292/001292087/dv/kplr001292087-001-2014...   \n",
       "24     2015-01-15  001/001573/001573174/dv/kplr001573174-002-2014...   \n",
       "25     2015-01-15  001/001574/001574792/dv/kplr001574792-001-2014...   \n",
       "26     2015-01-15  001/001575/001575690/dv/kplr001575690-001-2014...   \n",
       "27     2015-01-15  001/001575/001575873/dv/kplr001575873-001-2014...   \n",
       "28     2015-01-15  001/001865/001865567/dv/kplr001865567-001-2014...   \n",
       "29     2015-01-15  001/001865/001865567/dv/kplr001865567-002-2014...   \n",
       "...           ...                                                ...   \n",
       "20337  2015-01-15  012/012508/012508604/dv/kplr012508604-002-2014...   \n",
       "20338  2015-01-15  012/012508/012508604/dv/kplr012508604-003-2014...   \n",
       "20339  2015-01-15  012/012509/012509148/dv/kplr012509148-001-2014...   \n",
       "20340  2015-01-15  011/011671/011671226/dv/kplr011671226-002-2014...   \n",
       "20341  2015-01-15  011/011671/011671226/dv/kplr011671226-003-2014...   \n",
       "20342  2015-01-15  011/011671/011671226/dv/kplr011671226-004-2014...   \n",
       "20343  2015-01-15  011/011671/011671226/dv/kplr011671226-005-2014...   \n",
       "20344  2015-01-15  011/011671/011671429/dv/kplr011671429-001-2014...   \n",
       "20345  2015-01-15  011/011717/011717240/dv/kplr011717240-001-2014...   \n",
       "20346  2015-01-15  011/011717/011717240/dv/kplr011717240-002-2014...   \n",
       "20347  2015-01-15  011/011718/011718053/dv/kplr011718053-001-2014...   \n",
       "20348  2015-01-15  011/011718/011718144/dv/kplr011718144-001-2014...   \n",
       "20349  2015-01-15  011/011764/011764462/dv/kplr011764462-001-2014...   \n",
       "20350  2015-01-15  011/011766/011766996/dv/kplr011766996-001-2014...   \n",
       "20351  2015-01-15  011/011766/011766996/dv/kplr011766996-002-2014...   \n",
       "20352  2015-01-15  011/011766/011766996/dv/kplr011766996-003-2014...   \n",
       "20353  2015-01-15  011/011809/011809145/dv/kplr011809145-001-2014...   \n",
       "20354  2015-01-15  011/011809/011809179/dv/kplr011809179-001-2014...   \n",
       "20355  2015-01-15  011/011809/011809179/dv/kplr011809179-002-2014...   \n",
       "20356  2015-01-15  011/011809/011809346/dv/kplr011809346-001-2014...   \n",
       "20357  2015-01-15  011/011826/011826440/dv/kplr011826440-002-2014...   \n",
       "20358  2015-01-15  011/011852/011852982/dv/kplr011852982-001-2014...   \n",
       "20359  2015-01-15  011/011853/011853130/dv/kplr011853130-001-2014...   \n",
       "20360  2015-01-15  011/011905/011905761/dv/kplr011905761-001-2014...   \n",
       "20361  2015-01-15  011/011905/011905761/dv/kplr011905761-002-2014...   \n",
       "20362  2015-01-15  011/011906/011906217/dv/kplr011906217-001-2014...   \n",
       "20363  2015-01-15  011/011908/011908559/dv/kplr011908559-001-2014...   \n",
       "20364  2015-01-15  011/011923/011923562/dv/kplr011923562-002-2014...   \n",
       "20365  2015-01-15  011/011923/011923819/dv/kplr011923819-001-2014...   \n",
       "20366  2015-01-15  011/011923/011923819/dv/kplr011923819-002-2014...   \n",
       "\n",
       "                                        tce_datalink_dvr  tce_period  \\\n",
       "0      001/001162/001162345/dv/kplr001162345-20141002...    0.831850   \n",
       "1      001/001292/001292087/dv/kplr001292087-20141002...    1.095240   \n",
       "2      001/001293/001293031/dv/kplr001293031-20141002...    0.719273   \n",
       "3      001/001162/001162345/dv/kplr001162345-20141002...    0.831833   \n",
       "4      001/001164/001164109/dv/kplr001164109-20141002...  622.408000   \n",
       "5      000/000757/000757450/dv/kplr000757450-20141002...    8.884920   \n",
       "6      000/000892/000892667/dv/kplr000892667-20141002...    2.262110   \n",
       "7      000/000892/000892772/dv/kplr000892772-20141002...    5.092600   \n",
       "8      001/001026/001026032/dv/kplr001026032-20141002...    8.460440   \n",
       "9      001/001026/001026032/dv/kplr001026032-20141002...    4.230220   \n",
       "10     001/001026/001026133/dv/kplr001026133-20141002...    2.691910   \n",
       "11     001/001026/001026133/dv/kplr001026133-20141002...    1.346290   \n",
       "12     001/001028/001028018/dv/kplr001028018-20141002...    0.614378   \n",
       "13     001/001026/001026957/dv/kplr001026957-20141002...   21.761300   \n",
       "14     001/001160/001160891/dv/kplr001160891-20141002...    0.940463   \n",
       "15     001/001160/001160891/dv/kplr001160891-20141002...    0.940463   \n",
       "16     001/001160/001160891/dv/kplr001160891-20141002...    0.940520   \n",
       "17     001/001162/001162150/dv/kplr001162150-20141002...    0.778568   \n",
       "18     001/001162/001162150/dv/kplr001162150-20141002...    0.778574   \n",
       "19     001/001161/001161345/dv/kplr001161345-20141002...    4.287580   \n",
       "20     001/001162/001162150/dv/kplr001162150-20141002...    1.284530   \n",
       "21     001/001293/001293031/dv/kplr001293031-20141002...    1.078610   \n",
       "22     001/001162/001162345/dv/kplr001162345-20141002...    0.831777   \n",
       "23     001/001292/001292087/dv/kplr001292087-20141002...    0.547626   \n",
       "24     001/001573/001573174/dv/kplr001573174-20141002...    0.748347   \n",
       "25     001/001574/001574792/dv/kplr001574792-20141002...    1.391750   \n",
       "26     001/001575/001575690/dv/kplr001575690-20141002...    1.126200   \n",
       "27     001/001575/001575873/dv/kplr001575873-20141002...    0.629008   \n",
       "28     001/001865/001865567/dv/kplr001865567-20141002...    0.674634   \n",
       "29     001/001865/001865567/dv/kplr001865567-20141002...    1.349420   \n",
       "...                                                  ...         ...   \n",
       "20337  012/012508/012508604/dv/kplr012508604-20141002...    0.557521   \n",
       "20338  012/012508/012508604/dv/kplr012508604-20141002...    1.114970   \n",
       "20339  012/012509/012509148/dv/kplr012509148-20141002...    1.556300   \n",
       "20340  011/011671/011671226/dv/kplr011671226-20141002...    2.957120   \n",
       "20341  011/011671/011671226/dv/kplr011671226-20141002...    2.957150   \n",
       "20342  011/011671/011671226/dv/kplr011671226-20141002...    2.957100   \n",
       "20343  011/011671/011671226/dv/kplr011671226-20141002...    2.957170   \n",
       "20344  011/011671/011671429/dv/kplr011671429-20141002...  112.464000   \n",
       "20345  011/011717/011717240/dv/kplr011717240-20141002...    1.035500   \n",
       "20346  011/011717/011717240/dv/kplr011717240-20141002...    1.872050   \n",
       "20347  011/011718/011718053/dv/kplr011718053-20141002...    1.316180   \n",
       "20348  011/011718/011718144/dv/kplr011718144-20141002...   16.458100   \n",
       "20349  011/011764/011764462/dv/kplr011764462-20141002...    5.699200   \n",
       "20350  011/011766/011766996/dv/kplr011766996-20141002...    2.570150   \n",
       "20351  011/011766/011766996/dv/kplr011766996-20141002...    2.569230   \n",
       "20352  011/011766/011766996/dv/kplr011766996-20141002...    2.569240   \n",
       "20353  011/011809/011809145/dv/kplr011809145-20141002...    1.067020   \n",
       "20354  011/011809/011809179/dv/kplr011809179-20141002...    0.972957   \n",
       "20355  011/011809/011809179/dv/kplr011809179-20141002...    2.708050   \n",
       "20356  011/011809/011809346/dv/kplr011809346-20141002...    0.624575   \n",
       "20357  011/011826/011826440/dv/kplr011826440-20141002...    0.967130   \n",
       "20358  011/011852/011852982/dv/kplr011852982-20141002...   13.815000   \n",
       "20359  011/011853/011853130/dv/kplr011853130-20141002...   76.879400   \n",
       "20360  011/011905/011905761/dv/kplr011905761-20141002...    3.580240   \n",
       "20361  011/011905/011905761/dv/kplr011905761-20141002...    3.580330   \n",
       "20362  011/011906/011906217/dv/kplr011906217-20141002...   37.910200   \n",
       "20363  011/011908/011908559/dv/kplr011908559-20141002...    0.716006   \n",
       "20364  011/011923/011923562/dv/kplr011923562-20141002...    0.739472   \n",
       "20365  011/011923/011923819/dv/kplr011923819-20141002...   33.159500   \n",
       "20366  011/011923/011923819/dv/kplr011923819-20141002...   33.159200   \n",
       "\n",
       "       tce_period_err         ...          tce_dicco_mdec  tce_dicco_mdec_err  \\\n",
       "0        6.693070e-05         ...                0.504060              1.2900   \n",
       "1        1.009340e-05         ...               -0.075798              0.2120   \n",
       "2        1.388330e-05         ...                0.337380              0.2890   \n",
       "3        9.431020e-06         ...                0.167200              0.1830   \n",
       "4        8.547390e-03         ...               10.358000              0.0894   \n",
       "5        1.186150e-06         ...                0.072221              0.0697   \n",
       "6        2.726600e-05         ...                0.303870              0.1920   \n",
       "7        3.144550e-05         ...               -1.296300              0.1230   \n",
       "8        3.822250e-07         ...                0.051352              0.0669   \n",
       "9        7.327150e-07         ...               -0.003027              0.0678   \n",
       "10       2.128260e-05         ...               -0.010105              0.2830   \n",
       "11       2.253040e-05         ...               -0.747430              0.6710   \n",
       "12       9.733760e-06         ...                0.049380              0.2100   \n",
       "13       1.002540e-05         ...                0.149110              0.1570   \n",
       "14       2.619800e-05         ...                0.104310              0.3170   \n",
       "15       8.053180e-06         ...                0.223580              0.2560   \n",
       "16       7.876050e-06         ...                0.073798              0.1750   \n",
       "17       9.102830e-06         ...               -0.731240              0.6600   \n",
       "18       7.102600e-06         ...               -1.204600              0.1620   \n",
       "19       1.116950e-06         ...               -0.448280              0.2040   \n",
       "20       2.585230e-05         ...               -0.586720              0.2700   \n",
       "21       1.272980e-05         ...                0.208310              0.5190   \n",
       "22       1.136930e-05         ...                0.628050              0.2320   \n",
       "23       2.340840e-05         ...                0.119140              0.2030   \n",
       "24       7.786200e-06         ...                0.102230              0.1450   \n",
       "25       1.488460e-05         ...                0.270060              0.3060   \n",
       "26       1.569320e-07         ...                1.012700              0.3580   \n",
       "27       7.191270e-06         ...               10.172000              0.3450   \n",
       "28       1.044420e-05         ...                0.160620              0.2830   \n",
       "29       8.020030e-06         ...               -0.566710              0.1720   \n",
       "...               ...         ...                     ...                 ...   \n",
       "20337    0.000000e+00         ...                0.000000             -1.0000   \n",
       "20338    1.145120e-05         ...               -0.109260              0.0940   \n",
       "20339    1.155740e-05         ...                0.812390              0.6960   \n",
       "20340    9.978780e-06         ...               -0.203360              0.3000   \n",
       "20341    1.009300e-05         ...                0.281590              0.3410   \n",
       "20342    1.386450e-05         ...               -0.355560              0.2760   \n",
       "20343    0.000000e+00         ...                0.000000             -1.0000   \n",
       "20344    3.037760e-05         ...                0.217970              0.1350   \n",
       "20345    1.434610e-05         ...               -0.064943              1.6000   \n",
       "20346    2.638920e-05         ...                1.816600              0.9370   \n",
       "20347    1.237290e-05         ...               -0.907540              0.7570   \n",
       "20348    4.851470e-05         ...                0.384610              0.4630   \n",
       "20349    1.056740e-05         ...               -0.000521              0.2370   \n",
       "20350    3.561260e-05         ...                0.107490              0.6820   \n",
       "20351    2.162220e-05         ...                0.143910              0.2770   \n",
       "20352    2.707320e-05         ...                0.927950              0.6180   \n",
       "20353    1.023600e-05         ...               -1.266600              0.8420   \n",
       "20354    1.654570e-03         ...                0.184260              0.3260   \n",
       "20355    6.689970e-05         ...                0.510440              0.5830   \n",
       "20356    1.570140e-05         ...                0.010727              0.0850   \n",
       "20357    3.891750e-05         ...               -0.369430              0.3730   \n",
       "20358    1.980510e-05         ...               -0.062709              1.1500   \n",
       "20359    4.380300e-05         ...               -0.012056              0.0738   \n",
       "20360    6.820920e-05         ...               -0.104300              0.5680   \n",
       "20361    6.525820e-05         ...               -0.080262              0.2080   \n",
       "20362    1.999770e-06         ...                0.043427              0.0668   \n",
       "20363    1.242640e-05         ...                0.102790              0.4170   \n",
       "20364    1.068170e-05         ...               -0.147810              0.1660   \n",
       "20365    9.532960e-07         ...               -0.181860              0.0672   \n",
       "20366    9.280150e-07         ...               -0.181260              0.0672   \n",
       "\n",
       "       tce_dicco_msky  tce_dicco_msky_err  tce_dikco_mra  tce_dikco_mra_err  \\\n",
       "0            0.507070              1.1200      -0.183490             1.2100   \n",
       "1            0.097899              0.2090       0.206520             0.2250   \n",
       "2            0.444500              0.2240      -0.308090             0.1910   \n",
       "3            0.253410              0.1970       0.087066             0.2510   \n",
       "4           15.177000              0.0893     -11.060000             0.0892   \n",
       "5            0.072362              0.0694       0.069581             0.0719   \n",
       "6            0.411080              0.1370       0.139510             0.0883   \n",
       "7            4.920200              0.1610      -4.962200             0.1580   \n",
       "8            0.078983              0.0673      -0.004233             0.0737   \n",
       "9            0.051184              0.0676      -0.014758             0.0716   \n",
       "10           0.154550              0.2410       0.042965             0.2510   \n",
       "11           1.426300              0.5660       1.213400             0.6350   \n",
       "12           0.052617              0.2090       0.075046             0.0701   \n",
       "13           0.155260              0.1160      -0.078708             0.1220   \n",
       "14           0.255800              0.3960      -0.144430             0.3160   \n",
       "15           0.223670              0.2940       0.063502             0.2340   \n",
       "16           0.191460              0.2360      -0.057165             0.2190   \n",
       "17           1.624200              0.9160       1.851500             1.0500   \n",
       "18           1.911200              0.1740       1.749900             0.1640   \n",
       "19           0.755810              0.2680      -0.745250             0.3280   \n",
       "20           1.745700              0.3750       2.052400             0.3660   \n",
       "21           0.395070              0.3390       0.355910             0.3750   \n",
       "22           0.629640              0.2530      -0.166720             0.2670   \n",
       "23           0.159040              0.1540       0.205660             0.1450   \n",
       "24           0.129850              0.1350       0.014355             0.0850   \n",
       "25           0.275020              0.3020       0.118790             0.3040   \n",
       "26           1.619800              0.7430       0.037418             0.0701   \n",
       "27          10.349000              0.3240       1.821800             0.1370   \n",
       "28           0.181040              0.2710       0.191810             0.1130   \n",
       "29           0.572150              0.1710       0.248410             0.1190   \n",
       "...               ...                 ...            ...                ...   \n",
       "20337        0.000000             -1.0000       0.000000            -1.0000   \n",
       "20338        0.110940              0.0928      -0.064962             0.0783   \n",
       "20339        0.862270              0.6620       0.261980             0.3180   \n",
       "20340        0.329760              0.3970      -0.496030             0.3310   \n",
       "20341        0.323840              0.4890      -0.064916             0.4040   \n",
       "20342        0.391290              0.4000      -0.404980             0.3430   \n",
       "20343        0.000000             -1.0000       0.000000            -1.0000   \n",
       "20344        0.220220              0.2540       0.308590             0.2300   \n",
       "20345        1.241700              1.6000       1.106500             0.7150   \n",
       "20346        2.494700              0.6340      -1.873700             0.8510   \n",
       "20347        0.909970              0.5390      -0.097671             0.4160   \n",
       "20348        0.399770              0.5350      -0.082912             0.3510   \n",
       "20349        0.041172              0.2370       0.031105             0.1650   \n",
       "20350        0.146050              0.7950       0.061271             0.4770   \n",
       "20351        0.165280              0.2690       0.045429             0.2420   \n",
       "20352        0.998970              0.7860       0.330870             0.6980   \n",
       "20353        1.417400              0.8890       0.915380             0.5980   \n",
       "20354        0.394530              0.3390       0.369440             0.2710   \n",
       "20355        0.531790              0.6120      -0.118650             0.4300   \n",
       "20356        0.014216              0.0895       0.087154             0.0789   \n",
       "20357        0.372220              0.8260      -0.198650             0.8620   \n",
       "20358        0.071575              1.1700      -0.122460             0.1250   \n",
       "20359        0.020771              0.0761      -0.137600             0.0925   \n",
       "20360        0.178590              0.7920       0.122860             0.6040   \n",
       "20361        0.089655              0.2200      -0.058048             0.1620   \n",
       "20362        0.048061              0.0669      -0.164470             0.0689   \n",
       "20363        0.206710              0.6510      -0.239690             0.6460   \n",
       "20364        0.150500              0.1340      -0.140490             0.1380   \n",
       "20365        0.188820              0.0672      -0.092241             0.0672   \n",
       "20366        0.185860              0.0672      -0.087781             0.0673   \n",
       "\n",
       "       tce_dikco_mdec  tce_dikco_mdec_err  tce_dikco_msky  tce_dikco_msky_err  \n",
       "0            0.424890              1.3000        0.462820              1.1000  \n",
       "1           -0.077214              0.2380        0.220490              0.2200  \n",
       "2            0.283570              0.2240        0.418720              0.2070  \n",
       "3            0.106320              0.1720        0.137420              0.1960  \n",
       "4           10.437000              0.0894       15.207000              0.0893  \n",
       "5           -0.103820              0.0782        0.124980              0.0763  \n",
       "6            0.115410              0.1630        0.181060              0.1240  \n",
       "7           -1.555100              0.1170        5.200200              0.1650  \n",
       "8           -0.064448              0.0770        0.064587              0.0770  \n",
       "9           -0.137870              0.0841        0.138650              0.0840  \n",
       "10          -0.076421              0.2620        0.087671              0.2490  \n",
       "11          -0.785060              0.6510        1.445200              0.5700  \n",
       "12           0.173300              0.2050        0.188850              0.1310  \n",
       "13          -0.282140              0.1590        0.292920              0.1580  \n",
       "14           0.097590              0.3160        0.174310              0.3830  \n",
       "15           0.267290              0.2620        0.274730              0.2910  \n",
       "16           0.071770              0.1710        0.091754              0.2330  \n",
       "17          -1.255200              0.6740        2.236900              0.9280  \n",
       "18          -1.540000              0.1410        2.331000              0.1450  \n",
       "19          -0.323000              0.1970        0.812240              0.3050  \n",
       "20          -1.051400              0.2530        2.306100              0.3940  \n",
       "21           0.171660              0.5540        0.395150              0.3150  \n",
       "22           0.570610              0.2210        0.594470              0.2260  \n",
       "23           0.089230              0.1880        0.224180              0.1520  \n",
       "24           0.034881              0.1560        0.037720              0.1550  \n",
       "25           0.296830              0.3060        0.319720              0.3490  \n",
       "26          -0.162080              0.0711        0.166340              0.0704  \n",
       "27          10.058000              0.3630       10.221000              0.3410  \n",
       "28           0.246790              0.3090        0.312560              0.1320  \n",
       "29          -0.470940              0.1600        0.532440              0.1680  \n",
       "...               ...                 ...             ...                 ...  \n",
       "20337        0.000000             -1.0000        0.000000             -1.0000  \n",
       "20338       -0.167340              0.0945        0.179510              0.0942  \n",
       "20339        0.864440              0.8690        0.903270              0.8320  \n",
       "20340       -0.440050              0.2970        0.663090              0.4190  \n",
       "20341        0.037741              0.3730        0.075090              0.3970  \n",
       "20342       -0.560460              0.3000        0.691470              0.4370  \n",
       "20343        0.000000             -1.0000        0.000000             -1.0000  \n",
       "20344        0.282250              0.1460        0.418200              0.2580  \n",
       "20345       -0.006294              1.4700        1.106500              1.5000  \n",
       "20346        0.805420              0.7360        2.039500              0.7640  \n",
       "20347       -0.882480              0.8180        0.887870              0.5660  \n",
       "20348        0.463960              0.4580        0.471310              0.5160  \n",
       "20349        0.257480              0.2450        0.259350              0.2440  \n",
       "20350        0.153500              0.6340        0.165280              0.7560  \n",
       "20351        0.186720              0.2690        0.192160              0.2670  \n",
       "20352        0.973240              0.5950        1.027900              0.7060  \n",
       "20353       -1.707600              0.8290        1.937400              0.8810  \n",
       "20354        0.117130              0.3580        0.387570              0.3250  \n",
       "20355        0.428900              0.5940        0.445010              0.6130  \n",
       "20356       -0.023840              0.0880        0.090355              0.0786  \n",
       "20357       -0.549660              0.3720        0.584460              0.7550  \n",
       "20358       -0.244840              0.1500        0.273760              0.1430  \n",
       "20359       -0.070176              0.0766        0.154460              0.0954  \n",
       "20360       -0.158370              0.5940        0.200430              0.8170  \n",
       "20361       -0.084028              0.2160        0.102130              0.2310  \n",
       "20362       -0.027880              0.0773        0.166820              0.0691  \n",
       "20363        0.157510              0.3990        0.286810              0.6470  \n",
       "20364       -0.163850              0.1580        0.215830              0.1350  \n",
       "20365       -0.289220              0.0671        0.303570              0.0672  \n",
       "20366       -0.287540              0.0670        0.300640              0.0672  \n",
       "\n",
       "[20367 rows x 156 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kepler_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AFP    9596\n",
       "UNK    4630\n",
       "PC     3600\n",
       "NTP    2541\n",
       "Name: av_training_set, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kepler_data['av_training_set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20367, 156)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kepler_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_data = kepler_data.drop(['tce_rogue_flag', 'tce_delivname', 'rowupdate', \n",
    "                           'tce_datalink_dvs', 'tce_datalink_dvr', 'tce_steff_prov', \n",
    "                           'tce_slogg_prov', 'tce_smet_prov','tce_sradius_prov', 'tce_limbdark_mod',\n",
    "                           'tce_trans_mod', 'tce_eccen', 'tce_eccen_err', 'tce_longp', 'tce_longp_err'\n",
    "                          ],axis=1)\n",
    "# drop some columns that we will not be using\n",
    "# starting with all NA columns\n",
    "k_data.dropna(axis=1,how='any', inplace=True)\n",
    "\n",
    "# drop the Autovetter stuff\n",
    "autovetter_cols = ['av_vf_pc', 'av_vf_pc_err', 'av_vf_afp',\n",
    "                   'av_vf_afp_err', 'av_vf_ntp', 'av_vf_ntp_err', \n",
    "                   'av_pp_pc','av_pp_afp', 'av_pp_ntp', \n",
    "                   'av_training_set', 'av_pred_class']\n",
    "# drop all autovetter columns execpt the training set - as those are manually set - take as \"truth\"\n",
    "\n",
    "k_data = k_data.drop(['av_vf_pc', 'av_vf_pc_err', 'av_vf_afp','av_vf_afp_err',\n",
    "                           'av_vf_ntp', 'av_vf_ntp_err', 'av_pp_pc','av_pp_afp', \n",
    "                           'av_pp_ntp', 'av_pred_class'],axis=1)\n",
    "\n",
    "col_names = k_data.columns.values\n",
    "\n",
    "k_data = pd.get_dummies(k_data, columns=['av_training_set'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14367, 93)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                4700      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 40)                2040      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                1230      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 124       \n",
      "=================================================================\n",
      "Total params: 10,644\n",
      "Trainable params: 10,644\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k_data = shuffle(k_data)\n",
    "\n",
    "# ids = k_data[['rowid', 'kepid']]\n",
    "\n",
    "# train and test split\n",
    "y = k_data[['av_training_set_AFP','av_training_set_NTP','av_training_set_PC', 'av_training_set_UNK']]\n",
    "\n",
    "# update to be categorical\n",
    "x = k_data\n",
    "x = k_data.drop(['av_training_set_AFP','av_training_set_NTP','av_training_set_PC', 'av_training_set_UNK'], axis=1)\n",
    "#x = k_data.drop(['rowid','kepid'], axis=1)\n",
    "#x = k_data[['tce_mesmad','tce_maxmes', 'tce_minmes', 'tce_maxmesd', 'tce_minmesd', 'tce_plnt_num']]\n",
    "x = (x - x.mean()) / (x.max() - x.min())\n",
    "\n",
    "train_test_split_size = 5000\n",
    "x_test = x[:train_test_split_size]\n",
    "y_test = y[:train_test_split_size]\n",
    "\n",
    "x_train = x[train_test_split_size:]\n",
    "y_train = y[train_test_split_size:]\n",
    "\n",
    "train_val_split_size = 1000\n",
    "\n",
    "x_val = x_train[:train_val_split_size]\n",
    "y_val = y_train[:train_val_split_size]\n",
    "x_train = x_train[train_val_split_size:]\n",
    "y_train = y_train[train_val_split_size:]\n",
    "\n",
    "display(x_train.shape)\n",
    "\n",
    "# be really dumb and just attempt to pass into a net\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50,kernel_initializer='random_normal',\n",
    "                       kernel_regularizer=regularizers.l2(0.01),\n",
    "                       activation='relu', \n",
    "                       input_shape=(x_train.shape[1],)))\n",
    "model.add(layers.Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(layers.Dense(40, activation='relu', kernel_regularizer=regularizers.l2(0.01),))\n",
    "model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.l2(0.01),))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.001, nesterov=False)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14367 samples, validate on 1000 samples\n",
      "Epoch 1/2000\n",
      "14367/14367 [==============================] - 3s 189us/step - loss: 2.5590 - acc: 0.4314 - val_loss: 2.2859 - val_acc: 0.4750\n",
      "Epoch 2/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 2.0980 - acc: 0.4679 - val_loss: 1.9039 - val_acc: 0.4750\n",
      "Epoch 3/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.7819 - acc: 0.4753 - val_loss: 1.6337 - val_acc: 0.5570\n",
      "Epoch 4/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.5395 - acc: 0.5594 - val_loss: 1.4074 - val_acc: 0.6000\n",
      "Epoch 5/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.3623 - acc: 0.5774 - val_loss: 1.2695 - val_acc: 0.6140\n",
      "Epoch 6/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.2633 - acc: 0.5947 - val_loss: 1.1879 - val_acc: 0.6380\n",
      "Epoch 7/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.1963 - acc: 0.6069 - val_loss: 1.1369 - val_acc: 0.6450\n",
      "Epoch 8/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.1523 - acc: 0.6117 - val_loss: 1.0986 - val_acc: 0.6390\n",
      "Epoch 9/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 1.1213 - acc: 0.6086 - val_loss: 1.0782 - val_acc: 0.6460\n",
      "Epoch 10/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0977 - acc: 0.6144 - val_loss: 1.0636 - val_acc: 0.6280\n",
      "Epoch 11/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 1.0856 - acc: 0.6088 - val_loss: 1.0477 - val_acc: 0.6380\n",
      "Epoch 12/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0713 - acc: 0.6130 - val_loss: 1.0300 - val_acc: 0.6450\n",
      "Epoch 13/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0625 - acc: 0.6142 - val_loss: 1.0243 - val_acc: 0.6480\n",
      "Epoch 14/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 1.0559 - acc: 0.6147 - val_loss: 1.0178 - val_acc: 0.6460\n",
      "Epoch 15/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0497 - acc: 0.6159 - val_loss: 1.0104 - val_acc: 0.6500\n",
      "Epoch 16/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0443 - acc: 0.6177 - val_loss: 1.0080 - val_acc: 0.6510\n",
      "Epoch 17/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0440 - acc: 0.6192 - val_loss: 1.0053 - val_acc: 0.6500\n",
      "Epoch 18/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0393 - acc: 0.6186 - val_loss: 1.0067 - val_acc: 0.6450\n",
      "Epoch 19/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0346 - acc: 0.6170 - val_loss: 1.0036 - val_acc: 0.6560\n",
      "Epoch 20/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0312 - acc: 0.6203 - val_loss: 0.9946 - val_acc: 0.6460\n",
      "Epoch 21/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0292 - acc: 0.6204 - val_loss: 0.9962 - val_acc: 0.6540\n",
      "Epoch 22/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0264 - acc: 0.6200 - val_loss: 1.0006 - val_acc: 0.6510\n",
      "Epoch 23/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0325 - acc: 0.6184 - val_loss: 0.9898 - val_acc: 0.6480\n",
      "Epoch 24/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0238 - acc: 0.6220 - val_loss: 0.9859 - val_acc: 0.6530\n",
      "Epoch 25/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0232 - acc: 0.6211 - val_loss: 0.9870 - val_acc: 0.6480\n",
      "Epoch 26/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0182 - acc: 0.6222 - val_loss: 0.9907 - val_acc: 0.6420\n",
      "Epoch 27/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0205 - acc: 0.6207 - val_loss: 0.9906 - val_acc: 0.6520\n",
      "Epoch 28/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0168 - acc: 0.6221 - val_loss: 0.9837 - val_acc: 0.6500\n",
      "Epoch 29/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0172 - acc: 0.6206 - val_loss: 0.9978 - val_acc: 0.6380\n",
      "Epoch 30/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0226 - acc: 0.6153 - val_loss: 0.9966 - val_acc: 0.6280\n",
      "Epoch 31/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 1.0155 - acc: 0.6194 - val_loss: 0.9786 - val_acc: 0.6490\n",
      "Epoch 32/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0097 - acc: 0.6256 - val_loss: 0.9836 - val_acc: 0.6440\n",
      "Epoch 33/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0104 - acc: 0.6230 - val_loss: 0.9801 - val_acc: 0.6490\n",
      "Epoch 34/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0066 - acc: 0.6232 - val_loss: 0.9742 - val_acc: 0.6540\n",
      "Epoch 35/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0054 - acc: 0.6258 - val_loss: 0.9721 - val_acc: 0.6530\n",
      "Epoch 36/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.0049 - acc: 0.6261 - val_loss: 0.9752 - val_acc: 0.6500\n",
      "Epoch 37/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 1.0029 - acc: 0.6268 - val_loss: 0.9676 - val_acc: 0.6550\n",
      "Epoch 38/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0032 - acc: 0.6240 - val_loss: 0.9679 - val_acc: 0.6530\n",
      "Epoch 39/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0005 - acc: 0.6267 - val_loss: 0.9657 - val_acc: 0.6540\n",
      "Epoch 40/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9995 - acc: 0.6251 - val_loss: 0.9736 - val_acc: 0.6450\n",
      "Epoch 41/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0006 - acc: 0.6253 - val_loss: 0.9667 - val_acc: 0.6590\n",
      "Epoch 42/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9977 - acc: 0.6250 - val_loss: 0.9672 - val_acc: 0.6520\n",
      "Epoch 43/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 1.0010 - acc: 0.6255 - val_loss: 0.9716 - val_acc: 0.6650\n",
      "Epoch 44/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.9964 - acc: 0.6269 - val_loss: 0.9635 - val_acc: 0.6530\n",
      "Epoch 45/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9948 - acc: 0.6262 - val_loss: 0.9619 - val_acc: 0.6530\n",
      "Epoch 46/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9947 - acc: 0.6277 - val_loss: 0.9684 - val_acc: 0.6530\n",
      "Epoch 47/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9980 - acc: 0.6243 - val_loss: 0.9630 - val_acc: 0.6520\n",
      "Epoch 48/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9923 - acc: 0.6291 - val_loss: 0.9651 - val_acc: 0.6450\n",
      "Epoch 49/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9914 - acc: 0.6272 - val_loss: 0.9616 - val_acc: 0.6610\n",
      "Epoch 50/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9901 - acc: 0.6275 - val_loss: 0.9627 - val_acc: 0.6490\n",
      "Epoch 51/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9920 - acc: 0.6255 - val_loss: 0.9657 - val_acc: 0.6610\n",
      "Epoch 52/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9891 - acc: 0.6307 - val_loss: 0.9680 - val_acc: 0.6580\n",
      "Epoch 53/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9891 - acc: 0.6297 - val_loss: 0.9583 - val_acc: 0.6580\n",
      "Epoch 54/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9877 - acc: 0.6280 - val_loss: 0.9659 - val_acc: 0.6430\n",
      "Epoch 55/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9906 - acc: 0.6273 - val_loss: 0.9547 - val_acc: 0.6580\n",
      "Epoch 56/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9886 - acc: 0.6310 - val_loss: 0.9609 - val_acc: 0.6480\n",
      "Epoch 57/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9887 - acc: 0.6272 - val_loss: 0.9552 - val_acc: 0.6560\n",
      "Epoch 58/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9860 - acc: 0.6274 - val_loss: 0.9595 - val_acc: 0.6480\n",
      "Epoch 59/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9835 - acc: 0.6270 - val_loss: 0.9498 - val_acc: 0.6590\n",
      "Epoch 60/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9811 - acc: 0.6290 - val_loss: 0.9526 - val_acc: 0.6520\n",
      "Epoch 61/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9839 - acc: 0.6284 - val_loss: 0.9627 - val_acc: 0.6500\n",
      "Epoch 62/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9845 - acc: 0.6267 - val_loss: 0.9513 - val_acc: 0.6600\n",
      "Epoch 63/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9791 - acc: 0.6324 - val_loss: 0.9728 - val_acc: 0.6490\n",
      "Epoch 64/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9858 - acc: 0.6256 - val_loss: 0.9670 - val_acc: 0.6630\n",
      "Epoch 65/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9811 - acc: 0.6313 - val_loss: 0.9503 - val_acc: 0.6640\n",
      "Epoch 66/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9790 - acc: 0.6296 - val_loss: 0.9491 - val_acc: 0.6570\n",
      "Epoch 67/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9764 - acc: 0.6306 - val_loss: 0.9494 - val_acc: 0.6640\n",
      "Epoch 68/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9763 - acc: 0.6310 - val_loss: 0.9444 - val_acc: 0.6590\n",
      "Epoch 69/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9797 - acc: 0.6292 - val_loss: 0.9467 - val_acc: 0.6590\n",
      "Epoch 70/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9767 - acc: 0.6291 - val_loss: 0.9469 - val_acc: 0.6560\n",
      "Epoch 71/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9747 - acc: 0.6325 - val_loss: 0.9552 - val_acc: 0.6520\n",
      "Epoch 72/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9749 - acc: 0.6314 - val_loss: 0.9484 - val_acc: 0.6580\n",
      "Epoch 73/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9785 - acc: 0.6298 - val_loss: 0.9439 - val_acc: 0.6600\n",
      "Epoch 74/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9733 - acc: 0.6304 - val_loss: 0.9457 - val_acc: 0.6580\n",
      "Epoch 75/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9716 - acc: 0.6342 - val_loss: 0.9411 - val_acc: 0.6650\n",
      "Epoch 76/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9724 - acc: 0.6310 - val_loss: 0.9465 - val_acc: 0.6570\n",
      "Epoch 77/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9740 - acc: 0.6333 - val_loss: 0.9427 - val_acc: 0.6590\n",
      "Epoch 78/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9715 - acc: 0.6344 - val_loss: 0.9527 - val_acc: 0.6640\n",
      "Epoch 79/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9722 - acc: 0.6319 - val_loss: 0.9417 - val_acc: 0.6600\n",
      "Epoch 80/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.9690 - acc: 0.6308 - val_loss: 0.9426 - val_acc: 0.6640\n",
      "Epoch 81/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9686 - acc: 0.6343 - val_loss: 0.9383 - val_acc: 0.6660\n",
      "Epoch 82/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9675 - acc: 0.6373 - val_loss: 0.9447 - val_acc: 0.6570\n",
      "Epoch 83/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9684 - acc: 0.6301 - val_loss: 0.9431 - val_acc: 0.6620\n",
      "Epoch 84/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9669 - acc: 0.6343 - val_loss: 0.9406 - val_acc: 0.6630\n",
      "Epoch 85/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9679 - acc: 0.6323 - val_loss: 0.9438 - val_acc: 0.6590\n",
      "Epoch 86/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9664 - acc: 0.6342 - val_loss: 0.9393 - val_acc: 0.6620\n",
      "Epoch 87/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9650 - acc: 0.6340 - val_loss: 0.9367 - val_acc: 0.6680\n",
      "Epoch 88/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9633 - acc: 0.6366 - val_loss: 0.9353 - val_acc: 0.6620\n",
      "Epoch 89/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9662 - acc: 0.6367 - val_loss: 0.9412 - val_acc: 0.6630\n",
      "Epoch 90/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9658 - acc: 0.6322 - val_loss: 0.9354 - val_acc: 0.6670\n",
      "Epoch 91/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9626 - acc: 0.6360 - val_loss: 0.9367 - val_acc: 0.6660\n",
      "Epoch 92/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9633 - acc: 0.6375 - val_loss: 0.9372 - val_acc: 0.6670\n",
      "Epoch 93/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9641 - acc: 0.6344 - val_loss: 0.9415 - val_acc: 0.6750\n",
      "Epoch 94/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9610 - acc: 0.6391 - val_loss: 0.9314 - val_acc: 0.6610\n",
      "Epoch 95/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9597 - acc: 0.6371 - val_loss: 0.9369 - val_acc: 0.6680\n",
      "Epoch 96/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9599 - acc: 0.6357 - val_loss: 0.9331 - val_acc: 0.6670\n",
      "Epoch 97/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9621 - acc: 0.6358 - val_loss: 0.9386 - val_acc: 0.6640\n",
      "Epoch 98/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9612 - acc: 0.6360 - val_loss: 0.9360 - val_acc: 0.6690\n",
      "Epoch 99/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9627 - acc: 0.6338 - val_loss: 0.9364 - val_acc: 0.6580\n",
      "Epoch 100/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9578 - acc: 0.6349 - val_loss: 0.9288 - val_acc: 0.6630\n",
      "Epoch 101/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9590 - acc: 0.6336 - val_loss: 0.9372 - val_acc: 0.6550\n",
      "Epoch 102/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9594 - acc: 0.6380 - val_loss: 0.9328 - val_acc: 0.6610\n",
      "Epoch 103/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9561 - acc: 0.6358 - val_loss: 0.9344 - val_acc: 0.6650\n",
      "Epoch 104/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9586 - acc: 0.6329 - val_loss: 0.9397 - val_acc: 0.6710\n",
      "Epoch 105/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9564 - acc: 0.6376 - val_loss: 0.9317 - val_acc: 0.6610\n",
      "Epoch 106/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9614 - acc: 0.6342 - val_loss: 0.9363 - val_acc: 0.6620\n",
      "Epoch 107/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9613 - acc: 0.6356 - val_loss: 0.9361 - val_acc: 0.6540\n",
      "Epoch 108/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9564 - acc: 0.6337 - val_loss: 0.9371 - val_acc: 0.6690\n",
      "Epoch 109/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9549 - acc: 0.6395 - val_loss: 0.9332 - val_acc: 0.6610\n",
      "Epoch 110/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9536 - acc: 0.6346 - val_loss: 0.9283 - val_acc: 0.6560\n",
      "Epoch 111/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9544 - acc: 0.6379 - val_loss: 0.9577 - val_acc: 0.6380\n",
      "Epoch 112/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9578 - acc: 0.6338 - val_loss: 0.9286 - val_acc: 0.6720\n",
      "Epoch 113/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9515 - acc: 0.6384 - val_loss: 0.9312 - val_acc: 0.6650\n",
      "Epoch 114/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9539 - acc: 0.6389 - val_loss: 0.9398 - val_acc: 0.6500\n",
      "Epoch 115/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9530 - acc: 0.6375 - val_loss: 0.9300 - val_acc: 0.6680\n",
      "Epoch 116/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9539 - acc: 0.6364 - val_loss: 0.9412 - val_acc: 0.6560\n",
      "Epoch 117/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9538 - acc: 0.6370 - val_loss: 0.9324 - val_acc: 0.6660\n",
      "Epoch 118/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9520 - acc: 0.6397 - val_loss: 0.9276 - val_acc: 0.6640\n",
      "Epoch 119/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9552 - acc: 0.6368 - val_loss: 0.9261 - val_acc: 0.6670\n",
      "Epoch 120/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9507 - acc: 0.6410 - val_loss: 0.9336 - val_acc: 0.6710\n",
      "Epoch 121/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9497 - acc: 0.6389 - val_loss: 0.9259 - val_acc: 0.6700\n",
      "Epoch 122/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9504 - acc: 0.6385 - val_loss: 0.9235 - val_acc: 0.6700\n",
      "Epoch 123/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9473 - acc: 0.6418 - val_loss: 0.9312 - val_acc: 0.6570\n",
      "Epoch 124/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9501 - acc: 0.6378 - val_loss: 0.9232 - val_acc: 0.6640\n",
      "Epoch 125/2000\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.9457 - acc: 0.6401 - val_loss: 0.9403 - val_acc: 0.6580\n",
      "Epoch 126/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9537 - acc: 0.6370 - val_loss: 0.9207 - val_acc: 0.6720\n",
      "Epoch 127/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9457 - acc: 0.6395 - val_loss: 0.9234 - val_acc: 0.6620\n",
      "Epoch 128/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9461 - acc: 0.6405 - val_loss: 0.9208 - val_acc: 0.6670\n",
      "Epoch 129/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9442 - acc: 0.6430 - val_loss: 0.9243 - val_acc: 0.6690\n",
      "Epoch 130/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9457 - acc: 0.6415 - val_loss: 0.9325 - val_acc: 0.6630\n",
      "Epoch 131/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9496 - acc: 0.6380 - val_loss: 0.9261 - val_acc: 0.6600\n",
      "Epoch 132/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9453 - acc: 0.6403 - val_loss: 0.9253 - val_acc: 0.6690\n",
      "Epoch 133/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9461 - acc: 0.6389 - val_loss: 0.9301 - val_acc: 0.6610\n",
      "Epoch 134/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.9442 - acc: 0.6410 - val_loss: 0.9291 - val_acc: 0.6580\n",
      "Epoch 135/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9455 - acc: 0.6423 - val_loss: 0.9269 - val_acc: 0.6730\n",
      "Epoch 136/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9445 - acc: 0.6424 - val_loss: 0.9286 - val_acc: 0.6580\n",
      "Epoch 137/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9450 - acc: 0.6411 - val_loss: 0.9227 - val_acc: 0.6620\n",
      "Epoch 138/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9442 - acc: 0.6413 - val_loss: 0.9203 - val_acc: 0.6640\n",
      "Epoch 139/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9413 - acc: 0.6394 - val_loss: 0.9239 - val_acc: 0.6690\n",
      "Epoch 140/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9417 - acc: 0.6440 - val_loss: 0.9264 - val_acc: 0.6670\n",
      "Epoch 141/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.9445 - acc: 0.6422 - val_loss: 0.9284 - val_acc: 0.6640\n",
      "Epoch 142/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9443 - acc: 0.6402 - val_loss: 0.9210 - val_acc: 0.6610\n",
      "Epoch 143/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9417 - acc: 0.6410 - val_loss: 0.9277 - val_acc: 0.6580\n",
      "Epoch 144/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9427 - acc: 0.6389 - val_loss: 0.9195 - val_acc: 0.6640\n",
      "Epoch 145/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9393 - acc: 0.6419 - val_loss: 0.9182 - val_acc: 0.6610\n",
      "Epoch 146/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9416 - acc: 0.6399 - val_loss: 0.9148 - val_acc: 0.6580\n",
      "Epoch 147/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9416 - acc: 0.6399 - val_loss: 0.9265 - val_acc: 0.6570\n",
      "Epoch 148/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9398 - acc: 0.6418 - val_loss: 0.9167 - val_acc: 0.6750\n",
      "Epoch 149/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9377 - acc: 0.6417 - val_loss: 0.9166 - val_acc: 0.6720\n",
      "Epoch 150/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9385 - acc: 0.6450 - val_loss: 0.9190 - val_acc: 0.6610\n",
      "Epoch 151/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9388 - acc: 0.6440 - val_loss: 0.9244 - val_acc: 0.6590\n",
      "Epoch 152/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9402 - acc: 0.6420 - val_loss: 0.9164 - val_acc: 0.6710\n",
      "Epoch 153/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9382 - acc: 0.6421 - val_loss: 0.9149 - val_acc: 0.6630\n",
      "Epoch 154/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9399 - acc: 0.6413 - val_loss: 0.9224 - val_acc: 0.6730\n",
      "Epoch 155/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9394 - acc: 0.6417 - val_loss: 0.9276 - val_acc: 0.6670\n",
      "Epoch 156/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9420 - acc: 0.6427 - val_loss: 0.9188 - val_acc: 0.6690\n",
      "Epoch 157/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9380 - acc: 0.6431 - val_loss: 0.9177 - val_acc: 0.6700\n",
      "Epoch 158/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9391 - acc: 0.6393 - val_loss: 0.9197 - val_acc: 0.6740\n",
      "Epoch 159/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9371 - acc: 0.6437 - val_loss: 0.9180 - val_acc: 0.6710\n",
      "Epoch 160/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9376 - acc: 0.6410 - val_loss: 0.9275 - val_acc: 0.6700\n",
      "Epoch 161/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9385 - acc: 0.6450 - val_loss: 0.9185 - val_acc: 0.6620\n",
      "Epoch 162/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9374 - acc: 0.6407 - val_loss: 0.9153 - val_acc: 0.6750\n",
      "Epoch 163/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9377 - acc: 0.6440 - val_loss: 0.9192 - val_acc: 0.6670\n",
      "Epoch 164/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9354 - acc: 0.6420 - val_loss: 0.9164 - val_acc: 0.6680\n",
      "Epoch 165/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9357 - acc: 0.6450 - val_loss: 0.9131 - val_acc: 0.6690\n",
      "Epoch 166/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9371 - acc: 0.6440 - val_loss: 0.9137 - val_acc: 0.6740\n",
      "Epoch 167/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9368 - acc: 0.6410 - val_loss: 0.9134 - val_acc: 0.6690\n",
      "Epoch 168/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9332 - acc: 0.6451 - val_loss: 0.9194 - val_acc: 0.6730\n",
      "Epoch 169/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9333 - acc: 0.6474 - val_loss: 0.9143 - val_acc: 0.6670\n",
      "Epoch 170/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9339 - acc: 0.6447 - val_loss: 0.9169 - val_acc: 0.6570\n",
      "Epoch 171/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9363 - acc: 0.6432 - val_loss: 0.9177 - val_acc: 0.6710\n",
      "Epoch 172/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9351 - acc: 0.6430 - val_loss: 0.9184 - val_acc: 0.6650\n",
      "Epoch 173/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9417 - acc: 0.6389 - val_loss: 0.9119 - val_acc: 0.6740\n",
      "Epoch 174/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9333 - acc: 0.6427 - val_loss: 0.9184 - val_acc: 0.6720\n",
      "Epoch 175/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9327 - acc: 0.6471 - val_loss: 0.9098 - val_acc: 0.6760\n",
      "Epoch 176/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9318 - acc: 0.6473 - val_loss: 0.9115 - val_acc: 0.6620\n",
      "Epoch 177/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.9343 - acc: 0.6479 - val_loss: 0.9257 - val_acc: 0.6570\n",
      "Epoch 178/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9370 - acc: 0.6395 - val_loss: 0.9092 - val_acc: 0.6790\n",
      "Epoch 179/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9325 - acc: 0.6417 - val_loss: 0.9153 - val_acc: 0.6660\n",
      "Epoch 180/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9302 - acc: 0.6452 - val_loss: 0.9168 - val_acc: 0.6670\n",
      "Epoch 181/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9379 - acc: 0.6411 - val_loss: 0.9126 - val_acc: 0.6720\n",
      "Epoch 182/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9346 - acc: 0.6433 - val_loss: 0.9151 - val_acc: 0.6680\n",
      "Epoch 183/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9341 - acc: 0.6416 - val_loss: 0.9155 - val_acc: 0.6720\n",
      "Epoch 184/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9341 - acc: 0.6410 - val_loss: 0.9107 - val_acc: 0.6760\n",
      "Epoch 185/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9291 - acc: 0.6470 - val_loss: 0.9063 - val_acc: 0.6710\n",
      "Epoch 186/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9287 - acc: 0.6453 - val_loss: 0.9073 - val_acc: 0.6770\n",
      "Epoch 187/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9312 - acc: 0.6452 - val_loss: 0.9101 - val_acc: 0.6690\n",
      "Epoch 188/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9282 - acc: 0.6467 - val_loss: 0.9374 - val_acc: 0.6440\n",
      "Epoch 189/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9431 - acc: 0.6367 - val_loss: 0.9160 - val_acc: 0.6690\n",
      "Epoch 190/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9280 - acc: 0.6445 - val_loss: 0.9072 - val_acc: 0.6730\n",
      "Epoch 191/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9305 - acc: 0.6432 - val_loss: 0.9086 - val_acc: 0.6720\n",
      "Epoch 192/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9267 - acc: 0.6481 - val_loss: 0.9112 - val_acc: 0.6640\n",
      "Epoch 193/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9279 - acc: 0.6482 - val_loss: 0.9086 - val_acc: 0.6670\n",
      "Epoch 194/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9281 - acc: 0.6481 - val_loss: 0.9136 - val_acc: 0.6620\n",
      "Epoch 195/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9277 - acc: 0.6469 - val_loss: 0.9147 - val_acc: 0.6580\n",
      "Epoch 196/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9282 - acc: 0.6467 - val_loss: 0.9068 - val_acc: 0.6790\n",
      "Epoch 197/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9253 - acc: 0.6481 - val_loss: 0.9082 - val_acc: 0.6730\n",
      "Epoch 198/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9263 - acc: 0.6479 - val_loss: 0.9066 - val_acc: 0.6800\n",
      "Epoch 199/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9304 - acc: 0.6452 - val_loss: 0.9291 - val_acc: 0.6520\n",
      "Epoch 200/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9330 - acc: 0.6477 - val_loss: 0.9122 - val_acc: 0.6620\n",
      "Epoch 201/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9269 - acc: 0.6446 - val_loss: 0.9055 - val_acc: 0.6710\n",
      "Epoch 202/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9241 - acc: 0.6480 - val_loss: 0.9042 - val_acc: 0.6770\n",
      "Epoch 203/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9244 - acc: 0.6471 - val_loss: 0.9058 - val_acc: 0.6720\n",
      "Epoch 204/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9253 - acc: 0.6482 - val_loss: 0.9040 - val_acc: 0.6770\n",
      "Epoch 205/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9265 - acc: 0.6490 - val_loss: 0.9035 - val_acc: 0.6780\n",
      "Epoch 206/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9268 - acc: 0.6454 - val_loss: 0.9079 - val_acc: 0.6610\n",
      "Epoch 207/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9268 - acc: 0.6463 - val_loss: 0.9141 - val_acc: 0.6600\n",
      "Epoch 208/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9297 - acc: 0.6479 - val_loss: 0.9025 - val_acc: 0.6740\n",
      "Epoch 209/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9267 - acc: 0.6466 - val_loss: 0.9057 - val_acc: 0.6720\n",
      "Epoch 210/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9236 - acc: 0.6467 - val_loss: 0.9032 - val_acc: 0.6740\n",
      "Epoch 211/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9254 - acc: 0.6472 - val_loss: 0.9053 - val_acc: 0.6700\n",
      "Epoch 212/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9234 - acc: 0.6483 - val_loss: 0.9035 - val_acc: 0.6750\n",
      "Epoch 213/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9234 - acc: 0.6463 - val_loss: 0.9180 - val_acc: 0.6530\n",
      "Epoch 214/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9243 - acc: 0.6429 - val_loss: 0.9065 - val_acc: 0.6680\n",
      "Epoch 215/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9213 - acc: 0.6475 - val_loss: 0.9114 - val_acc: 0.6660\n",
      "Epoch 216/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9276 - acc: 0.6450 - val_loss: 0.9079 - val_acc: 0.6640\n",
      "Epoch 217/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9225 - acc: 0.6487 - val_loss: 0.9216 - val_acc: 0.6490\n",
      "Epoch 218/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9250 - acc: 0.6461 - val_loss: 0.9019 - val_acc: 0.6640\n",
      "Epoch 219/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9225 - acc: 0.6475 - val_loss: 0.9047 - val_acc: 0.6640\n",
      "Epoch 220/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9245 - acc: 0.6442 - val_loss: 0.9003 - val_acc: 0.6760\n",
      "Epoch 221/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9213 - acc: 0.6482 - val_loss: 0.9013 - val_acc: 0.6650\n",
      "Epoch 222/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9217 - acc: 0.6465 - val_loss: 0.8980 - val_acc: 0.6790\n",
      "Epoch 223/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9202 - acc: 0.6466 - val_loss: 0.9101 - val_acc: 0.6700\n",
      "Epoch 224/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9230 - acc: 0.6472 - val_loss: 0.9079 - val_acc: 0.6700\n",
      "Epoch 225/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9208 - acc: 0.6500 - val_loss: 0.9000 - val_acc: 0.6700\n",
      "Epoch 226/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9193 - acc: 0.6493 - val_loss: 0.9048 - val_acc: 0.6700\n",
      "Epoch 227/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9192 - acc: 0.6496 - val_loss: 0.9015 - val_acc: 0.6670\n",
      "Epoch 228/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9203 - acc: 0.6482 - val_loss: 0.8978 - val_acc: 0.6710\n",
      "Epoch 229/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9190 - acc: 0.6490 - val_loss: 0.8985 - val_acc: 0.6730\n",
      "Epoch 230/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9188 - acc: 0.6491 - val_loss: 0.9080 - val_acc: 0.6640\n",
      "Epoch 231/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9213 - acc: 0.6489 - val_loss: 0.8983 - val_acc: 0.6710\n",
      "Epoch 232/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9192 - acc: 0.6504 - val_loss: 0.8981 - val_acc: 0.6760\n",
      "Epoch 233/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9190 - acc: 0.6489 - val_loss: 0.8980 - val_acc: 0.6740\n",
      "Epoch 234/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9200 - acc: 0.6475 - val_loss: 0.9006 - val_acc: 0.6750\n",
      "Epoch 235/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9182 - acc: 0.6487 - val_loss: 0.9031 - val_acc: 0.6690\n",
      "Epoch 236/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9166 - acc: 0.6520 - val_loss: 0.9110 - val_acc: 0.6640\n",
      "Epoch 237/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9207 - acc: 0.6493 - val_loss: 0.9023 - val_acc: 0.6740\n",
      "Epoch 238/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9182 - acc: 0.6484 - val_loss: 0.9022 - val_acc: 0.6730\n",
      "Epoch 239/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9192 - acc: 0.6493 - val_loss: 0.9056 - val_acc: 0.6740\n",
      "Epoch 240/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9211 - acc: 0.6487 - val_loss: 0.9052 - val_acc: 0.6740\n",
      "Epoch 241/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9192 - acc: 0.6470 - val_loss: 0.9005 - val_acc: 0.6710\n",
      "Epoch 242/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9244 - acc: 0.6429 - val_loss: 0.9009 - val_acc: 0.6700\n",
      "Epoch 243/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9180 - acc: 0.6475 - val_loss: 0.8980 - val_acc: 0.6630\n",
      "Epoch 244/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9156 - acc: 0.6519 - val_loss: 0.9029 - val_acc: 0.6640\n",
      "Epoch 245/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9164 - acc: 0.6484 - val_loss: 0.9044 - val_acc: 0.6700\n",
      "Epoch 246/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9185 - acc: 0.6488 - val_loss: 0.8984 - val_acc: 0.6740\n",
      "Epoch 247/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9157 - acc: 0.6509 - val_loss: 0.8963 - val_acc: 0.6690\n",
      "Epoch 248/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9169 - acc: 0.6509 - val_loss: 0.9182 - val_acc: 0.6570\n",
      "Epoch 249/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9190 - acc: 0.6486 - val_loss: 0.9123 - val_acc: 0.6580\n",
      "Epoch 250/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9174 - acc: 0.6510 - val_loss: 0.9062 - val_acc: 0.6660\n",
      "Epoch 251/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9138 - acc: 0.6521 - val_loss: 0.8988 - val_acc: 0.6710\n",
      "Epoch 252/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9163 - acc: 0.6500 - val_loss: 0.9017 - val_acc: 0.6670\n",
      "Epoch 253/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9190 - acc: 0.6475 - val_loss: 0.9019 - val_acc: 0.6640\n",
      "Epoch 254/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9172 - acc: 0.6484 - val_loss: 0.8978 - val_acc: 0.6730\n",
      "Epoch 255/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9159 - acc: 0.6475 - val_loss: 0.8948 - val_acc: 0.6770\n",
      "Epoch 256/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9139 - acc: 0.6539 - val_loss: 0.8992 - val_acc: 0.6730\n",
      "Epoch 257/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9154 - acc: 0.6508 - val_loss: 0.8949 - val_acc: 0.6720\n",
      "Epoch 258/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9162 - acc: 0.6509 - val_loss: 0.8969 - val_acc: 0.6770\n",
      "Epoch 259/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9143 - acc: 0.6493 - val_loss: 0.8962 - val_acc: 0.6780\n",
      "Epoch 260/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9174 - acc: 0.6474 - val_loss: 0.8949 - val_acc: 0.6730\n",
      "Epoch 261/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9136 - acc: 0.6527 - val_loss: 0.8972 - val_acc: 0.6700\n",
      "Epoch 262/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9189 - acc: 0.6490 - val_loss: 0.9014 - val_acc: 0.6760\n",
      "Epoch 263/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9125 - acc: 0.6495 - val_loss: 0.9015 - val_acc: 0.6700\n",
      "Epoch 264/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9142 - acc: 0.6515 - val_loss: 0.8923 - val_acc: 0.6760\n",
      "Epoch 265/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9129 - acc: 0.6532 - val_loss: 0.8972 - val_acc: 0.6690\n",
      "Epoch 266/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9145 - acc: 0.6508 - val_loss: 0.8913 - val_acc: 0.6740\n",
      "Epoch 267/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9111 - acc: 0.6529 - val_loss: 0.9004 - val_acc: 0.6680\n",
      "Epoch 268/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9132 - acc: 0.6507 - val_loss: 0.8982 - val_acc: 0.6690\n",
      "Epoch 269/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9133 - acc: 0.6546 - val_loss: 0.8914 - val_acc: 0.6750\n",
      "Epoch 270/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9135 - acc: 0.6512 - val_loss: 0.8936 - val_acc: 0.6730\n",
      "Epoch 271/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9131 - acc: 0.6537 - val_loss: 0.9036 - val_acc: 0.6630\n",
      "Epoch 272/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9172 - acc: 0.6481 - val_loss: 0.8936 - val_acc: 0.6690\n",
      "Epoch 273/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9130 - acc: 0.6493 - val_loss: 0.8892 - val_acc: 0.6720\n",
      "Epoch 274/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.9123 - acc: 0.6523 - val_loss: 0.8943 - val_acc: 0.6670\n",
      "Epoch 275/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9116 - acc: 0.6507 - val_loss: 0.8932 - val_acc: 0.6760\n",
      "Epoch 276/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9098 - acc: 0.6528 - val_loss: 0.8934 - val_acc: 0.6740\n",
      "Epoch 277/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9110 - acc: 0.6504 - val_loss: 0.8933 - val_acc: 0.6730\n",
      "Epoch 278/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9120 - acc: 0.6533 - val_loss: 0.8961 - val_acc: 0.6690\n",
      "Epoch 279/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9087 - acc: 0.6566 - val_loss: 0.8905 - val_acc: 0.6710\n",
      "Epoch 280/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9113 - acc: 0.6514 - val_loss: 0.8923 - val_acc: 0.6740\n",
      "Epoch 281/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9097 - acc: 0.6548 - val_loss: 0.9050 - val_acc: 0.6660\n",
      "Epoch 282/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9140 - acc: 0.6491 - val_loss: 0.8870 - val_acc: 0.6830\n",
      "Epoch 283/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9100 - acc: 0.6500 - val_loss: 0.8942 - val_acc: 0.6730\n",
      "Epoch 284/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9115 - acc: 0.6530 - val_loss: 0.8965 - val_acc: 0.6710\n",
      "Epoch 285/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9081 - acc: 0.6537 - val_loss: 0.8963 - val_acc: 0.6740\n",
      "Epoch 286/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9094 - acc: 0.6539 - val_loss: 0.8922 - val_acc: 0.6730\n",
      "Epoch 287/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9081 - acc: 0.6559 - val_loss: 0.8925 - val_acc: 0.6730\n",
      "Epoch 288/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9084 - acc: 0.6536 - val_loss: 0.8942 - val_acc: 0.6690\n",
      "Epoch 289/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9090 - acc: 0.6537 - val_loss: 0.8943 - val_acc: 0.6700\n",
      "Epoch 290/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9102 - acc: 0.6514 - val_loss: 0.8938 - val_acc: 0.6730\n",
      "Epoch 291/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9135 - acc: 0.6498 - val_loss: 0.8963 - val_acc: 0.6690\n",
      "Epoch 292/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9090 - acc: 0.6522 - val_loss: 0.8918 - val_acc: 0.6740\n",
      "Epoch 293/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9074 - acc: 0.6501 - val_loss: 0.8915 - val_acc: 0.6740\n",
      "Epoch 294/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9086 - acc: 0.6532 - val_loss: 0.8885 - val_acc: 0.6760\n",
      "Epoch 295/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9066 - acc: 0.6557 - val_loss: 0.8913 - val_acc: 0.6690\n",
      "Epoch 296/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9064 - acc: 0.6537 - val_loss: 0.8963 - val_acc: 0.6650\n",
      "Epoch 297/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9066 - acc: 0.6543 - val_loss: 0.8922 - val_acc: 0.6760\n",
      "Epoch 298/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9048 - acc: 0.6555 - val_loss: 0.9007 - val_acc: 0.6660\n",
      "Epoch 299/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9071 - acc: 0.6537 - val_loss: 0.8899 - val_acc: 0.6700\n",
      "Epoch 300/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9052 - acc: 0.6525 - val_loss: 0.8903 - val_acc: 0.6690\n",
      "Epoch 301/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9064 - acc: 0.6541 - val_loss: 0.8884 - val_acc: 0.6700\n",
      "Epoch 302/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9049 - acc: 0.6565 - val_loss: 0.9031 - val_acc: 0.6700\n",
      "Epoch 303/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9086 - acc: 0.6539 - val_loss: 0.8913 - val_acc: 0.6710\n",
      "Epoch 304/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9070 - acc: 0.6548 - val_loss: 0.8912 - val_acc: 0.6780\n",
      "Epoch 305/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9087 - acc: 0.6507 - val_loss: 0.8974 - val_acc: 0.6780\n",
      "Epoch 306/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9063 - acc: 0.6540 - val_loss: 0.8910 - val_acc: 0.6660\n",
      "Epoch 307/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9056 - acc: 0.6528 - val_loss: 0.8897 - val_acc: 0.6720\n",
      "Epoch 308/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9043 - acc: 0.6548 - val_loss: 0.8969 - val_acc: 0.6660\n",
      "Epoch 309/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9042 - acc: 0.6546 - val_loss: 0.8952 - val_acc: 0.6630\n",
      "Epoch 310/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9081 - acc: 0.6532 - val_loss: 0.8964 - val_acc: 0.6690\n",
      "Epoch 311/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9048 - acc: 0.6539 - val_loss: 0.8935 - val_acc: 0.6710\n",
      "Epoch 312/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9042 - acc: 0.6542 - val_loss: 0.8898 - val_acc: 0.6720\n",
      "Epoch 313/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9025 - acc: 0.6545 - val_loss: 0.8903 - val_acc: 0.6710\n",
      "Epoch 314/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9032 - acc: 0.6548 - val_loss: 0.8906 - val_acc: 0.6710\n",
      "Epoch 315/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9024 - acc: 0.6566 - val_loss: 0.8859 - val_acc: 0.6730\n",
      "Epoch 316/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9025 - acc: 0.6559 - val_loss: 0.8986 - val_acc: 0.6780\n",
      "Epoch 317/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9048 - acc: 0.6550 - val_loss: 0.8942 - val_acc: 0.6670\n",
      "Epoch 318/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9015 - acc: 0.6571 - val_loss: 0.9004 - val_acc: 0.6590\n",
      "Epoch 319/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9057 - acc: 0.6548 - val_loss: 0.8958 - val_acc: 0.6660\n",
      "Epoch 320/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9044 - acc: 0.6539 - val_loss: 0.8912 - val_acc: 0.6690\n",
      "Epoch 321/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9016 - acc: 0.6572 - val_loss: 0.8992 - val_acc: 0.6720\n",
      "Epoch 322/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9020 - acc: 0.6550 - val_loss: 0.8917 - val_acc: 0.6770\n",
      "Epoch 323/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9028 - acc: 0.6543 - val_loss: 0.8988 - val_acc: 0.6720\n",
      "Epoch 324/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9061 - acc: 0.6544 - val_loss: 0.8872 - val_acc: 0.6680\n",
      "Epoch 325/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9015 - acc: 0.6553 - val_loss: 0.8878 - val_acc: 0.6740\n",
      "Epoch 326/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9016 - acc: 0.6558 - val_loss: 0.8952 - val_acc: 0.6620\n",
      "Epoch 327/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9042 - acc: 0.6556 - val_loss: 0.8896 - val_acc: 0.6690\n",
      "Epoch 328/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9016 - acc: 0.6562 - val_loss: 0.9084 - val_acc: 0.6600\n",
      "Epoch 329/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9046 - acc: 0.6526 - val_loss: 0.8906 - val_acc: 0.6710\n",
      "Epoch 330/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9028 - acc: 0.6558 - val_loss: 0.8872 - val_acc: 0.6770\n",
      "Epoch 331/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8998 - acc: 0.6558 - val_loss: 0.8915 - val_acc: 0.6730\n",
      "Epoch 332/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9015 - acc: 0.6566 - val_loss: 0.8850 - val_acc: 0.6750\n",
      "Epoch 333/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8999 - acc: 0.6565 - val_loss: 0.8898 - val_acc: 0.6740\n",
      "Epoch 334/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9035 - acc: 0.6551 - val_loss: 0.8912 - val_acc: 0.6780\n",
      "Epoch 335/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8988 - acc: 0.6555 - val_loss: 0.8924 - val_acc: 0.6680\n",
      "Epoch 336/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9006 - acc: 0.6537 - val_loss: 0.8862 - val_acc: 0.6840\n",
      "Epoch 337/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8999 - acc: 0.6591 - val_loss: 0.8953 - val_acc: 0.6710\n",
      "Epoch 338/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9042 - acc: 0.6541 - val_loss: 0.8878 - val_acc: 0.6740\n",
      "Epoch 339/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9004 - acc: 0.6542 - val_loss: 0.8895 - val_acc: 0.6700\n",
      "Epoch 340/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8991 - acc: 0.6544 - val_loss: 0.8862 - val_acc: 0.6780\n",
      "Epoch 341/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8990 - acc: 0.6561 - val_loss: 0.8911 - val_acc: 0.6720\n",
      "Epoch 342/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8998 - acc: 0.6569 - val_loss: 0.8919 - val_acc: 0.6800\n",
      "Epoch 343/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9033 - acc: 0.6493 - val_loss: 0.8897 - val_acc: 0.6760\n",
      "Epoch 344/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9000 - acc: 0.6573 - val_loss: 0.8885 - val_acc: 0.6670\n",
      "Epoch 345/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9043 - acc: 0.6496 - val_loss: 0.8927 - val_acc: 0.6690\n",
      "Epoch 346/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.9002 - acc: 0.6561 - val_loss: 0.8934 - val_acc: 0.6710\n",
      "Epoch 347/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9032 - acc: 0.6519 - val_loss: 0.8927 - val_acc: 0.6750\n",
      "Epoch 348/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9007 - acc: 0.6563 - val_loss: 0.8854 - val_acc: 0.6780\n",
      "Epoch 349/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8964 - acc: 0.6587 - val_loss: 0.8873 - val_acc: 0.6770\n",
      "Epoch 350/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8989 - acc: 0.6568 - val_loss: 0.8884 - val_acc: 0.6720\n",
      "Epoch 351/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9005 - acc: 0.6585 - val_loss: 0.8860 - val_acc: 0.6780\n",
      "Epoch 352/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8977 - acc: 0.6572 - val_loss: 0.8908 - val_acc: 0.6760\n",
      "Epoch 353/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8976 - acc: 0.6570 - val_loss: 0.8961 - val_acc: 0.6670\n",
      "Epoch 354/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8969 - acc: 0.6583 - val_loss: 0.8872 - val_acc: 0.6690\n",
      "Epoch 355/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8971 - acc: 0.6578 - val_loss: 0.9023 - val_acc: 0.6550\n",
      "Epoch 356/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9025 - acc: 0.6503 - val_loss: 0.8865 - val_acc: 0.6760\n",
      "Epoch 357/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8983 - acc: 0.6543 - val_loss: 0.8949 - val_acc: 0.6710\n",
      "Epoch 358/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8982 - acc: 0.6573 - val_loss: 0.8918 - val_acc: 0.6660\n",
      "Epoch 359/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8963 - acc: 0.6578 - val_loss: 0.9040 - val_acc: 0.6670\n",
      "Epoch 360/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9011 - acc: 0.6507 - val_loss: 0.8847 - val_acc: 0.6640\n",
      "Epoch 361/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8966 - acc: 0.6578 - val_loss: 0.8999 - val_acc: 0.6700\n",
      "Epoch 362/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8979 - acc: 0.6584 - val_loss: 0.8989 - val_acc: 0.6640\n",
      "Epoch 363/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9012 - acc: 0.6553 - val_loss: 0.8939 - val_acc: 0.6710\n",
      "Epoch 364/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9031 - acc: 0.6550 - val_loss: 0.8890 - val_acc: 0.6660\n",
      "Epoch 365/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8960 - acc: 0.6570 - val_loss: 0.8893 - val_acc: 0.6700\n",
      "Epoch 366/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8965 - acc: 0.6568 - val_loss: 0.8884 - val_acc: 0.6760\n",
      "Epoch 367/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8964 - acc: 0.6565 - val_loss: 0.8892 - val_acc: 0.6730\n",
      "Epoch 368/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8954 - acc: 0.6587 - val_loss: 0.8893 - val_acc: 0.6590\n",
      "Epoch 369/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8985 - acc: 0.6540 - val_loss: 0.8888 - val_acc: 0.6680\n",
      "Epoch 370/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8973 - acc: 0.6571 - val_loss: 0.8897 - val_acc: 0.6660\n",
      "Epoch 371/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8962 - acc: 0.6560 - val_loss: 0.8845 - val_acc: 0.6720\n",
      "Epoch 372/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8940 - acc: 0.6589 - val_loss: 0.8834 - val_acc: 0.6720\n",
      "Epoch 373/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8961 - acc: 0.6579 - val_loss: 0.8929 - val_acc: 0.6780\n",
      "Epoch 374/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8980 - acc: 0.6586 - val_loss: 0.8866 - val_acc: 0.6660\n",
      "Epoch 375/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8983 - acc: 0.6589 - val_loss: 0.8859 - val_acc: 0.6720\n",
      "Epoch 376/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8949 - acc: 0.6580 - val_loss: 0.8875 - val_acc: 0.6710\n",
      "Epoch 377/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8963 - acc: 0.6576 - val_loss: 0.8882 - val_acc: 0.6730\n",
      "Epoch 378/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8996 - acc: 0.6531 - val_loss: 0.8857 - val_acc: 0.6760\n",
      "Epoch 379/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8977 - acc: 0.6538 - val_loss: 0.8931 - val_acc: 0.6650\n",
      "Epoch 380/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8954 - acc: 0.6584 - val_loss: 0.8860 - val_acc: 0.6790\n",
      "Epoch 381/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8932 - acc: 0.6612 - val_loss: 0.8881 - val_acc: 0.6760\n",
      "Epoch 382/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8926 - acc: 0.6594 - val_loss: 0.8866 - val_acc: 0.6750\n",
      "Epoch 383/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8932 - acc: 0.6567 - val_loss: 0.8902 - val_acc: 0.6620\n",
      "Epoch 384/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8937 - acc: 0.6567 - val_loss: 0.8905 - val_acc: 0.6720\n",
      "Epoch 385/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8955 - acc: 0.6579 - val_loss: 0.8940 - val_acc: 0.6760\n",
      "Epoch 386/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8955 - acc: 0.6566 - val_loss: 0.8812 - val_acc: 0.6730\n",
      "Epoch 387/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8930 - acc: 0.6578 - val_loss: 0.8895 - val_acc: 0.6670\n",
      "Epoch 388/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8974 - acc: 0.6553 - val_loss: 0.8812 - val_acc: 0.6750\n",
      "Epoch 389/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8925 - acc: 0.6583 - val_loss: 0.8940 - val_acc: 0.6510\n",
      "Epoch 390/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.9063 - acc: 0.6477 - val_loss: 0.8903 - val_acc: 0.6680\n",
      "Epoch 391/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8960 - acc: 0.6575 - val_loss: 0.8799 - val_acc: 0.6760\n",
      "Epoch 392/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8934 - acc: 0.6596 - val_loss: 0.8966 - val_acc: 0.6700\n",
      "Epoch 393/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8921 - acc: 0.6586 - val_loss: 0.8843 - val_acc: 0.6780\n",
      "Epoch 394/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8929 - acc: 0.6616 - val_loss: 0.8924 - val_acc: 0.6690\n",
      "Epoch 395/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8938 - acc: 0.6593 - val_loss: 0.8844 - val_acc: 0.6780\n",
      "Epoch 396/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8915 - acc: 0.6598 - val_loss: 0.8876 - val_acc: 0.6610\n",
      "Epoch 397/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8977 - acc: 0.6536 - val_loss: 0.8865 - val_acc: 0.6710\n",
      "Epoch 398/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8949 - acc: 0.6553 - val_loss: 0.8868 - val_acc: 0.6660\n",
      "Epoch 399/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8925 - acc: 0.6577 - val_loss: 0.8792 - val_acc: 0.6690\n",
      "Epoch 400/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8927 - acc: 0.6602 - val_loss: 0.8966 - val_acc: 0.6650\n",
      "Epoch 401/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8943 - acc: 0.6585 - val_loss: 0.8920 - val_acc: 0.6700\n",
      "Epoch 402/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8931 - acc: 0.6571 - val_loss: 0.8860 - val_acc: 0.6720\n",
      "Epoch 403/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8929 - acc: 0.6588 - val_loss: 0.8876 - val_acc: 0.6740\n",
      "Epoch 404/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8916 - acc: 0.6603 - val_loss: 0.8883 - val_acc: 0.6690\n",
      "Epoch 405/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8930 - acc: 0.6580 - val_loss: 0.8804 - val_acc: 0.6790\n",
      "Epoch 406/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8910 - acc: 0.6582 - val_loss: 0.8815 - val_acc: 0.6820\n",
      "Epoch 407/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8942 - acc: 0.6564 - val_loss: 0.8826 - val_acc: 0.6680\n",
      "Epoch 408/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8929 - acc: 0.6582 - val_loss: 0.8810 - val_acc: 0.6730\n",
      "Epoch 409/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8945 - acc: 0.6572 - val_loss: 0.8799 - val_acc: 0.6710\n",
      "Epoch 410/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8913 - acc: 0.6584 - val_loss: 0.8853 - val_acc: 0.6810\n",
      "Epoch 411/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8954 - acc: 0.6551 - val_loss: 0.8908 - val_acc: 0.6510\n",
      "Epoch 412/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8928 - acc: 0.6569 - val_loss: 0.8808 - val_acc: 0.6760\n",
      "Epoch 413/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8893 - acc: 0.6633 - val_loss: 0.8820 - val_acc: 0.6750\n",
      "Epoch 414/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8903 - acc: 0.6585 - val_loss: 0.8816 - val_acc: 0.6820\n",
      "Epoch 415/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8902 - acc: 0.6609 - val_loss: 0.8834 - val_acc: 0.6780\n",
      "Epoch 416/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8898 - acc: 0.6589 - val_loss: 0.8824 - val_acc: 0.6870\n",
      "Epoch 417/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8894 - acc: 0.6594 - val_loss: 0.8803 - val_acc: 0.6790\n",
      "Epoch 418/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8914 - acc: 0.6561 - val_loss: 0.8789 - val_acc: 0.6760\n",
      "Epoch 419/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8903 - acc: 0.6602 - val_loss: 0.8774 - val_acc: 0.6820\n",
      "Epoch 420/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8887 - acc: 0.6589 - val_loss: 0.8944 - val_acc: 0.6670\n",
      "Epoch 421/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8934 - acc: 0.6596 - val_loss: 0.8932 - val_acc: 0.6730\n",
      "Epoch 422/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8951 - acc: 0.6555 - val_loss: 0.8823 - val_acc: 0.6800\n",
      "Epoch 423/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8953 - acc: 0.6570 - val_loss: 0.8929 - val_acc: 0.6640\n",
      "Epoch 424/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8888 - acc: 0.6591 - val_loss: 0.8748 - val_acc: 0.6690\n",
      "Epoch 425/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8914 - acc: 0.6555 - val_loss: 0.8800 - val_acc: 0.6760\n",
      "Epoch 426/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8904 - acc: 0.6610 - val_loss: 0.8771 - val_acc: 0.6750\n",
      "Epoch 427/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8885 - acc: 0.6606 - val_loss: 0.8797 - val_acc: 0.6790\n",
      "Epoch 428/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8893 - acc: 0.6605 - val_loss: 0.8767 - val_acc: 0.6780\n",
      "Epoch 429/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8862 - acc: 0.6633 - val_loss: 0.8766 - val_acc: 0.6810\n",
      "Epoch 430/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8876 - acc: 0.6609 - val_loss: 0.8806 - val_acc: 0.6550\n",
      "Epoch 431/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8910 - acc: 0.6589 - val_loss: 0.8863 - val_acc: 0.6720\n",
      "Epoch 432/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8922 - acc: 0.6557 - val_loss: 0.8872 - val_acc: 0.6670\n",
      "Epoch 433/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8922 - acc: 0.6605 - val_loss: 0.8843 - val_acc: 0.6690\n",
      "Epoch 434/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8880 - acc: 0.6599 - val_loss: 0.8784 - val_acc: 0.6760\n",
      "Epoch 435/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8870 - acc: 0.6620 - val_loss: 0.8815 - val_acc: 0.6780\n",
      "Epoch 436/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8927 - acc: 0.6569 - val_loss: 0.8767 - val_acc: 0.6750\n",
      "Epoch 437/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8860 - acc: 0.6637 - val_loss: 0.8765 - val_acc: 0.6800\n",
      "Epoch 438/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8869 - acc: 0.6609 - val_loss: 0.8799 - val_acc: 0.6720\n",
      "Epoch 439/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8876 - acc: 0.6606 - val_loss: 0.8840 - val_acc: 0.6810\n",
      "Epoch 440/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8872 - acc: 0.6626 - val_loss: 0.8811 - val_acc: 0.6730\n",
      "Epoch 441/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8864 - acc: 0.6594 - val_loss: 0.8775 - val_acc: 0.6710\n",
      "Epoch 442/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8884 - acc: 0.6602 - val_loss: 0.8801 - val_acc: 0.6770\n",
      "Epoch 443/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8871 - acc: 0.6641 - val_loss: 0.8856 - val_acc: 0.6770\n",
      "Epoch 444/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8893 - acc: 0.6582 - val_loss: 0.8838 - val_acc: 0.6760\n",
      "Epoch 445/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8901 - acc: 0.6585 - val_loss: 0.8792 - val_acc: 0.6770\n",
      "Epoch 446/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8910 - acc: 0.6553 - val_loss: 0.8804 - val_acc: 0.6780\n",
      "Epoch 447/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8855 - acc: 0.6622 - val_loss: 0.8740 - val_acc: 0.6730\n",
      "Epoch 448/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8860 - acc: 0.6610 - val_loss: 0.8749 - val_acc: 0.6800\n",
      "Epoch 449/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8857 - acc: 0.6605 - val_loss: 0.8772 - val_acc: 0.6690\n",
      "Epoch 450/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8846 - acc: 0.6639 - val_loss: 0.8790 - val_acc: 0.6720\n",
      "Epoch 451/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8863 - acc: 0.6618 - val_loss: 0.8770 - val_acc: 0.6820\n",
      "Epoch 452/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8851 - acc: 0.6619 - val_loss: 0.8760 - val_acc: 0.6780\n",
      "Epoch 453/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8872 - acc: 0.6546 - val_loss: 0.8830 - val_acc: 0.6620\n",
      "Epoch 454/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8902 - acc: 0.6613 - val_loss: 0.8763 - val_acc: 0.6800\n",
      "Epoch 455/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8864 - acc: 0.6591 - val_loss: 0.8818 - val_acc: 0.6830\n",
      "Epoch 456/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8923 - acc: 0.6553 - val_loss: 0.8788 - val_acc: 0.6790\n",
      "Epoch 457/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8861 - acc: 0.6602 - val_loss: 0.8788 - val_acc: 0.6740\n",
      "Epoch 458/2000\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.8857 - acc: 0.6592 - val_loss: 0.8865 - val_acc: 0.6720\n",
      "Epoch 459/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8870 - acc: 0.6603 - val_loss: 0.8749 - val_acc: 0.6730\n",
      "Epoch 460/2000\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.8851 - acc: 0.6621 - val_loss: 0.8727 - val_acc: 0.6780\n",
      "Epoch 461/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8854 - acc: 0.6575 - val_loss: 0.8849 - val_acc: 0.6650\n",
      "Epoch 462/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8853 - acc: 0.6615 - val_loss: 0.8789 - val_acc: 0.6780\n",
      "Epoch 463/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8848 - acc: 0.6597 - val_loss: 0.8783 - val_acc: 0.6800\n",
      "Epoch 464/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8848 - acc: 0.6623 - val_loss: 0.8775 - val_acc: 0.6800\n",
      "Epoch 465/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8840 - acc: 0.6631 - val_loss: 0.8780 - val_acc: 0.6730\n",
      "Epoch 466/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8872 - acc: 0.6623 - val_loss: 0.8787 - val_acc: 0.6760\n",
      "Epoch 467/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8863 - acc: 0.6585 - val_loss: 0.8939 - val_acc: 0.6700\n",
      "Epoch 468/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8884 - acc: 0.6610 - val_loss: 0.8809 - val_acc: 0.6770\n",
      "Epoch 469/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8863 - acc: 0.6624 - val_loss: 0.8817 - val_acc: 0.6740\n",
      "Epoch 470/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8858 - acc: 0.6608 - val_loss: 0.8754 - val_acc: 0.6730\n",
      "Epoch 471/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8829 - acc: 0.6626 - val_loss: 0.8729 - val_acc: 0.6780\n",
      "Epoch 472/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8838 - acc: 0.6619 - val_loss: 0.8763 - val_acc: 0.6850\n",
      "Epoch 473/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8842 - acc: 0.6626 - val_loss: 0.8853 - val_acc: 0.6750\n",
      "Epoch 474/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8850 - acc: 0.6585 - val_loss: 0.8719 - val_acc: 0.6850\n",
      "Epoch 475/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8832 - acc: 0.6637 - val_loss: 0.8757 - val_acc: 0.6840\n",
      "Epoch 476/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8838 - acc: 0.6622 - val_loss: 0.8711 - val_acc: 0.6850\n",
      "Epoch 477/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8822 - acc: 0.6644 - val_loss: 0.8741 - val_acc: 0.6760\n",
      "Epoch 478/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8840 - acc: 0.6591 - val_loss: 0.8819 - val_acc: 0.6540\n",
      "Epoch 479/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8843 - acc: 0.6626 - val_loss: 0.8763 - val_acc: 0.6780\n",
      "Epoch 480/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8834 - acc: 0.6602 - val_loss: 0.8770 - val_acc: 0.6580\n",
      "Epoch 481/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8833 - acc: 0.6600 - val_loss: 0.8921 - val_acc: 0.6750\n",
      "Epoch 482/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8867 - acc: 0.6603 - val_loss: 0.8731 - val_acc: 0.6730\n",
      "Epoch 483/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8875 - acc: 0.6595 - val_loss: 0.8943 - val_acc: 0.6630\n",
      "Epoch 484/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8858 - acc: 0.6605 - val_loss: 0.8875 - val_acc: 0.6670\n",
      "Epoch 485/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8838 - acc: 0.6607 - val_loss: 0.8733 - val_acc: 0.6850\n",
      "Epoch 486/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8823 - acc: 0.6624 - val_loss: 0.8785 - val_acc: 0.6780\n",
      "Epoch 487/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8852 - acc: 0.6612 - val_loss: 0.8761 - val_acc: 0.6790\n",
      "Epoch 488/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8837 - acc: 0.6605 - val_loss: 0.8814 - val_acc: 0.6750\n",
      "Epoch 489/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8933 - acc: 0.6525 - val_loss: 0.8746 - val_acc: 0.6780\n",
      "Epoch 490/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8849 - acc: 0.6605 - val_loss: 0.8735 - val_acc: 0.6810\n",
      "Epoch 491/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8801 - acc: 0.6621 - val_loss: 0.8745 - val_acc: 0.6790\n",
      "Epoch 492/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8818 - acc: 0.6637 - val_loss: 0.8792 - val_acc: 0.6840\n",
      "Epoch 493/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8833 - acc: 0.6596 - val_loss: 0.8821 - val_acc: 0.6690\n",
      "Epoch 494/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8863 - acc: 0.6602 - val_loss: 0.8758 - val_acc: 0.6790\n",
      "Epoch 495/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8797 - acc: 0.6633 - val_loss: 0.8753 - val_acc: 0.6750\n",
      "Epoch 496/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8846 - acc: 0.6608 - val_loss: 0.8761 - val_acc: 0.6740\n",
      "Epoch 497/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8806 - acc: 0.6639 - val_loss: 0.8787 - val_acc: 0.6770\n",
      "Epoch 498/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8829 - acc: 0.6582 - val_loss: 0.8708 - val_acc: 0.6680\n",
      "Epoch 499/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8836 - acc: 0.6597 - val_loss: 0.8708 - val_acc: 0.6640\n",
      "Epoch 500/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8808 - acc: 0.6636 - val_loss: 0.8763 - val_acc: 0.6730\n",
      "Epoch 501/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8809 - acc: 0.6631 - val_loss: 0.8750 - val_acc: 0.6730\n",
      "Epoch 502/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8796 - acc: 0.6666 - val_loss: 0.8771 - val_acc: 0.6720\n",
      "Epoch 503/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8820 - acc: 0.6624 - val_loss: 0.8816 - val_acc: 0.6730\n",
      "Epoch 504/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8836 - acc: 0.6598 - val_loss: 0.8749 - val_acc: 0.6750\n",
      "Epoch 505/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8799 - acc: 0.6646 - val_loss: 0.8819 - val_acc: 0.6720\n",
      "Epoch 506/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8803 - acc: 0.6625 - val_loss: 0.8733 - val_acc: 0.6710\n",
      "Epoch 507/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8812 - acc: 0.6625 - val_loss: 0.8701 - val_acc: 0.6740\n",
      "Epoch 508/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8798 - acc: 0.6649 - val_loss: 0.8899 - val_acc: 0.6670\n",
      "Epoch 509/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8864 - acc: 0.6610 - val_loss: 0.8695 - val_acc: 0.6690\n",
      "Epoch 510/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8805 - acc: 0.6624 - val_loss: 0.8835 - val_acc: 0.6610\n",
      "Epoch 511/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8804 - acc: 0.6639 - val_loss: 0.8733 - val_acc: 0.6760\n",
      "Epoch 512/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8817 - acc: 0.6625 - val_loss: 0.8824 - val_acc: 0.6700\n",
      "Epoch 513/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8814 - acc: 0.6632 - val_loss: 0.8749 - val_acc: 0.6760\n",
      "Epoch 514/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8795 - acc: 0.6624 - val_loss: 0.8752 - val_acc: 0.6770\n",
      "Epoch 515/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8798 - acc: 0.6627 - val_loss: 0.8746 - val_acc: 0.6760\n",
      "Epoch 516/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8821 - acc: 0.6619 - val_loss: 0.8739 - val_acc: 0.6740\n",
      "Epoch 517/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8799 - acc: 0.6625 - val_loss: 0.8692 - val_acc: 0.6830\n",
      "Epoch 518/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8820 - acc: 0.6617 - val_loss: 0.8736 - val_acc: 0.6810\n",
      "Epoch 519/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8788 - acc: 0.6610 - val_loss: 0.8696 - val_acc: 0.6770\n",
      "Epoch 520/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8776 - acc: 0.6637 - val_loss: 0.8706 - val_acc: 0.6750\n",
      "Epoch 521/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8825 - acc: 0.6628 - val_loss: 0.8756 - val_acc: 0.6760\n",
      "Epoch 522/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8798 - acc: 0.6621 - val_loss: 0.8711 - val_acc: 0.6790\n",
      "Epoch 523/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8791 - acc: 0.6630 - val_loss: 0.8749 - val_acc: 0.6720\n",
      "Epoch 524/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8790 - acc: 0.6653 - val_loss: 0.8691 - val_acc: 0.6840\n",
      "Epoch 525/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8794 - acc: 0.6633 - val_loss: 0.8720 - val_acc: 0.6670\n",
      "Epoch 526/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8823 - acc: 0.6619 - val_loss: 0.8701 - val_acc: 0.6760\n",
      "Epoch 527/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8796 - acc: 0.6642 - val_loss: 0.8716 - val_acc: 0.6790\n",
      "Epoch 528/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8777 - acc: 0.6649 - val_loss: 0.8681 - val_acc: 0.6700\n",
      "Epoch 529/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8811 - acc: 0.6646 - val_loss: 0.8697 - val_acc: 0.6840\n",
      "Epoch 530/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8772 - acc: 0.6656 - val_loss: 0.8695 - val_acc: 0.6820\n",
      "Epoch 531/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8777 - acc: 0.6667 - val_loss: 0.8830 - val_acc: 0.6730\n",
      "Epoch 532/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8798 - acc: 0.6630 - val_loss: 0.8780 - val_acc: 0.6750\n",
      "Epoch 533/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8782 - acc: 0.6641 - val_loss: 0.8726 - val_acc: 0.6820\n",
      "Epoch 534/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8779 - acc: 0.6643 - val_loss: 0.8721 - val_acc: 0.6690\n",
      "Epoch 535/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8798 - acc: 0.6626 - val_loss: 0.8754 - val_acc: 0.6770\n",
      "Epoch 536/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8783 - acc: 0.6623 - val_loss: 0.8709 - val_acc: 0.6760\n",
      "Epoch 537/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8764 - acc: 0.6638 - val_loss: 0.8729 - val_acc: 0.6850\n",
      "Epoch 538/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8804 - acc: 0.6619 - val_loss: 0.8698 - val_acc: 0.6780\n",
      "Epoch 539/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8780 - acc: 0.6656 - val_loss: 0.8793 - val_acc: 0.6730\n",
      "Epoch 540/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8782 - acc: 0.6619 - val_loss: 0.8736 - val_acc: 0.6700\n",
      "Epoch 541/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8787 - acc: 0.6634 - val_loss: 0.8749 - val_acc: 0.6740\n",
      "Epoch 542/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8786 - acc: 0.6656 - val_loss: 0.8710 - val_acc: 0.6800\n",
      "Epoch 543/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8818 - acc: 0.6607 - val_loss: 0.8735 - val_acc: 0.6730\n",
      "Epoch 544/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8769 - acc: 0.6653 - val_loss: 0.8850 - val_acc: 0.6730\n",
      "Epoch 545/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8811 - acc: 0.6638 - val_loss: 0.8756 - val_acc: 0.6640\n",
      "Epoch 546/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8789 - acc: 0.6652 - val_loss: 0.8721 - val_acc: 0.6740\n",
      "Epoch 547/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8769 - acc: 0.6656 - val_loss: 0.8776 - val_acc: 0.6650\n",
      "Epoch 548/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8776 - acc: 0.6635 - val_loss: 0.8703 - val_acc: 0.6740\n",
      "Epoch 549/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8761 - acc: 0.6642 - val_loss: 0.8663 - val_acc: 0.6670\n",
      "Epoch 550/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8772 - acc: 0.6604 - val_loss: 0.8727 - val_acc: 0.6670\n",
      "Epoch 551/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8780 - acc: 0.6633 - val_loss: 0.8725 - val_acc: 0.6660\n",
      "Epoch 552/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8748 - acc: 0.6653 - val_loss: 0.8696 - val_acc: 0.6760\n",
      "Epoch 553/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8755 - acc: 0.6638 - val_loss: 0.8744 - val_acc: 0.6700\n",
      "Epoch 554/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8766 - acc: 0.6640 - val_loss: 0.8812 - val_acc: 0.6770\n",
      "Epoch 555/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8771 - acc: 0.6649 - val_loss: 0.8767 - val_acc: 0.6730\n",
      "Epoch 556/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8767 - acc: 0.6648 - val_loss: 0.8765 - val_acc: 0.6700\n",
      "Epoch 557/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8786 - acc: 0.6619 - val_loss: 0.8761 - val_acc: 0.6630\n",
      "Epoch 558/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8803 - acc: 0.6602 - val_loss: 0.8760 - val_acc: 0.6790\n",
      "Epoch 559/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8804 - acc: 0.6605 - val_loss: 0.8697 - val_acc: 0.6780\n",
      "Epoch 560/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8738 - acc: 0.6654 - val_loss: 0.8762 - val_acc: 0.6770\n",
      "Epoch 561/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8767 - acc: 0.6660 - val_loss: 0.8722 - val_acc: 0.6600\n",
      "Epoch 562/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8786 - acc: 0.6613 - val_loss: 0.8762 - val_acc: 0.6770\n",
      "Epoch 563/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8842 - acc: 0.6589 - val_loss: 0.8887 - val_acc: 0.6690\n",
      "Epoch 564/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8798 - acc: 0.6621 - val_loss: 0.8680 - val_acc: 0.6840\n",
      "Epoch 565/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8799 - acc: 0.6613 - val_loss: 0.8676 - val_acc: 0.6700\n",
      "Epoch 566/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8774 - acc: 0.6652 - val_loss: 0.8680 - val_acc: 0.6860\n",
      "Epoch 567/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8755 - acc: 0.6645 - val_loss: 0.8694 - val_acc: 0.6780\n",
      "Epoch 568/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8779 - acc: 0.6610 - val_loss: 0.8868 - val_acc: 0.6610\n",
      "Epoch 569/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8800 - acc: 0.6635 - val_loss: 0.8726 - val_acc: 0.6770\n",
      "Epoch 570/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8753 - acc: 0.6646 - val_loss: 0.8739 - val_acc: 0.6740\n",
      "Epoch 571/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8757 - acc: 0.6630 - val_loss: 0.8739 - val_acc: 0.6700\n",
      "Epoch 572/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8746 - acc: 0.6635 - val_loss: 0.8671 - val_acc: 0.6830\n",
      "Epoch 573/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8744 - acc: 0.6621 - val_loss: 0.8803 - val_acc: 0.6670\n",
      "Epoch 574/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8769 - acc: 0.6623 - val_loss: 0.8747 - val_acc: 0.6760\n",
      "Epoch 575/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8758 - acc: 0.6635 - val_loss: 0.8655 - val_acc: 0.6830\n",
      "Epoch 576/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8733 - acc: 0.6667 - val_loss: 0.8714 - val_acc: 0.6810\n",
      "Epoch 577/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8781 - acc: 0.6632 - val_loss: 0.8755 - val_acc: 0.6730\n",
      "Epoch 578/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8808 - acc: 0.6599 - val_loss: 0.8662 - val_acc: 0.6780\n",
      "Epoch 579/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8733 - acc: 0.6651 - val_loss: 0.8834 - val_acc: 0.6720\n",
      "Epoch 580/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8787 - acc: 0.6610 - val_loss: 0.8822 - val_acc: 0.6740\n",
      "Epoch 581/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8777 - acc: 0.6618 - val_loss: 0.8689 - val_acc: 0.6780\n",
      "Epoch 582/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8783 - acc: 0.6635 - val_loss: 0.8739 - val_acc: 0.6760\n",
      "Epoch 583/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8778 - acc: 0.6619 - val_loss: 0.8869 - val_acc: 0.6640\n",
      "Epoch 584/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8886 - acc: 0.6600 - val_loss: 0.8683 - val_acc: 0.6780\n",
      "Epoch 585/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8735 - acc: 0.6642 - val_loss: 0.8682 - val_acc: 0.6830\n",
      "Epoch 586/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8753 - acc: 0.6645 - val_loss: 0.8731 - val_acc: 0.6730\n",
      "Epoch 587/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8792 - acc: 0.6604 - val_loss: 0.8777 - val_acc: 0.6760\n",
      "Epoch 588/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8732 - acc: 0.6653 - val_loss: 0.8676 - val_acc: 0.6680\n",
      "Epoch 589/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8735 - acc: 0.6660 - val_loss: 0.8651 - val_acc: 0.6730\n",
      "Epoch 590/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8744 - acc: 0.6632 - val_loss: 0.8698 - val_acc: 0.6800\n",
      "Epoch 591/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8755 - acc: 0.6644 - val_loss: 0.8705 - val_acc: 0.6740\n",
      "Epoch 592/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8721 - acc: 0.6669 - val_loss: 0.8684 - val_acc: 0.6790\n",
      "Epoch 593/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8722 - acc: 0.6653 - val_loss: 0.8662 - val_acc: 0.6730\n",
      "Epoch 594/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8802 - acc: 0.6587 - val_loss: 0.8624 - val_acc: 0.6770\n",
      "Epoch 595/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8718 - acc: 0.6667 - val_loss: 0.8679 - val_acc: 0.6740\n",
      "Epoch 596/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8740 - acc: 0.6649 - val_loss: 0.8677 - val_acc: 0.6800\n",
      "Epoch 597/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8720 - acc: 0.6659 - val_loss: 0.8733 - val_acc: 0.6780\n",
      "Epoch 598/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8757 - acc: 0.6626 - val_loss: 0.8680 - val_acc: 0.6760\n",
      "Epoch 599/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8724 - acc: 0.6663 - val_loss: 0.8739 - val_acc: 0.6760\n",
      "Epoch 600/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8745 - acc: 0.6640 - val_loss: 0.8726 - val_acc: 0.6660\n",
      "Epoch 601/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8739 - acc: 0.6642 - val_loss: 0.8787 - val_acc: 0.6680\n",
      "Epoch 602/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8755 - acc: 0.6637 - val_loss: 0.8682 - val_acc: 0.6740\n",
      "Epoch 603/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8715 - acc: 0.6643 - val_loss: 0.8689 - val_acc: 0.6750\n",
      "Epoch 604/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8713 - acc: 0.6624 - val_loss: 0.8743 - val_acc: 0.6670\n",
      "Epoch 605/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8774 - acc: 0.6619 - val_loss: 0.8692 - val_acc: 0.6740\n",
      "Epoch 606/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8730 - acc: 0.6673 - val_loss: 0.8649 - val_acc: 0.6780\n",
      "Epoch 607/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8706 - acc: 0.6683 - val_loss: 0.8713 - val_acc: 0.6720\n",
      "Epoch 608/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8714 - acc: 0.6674 - val_loss: 0.8692 - val_acc: 0.6690\n",
      "Epoch 609/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8710 - acc: 0.6674 - val_loss: 0.8711 - val_acc: 0.6760\n",
      "Epoch 610/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8720 - acc: 0.6656 - val_loss: 0.8792 - val_acc: 0.6590\n",
      "Epoch 611/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8756 - acc: 0.6629 - val_loss: 0.8704 - val_acc: 0.6790\n",
      "Epoch 612/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8727 - acc: 0.6660 - val_loss: 0.8706 - val_acc: 0.6720\n",
      "Epoch 613/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8720 - acc: 0.6705 - val_loss: 0.8851 - val_acc: 0.6680\n",
      "Epoch 614/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8735 - acc: 0.6678 - val_loss: 0.8775 - val_acc: 0.6760\n",
      "Epoch 615/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8751 - acc: 0.6648 - val_loss: 0.8684 - val_acc: 0.6770\n",
      "Epoch 616/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8745 - acc: 0.6626 - val_loss: 0.8712 - val_acc: 0.6710\n",
      "Epoch 617/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8702 - acc: 0.6661 - val_loss: 0.8695 - val_acc: 0.6720\n",
      "Epoch 618/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8728 - acc: 0.6654 - val_loss: 0.8722 - val_acc: 0.6760\n",
      "Epoch 619/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8819 - acc: 0.6619 - val_loss: 0.8746 - val_acc: 0.6770\n",
      "Epoch 620/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8741 - acc: 0.6646 - val_loss: 0.8702 - val_acc: 0.6750\n",
      "Epoch 621/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8706 - acc: 0.6651 - val_loss: 0.8628 - val_acc: 0.6770\n",
      "Epoch 622/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8734 - acc: 0.6674 - val_loss: 0.8700 - val_acc: 0.6740\n",
      "Epoch 623/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8712 - acc: 0.6646 - val_loss: 0.8630 - val_acc: 0.6700\n",
      "Epoch 624/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8720 - acc: 0.6662 - val_loss: 0.8695 - val_acc: 0.6800\n",
      "Epoch 625/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8707 - acc: 0.6672 - val_loss: 0.8661 - val_acc: 0.6810\n",
      "Epoch 626/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8758 - acc: 0.6619 - val_loss: 0.8678 - val_acc: 0.6810\n",
      "Epoch 627/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8724 - acc: 0.6662 - val_loss: 0.8703 - val_acc: 0.6710\n",
      "Epoch 628/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8728 - acc: 0.6665 - val_loss: 0.8669 - val_acc: 0.6770\n",
      "Epoch 629/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8735 - acc: 0.6660 - val_loss: 0.8670 - val_acc: 0.6710\n",
      "Epoch 630/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8705 - acc: 0.6665 - val_loss: 0.8638 - val_acc: 0.6710\n",
      "Epoch 631/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8708 - acc: 0.6657 - val_loss: 0.8695 - val_acc: 0.6720\n",
      "Epoch 632/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8742 - acc: 0.6656 - val_loss: 0.8651 - val_acc: 0.6740\n",
      "Epoch 633/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8728 - acc: 0.6635 - val_loss: 0.8706 - val_acc: 0.6710\n",
      "Epoch 634/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8692 - acc: 0.6671 - val_loss: 0.8629 - val_acc: 0.6770\n",
      "Epoch 635/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8717 - acc: 0.6691 - val_loss: 0.8680 - val_acc: 0.6670\n",
      "Epoch 636/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8713 - acc: 0.6674 - val_loss: 0.8636 - val_acc: 0.6750\n",
      "Epoch 637/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8724 - acc: 0.6647 - val_loss: 0.8645 - val_acc: 0.6760\n",
      "Epoch 638/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8701 - acc: 0.6637 - val_loss: 0.8641 - val_acc: 0.6770\n",
      "Epoch 639/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8702 - acc: 0.6662 - val_loss: 0.8684 - val_acc: 0.6660\n",
      "Epoch 640/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8747 - acc: 0.6608 - val_loss: 0.8642 - val_acc: 0.6730\n",
      "Epoch 641/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8700 - acc: 0.6697 - val_loss: 0.8724 - val_acc: 0.6670\n",
      "Epoch 642/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8731 - acc: 0.6679 - val_loss: 0.8684 - val_acc: 0.6810\n",
      "Epoch 643/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8708 - acc: 0.6643 - val_loss: 0.8674 - val_acc: 0.6670\n",
      "Epoch 644/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8702 - acc: 0.6662 - val_loss: 0.8662 - val_acc: 0.6780\n",
      "Epoch 645/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8689 - acc: 0.6678 - val_loss: 0.8680 - val_acc: 0.6720\n",
      "Epoch 646/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8703 - acc: 0.6669 - val_loss: 0.8727 - val_acc: 0.6690\n",
      "Epoch 647/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8760 - acc: 0.6667 - val_loss: 0.8670 - val_acc: 0.6670\n",
      "Epoch 648/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8709 - acc: 0.6675 - val_loss: 0.8642 - val_acc: 0.6760\n",
      "Epoch 649/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8705 - acc: 0.6668 - val_loss: 0.8793 - val_acc: 0.6710\n",
      "Epoch 650/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8739 - acc: 0.6638 - val_loss: 0.8771 - val_acc: 0.6720\n",
      "Epoch 651/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8721 - acc: 0.6676 - val_loss: 0.8635 - val_acc: 0.6770\n",
      "Epoch 652/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8687 - acc: 0.6687 - val_loss: 0.8716 - val_acc: 0.6670\n",
      "Epoch 653/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8699 - acc: 0.6672 - val_loss: 0.8608 - val_acc: 0.6780\n",
      "Epoch 654/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8703 - acc: 0.6650 - val_loss: 0.8666 - val_acc: 0.6760\n",
      "Epoch 655/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8693 - acc: 0.6658 - val_loss: 0.8740 - val_acc: 0.6720\n",
      "Epoch 656/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8700 - acc: 0.6711 - val_loss: 0.8682 - val_acc: 0.6740\n",
      "Epoch 657/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8701 - acc: 0.6652 - val_loss: 0.8678 - val_acc: 0.6830\n",
      "Epoch 658/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8727 - acc: 0.6660 - val_loss: 0.8763 - val_acc: 0.6660\n",
      "Epoch 659/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8717 - acc: 0.6691 - val_loss: 0.8661 - val_acc: 0.6780\n",
      "Epoch 660/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8685 - acc: 0.6692 - val_loss: 0.8634 - val_acc: 0.6750\n",
      "Epoch 661/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8673 - acc: 0.6704 - val_loss: 0.8673 - val_acc: 0.6770\n",
      "Epoch 662/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8711 - acc: 0.6658 - val_loss: 0.8735 - val_acc: 0.6730\n",
      "Epoch 663/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8747 - acc: 0.6649 - val_loss: 0.8774 - val_acc: 0.6690\n",
      "Epoch 664/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8724 - acc: 0.6626 - val_loss: 0.8628 - val_acc: 0.6770\n",
      "Epoch 665/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8678 - acc: 0.6682 - val_loss: 0.8630 - val_acc: 0.6800\n",
      "Epoch 666/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8677 - acc: 0.6682 - val_loss: 0.8632 - val_acc: 0.6700\n",
      "Epoch 667/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8674 - acc: 0.6675 - val_loss: 0.8614 - val_acc: 0.6690\n",
      "Epoch 668/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8703 - acc: 0.6657 - val_loss: 0.8640 - val_acc: 0.6760\n",
      "Epoch 669/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8684 - acc: 0.6671 - val_loss: 0.8720 - val_acc: 0.6610\n",
      "Epoch 670/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8716 - acc: 0.6610 - val_loss: 0.8902 - val_acc: 0.6650\n",
      "Epoch 671/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8728 - acc: 0.6600 - val_loss: 0.8657 - val_acc: 0.6770\n",
      "Epoch 672/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8668 - acc: 0.6696 - val_loss: 0.8745 - val_acc: 0.6620\n",
      "Epoch 673/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8695 - acc: 0.6661 - val_loss: 0.8619 - val_acc: 0.6820\n",
      "Epoch 674/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8670 - acc: 0.6660 - val_loss: 0.8628 - val_acc: 0.6770\n",
      "Epoch 675/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8696 - acc: 0.6653 - val_loss: 0.8626 - val_acc: 0.6830\n",
      "Epoch 676/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8665 - acc: 0.6682 - val_loss: 0.8646 - val_acc: 0.6690\n",
      "Epoch 677/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8669 - acc: 0.6688 - val_loss: 0.8619 - val_acc: 0.6740\n",
      "Epoch 678/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8651 - acc: 0.6693 - val_loss: 0.8653 - val_acc: 0.6720\n",
      "Epoch 679/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8664 - acc: 0.6683 - val_loss: 0.8669 - val_acc: 0.6660\n",
      "Epoch 680/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8681 - acc: 0.6692 - val_loss: 0.8594 - val_acc: 0.6750\n",
      "Epoch 681/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8677 - acc: 0.6668 - val_loss: 0.8621 - val_acc: 0.6790\n",
      "Epoch 682/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8681 - acc: 0.6684 - val_loss: 0.8663 - val_acc: 0.6730\n",
      "Epoch 683/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8714 - acc: 0.6633 - val_loss: 0.8685 - val_acc: 0.6700\n",
      "Epoch 684/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8733 - acc: 0.6617 - val_loss: 0.8630 - val_acc: 0.6710\n",
      "Epoch 685/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8710 - acc: 0.6669 - val_loss: 0.8708 - val_acc: 0.6670\n",
      "Epoch 686/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8720 - acc: 0.6647 - val_loss: 0.8648 - val_acc: 0.6770\n",
      "Epoch 687/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8681 - acc: 0.6646 - val_loss: 0.8616 - val_acc: 0.6770\n",
      "Epoch 688/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8667 - acc: 0.6687 - val_loss: 0.8588 - val_acc: 0.6750\n",
      "Epoch 689/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8685 - acc: 0.6683 - val_loss: 0.8600 - val_acc: 0.6720\n",
      "Epoch 690/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8651 - acc: 0.6683 - val_loss: 0.8694 - val_acc: 0.6630\n",
      "Epoch 691/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8667 - acc: 0.6696 - val_loss: 0.8648 - val_acc: 0.6780\n",
      "Epoch 692/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8680 - acc: 0.6682 - val_loss: 0.8766 - val_acc: 0.6710\n",
      "Epoch 693/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8714 - acc: 0.6671 - val_loss: 0.8730 - val_acc: 0.6650\n",
      "Epoch 694/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8722 - acc: 0.6617 - val_loss: 0.8676 - val_acc: 0.6720\n",
      "Epoch 695/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8691 - acc: 0.6651 - val_loss: 0.8647 - val_acc: 0.6720\n",
      "Epoch 696/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8669 - acc: 0.6659 - val_loss: 0.8630 - val_acc: 0.6750\n",
      "Epoch 697/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8672 - acc: 0.6658 - val_loss: 0.8783 - val_acc: 0.6690\n",
      "Epoch 698/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8728 - acc: 0.6641 - val_loss: 0.8591 - val_acc: 0.6740\n",
      "Epoch 699/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8667 - acc: 0.6678 - val_loss: 0.8631 - val_acc: 0.6750\n",
      "Epoch 700/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8676 - acc: 0.6678 - val_loss: 0.8645 - val_acc: 0.6760\n",
      "Epoch 701/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8663 - acc: 0.6692 - val_loss: 0.8614 - val_acc: 0.6760\n",
      "Epoch 702/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8662 - acc: 0.6690 - val_loss: 0.8737 - val_acc: 0.6620\n",
      "Epoch 703/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8683 - acc: 0.6672 - val_loss: 0.8678 - val_acc: 0.6720\n",
      "Epoch 704/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8673 - acc: 0.6677 - val_loss: 0.8716 - val_acc: 0.6640\n",
      "Epoch 705/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8713 - acc: 0.6645 - val_loss: 0.8594 - val_acc: 0.6740\n",
      "Epoch 706/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8654 - acc: 0.6687 - val_loss: 0.8665 - val_acc: 0.6730\n",
      "Epoch 707/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8687 - acc: 0.6641 - val_loss: 0.8639 - val_acc: 0.6790\n",
      "Epoch 708/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8671 - acc: 0.6687 - val_loss: 0.8686 - val_acc: 0.6630\n",
      "Epoch 709/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8674 - acc: 0.6691 - val_loss: 0.8605 - val_acc: 0.6790\n",
      "Epoch 710/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8663 - acc: 0.6696 - val_loss: 0.8624 - val_acc: 0.6720\n",
      "Epoch 711/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8638 - acc: 0.6678 - val_loss: 0.8641 - val_acc: 0.6900\n",
      "Epoch 712/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8662 - acc: 0.6669 - val_loss: 0.8613 - val_acc: 0.6750\n",
      "Epoch 713/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8668 - acc: 0.6695 - val_loss: 0.8646 - val_acc: 0.6760\n",
      "Epoch 714/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8668 - acc: 0.6645 - val_loss: 0.8633 - val_acc: 0.6730\n",
      "Epoch 715/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8635 - acc: 0.6720 - val_loss: 0.8614 - val_acc: 0.6760\n",
      "Epoch 716/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8641 - acc: 0.6710 - val_loss: 0.8629 - val_acc: 0.6730\n",
      "Epoch 717/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8695 - acc: 0.6649 - val_loss: 0.8666 - val_acc: 0.6690\n",
      "Epoch 718/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8644 - acc: 0.6697 - val_loss: 0.8769 - val_acc: 0.6770\n",
      "Epoch 719/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8694 - acc: 0.6658 - val_loss: 0.8619 - val_acc: 0.6760\n",
      "Epoch 720/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8664 - acc: 0.6701 - val_loss: 0.8637 - val_acc: 0.6590\n",
      "Epoch 721/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8672 - acc: 0.6673 - val_loss: 0.8636 - val_acc: 0.6710\n",
      "Epoch 722/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8637 - acc: 0.6695 - val_loss: 0.8586 - val_acc: 0.6820\n",
      "Epoch 723/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8635 - acc: 0.6695 - val_loss: 0.8569 - val_acc: 0.6760\n",
      "Epoch 724/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8645 - acc: 0.6707 - val_loss: 0.8570 - val_acc: 0.6790\n",
      "Epoch 725/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8629 - acc: 0.6705 - val_loss: 0.8622 - val_acc: 0.6790\n",
      "Epoch 726/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8629 - acc: 0.6692 - val_loss: 0.8598 - val_acc: 0.6770\n",
      "Epoch 727/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8652 - acc: 0.6672 - val_loss: 0.8664 - val_acc: 0.6750\n",
      "Epoch 728/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8663 - acc: 0.6662 - val_loss: 0.8647 - val_acc: 0.6690\n",
      "Epoch 729/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8632 - acc: 0.6672 - val_loss: 0.8608 - val_acc: 0.6750\n",
      "Epoch 730/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8635 - acc: 0.6678 - val_loss: 0.8660 - val_acc: 0.6700\n",
      "Epoch 731/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8649 - acc: 0.6690 - val_loss: 0.8581 - val_acc: 0.6770\n",
      "Epoch 732/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8626 - acc: 0.6715 - val_loss: 0.8630 - val_acc: 0.6810\n",
      "Epoch 733/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8622 - acc: 0.6670 - val_loss: 0.8628 - val_acc: 0.6770\n",
      "Epoch 734/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8663 - acc: 0.6684 - val_loss: 0.8648 - val_acc: 0.6710\n",
      "Epoch 735/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8649 - acc: 0.6668 - val_loss: 0.8636 - val_acc: 0.6780\n",
      "Epoch 736/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8664 - acc: 0.6681 - val_loss: 0.8771 - val_acc: 0.6630\n",
      "Epoch 737/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8676 - acc: 0.6657 - val_loss: 0.8621 - val_acc: 0.6770\n",
      "Epoch 738/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8641 - acc: 0.6674 - val_loss: 0.8628 - val_acc: 0.6770\n",
      "Epoch 739/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8622 - acc: 0.6697 - val_loss: 0.8664 - val_acc: 0.6710\n",
      "Epoch 740/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8653 - acc: 0.6688 - val_loss: 0.8708 - val_acc: 0.6710\n",
      "Epoch 741/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8667 - acc: 0.6694 - val_loss: 0.8560 - val_acc: 0.6750\n",
      "Epoch 742/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8630 - acc: 0.6692 - val_loss: 0.8633 - val_acc: 0.6760\n",
      "Epoch 743/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8634 - acc: 0.6713 - val_loss: 0.8581 - val_acc: 0.6720\n",
      "Epoch 744/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8638 - acc: 0.6666 - val_loss: 0.8590 - val_acc: 0.6760\n",
      "Epoch 745/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8652 - acc: 0.6657 - val_loss: 0.8540 - val_acc: 0.6740\n",
      "Epoch 746/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8637 - acc: 0.6669 - val_loss: 0.8602 - val_acc: 0.6810\n",
      "Epoch 747/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8666 - acc: 0.6680 - val_loss: 0.8625 - val_acc: 0.6740\n",
      "Epoch 748/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8643 - acc: 0.6659 - val_loss: 0.8551 - val_acc: 0.6810\n",
      "Epoch 749/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8627 - acc: 0.6703 - val_loss: 0.8547 - val_acc: 0.6730\n",
      "Epoch 750/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8637 - acc: 0.6680 - val_loss: 0.8630 - val_acc: 0.6760\n",
      "Epoch 751/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8639 - acc: 0.6704 - val_loss: 0.8647 - val_acc: 0.6800\n",
      "Epoch 752/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8655 - acc: 0.6672 - val_loss: 0.8660 - val_acc: 0.6710\n",
      "Epoch 753/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8661 - acc: 0.6675 - val_loss: 0.8604 - val_acc: 0.6780\n",
      "Epoch 754/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8651 - acc: 0.6689 - val_loss: 0.8647 - val_acc: 0.6750\n",
      "Epoch 755/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8638 - acc: 0.6677 - val_loss: 0.8564 - val_acc: 0.6780\n",
      "Epoch 756/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8627 - acc: 0.6703 - val_loss: 0.8560 - val_acc: 0.6790\n",
      "Epoch 757/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8618 - acc: 0.6720 - val_loss: 0.8524 - val_acc: 0.6860\n",
      "Epoch 758/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8608 - acc: 0.6692 - val_loss: 0.8584 - val_acc: 0.6770\n",
      "Epoch 759/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8667 - acc: 0.6677 - val_loss: 0.8645 - val_acc: 0.6810\n",
      "Epoch 760/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8646 - acc: 0.6681 - val_loss: 0.8689 - val_acc: 0.6730\n",
      "Epoch 761/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8619 - acc: 0.6709 - val_loss: 0.8550 - val_acc: 0.6770\n",
      "Epoch 762/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8613 - acc: 0.6696 - val_loss: 0.8576 - val_acc: 0.6720\n",
      "Epoch 763/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8621 - acc: 0.6701 - val_loss: 0.8594 - val_acc: 0.6740\n",
      "Epoch 764/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8668 - acc: 0.6669 - val_loss: 0.8663 - val_acc: 0.6730\n",
      "Epoch 765/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8654 - acc: 0.6658 - val_loss: 0.8727 - val_acc: 0.6670\n",
      "Epoch 766/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8639 - acc: 0.6683 - val_loss: 0.8565 - val_acc: 0.6770\n",
      "Epoch 767/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8630 - acc: 0.6654 - val_loss: 0.8602 - val_acc: 0.6750\n",
      "Epoch 768/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8617 - acc: 0.6715 - val_loss: 0.8594 - val_acc: 0.6770\n",
      "Epoch 769/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8620 - acc: 0.6697 - val_loss: 0.8611 - val_acc: 0.6770\n",
      "Epoch 770/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8653 - acc: 0.6638 - val_loss: 0.8577 - val_acc: 0.6740\n",
      "Epoch 771/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8623 - acc: 0.6675 - val_loss: 0.8586 - val_acc: 0.6810\n",
      "Epoch 772/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8595 - acc: 0.6731 - val_loss: 0.8573 - val_acc: 0.6750\n",
      "Epoch 773/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8605 - acc: 0.6729 - val_loss: 0.8647 - val_acc: 0.6800\n",
      "Epoch 774/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8630 - acc: 0.6668 - val_loss: 0.8594 - val_acc: 0.6720\n",
      "Epoch 775/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8657 - acc: 0.6673 - val_loss: 0.8575 - val_acc: 0.6740\n",
      "Epoch 776/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8635 - acc: 0.6653 - val_loss: 0.8608 - val_acc: 0.6760\n",
      "Epoch 777/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8612 - acc: 0.6712 - val_loss: 0.8560 - val_acc: 0.6840\n",
      "Epoch 778/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8603 - acc: 0.6704 - val_loss: 0.8575 - val_acc: 0.6700\n",
      "Epoch 779/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8598 - acc: 0.6704 - val_loss: 0.8550 - val_acc: 0.6800\n",
      "Epoch 780/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8607 - acc: 0.6704 - val_loss: 0.8617 - val_acc: 0.6710\n",
      "Epoch 781/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8644 - acc: 0.6691 - val_loss: 0.8544 - val_acc: 0.6780\n",
      "Epoch 782/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8614 - acc: 0.6693 - val_loss: 0.8587 - val_acc: 0.6690\n",
      "Epoch 783/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8665 - acc: 0.6683 - val_loss: 0.8736 - val_acc: 0.6720\n",
      "Epoch 784/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8616 - acc: 0.6717 - val_loss: 0.8631 - val_acc: 0.6850\n",
      "Epoch 785/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8635 - acc: 0.6671 - val_loss: 0.8561 - val_acc: 0.6790\n",
      "Epoch 786/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8637 - acc: 0.6663 - val_loss: 0.8687 - val_acc: 0.6670\n",
      "Epoch 787/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8651 - acc: 0.6690 - val_loss: 0.8569 - val_acc: 0.6640\n",
      "Epoch 788/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8640 - acc: 0.6651 - val_loss: 0.8568 - val_acc: 0.6740\n",
      "Epoch 789/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8591 - acc: 0.6695 - val_loss: 0.8592 - val_acc: 0.6770\n",
      "Epoch 790/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8641 - acc: 0.6674 - val_loss: 0.8654 - val_acc: 0.6660\n",
      "Epoch 791/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8602 - acc: 0.6701 - val_loss: 0.8644 - val_acc: 0.6750\n",
      "Epoch 792/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8609 - acc: 0.6665 - val_loss: 0.8794 - val_acc: 0.6690\n",
      "Epoch 793/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8615 - acc: 0.6674 - val_loss: 0.8680 - val_acc: 0.6840\n",
      "Epoch 794/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8626 - acc: 0.6717 - val_loss: 0.8570 - val_acc: 0.6800\n",
      "Epoch 795/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8623 - acc: 0.6700 - val_loss: 0.8607 - val_acc: 0.6730\n",
      "Epoch 796/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8637 - acc: 0.6682 - val_loss: 0.8712 - val_acc: 0.6660\n",
      "Epoch 797/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8619 - acc: 0.6684 - val_loss: 0.8535 - val_acc: 0.6740\n",
      "Epoch 798/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8614 - acc: 0.6695 - val_loss: 0.8566 - val_acc: 0.6820\n",
      "Epoch 799/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8585 - acc: 0.6719 - val_loss: 0.8638 - val_acc: 0.6730\n",
      "Epoch 800/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8665 - acc: 0.6683 - val_loss: 0.8600 - val_acc: 0.6760\n",
      "Epoch 801/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8595 - acc: 0.6717 - val_loss: 0.8521 - val_acc: 0.6830\n",
      "Epoch 802/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8587 - acc: 0.6717 - val_loss: 0.8559 - val_acc: 0.6750\n",
      "Epoch 803/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8592 - acc: 0.6689 - val_loss: 0.8605 - val_acc: 0.6740\n",
      "Epoch 804/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8606 - acc: 0.6701 - val_loss: 0.8558 - val_acc: 0.6770\n",
      "Epoch 805/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8613 - acc: 0.6710 - val_loss: 0.8598 - val_acc: 0.6820\n",
      "Epoch 806/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8589 - acc: 0.6752 - val_loss: 0.8633 - val_acc: 0.6770\n",
      "Epoch 807/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8598 - acc: 0.6701 - val_loss: 0.8587 - val_acc: 0.6790\n",
      "Epoch 808/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8599 - acc: 0.6689 - val_loss: 0.8614 - val_acc: 0.6750\n",
      "Epoch 809/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8672 - acc: 0.6659 - val_loss: 0.8578 - val_acc: 0.6750\n",
      "Epoch 810/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8598 - acc: 0.6715 - val_loss: 0.8551 - val_acc: 0.6740\n",
      "Epoch 811/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8585 - acc: 0.6721 - val_loss: 0.8543 - val_acc: 0.6770\n",
      "Epoch 812/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8581 - acc: 0.6716 - val_loss: 0.8643 - val_acc: 0.6760\n",
      "Epoch 813/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8617 - acc: 0.6646 - val_loss: 0.8661 - val_acc: 0.6650\n",
      "Epoch 814/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8598 - acc: 0.6704 - val_loss: 0.8725 - val_acc: 0.6750\n",
      "Epoch 815/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8610 - acc: 0.6699 - val_loss: 0.8647 - val_acc: 0.6700\n",
      "Epoch 816/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8597 - acc: 0.6710 - val_loss: 0.8575 - val_acc: 0.6770\n",
      "Epoch 817/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8596 - acc: 0.6700 - val_loss: 0.8614 - val_acc: 0.6770\n",
      "Epoch 818/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8588 - acc: 0.6693 - val_loss: 0.8581 - val_acc: 0.6780\n",
      "Epoch 819/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8571 - acc: 0.6718 - val_loss: 0.8553 - val_acc: 0.6710\n",
      "Epoch 820/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8569 - acc: 0.6733 - val_loss: 0.8685 - val_acc: 0.6760\n",
      "Epoch 821/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8604 - acc: 0.6705 - val_loss: 0.8590 - val_acc: 0.6780\n",
      "Epoch 822/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8591 - acc: 0.6694 - val_loss: 0.8546 - val_acc: 0.6690\n",
      "Epoch 823/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8580 - acc: 0.6727 - val_loss: 0.8599 - val_acc: 0.6800\n",
      "Epoch 824/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8604 - acc: 0.6707 - val_loss: 0.8605 - val_acc: 0.6770\n",
      "Epoch 825/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8607 - acc: 0.6723 - val_loss: 0.8501 - val_acc: 0.6750\n",
      "Epoch 826/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8591 - acc: 0.6695 - val_loss: 0.8630 - val_acc: 0.6710\n",
      "Epoch 827/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8611 - acc: 0.6690 - val_loss: 0.8579 - val_acc: 0.6740\n",
      "Epoch 828/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8578 - acc: 0.6746 - val_loss: 0.8574 - val_acc: 0.6760\n",
      "Epoch 829/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8605 - acc: 0.6722 - val_loss: 0.8601 - val_acc: 0.6740\n",
      "Epoch 830/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8611 - acc: 0.6695 - val_loss: 0.8563 - val_acc: 0.6700\n",
      "Epoch 831/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8597 - acc: 0.6666 - val_loss: 0.8600 - val_acc: 0.6670\n",
      "Epoch 832/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8605 - acc: 0.6697 - val_loss: 0.8571 - val_acc: 0.6760\n",
      "Epoch 833/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8574 - acc: 0.6705 - val_loss: 0.8582 - val_acc: 0.6720\n",
      "Epoch 834/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8589 - acc: 0.6710 - val_loss: 0.8658 - val_acc: 0.6700\n",
      "Epoch 835/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8598 - acc: 0.6719 - val_loss: 0.8647 - val_acc: 0.6650\n",
      "Epoch 836/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8624 - acc: 0.6688 - val_loss: 0.8551 - val_acc: 0.6740\n",
      "Epoch 837/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8626 - acc: 0.6699 - val_loss: 0.8618 - val_acc: 0.6820\n",
      "Epoch 838/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8608 - acc: 0.6684 - val_loss: 0.8537 - val_acc: 0.6760\n",
      "Epoch 839/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8594 - acc: 0.6669 - val_loss: 0.8574 - val_acc: 0.6740\n",
      "Epoch 840/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8608 - acc: 0.6700 - val_loss: 0.8518 - val_acc: 0.6770\n",
      "Epoch 841/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8562 - acc: 0.6720 - val_loss: 0.8511 - val_acc: 0.6800\n",
      "Epoch 842/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8561 - acc: 0.6740 - val_loss: 0.8597 - val_acc: 0.6750\n",
      "Epoch 843/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8579 - acc: 0.6738 - val_loss: 0.8566 - val_acc: 0.6720\n",
      "Epoch 844/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8574 - acc: 0.6722 - val_loss: 0.8533 - val_acc: 0.6740\n",
      "Epoch 845/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8598 - acc: 0.6696 - val_loss: 0.8612 - val_acc: 0.6770\n",
      "Epoch 846/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8579 - acc: 0.6698 - val_loss: 0.8559 - val_acc: 0.6730\n",
      "Epoch 847/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8547 - acc: 0.6743 - val_loss: 0.8601 - val_acc: 0.6730\n",
      "Epoch 848/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8572 - acc: 0.6715 - val_loss: 0.8561 - val_acc: 0.6690\n",
      "Epoch 849/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8580 - acc: 0.6711 - val_loss: 0.8555 - val_acc: 0.6760\n",
      "Epoch 850/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8562 - acc: 0.6720 - val_loss: 0.8565 - val_acc: 0.6870\n",
      "Epoch 851/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8565 - acc: 0.6694 - val_loss: 0.8496 - val_acc: 0.6750\n",
      "Epoch 852/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8577 - acc: 0.6722 - val_loss: 0.8696 - val_acc: 0.6710\n",
      "Epoch 853/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8587 - acc: 0.6697 - val_loss: 0.8564 - val_acc: 0.6780\n",
      "Epoch 854/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8546 - acc: 0.6765 - val_loss: 0.8565 - val_acc: 0.6800\n",
      "Epoch 855/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8579 - acc: 0.6726 - val_loss: 0.8574 - val_acc: 0.6750\n",
      "Epoch 856/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8630 - acc: 0.6708 - val_loss: 0.8617 - val_acc: 0.6690\n",
      "Epoch 857/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8556 - acc: 0.6703 - val_loss: 0.8602 - val_acc: 0.6780\n",
      "Epoch 858/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8567 - acc: 0.6722 - val_loss: 0.8546 - val_acc: 0.6790\n",
      "Epoch 859/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8592 - acc: 0.6727 - val_loss: 0.8620 - val_acc: 0.6730\n",
      "Epoch 860/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8579 - acc: 0.6712 - val_loss: 0.8652 - val_acc: 0.6700\n",
      "Epoch 861/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8601 - acc: 0.6707 - val_loss: 0.8628 - val_acc: 0.6670\n",
      "Epoch 862/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8602 - acc: 0.6681 - val_loss: 0.8504 - val_acc: 0.6740\n",
      "Epoch 863/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8552 - acc: 0.6706 - val_loss: 0.8567 - val_acc: 0.6710\n",
      "Epoch 864/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8592 - acc: 0.6714 - val_loss: 0.8621 - val_acc: 0.6740\n",
      "Epoch 865/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8574 - acc: 0.6723 - val_loss: 0.8632 - val_acc: 0.6690\n",
      "Epoch 866/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8570 - acc: 0.6701 - val_loss: 0.8643 - val_acc: 0.6700\n",
      "Epoch 867/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8604 - acc: 0.6663 - val_loss: 0.8596 - val_acc: 0.6750\n",
      "Epoch 868/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8544 - acc: 0.6753 - val_loss: 0.8605 - val_acc: 0.6720\n",
      "Epoch 869/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8596 - acc: 0.6677 - val_loss: 0.8606 - val_acc: 0.6650\n",
      "Epoch 870/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8565 - acc: 0.6741 - val_loss: 0.8582 - val_acc: 0.6750\n",
      "Epoch 871/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8549 - acc: 0.6734 - val_loss: 0.8509 - val_acc: 0.6740\n",
      "Epoch 872/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8566 - acc: 0.6722 - val_loss: 0.8590 - val_acc: 0.6700\n",
      "Epoch 873/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8571 - acc: 0.6727 - val_loss: 0.8541 - val_acc: 0.6760\n",
      "Epoch 874/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8556 - acc: 0.6688 - val_loss: 0.8673 - val_acc: 0.6750\n",
      "Epoch 875/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8592 - acc: 0.6722 - val_loss: 0.8543 - val_acc: 0.6730\n",
      "Epoch 876/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8607 - acc: 0.6679 - val_loss: 0.8557 - val_acc: 0.6780\n",
      "Epoch 877/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8586 - acc: 0.6696 - val_loss: 0.8536 - val_acc: 0.6720\n",
      "Epoch 878/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8581 - acc: 0.6706 - val_loss: 0.8662 - val_acc: 0.6690\n",
      "Epoch 879/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8613 - acc: 0.6685 - val_loss: 0.8568 - val_acc: 0.6790\n",
      "Epoch 880/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8557 - acc: 0.6766 - val_loss: 0.8500 - val_acc: 0.6730\n",
      "Epoch 881/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8567 - acc: 0.6704 - val_loss: 0.8556 - val_acc: 0.6740\n",
      "Epoch 882/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8554 - acc: 0.6738 - val_loss: 0.8537 - val_acc: 0.6790\n",
      "Epoch 883/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8600 - acc: 0.6683 - val_loss: 0.8513 - val_acc: 0.6750\n",
      "Epoch 884/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8558 - acc: 0.6731 - val_loss: 0.8535 - val_acc: 0.6750\n",
      "Epoch 885/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8572 - acc: 0.6711 - val_loss: 0.8482 - val_acc: 0.6730\n",
      "Epoch 886/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8538 - acc: 0.6743 - val_loss: 0.8644 - val_acc: 0.6680\n",
      "Epoch 887/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8565 - acc: 0.6727 - val_loss: 0.8511 - val_acc: 0.6790\n",
      "Epoch 888/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8563 - acc: 0.6738 - val_loss: 0.8508 - val_acc: 0.6760\n",
      "Epoch 889/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8559 - acc: 0.6744 - val_loss: 0.8588 - val_acc: 0.6760\n",
      "Epoch 890/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8538 - acc: 0.6745 - val_loss: 0.8561 - val_acc: 0.6690\n",
      "Epoch 891/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8574 - acc: 0.6691 - val_loss: 0.8646 - val_acc: 0.6780\n",
      "Epoch 892/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8572 - acc: 0.6690 - val_loss: 0.8514 - val_acc: 0.6780\n",
      "Epoch 893/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8575 - acc: 0.6710 - val_loss: 0.8535 - val_acc: 0.6760\n",
      "Epoch 894/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8553 - acc: 0.6740 - val_loss: 0.8615 - val_acc: 0.6740\n",
      "Epoch 895/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8555 - acc: 0.6727 - val_loss: 0.8528 - val_acc: 0.6830\n",
      "Epoch 896/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8550 - acc: 0.6723 - val_loss: 0.8712 - val_acc: 0.6720\n",
      "Epoch 897/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8640 - acc: 0.6656 - val_loss: 0.8572 - val_acc: 0.6760\n",
      "Epoch 898/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8552 - acc: 0.6727 - val_loss: 0.8640 - val_acc: 0.6680\n",
      "Epoch 899/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8593 - acc: 0.6681 - val_loss: 0.8665 - val_acc: 0.6750\n",
      "Epoch 900/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8560 - acc: 0.6724 - val_loss: 0.8618 - val_acc: 0.6710\n",
      "Epoch 901/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8541 - acc: 0.6723 - val_loss: 0.8576 - val_acc: 0.6740\n",
      "Epoch 902/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8524 - acc: 0.6742 - val_loss: 0.8589 - val_acc: 0.6680\n",
      "Epoch 903/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8543 - acc: 0.6731 - val_loss: 0.8543 - val_acc: 0.6730\n",
      "Epoch 904/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8569 - acc: 0.6745 - val_loss: 0.8528 - val_acc: 0.6710\n",
      "Epoch 905/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8553 - acc: 0.6733 - val_loss: 0.8597 - val_acc: 0.6770\n",
      "Epoch 906/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8550 - acc: 0.6714 - val_loss: 0.8556 - val_acc: 0.6800\n",
      "Epoch 907/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8540 - acc: 0.6724 - val_loss: 0.8552 - val_acc: 0.6840\n",
      "Epoch 908/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8534 - acc: 0.6749 - val_loss: 0.8548 - val_acc: 0.6750\n",
      "Epoch 909/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8512 - acc: 0.6747 - val_loss: 0.8555 - val_acc: 0.6720\n",
      "Epoch 910/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8550 - acc: 0.6739 - val_loss: 0.8592 - val_acc: 0.6740\n",
      "Epoch 911/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8568 - acc: 0.6706 - val_loss: 0.8706 - val_acc: 0.6670\n",
      "Epoch 912/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8592 - acc: 0.6698 - val_loss: 0.8540 - val_acc: 0.6760\n",
      "Epoch 913/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8542 - acc: 0.6715 - val_loss: 0.8510 - val_acc: 0.6800\n",
      "Epoch 914/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8519 - acc: 0.6777 - val_loss: 0.8486 - val_acc: 0.6760\n",
      "Epoch 915/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8556 - acc: 0.6728 - val_loss: 0.8511 - val_acc: 0.6790\n",
      "Epoch 916/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8539 - acc: 0.6741 - val_loss: 0.8600 - val_acc: 0.6720\n",
      "Epoch 917/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8587 - acc: 0.6697 - val_loss: 0.8608 - val_acc: 0.6700\n",
      "Epoch 918/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8557 - acc: 0.6688 - val_loss: 0.8498 - val_acc: 0.6710\n",
      "Epoch 919/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8520 - acc: 0.6731 - val_loss: 0.8499 - val_acc: 0.6780\n",
      "Epoch 920/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8563 - acc: 0.6702 - val_loss: 0.8572 - val_acc: 0.6750\n",
      "Epoch 921/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8530 - acc: 0.6761 - val_loss: 0.8561 - val_acc: 0.6700\n",
      "Epoch 922/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8552 - acc: 0.6700 - val_loss: 0.8562 - val_acc: 0.6660\n",
      "Epoch 923/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8548 - acc: 0.6731 - val_loss: 0.8619 - val_acc: 0.6830\n",
      "Epoch 924/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8548 - acc: 0.6718 - val_loss: 0.8569 - val_acc: 0.6780\n",
      "Epoch 925/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8515 - acc: 0.6782 - val_loss: 0.8509 - val_acc: 0.6890\n",
      "Epoch 926/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8549 - acc: 0.6749 - val_loss: 0.8525 - val_acc: 0.6680\n",
      "Epoch 927/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8563 - acc: 0.6712 - val_loss: 0.8659 - val_acc: 0.6670\n",
      "Epoch 928/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8595 - acc: 0.6748 - val_loss: 0.8568 - val_acc: 0.6810\n",
      "Epoch 929/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8528 - acc: 0.6732 - val_loss: 0.8562 - val_acc: 0.6720\n",
      "Epoch 930/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8526 - acc: 0.6752 - val_loss: 0.8488 - val_acc: 0.6780\n",
      "Epoch 931/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8510 - acc: 0.6822 - val_loss: 0.8480 - val_acc: 0.6710\n",
      "Epoch 932/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8530 - acc: 0.6741 - val_loss: 0.8590 - val_acc: 0.6740\n",
      "Epoch 933/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8511 - acc: 0.6775 - val_loss: 0.8537 - val_acc: 0.6750\n",
      "Epoch 934/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8514 - acc: 0.6747 - val_loss: 0.8518 - val_acc: 0.6760\n",
      "Epoch 935/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8529 - acc: 0.6761 - val_loss: 0.8748 - val_acc: 0.6710\n",
      "Epoch 936/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8609 - acc: 0.6665 - val_loss: 0.8578 - val_acc: 0.6700\n",
      "Epoch 937/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8527 - acc: 0.6743 - val_loss: 0.8545 - val_acc: 0.6680\n",
      "Epoch 938/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8516 - acc: 0.6745 - val_loss: 0.8521 - val_acc: 0.6740\n",
      "Epoch 939/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8618 - acc: 0.6664 - val_loss: 0.8538 - val_acc: 0.6800\n",
      "Epoch 940/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8517 - acc: 0.6757 - val_loss: 0.8506 - val_acc: 0.6770\n",
      "Epoch 941/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8526 - acc: 0.6750 - val_loss: 0.8492 - val_acc: 0.6700\n",
      "Epoch 942/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8513 - acc: 0.6744 - val_loss: 0.8488 - val_acc: 0.6720\n",
      "Epoch 943/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8522 - acc: 0.6775 - val_loss: 0.8493 - val_acc: 0.6670\n",
      "Epoch 944/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8537 - acc: 0.6727 - val_loss: 0.8460 - val_acc: 0.6840\n",
      "Epoch 945/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8520 - acc: 0.6742 - val_loss: 0.8495 - val_acc: 0.6740\n",
      "Epoch 946/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8517 - acc: 0.6747 - val_loss: 0.8464 - val_acc: 0.6740\n",
      "Epoch 947/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8508 - acc: 0.6733 - val_loss: 0.8566 - val_acc: 0.6790\n",
      "Epoch 948/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8536 - acc: 0.6729 - val_loss: 0.8594 - val_acc: 0.6730\n",
      "Epoch 949/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8516 - acc: 0.6731 - val_loss: 0.8539 - val_acc: 0.6670\n",
      "Epoch 950/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8531 - acc: 0.6722 - val_loss: 0.8551 - val_acc: 0.6740\n",
      "Epoch 951/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8492 - acc: 0.6805 - val_loss: 0.8505 - val_acc: 0.6840\n",
      "Epoch 952/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8517 - acc: 0.6759 - val_loss: 0.8571 - val_acc: 0.6740\n",
      "Epoch 953/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8563 - acc: 0.6704 - val_loss: 0.8680 - val_acc: 0.6630\n",
      "Epoch 954/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8537 - acc: 0.6738 - val_loss: 0.8532 - val_acc: 0.6700\n",
      "Epoch 955/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8540 - acc: 0.6717 - val_loss: 0.8529 - val_acc: 0.6840\n",
      "Epoch 956/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8505 - acc: 0.6779 - val_loss: 0.8487 - val_acc: 0.6830\n",
      "Epoch 957/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8498 - acc: 0.6803 - val_loss: 0.8506 - val_acc: 0.6780\n",
      "Epoch 958/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8508 - acc: 0.6747 - val_loss: 0.8511 - val_acc: 0.6750\n",
      "Epoch 959/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8513 - acc: 0.6763 - val_loss: 0.8482 - val_acc: 0.6810\n",
      "Epoch 960/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8487 - acc: 0.6791 - val_loss: 0.8550 - val_acc: 0.6750\n",
      "Epoch 961/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8541 - acc: 0.6741 - val_loss: 0.8503 - val_acc: 0.6740\n",
      "Epoch 962/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8531 - acc: 0.6736 - val_loss: 0.8562 - val_acc: 0.6660\n",
      "Epoch 963/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8521 - acc: 0.6747 - val_loss: 0.8467 - val_acc: 0.6720\n",
      "Epoch 964/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8504 - acc: 0.6743 - val_loss: 0.8760 - val_acc: 0.6720\n",
      "Epoch 965/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8561 - acc: 0.6726 - val_loss: 0.8542 - val_acc: 0.6620\n",
      "Epoch 966/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8537 - acc: 0.6729 - val_loss: 0.8511 - val_acc: 0.6720\n",
      "Epoch 967/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8510 - acc: 0.6781 - val_loss: 0.8440 - val_acc: 0.6750\n",
      "Epoch 968/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8488 - acc: 0.6782 - val_loss: 0.8498 - val_acc: 0.6780\n",
      "Epoch 969/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8497 - acc: 0.6768 - val_loss: 0.8520 - val_acc: 0.6860\n",
      "Epoch 970/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8525 - acc: 0.6763 - val_loss: 0.8675 - val_acc: 0.6740\n",
      "Epoch 971/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8586 - acc: 0.6649 - val_loss: 0.8544 - val_acc: 0.6740\n",
      "Epoch 972/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8511 - acc: 0.6741 - val_loss: 0.8516 - val_acc: 0.6770\n",
      "Epoch 973/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8529 - acc: 0.6741 - val_loss: 0.8473 - val_acc: 0.6750\n",
      "Epoch 974/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8524 - acc: 0.6752 - val_loss: 0.8558 - val_acc: 0.6850\n",
      "Epoch 975/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8518 - acc: 0.6766 - val_loss: 0.8518 - val_acc: 0.6730\n",
      "Epoch 976/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8493 - acc: 0.6761 - val_loss: 0.8564 - val_acc: 0.6670\n",
      "Epoch 977/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8514 - acc: 0.6751 - val_loss: 0.8576 - val_acc: 0.6860\n",
      "Epoch 978/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8494 - acc: 0.6769 - val_loss: 0.8538 - val_acc: 0.6700\n",
      "Epoch 979/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8487 - acc: 0.6779 - val_loss: 0.8478 - val_acc: 0.6770\n",
      "Epoch 980/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8504 - acc: 0.6743 - val_loss: 0.8514 - val_acc: 0.6820\n",
      "Epoch 981/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8516 - acc: 0.6762 - val_loss: 0.8527 - val_acc: 0.6790\n",
      "Epoch 982/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8480 - acc: 0.6752 - val_loss: 0.8523 - val_acc: 0.6750\n",
      "Epoch 983/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8483 - acc: 0.6778 - val_loss: 0.8522 - val_acc: 0.6890\n",
      "Epoch 984/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8525 - acc: 0.6750 - val_loss: 0.8550 - val_acc: 0.6730\n",
      "Epoch 985/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8509 - acc: 0.6749 - val_loss: 0.8522 - val_acc: 0.6880\n",
      "Epoch 986/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8521 - acc: 0.6719 - val_loss: 0.8508 - val_acc: 0.6650\n",
      "Epoch 987/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8483 - acc: 0.6763 - val_loss: 0.8467 - val_acc: 0.6810\n",
      "Epoch 988/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8474 - acc: 0.6798 - val_loss: 0.8547 - val_acc: 0.6770\n",
      "Epoch 989/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8500 - acc: 0.6761 - val_loss: 0.8748 - val_acc: 0.6690\n",
      "Epoch 990/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8567 - acc: 0.6734 - val_loss: 0.8588 - val_acc: 0.6710\n",
      "Epoch 991/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8483 - acc: 0.6752 - val_loss: 0.8538 - val_acc: 0.6730\n",
      "Epoch 992/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8486 - acc: 0.6768 - val_loss: 0.8614 - val_acc: 0.6720\n",
      "Epoch 993/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8479 - acc: 0.6786 - val_loss: 0.8648 - val_acc: 0.6720\n",
      "Epoch 994/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8527 - acc: 0.6739 - val_loss: 0.8551 - val_acc: 0.6790\n",
      "Epoch 995/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8563 - acc: 0.6731 - val_loss: 0.8539 - val_acc: 0.6790\n",
      "Epoch 996/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8519 - acc: 0.6727 - val_loss: 0.8532 - val_acc: 0.6740\n",
      "Epoch 997/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8536 - acc: 0.6763 - val_loss: 0.8525 - val_acc: 0.6650\n",
      "Epoch 998/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8475 - acc: 0.6775 - val_loss: 0.8559 - val_acc: 0.6790\n",
      "Epoch 999/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8553 - acc: 0.6741 - val_loss: 0.8445 - val_acc: 0.6760\n",
      "Epoch 1000/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8484 - acc: 0.6761 - val_loss: 0.8453 - val_acc: 0.6700\n",
      "Epoch 1001/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8523 - acc: 0.6774 - val_loss: 0.8517 - val_acc: 0.6700\n",
      "Epoch 1002/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8503 - acc: 0.6758 - val_loss: 0.8489 - val_acc: 0.6800\n",
      "Epoch 1003/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8459 - acc: 0.6790 - val_loss: 0.8492 - val_acc: 0.6750\n",
      "Epoch 1004/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8518 - acc: 0.6738 - val_loss: 0.8482 - val_acc: 0.6760\n",
      "Epoch 1005/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8528 - acc: 0.6707 - val_loss: 0.8512 - val_acc: 0.6720\n",
      "Epoch 1006/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8498 - acc: 0.6753 - val_loss: 0.8556 - val_acc: 0.6740\n",
      "Epoch 1007/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8474 - acc: 0.6786 - val_loss: 0.8454 - val_acc: 0.6710\n",
      "Epoch 1008/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8480 - acc: 0.6757 - val_loss: 0.8502 - val_acc: 0.6760\n",
      "Epoch 1009/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8472 - acc: 0.6772 - val_loss: 0.8568 - val_acc: 0.6760\n",
      "Epoch 1010/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8507 - acc: 0.6770 - val_loss: 0.8477 - val_acc: 0.6860\n",
      "Epoch 1011/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8481 - acc: 0.6768 - val_loss: 0.8460 - val_acc: 0.6720\n",
      "Epoch 1012/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8482 - acc: 0.6756 - val_loss: 0.8471 - val_acc: 0.6790\n",
      "Epoch 1013/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8483 - acc: 0.6798 - val_loss: 0.8480 - val_acc: 0.6700\n",
      "Epoch 1014/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8473 - acc: 0.6767 - val_loss: 0.8461 - val_acc: 0.6790\n",
      "Epoch 1015/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8505 - acc: 0.6747 - val_loss: 0.8486 - val_acc: 0.6780\n",
      "Epoch 1016/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8582 - acc: 0.6691 - val_loss: 0.8454 - val_acc: 0.6660\n",
      "Epoch 1017/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8476 - acc: 0.6759 - val_loss: 0.8548 - val_acc: 0.6790\n",
      "Epoch 1018/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8513 - acc: 0.6759 - val_loss: 0.8530 - val_acc: 0.6680\n",
      "Epoch 1019/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8492 - acc: 0.6740 - val_loss: 0.8578 - val_acc: 0.6760\n",
      "Epoch 1020/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8548 - acc: 0.6725 - val_loss: 0.8502 - val_acc: 0.6680\n",
      "Epoch 1021/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8472 - acc: 0.6788 - val_loss: 0.8465 - val_acc: 0.6730\n",
      "Epoch 1022/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8469 - acc: 0.6789 - val_loss: 0.8455 - val_acc: 0.6780\n",
      "Epoch 1023/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8478 - acc: 0.6735 - val_loss: 0.8546 - val_acc: 0.6750\n",
      "Epoch 1024/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8501 - acc: 0.6738 - val_loss: 0.8541 - val_acc: 0.6730\n",
      "Epoch 1025/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8465 - acc: 0.6786 - val_loss: 0.8462 - val_acc: 0.6790\n",
      "Epoch 1026/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8463 - acc: 0.6779 - val_loss: 0.8598 - val_acc: 0.6730\n",
      "Epoch 1027/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8528 - acc: 0.6758 - val_loss: 0.8543 - val_acc: 0.6750\n",
      "Epoch 1028/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8488 - acc: 0.6760 - val_loss: 0.8535 - val_acc: 0.6750\n",
      "Epoch 1029/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8472 - acc: 0.6746 - val_loss: 0.8528 - val_acc: 0.6780\n",
      "Epoch 1030/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8479 - acc: 0.6788 - val_loss: 0.8502 - val_acc: 0.6840\n",
      "Epoch 1031/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8481 - acc: 0.6771 - val_loss: 0.8449 - val_acc: 0.6740\n",
      "Epoch 1032/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8488 - acc: 0.6760 - val_loss: 0.8576 - val_acc: 0.6730\n",
      "Epoch 1033/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8512 - acc: 0.6747 - val_loss: 0.8464 - val_acc: 0.6820\n",
      "Epoch 1034/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8479 - acc: 0.6777 - val_loss: 0.8461 - val_acc: 0.6870\n",
      "Epoch 1035/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8481 - acc: 0.6759 - val_loss: 0.8508 - val_acc: 0.6760\n",
      "Epoch 1036/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8514 - acc: 0.6748 - val_loss: 0.8522 - val_acc: 0.6750\n",
      "Epoch 1037/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8484 - acc: 0.6744 - val_loss: 0.8520 - val_acc: 0.6780\n",
      "Epoch 1038/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8472 - acc: 0.6800 - val_loss: 0.8443 - val_acc: 0.6770\n",
      "Epoch 1039/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8468 - acc: 0.6773 - val_loss: 0.8409 - val_acc: 0.6840\n",
      "Epoch 1040/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8463 - acc: 0.6780 - val_loss: 0.8493 - val_acc: 0.6700\n",
      "Epoch 1041/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8477 - acc: 0.6763 - val_loss: 0.8474 - val_acc: 0.6780\n",
      "Epoch 1042/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8508 - acc: 0.6736 - val_loss: 0.8565 - val_acc: 0.6690\n",
      "Epoch 1043/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8504 - acc: 0.6733 - val_loss: 0.8463 - val_acc: 0.6760\n",
      "Epoch 1044/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8520 - acc: 0.6739 - val_loss: 0.8448 - val_acc: 0.6760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1045/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8511 - acc: 0.6742 - val_loss: 0.8493 - val_acc: 0.6830\n",
      "Epoch 1046/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8460 - acc: 0.6782 - val_loss: 0.8640 - val_acc: 0.6720\n",
      "Epoch 1047/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8550 - acc: 0.6732 - val_loss: 0.8480 - val_acc: 0.6780\n",
      "Epoch 1048/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8538 - acc: 0.6721 - val_loss: 0.8552 - val_acc: 0.6730\n",
      "Epoch 1049/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8534 - acc: 0.6740 - val_loss: 0.8679 - val_acc: 0.6730\n",
      "Epoch 1050/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8504 - acc: 0.6723 - val_loss: 0.8452 - val_acc: 0.6850\n",
      "Epoch 1051/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8486 - acc: 0.6773 - val_loss: 0.8506 - val_acc: 0.6770\n",
      "Epoch 1052/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8454 - acc: 0.6767 - val_loss: 0.8510 - val_acc: 0.6800\n",
      "Epoch 1053/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8463 - acc: 0.6780 - val_loss: 0.8467 - val_acc: 0.6880\n",
      "Epoch 1054/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8497 - acc: 0.6726 - val_loss: 0.8467 - val_acc: 0.6750\n",
      "Epoch 1055/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8476 - acc: 0.6757 - val_loss: 0.8472 - val_acc: 0.6740\n",
      "Epoch 1056/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8497 - acc: 0.6747 - val_loss: 0.8399 - val_acc: 0.6820\n",
      "Epoch 1057/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8456 - acc: 0.6779 - val_loss: 0.8499 - val_acc: 0.6810\n",
      "Epoch 1058/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8468 - acc: 0.6767 - val_loss: 0.8638 - val_acc: 0.6730\n",
      "Epoch 1059/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8506 - acc: 0.6777 - val_loss: 0.8494 - val_acc: 0.6750\n",
      "Epoch 1060/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8497 - acc: 0.6757 - val_loss: 0.8438 - val_acc: 0.6860\n",
      "Epoch 1061/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8454 - acc: 0.6800 - val_loss: 0.8493 - val_acc: 0.6790\n",
      "Epoch 1062/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8456 - acc: 0.6788 - val_loss: 0.8521 - val_acc: 0.6750\n",
      "Epoch 1063/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8485 - acc: 0.6760 - val_loss: 0.8428 - val_acc: 0.6810\n",
      "Epoch 1064/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8462 - acc: 0.6772 - val_loss: 0.8425 - val_acc: 0.6800\n",
      "Epoch 1065/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8492 - acc: 0.6740 - val_loss: 0.8463 - val_acc: 0.6670\n",
      "Epoch 1066/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8478 - acc: 0.6755 - val_loss: 0.8448 - val_acc: 0.6820\n",
      "Epoch 1067/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8471 - acc: 0.6766 - val_loss: 0.8482 - val_acc: 0.6780\n",
      "Epoch 1068/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8449 - acc: 0.6770 - val_loss: 0.8426 - val_acc: 0.6800\n",
      "Epoch 1069/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8445 - acc: 0.6794 - val_loss: 0.8474 - val_acc: 0.6770\n",
      "Epoch 1070/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8440 - acc: 0.6779 - val_loss: 0.8462 - val_acc: 0.6790\n",
      "Epoch 1071/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8465 - acc: 0.6733 - val_loss: 0.8592 - val_acc: 0.6660\n",
      "Epoch 1072/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8489 - acc: 0.6740 - val_loss: 0.8491 - val_acc: 0.6740\n",
      "Epoch 1073/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8506 - acc: 0.6757 - val_loss: 0.8587 - val_acc: 0.6790\n",
      "Epoch 1074/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8544 - acc: 0.6715 - val_loss: 0.8493 - val_acc: 0.6780\n",
      "Epoch 1075/2000\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.8446 - acc: 0.6779 - val_loss: 0.8438 - val_acc: 0.6770\n",
      "Epoch 1076/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8460 - acc: 0.6799 - val_loss: 0.8432 - val_acc: 0.6830\n",
      "Epoch 1077/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8532 - acc: 0.6747 - val_loss: 0.8559 - val_acc: 0.6770\n",
      "Epoch 1078/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8511 - acc: 0.6729 - val_loss: 0.8505 - val_acc: 0.6720\n",
      "Epoch 1079/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8501 - acc: 0.6735 - val_loss: 0.8568 - val_acc: 0.6730\n",
      "Epoch 1080/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8468 - acc: 0.6772 - val_loss: 0.8433 - val_acc: 0.6810\n",
      "Epoch 1081/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8438 - acc: 0.6807 - val_loss: 0.8444 - val_acc: 0.6780\n",
      "Epoch 1082/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8499 - acc: 0.6769 - val_loss: 0.8601 - val_acc: 0.6770\n",
      "Epoch 1083/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8480 - acc: 0.6759 - val_loss: 0.8461 - val_acc: 0.6730\n",
      "Epoch 1084/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8438 - acc: 0.6796 - val_loss: 0.8458 - val_acc: 0.6800\n",
      "Epoch 1085/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8452 - acc: 0.6795 - val_loss: 0.8486 - val_acc: 0.6820\n",
      "Epoch 1086/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8475 - acc: 0.6768 - val_loss: 0.8500 - val_acc: 0.6820\n",
      "Epoch 1087/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8487 - acc: 0.6752 - val_loss: 0.8396 - val_acc: 0.6800\n",
      "Epoch 1088/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8468 - acc: 0.6793 - val_loss: 0.8443 - val_acc: 0.6810\n",
      "Epoch 1089/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8447 - acc: 0.6779 - val_loss: 0.8431 - val_acc: 0.6740\n",
      "Epoch 1090/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8456 - acc: 0.6800 - val_loss: 0.8472 - val_acc: 0.6790\n",
      "Epoch 1091/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8523 - acc: 0.6743 - val_loss: 0.8484 - val_acc: 0.6750\n",
      "Epoch 1092/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8467 - acc: 0.6787 - val_loss: 0.8454 - val_acc: 0.6830\n",
      "Epoch 1093/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8441 - acc: 0.6791 - val_loss: 0.8419 - val_acc: 0.6800\n",
      "Epoch 1094/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8443 - acc: 0.6784 - val_loss: 0.8417 - val_acc: 0.6770\n",
      "Epoch 1095/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8437 - acc: 0.6757 - val_loss: 0.8505 - val_acc: 0.6770\n",
      "Epoch 1096/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8459 - acc: 0.6794 - val_loss: 0.8467 - val_acc: 0.6850\n",
      "Epoch 1097/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8459 - acc: 0.6765 - val_loss: 0.8469 - val_acc: 0.6700\n",
      "Epoch 1098/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8468 - acc: 0.6783 - val_loss: 0.8463 - val_acc: 0.6760\n",
      "Epoch 1099/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8461 - acc: 0.6788 - val_loss: 0.8458 - val_acc: 0.6770\n",
      "Epoch 1100/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8439 - acc: 0.6768 - val_loss: 0.8604 - val_acc: 0.6690\n",
      "Epoch 1101/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8455 - acc: 0.6784 - val_loss: 0.8469 - val_acc: 0.6840\n",
      "Epoch 1102/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8438 - acc: 0.6813 - val_loss: 0.8480 - val_acc: 0.6730\n",
      "Epoch 1103/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8451 - acc: 0.6786 - val_loss: 0.8418 - val_acc: 0.6750\n",
      "Epoch 1104/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8427 - acc: 0.6784 - val_loss: 0.8608 - val_acc: 0.6720\n",
      "Epoch 1105/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8503 - acc: 0.6764 - val_loss: 0.8424 - val_acc: 0.6830\n",
      "Epoch 1106/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8449 - acc: 0.6783 - val_loss: 0.8428 - val_acc: 0.6730\n",
      "Epoch 1107/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8445 - acc: 0.6786 - val_loss: 0.8411 - val_acc: 0.6830\n",
      "Epoch 1108/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8420 - acc: 0.6812 - val_loss: 0.8420 - val_acc: 0.6760\n",
      "Epoch 1109/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8454 - acc: 0.6783 - val_loss: 0.8429 - val_acc: 0.6850\n",
      "Epoch 1110/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8443 - acc: 0.6798 - val_loss: 0.8417 - val_acc: 0.6760\n",
      "Epoch 1111/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8443 - acc: 0.6791 - val_loss: 0.8494 - val_acc: 0.6760\n",
      "Epoch 1112/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8460 - acc: 0.6776 - val_loss: 0.8509 - val_acc: 0.6820\n",
      "Epoch 1113/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8437 - acc: 0.6778 - val_loss: 0.8450 - val_acc: 0.6770\n",
      "Epoch 1114/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8437 - acc: 0.6791 - val_loss: 0.8495 - val_acc: 0.6760\n",
      "Epoch 1115/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8441 - acc: 0.6799 - val_loss: 0.8456 - val_acc: 0.6790\n",
      "Epoch 1116/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8449 - acc: 0.6770 - val_loss: 0.8479 - val_acc: 0.6780\n",
      "Epoch 1117/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8424 - acc: 0.6790 - val_loss: 0.8523 - val_acc: 0.6760\n",
      "Epoch 1118/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8478 - acc: 0.6770 - val_loss: 0.8605 - val_acc: 0.6750\n",
      "Epoch 1119/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8570 - acc: 0.6733 - val_loss: 0.8489 - val_acc: 0.6710\n",
      "Epoch 1120/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8492 - acc: 0.6755 - val_loss: 0.8570 - val_acc: 0.6740\n",
      "Epoch 1121/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8434 - acc: 0.6793 - val_loss: 0.8435 - val_acc: 0.6810\n",
      "Epoch 1122/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8456 - acc: 0.6749 - val_loss: 0.8561 - val_acc: 0.6730\n",
      "Epoch 1123/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8530 - acc: 0.6718 - val_loss: 0.8611 - val_acc: 0.6690\n",
      "Epoch 1124/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8461 - acc: 0.6741 - val_loss: 0.8489 - val_acc: 0.6710\n",
      "Epoch 1125/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8440 - acc: 0.6788 - val_loss: 0.8544 - val_acc: 0.6770\n",
      "Epoch 1126/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8427 - acc: 0.6794 - val_loss: 0.8556 - val_acc: 0.6750\n",
      "Epoch 1127/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8443 - acc: 0.6759 - val_loss: 0.8508 - val_acc: 0.6750\n",
      "Epoch 1128/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8459 - acc: 0.6780 - val_loss: 0.8530 - val_acc: 0.6710\n",
      "Epoch 1129/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8475 - acc: 0.6770 - val_loss: 0.8439 - val_acc: 0.6840\n",
      "Epoch 1130/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8417 - acc: 0.6785 - val_loss: 0.8504 - val_acc: 0.6810\n",
      "Epoch 1131/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8502 - acc: 0.6767 - val_loss: 0.8639 - val_acc: 0.6700\n",
      "Epoch 1132/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8446 - acc: 0.6761 - val_loss: 0.8467 - val_acc: 0.6800\n",
      "Epoch 1133/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8437 - acc: 0.6780 - val_loss: 0.8532 - val_acc: 0.6790\n",
      "Epoch 1134/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8452 - acc: 0.6747 - val_loss: 0.8568 - val_acc: 0.6710\n",
      "Epoch 1135/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8447 - acc: 0.6782 - val_loss: 0.8556 - val_acc: 0.6730\n",
      "Epoch 1136/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8463 - acc: 0.6752 - val_loss: 0.8429 - val_acc: 0.6840\n",
      "Epoch 1137/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8456 - acc: 0.6747 - val_loss: 0.8493 - val_acc: 0.6720\n",
      "Epoch 1138/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8433 - acc: 0.6799 - val_loss: 0.8562 - val_acc: 0.6670\n",
      "Epoch 1139/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8424 - acc: 0.6786 - val_loss: 0.8455 - val_acc: 0.6850\n",
      "Epoch 1140/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8409 - acc: 0.6795 - val_loss: 0.8443 - val_acc: 0.6750\n",
      "Epoch 1141/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8418 - acc: 0.6791 - val_loss: 0.8500 - val_acc: 0.6790\n",
      "Epoch 1142/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8421 - acc: 0.6800 - val_loss: 0.8443 - val_acc: 0.6760\n",
      "Epoch 1143/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8423 - acc: 0.6780 - val_loss: 0.8423 - val_acc: 0.6800\n",
      "Epoch 1144/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8439 - acc: 0.6798 - val_loss: 0.8424 - val_acc: 0.6780\n",
      "Epoch 1145/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8434 - acc: 0.6790 - val_loss: 0.8456 - val_acc: 0.6730\n",
      "Epoch 1146/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8430 - acc: 0.6800 - val_loss: 0.8549 - val_acc: 0.6760\n",
      "Epoch 1147/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8433 - acc: 0.6777 - val_loss: 0.8437 - val_acc: 0.6810\n",
      "Epoch 1148/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8479 - acc: 0.6758 - val_loss: 0.8491 - val_acc: 0.6760\n",
      "Epoch 1149/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8443 - acc: 0.6799 - val_loss: 0.8457 - val_acc: 0.6690\n",
      "Epoch 1150/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8451 - acc: 0.6767 - val_loss: 0.8484 - val_acc: 0.6820\n",
      "Epoch 1151/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8447 - acc: 0.6786 - val_loss: 0.8455 - val_acc: 0.6840\n",
      "Epoch 1152/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8417 - acc: 0.6793 - val_loss: 0.8444 - val_acc: 0.6700\n",
      "Epoch 1153/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8462 - acc: 0.6795 - val_loss: 0.8425 - val_acc: 0.6760\n",
      "Epoch 1154/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8425 - acc: 0.6809 - val_loss: 0.8529 - val_acc: 0.6730\n",
      "Epoch 1155/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8434 - acc: 0.6791 - val_loss: 0.8673 - val_acc: 0.6670\n",
      "Epoch 1156/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8474 - acc: 0.6743 - val_loss: 0.8510 - val_acc: 0.6730\n",
      "Epoch 1157/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8427 - acc: 0.6777 - val_loss: 0.8420 - val_acc: 0.6760\n",
      "Epoch 1158/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8428 - acc: 0.6773 - val_loss: 0.8514 - val_acc: 0.6770\n",
      "Epoch 1159/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8436 - acc: 0.6794 - val_loss: 0.8521 - val_acc: 0.6760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1160/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8498 - acc: 0.6761 - val_loss: 0.8438 - val_acc: 0.6770\n",
      "Epoch 1161/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8432 - acc: 0.6802 - val_loss: 0.8530 - val_acc: 0.6800\n",
      "Epoch 1162/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8417 - acc: 0.6791 - val_loss: 0.8461 - val_acc: 0.6740\n",
      "Epoch 1163/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8412 - acc: 0.6804 - val_loss: 0.8545 - val_acc: 0.6730\n",
      "Epoch 1164/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8482 - acc: 0.6727 - val_loss: 0.8433 - val_acc: 0.6840\n",
      "Epoch 1165/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8456 - acc: 0.6775 - val_loss: 0.8437 - val_acc: 0.6820\n",
      "Epoch 1166/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8408 - acc: 0.6789 - val_loss: 0.8421 - val_acc: 0.6780\n",
      "Epoch 1167/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8431 - acc: 0.6773 - val_loss: 0.8461 - val_acc: 0.6860\n",
      "Epoch 1168/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8410 - acc: 0.6793 - val_loss: 0.8429 - val_acc: 0.6820\n",
      "Epoch 1169/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8416 - acc: 0.6807 - val_loss: 0.8460 - val_acc: 0.6800\n",
      "Epoch 1170/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8386 - acc: 0.6832 - val_loss: 0.8545 - val_acc: 0.6740\n",
      "Epoch 1171/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8430 - acc: 0.6793 - val_loss: 0.8475 - val_acc: 0.6770\n",
      "Epoch 1172/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8421 - acc: 0.6818 - val_loss: 0.8472 - val_acc: 0.6790\n",
      "Epoch 1173/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8427 - acc: 0.6802 - val_loss: 0.8465 - val_acc: 0.6810\n",
      "Epoch 1174/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8496 - acc: 0.6745 - val_loss: 0.8471 - val_acc: 0.6760\n",
      "Epoch 1175/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8431 - acc: 0.6804 - val_loss: 0.8522 - val_acc: 0.6790\n",
      "Epoch 1176/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8417 - acc: 0.6787 - val_loss: 0.8450 - val_acc: 0.6750\n",
      "Epoch 1177/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8520 - acc: 0.6727 - val_loss: 0.8514 - val_acc: 0.6740\n",
      "Epoch 1178/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8439 - acc: 0.6798 - val_loss: 0.8431 - val_acc: 0.6750\n",
      "Epoch 1179/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8413 - acc: 0.6791 - val_loss: 0.8486 - val_acc: 0.6780\n",
      "Epoch 1180/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8409 - acc: 0.6782 - val_loss: 0.8497 - val_acc: 0.6720\n",
      "Epoch 1181/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8433 - acc: 0.6779 - val_loss: 0.8442 - val_acc: 0.6780\n",
      "Epoch 1182/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8418 - acc: 0.6790 - val_loss: 0.8496 - val_acc: 0.6770\n",
      "Epoch 1183/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8414 - acc: 0.6820 - val_loss: 0.8445 - val_acc: 0.6720\n",
      "Epoch 1184/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8428 - acc: 0.6788 - val_loss: 0.8407 - val_acc: 0.6770\n",
      "Epoch 1185/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8400 - acc: 0.6795 - val_loss: 0.8465 - val_acc: 0.6740\n",
      "Epoch 1186/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8418 - acc: 0.6793 - val_loss: 0.8444 - val_acc: 0.6860\n",
      "Epoch 1187/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8424 - acc: 0.6792 - val_loss: 0.8428 - val_acc: 0.6760\n",
      "Epoch 1188/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8410 - acc: 0.6795 - val_loss: 0.8490 - val_acc: 0.6860\n",
      "Epoch 1189/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8429 - acc: 0.6794 - val_loss: 0.8562 - val_acc: 0.6780\n",
      "Epoch 1190/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8444 - acc: 0.6761 - val_loss: 0.8426 - val_acc: 0.6790\n",
      "Epoch 1191/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8430 - acc: 0.6777 - val_loss: 0.8475 - val_acc: 0.6820\n",
      "Epoch 1192/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8431 - acc: 0.6794 - val_loss: 0.8373 - val_acc: 0.6840\n",
      "Epoch 1193/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8443 - acc: 0.6749 - val_loss: 0.8449 - val_acc: 0.6730\n",
      "Epoch 1194/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8461 - acc: 0.6766 - val_loss: 0.8565 - val_acc: 0.6720\n",
      "Epoch 1195/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8412 - acc: 0.6791 - val_loss: 0.8460 - val_acc: 0.6690\n",
      "Epoch 1196/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8467 - acc: 0.6761 - val_loss: 0.8455 - val_acc: 0.6780\n",
      "Epoch 1197/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8454 - acc: 0.6781 - val_loss: 0.8380 - val_acc: 0.6800\n",
      "Epoch 1198/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8456 - acc: 0.6782 - val_loss: 0.8519 - val_acc: 0.6740\n",
      "Epoch 1199/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8439 - acc: 0.6769 - val_loss: 0.8475 - val_acc: 0.6730\n",
      "Epoch 1200/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8405 - acc: 0.6782 - val_loss: 0.8420 - val_acc: 0.6800\n",
      "Epoch 1201/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8402 - acc: 0.6800 - val_loss: 0.8395 - val_acc: 0.6770\n",
      "Epoch 1202/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8378 - acc: 0.6820 - val_loss: 0.8435 - val_acc: 0.6680\n",
      "Epoch 1203/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8398 - acc: 0.6791 - val_loss: 0.8432 - val_acc: 0.6710\n",
      "Epoch 1204/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8392 - acc: 0.6816 - val_loss: 0.8489 - val_acc: 0.6750\n",
      "Epoch 1205/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8413 - acc: 0.6783 - val_loss: 0.8401 - val_acc: 0.6820\n",
      "Epoch 1206/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8398 - acc: 0.6790 - val_loss: 0.8466 - val_acc: 0.6740\n",
      "Epoch 1207/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8395 - acc: 0.6816 - val_loss: 0.8435 - val_acc: 0.6770\n",
      "Epoch 1208/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8382 - acc: 0.6819 - val_loss: 0.8429 - val_acc: 0.6740\n",
      "Epoch 1209/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8408 - acc: 0.6832 - val_loss: 0.8428 - val_acc: 0.6800\n",
      "Epoch 1210/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8404 - acc: 0.6791 - val_loss: 0.8477 - val_acc: 0.6800\n",
      "Epoch 1211/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8412 - acc: 0.6805 - val_loss: 0.8471 - val_acc: 0.6740\n",
      "Epoch 1212/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8452 - acc: 0.6733 - val_loss: 0.8441 - val_acc: 0.6790\n",
      "Epoch 1213/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8403 - acc: 0.6822 - val_loss: 0.8372 - val_acc: 0.6850\n",
      "Epoch 1214/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8401 - acc: 0.6822 - val_loss: 0.8448 - val_acc: 0.6810\n",
      "Epoch 1215/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8384 - acc: 0.6791 - val_loss: 0.8384 - val_acc: 0.6800\n",
      "Epoch 1216/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8420 - acc: 0.6761 - val_loss: 0.8462 - val_acc: 0.6770\n",
      "Epoch 1217/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8434 - acc: 0.6789 - val_loss: 0.8513 - val_acc: 0.6790\n",
      "Epoch 1218/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8416 - acc: 0.6809 - val_loss: 0.8399 - val_acc: 0.6820\n",
      "Epoch 1219/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8388 - acc: 0.6792 - val_loss: 0.8396 - val_acc: 0.6780\n",
      "Epoch 1220/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8414 - acc: 0.6813 - val_loss: 0.8410 - val_acc: 0.6810\n",
      "Epoch 1221/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8396 - acc: 0.6802 - val_loss: 0.8418 - val_acc: 0.6810\n",
      "Epoch 1222/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8414 - acc: 0.6807 - val_loss: 0.8684 - val_acc: 0.6680\n",
      "Epoch 1223/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8422 - acc: 0.6791 - val_loss: 0.8445 - val_acc: 0.6780\n",
      "Epoch 1224/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8415 - acc: 0.6777 - val_loss: 0.8450 - val_acc: 0.6830\n",
      "Epoch 1225/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8414 - acc: 0.6794 - val_loss: 0.8408 - val_acc: 0.6830\n",
      "Epoch 1226/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8374 - acc: 0.6816 - val_loss: 0.8428 - val_acc: 0.6770\n",
      "Epoch 1227/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8391 - acc: 0.6802 - val_loss: 0.8443 - val_acc: 0.6770\n",
      "Epoch 1228/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8390 - acc: 0.6809 - val_loss: 0.8392 - val_acc: 0.6740\n",
      "Epoch 1229/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8428 - acc: 0.6770 - val_loss: 0.8438 - val_acc: 0.6820\n",
      "Epoch 1230/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8404 - acc: 0.6798 - val_loss: 0.8580 - val_acc: 0.6750\n",
      "Epoch 1231/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8403 - acc: 0.6799 - val_loss: 0.8524 - val_acc: 0.6740\n",
      "Epoch 1232/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8400 - acc: 0.6785 - val_loss: 0.8599 - val_acc: 0.6750\n",
      "Epoch 1233/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8460 - acc: 0.6761 - val_loss: 0.8422 - val_acc: 0.6790\n",
      "Epoch 1234/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8378 - acc: 0.6824 - val_loss: 0.8405 - val_acc: 0.6850\n",
      "Epoch 1235/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8376 - acc: 0.6804 - val_loss: 0.8417 - val_acc: 0.6780\n",
      "Epoch 1236/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8430 - acc: 0.6793 - val_loss: 0.8460 - val_acc: 0.6820\n",
      "Epoch 1237/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8390 - acc: 0.6807 - val_loss: 0.8434 - val_acc: 0.6760\n",
      "Epoch 1238/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8424 - acc: 0.6760 - val_loss: 0.8409 - val_acc: 0.6730\n",
      "Epoch 1239/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8396 - acc: 0.6802 - val_loss: 0.8399 - val_acc: 0.6730\n",
      "Epoch 1240/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8428 - acc: 0.6785 - val_loss: 0.8401 - val_acc: 0.6750\n",
      "Epoch 1241/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8393 - acc: 0.6796 - val_loss: 0.8548 - val_acc: 0.6720\n",
      "Epoch 1242/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8434 - acc: 0.6765 - val_loss: 0.8448 - val_acc: 0.6790\n",
      "Epoch 1243/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8382 - acc: 0.6789 - val_loss: 0.8588 - val_acc: 0.6660\n",
      "Epoch 1244/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8405 - acc: 0.6787 - val_loss: 0.8430 - val_acc: 0.6810\n",
      "Epoch 1245/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8395 - acc: 0.6818 - val_loss: 0.8402 - val_acc: 0.6700\n",
      "Epoch 1246/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8454 - acc: 0.6759 - val_loss: 0.8437 - val_acc: 0.6830\n",
      "Epoch 1247/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8435 - acc: 0.6776 - val_loss: 0.8496 - val_acc: 0.6790\n",
      "Epoch 1248/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8375 - acc: 0.6804 - val_loss: 0.8390 - val_acc: 0.6790\n",
      "Epoch 1249/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8379 - acc: 0.6812 - val_loss: 0.8406 - val_acc: 0.6710\n",
      "Epoch 1250/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8387 - acc: 0.6816 - val_loss: 0.8478 - val_acc: 0.6790\n",
      "Epoch 1251/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8418 - acc: 0.6760 - val_loss: 0.8377 - val_acc: 0.6760\n",
      "Epoch 1252/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8401 - acc: 0.6791 - val_loss: 0.8386 - val_acc: 0.6810\n",
      "Epoch 1253/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8377 - acc: 0.6819 - val_loss: 0.8405 - val_acc: 0.6730\n",
      "Epoch 1254/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8391 - acc: 0.6782 - val_loss: 0.8411 - val_acc: 0.6810\n",
      "Epoch 1255/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8410 - acc: 0.6802 - val_loss: 0.8434 - val_acc: 0.6780\n",
      "Epoch 1256/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8381 - acc: 0.6862 - val_loss: 0.8503 - val_acc: 0.6760\n",
      "Epoch 1257/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8389 - acc: 0.6795 - val_loss: 0.8508 - val_acc: 0.6820\n",
      "Epoch 1258/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8398 - acc: 0.6788 - val_loss: 0.8426 - val_acc: 0.6800\n",
      "Epoch 1259/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8391 - acc: 0.6806 - val_loss: 0.8472 - val_acc: 0.6780\n",
      "Epoch 1260/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8408 - acc: 0.6791 - val_loss: 0.8523 - val_acc: 0.6750\n",
      "Epoch 1261/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8395 - acc: 0.6815 - val_loss: 0.8408 - val_acc: 0.6740\n",
      "Epoch 1262/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8384 - acc: 0.6801 - val_loss: 0.8357 - val_acc: 0.6870\n",
      "Epoch 1263/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8376 - acc: 0.6792 - val_loss: 0.8550 - val_acc: 0.6740\n",
      "Epoch 1264/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8457 - acc: 0.6769 - val_loss: 0.8458 - val_acc: 0.6710\n",
      "Epoch 1265/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8409 - acc: 0.6781 - val_loss: 0.8435 - val_acc: 0.6740\n",
      "Epoch 1266/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8384 - acc: 0.6830 - val_loss: 0.8527 - val_acc: 0.6730\n",
      "Epoch 1267/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8376 - acc: 0.6809 - val_loss: 0.8464 - val_acc: 0.6740\n",
      "Epoch 1268/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8401 - acc: 0.6811 - val_loss: 0.8455 - val_acc: 0.6770\n",
      "Epoch 1269/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8406 - acc: 0.6788 - val_loss: 0.8390 - val_acc: 0.6800\n",
      "Epoch 1270/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8397 - acc: 0.6816 - val_loss: 0.8425 - val_acc: 0.6670\n",
      "Epoch 1271/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8405 - acc: 0.6799 - val_loss: 0.8454 - val_acc: 0.6770\n",
      "Epoch 1272/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8387 - acc: 0.6789 - val_loss: 0.8437 - val_acc: 0.6780\n",
      "Epoch 1273/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8424 - acc: 0.6761 - val_loss: 0.8420 - val_acc: 0.6760\n",
      "Epoch 1274/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8393 - acc: 0.6796 - val_loss: 0.8394 - val_acc: 0.6820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1275/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8372 - acc: 0.6803 - val_loss: 0.8494 - val_acc: 0.6750\n",
      "Epoch 1276/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8413 - acc: 0.6803 - val_loss: 0.8444 - val_acc: 0.6710\n",
      "Epoch 1277/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8362 - acc: 0.6799 - val_loss: 0.8481 - val_acc: 0.6820\n",
      "Epoch 1278/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8392 - acc: 0.6818 - val_loss: 0.8400 - val_acc: 0.6810\n",
      "Epoch 1279/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8355 - acc: 0.6810 - val_loss: 0.8419 - val_acc: 0.6840\n",
      "Epoch 1280/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8391 - acc: 0.6801 - val_loss: 0.8507 - val_acc: 0.6810\n",
      "Epoch 1281/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8391 - acc: 0.6774 - val_loss: 0.8400 - val_acc: 0.6830\n",
      "Epoch 1282/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8385 - acc: 0.6795 - val_loss: 0.8444 - val_acc: 0.6770\n",
      "Epoch 1283/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8372 - acc: 0.6820 - val_loss: 0.8392 - val_acc: 0.6740\n",
      "Epoch 1284/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8399 - acc: 0.6798 - val_loss: 0.8452 - val_acc: 0.6700\n",
      "Epoch 1285/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8423 - acc: 0.6777 - val_loss: 0.8451 - val_acc: 0.6740\n",
      "Epoch 1286/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8394 - acc: 0.6782 - val_loss: 0.8412 - val_acc: 0.6770\n",
      "Epoch 1287/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8370 - acc: 0.6788 - val_loss: 0.8405 - val_acc: 0.6750\n",
      "Epoch 1288/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8364 - acc: 0.6837 - val_loss: 0.8443 - val_acc: 0.6730\n",
      "Epoch 1289/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8418 - acc: 0.6761 - val_loss: 0.8404 - val_acc: 0.6760\n",
      "Epoch 1290/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8436 - acc: 0.6756 - val_loss: 0.8421 - val_acc: 0.6840\n",
      "Epoch 1291/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8410 - acc: 0.6798 - val_loss: 0.8391 - val_acc: 0.6740\n",
      "Epoch 1292/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8375 - acc: 0.6797 - val_loss: 0.8397 - val_acc: 0.6700\n",
      "Epoch 1293/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8368 - acc: 0.6829 - val_loss: 0.8480 - val_acc: 0.6730\n",
      "Epoch 1294/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8382 - acc: 0.6790 - val_loss: 0.8405 - val_acc: 0.6790\n",
      "Epoch 1295/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8387 - acc: 0.6795 - val_loss: 0.8442 - val_acc: 0.6740\n",
      "Epoch 1296/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8368 - acc: 0.6799 - val_loss: 0.8470 - val_acc: 0.6740\n",
      "Epoch 1297/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8389 - acc: 0.6797 - val_loss: 0.8415 - val_acc: 0.6800\n",
      "Epoch 1298/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8389 - acc: 0.6787 - val_loss: 0.8419 - val_acc: 0.6780\n",
      "Epoch 1299/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8356 - acc: 0.6831 - val_loss: 0.8556 - val_acc: 0.6800\n",
      "Epoch 1300/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8401 - acc: 0.6807 - val_loss: 0.8407 - val_acc: 0.6790\n",
      "Epoch 1301/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8384 - acc: 0.6785 - val_loss: 0.8384 - val_acc: 0.6740\n",
      "Epoch 1302/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8375 - acc: 0.6804 - val_loss: 0.8392 - val_acc: 0.6820\n",
      "Epoch 1303/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8357 - acc: 0.6808 - val_loss: 0.8419 - val_acc: 0.6760\n",
      "Epoch 1304/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8359 - acc: 0.6825 - val_loss: 0.8416 - val_acc: 0.6710\n",
      "Epoch 1305/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8370 - acc: 0.6831 - val_loss: 0.8386 - val_acc: 0.6850\n",
      "Epoch 1306/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8360 - acc: 0.6811 - val_loss: 0.8575 - val_acc: 0.6720\n",
      "Epoch 1307/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8416 - acc: 0.6792 - val_loss: 0.8440 - val_acc: 0.6810\n",
      "Epoch 1308/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8397 - acc: 0.6808 - val_loss: 0.8369 - val_acc: 0.6840\n",
      "Epoch 1309/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8383 - acc: 0.6819 - val_loss: 0.8415 - val_acc: 0.6870\n",
      "Epoch 1310/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8402 - acc: 0.6798 - val_loss: 0.8408 - val_acc: 0.6830\n",
      "Epoch 1311/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8386 - acc: 0.6810 - val_loss: 0.8452 - val_acc: 0.6800\n",
      "Epoch 1312/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8371 - acc: 0.6807 - val_loss: 0.8512 - val_acc: 0.6740\n",
      "Epoch 1313/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8470 - acc: 0.6772 - val_loss: 0.8388 - val_acc: 0.6860\n",
      "Epoch 1314/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8450 - acc: 0.6773 - val_loss: 0.8534 - val_acc: 0.6790\n",
      "Epoch 1315/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8381 - acc: 0.6784 - val_loss: 0.8424 - val_acc: 0.6740\n",
      "Epoch 1316/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8365 - acc: 0.6811 - val_loss: 0.8402 - val_acc: 0.6810\n",
      "Epoch 1317/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8357 - acc: 0.6796 - val_loss: 0.8607 - val_acc: 0.6710\n",
      "Epoch 1318/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8378 - acc: 0.6810 - val_loss: 0.8418 - val_acc: 0.6750\n",
      "Epoch 1319/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8359 - acc: 0.6798 - val_loss: 0.8460 - val_acc: 0.6730\n",
      "Epoch 1320/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8375 - acc: 0.6823 - val_loss: 0.8416 - val_acc: 0.6760\n",
      "Epoch 1321/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8364 - acc: 0.6810 - val_loss: 0.8418 - val_acc: 0.6680\n",
      "Epoch 1322/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8423 - acc: 0.6765 - val_loss: 0.8461 - val_acc: 0.6760\n",
      "Epoch 1323/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8350 - acc: 0.6838 - val_loss: 0.8628 - val_acc: 0.6680\n",
      "Epoch 1324/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8407 - acc: 0.6795 - val_loss: 0.8478 - val_acc: 0.6790\n",
      "Epoch 1325/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8388 - acc: 0.6788 - val_loss: 0.8360 - val_acc: 0.6840\n",
      "Epoch 1326/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8398 - acc: 0.6814 - val_loss: 0.8439 - val_acc: 0.6770\n",
      "Epoch 1327/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8408 - acc: 0.6799 - val_loss: 0.8447 - val_acc: 0.6840\n",
      "Epoch 1328/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8371 - acc: 0.6779 - val_loss: 0.8379 - val_acc: 0.6800\n",
      "Epoch 1329/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8351 - acc: 0.6840 - val_loss: 0.8397 - val_acc: 0.6780\n",
      "Epoch 1330/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8377 - acc: 0.6789 - val_loss: 0.8410 - val_acc: 0.6770\n",
      "Epoch 1331/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8361 - acc: 0.6824 - val_loss: 0.8349 - val_acc: 0.6810\n",
      "Epoch 1332/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8351 - acc: 0.6809 - val_loss: 0.8372 - val_acc: 0.6770\n",
      "Epoch 1333/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8349 - acc: 0.6806 - val_loss: 0.8395 - val_acc: 0.6800\n",
      "Epoch 1334/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8351 - acc: 0.6833 - val_loss: 0.8378 - val_acc: 0.6830\n",
      "Epoch 1335/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8365 - acc: 0.6823 - val_loss: 0.8419 - val_acc: 0.6800\n",
      "Epoch 1336/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8390 - acc: 0.6802 - val_loss: 0.8471 - val_acc: 0.6790\n",
      "Epoch 1337/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8376 - acc: 0.6788 - val_loss: 0.8365 - val_acc: 0.6800\n",
      "Epoch 1338/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8375 - acc: 0.6796 - val_loss: 0.8440 - val_acc: 0.6790\n",
      "Epoch 1339/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8346 - acc: 0.6832 - val_loss: 0.8440 - val_acc: 0.6790\n",
      "Epoch 1340/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8384 - acc: 0.6779 - val_loss: 0.8422 - val_acc: 0.6730\n",
      "Epoch 1341/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8356 - acc: 0.6823 - val_loss: 0.8439 - val_acc: 0.6740\n",
      "Epoch 1342/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8363 - acc: 0.6828 - val_loss: 0.8396 - val_acc: 0.6780\n",
      "Epoch 1343/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8419 - acc: 0.6807 - val_loss: 0.8435 - val_acc: 0.6810\n",
      "Epoch 1344/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8345 - acc: 0.6796 - val_loss: 0.8389 - val_acc: 0.6820\n",
      "Epoch 1345/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8341 - acc: 0.6824 - val_loss: 0.8474 - val_acc: 0.6790\n",
      "Epoch 1346/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8404 - acc: 0.6764 - val_loss: 0.8395 - val_acc: 0.6770\n",
      "Epoch 1347/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8349 - acc: 0.6830 - val_loss: 0.8474 - val_acc: 0.6800\n",
      "Epoch 1348/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8381 - acc: 0.6804 - val_loss: 0.8400 - val_acc: 0.6800\n",
      "Epoch 1349/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8364 - acc: 0.6790 - val_loss: 0.8434 - val_acc: 0.6790\n",
      "Epoch 1350/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8363 - acc: 0.6810 - val_loss: 0.8382 - val_acc: 0.6780\n",
      "Epoch 1351/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8333 - acc: 0.6827 - val_loss: 0.8402 - val_acc: 0.6780\n",
      "Epoch 1352/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8357 - acc: 0.6807 - val_loss: 0.8423 - val_acc: 0.6740\n",
      "Epoch 1353/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8342 - acc: 0.6820 - val_loss: 0.8403 - val_acc: 0.6810\n",
      "Epoch 1354/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8371 - acc: 0.6809 - val_loss: 0.8515 - val_acc: 0.6760\n",
      "Epoch 1355/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8370 - acc: 0.6818 - val_loss: 0.8378 - val_acc: 0.6820\n",
      "Epoch 1356/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8329 - acc: 0.6804 - val_loss: 0.8403 - val_acc: 0.6870\n",
      "Epoch 1357/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8353 - acc: 0.6813 - val_loss: 0.8531 - val_acc: 0.6770\n",
      "Epoch 1358/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8374 - acc: 0.6796 - val_loss: 0.8379 - val_acc: 0.6760\n",
      "Epoch 1359/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8347 - acc: 0.6813 - val_loss: 0.8471 - val_acc: 0.6730\n",
      "Epoch 1360/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8350 - acc: 0.6798 - val_loss: 0.8440 - val_acc: 0.6840\n",
      "Epoch 1361/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8359 - acc: 0.6784 - val_loss: 0.8384 - val_acc: 0.6790\n",
      "Epoch 1362/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8327 - acc: 0.6827 - val_loss: 0.8409 - val_acc: 0.6720\n",
      "Epoch 1363/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8377 - acc: 0.6809 - val_loss: 0.8470 - val_acc: 0.6740\n",
      "Epoch 1364/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8437 - acc: 0.6761 - val_loss: 0.8403 - val_acc: 0.6740\n",
      "Epoch 1365/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8356 - acc: 0.6833 - val_loss: 0.8522 - val_acc: 0.6710\n",
      "Epoch 1366/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8340 - acc: 0.6822 - val_loss: 0.8463 - val_acc: 0.6760\n",
      "Epoch 1367/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8351 - acc: 0.6830 - val_loss: 0.8467 - val_acc: 0.6780\n",
      "Epoch 1368/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8369 - acc: 0.6780 - val_loss: 0.8618 - val_acc: 0.6700\n",
      "Epoch 1369/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8384 - acc: 0.6800 - val_loss: 0.8345 - val_acc: 0.6850\n",
      "Epoch 1370/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8360 - acc: 0.6819 - val_loss: 0.8600 - val_acc: 0.6660\n",
      "Epoch 1371/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8409 - acc: 0.6743 - val_loss: 0.8390 - val_acc: 0.6810\n",
      "Epoch 1372/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8360 - acc: 0.6804 - val_loss: 0.8392 - val_acc: 0.6780\n",
      "Epoch 1373/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8363 - acc: 0.6855 - val_loss: 0.8405 - val_acc: 0.6830\n",
      "Epoch 1374/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8382 - acc: 0.6808 - val_loss: 0.8422 - val_acc: 0.6750\n",
      "Epoch 1375/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8385 - acc: 0.6806 - val_loss: 0.8459 - val_acc: 0.6790\n",
      "Epoch 1376/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8370 - acc: 0.6791 - val_loss: 0.8408 - val_acc: 0.6640\n",
      "Epoch 1377/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8374 - acc: 0.6785 - val_loss: 0.8448 - val_acc: 0.6850\n",
      "Epoch 1378/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8399 - acc: 0.6748 - val_loss: 0.8613 - val_acc: 0.6650\n",
      "Epoch 1379/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8389 - acc: 0.6800 - val_loss: 0.8374 - val_acc: 0.6880\n",
      "Epoch 1380/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8346 - acc: 0.6847 - val_loss: 0.8409 - val_acc: 0.6830\n",
      "Epoch 1381/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8340 - acc: 0.6804 - val_loss: 0.8416 - val_acc: 0.6750\n",
      "Epoch 1382/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8347 - acc: 0.6796 - val_loss: 0.8361 - val_acc: 0.6840\n",
      "Epoch 1383/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8355 - acc: 0.6834 - val_loss: 0.8636 - val_acc: 0.6690\n",
      "Epoch 1384/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8453 - acc: 0.6767 - val_loss: 0.8391 - val_acc: 0.6800\n",
      "Epoch 1385/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8324 - acc: 0.6846 - val_loss: 0.8355 - val_acc: 0.6760\n",
      "Epoch 1386/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8342 - acc: 0.6803 - val_loss: 0.8485 - val_acc: 0.6750\n",
      "Epoch 1387/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8345 - acc: 0.6830 - val_loss: 0.8391 - val_acc: 0.6800\n",
      "Epoch 1388/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8328 - acc: 0.6846 - val_loss: 0.8397 - val_acc: 0.6800\n",
      "Epoch 1389/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8377 - acc: 0.6779 - val_loss: 0.8543 - val_acc: 0.6770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1390/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8357 - acc: 0.6808 - val_loss: 0.8411 - val_acc: 0.6760\n",
      "Epoch 1391/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8352 - acc: 0.6825 - val_loss: 0.8363 - val_acc: 0.6850\n",
      "Epoch 1392/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8348 - acc: 0.6810 - val_loss: 0.8481 - val_acc: 0.6770\n",
      "Epoch 1393/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8352 - acc: 0.6819 - val_loss: 0.8400 - val_acc: 0.6760\n",
      "Epoch 1394/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8359 - acc: 0.6828 - val_loss: 0.8455 - val_acc: 0.6790\n",
      "Epoch 1395/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8335 - acc: 0.6814 - val_loss: 0.8406 - val_acc: 0.6800\n",
      "Epoch 1396/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8349 - acc: 0.6790 - val_loss: 0.8419 - val_acc: 0.6810\n",
      "Epoch 1397/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8345 - acc: 0.6842 - val_loss: 0.8356 - val_acc: 0.6790\n",
      "Epoch 1398/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8325 - acc: 0.6811 - val_loss: 0.8544 - val_acc: 0.6740\n",
      "Epoch 1399/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8390 - acc: 0.6809 - val_loss: 0.8375 - val_acc: 0.6820\n",
      "Epoch 1400/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8356 - acc: 0.6782 - val_loss: 0.8358 - val_acc: 0.6790\n",
      "Epoch 1401/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8341 - acc: 0.6827 - val_loss: 0.8407 - val_acc: 0.6750\n",
      "Epoch 1402/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8339 - acc: 0.6825 - val_loss: 0.8374 - val_acc: 0.6810\n",
      "Epoch 1403/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8377 - acc: 0.6798 - val_loss: 0.8508 - val_acc: 0.6760\n",
      "Epoch 1404/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8350 - acc: 0.6813 - val_loss: 0.8392 - val_acc: 0.6810\n",
      "Epoch 1405/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8322 - acc: 0.6842 - val_loss: 0.8419 - val_acc: 0.6830\n",
      "Epoch 1406/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8347 - acc: 0.6790 - val_loss: 0.8398 - val_acc: 0.6780\n",
      "Epoch 1407/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8357 - acc: 0.6822 - val_loss: 0.8407 - val_acc: 0.6820\n",
      "Epoch 1408/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8328 - acc: 0.6803 - val_loss: 0.8378 - val_acc: 0.6810\n",
      "Epoch 1409/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8355 - acc: 0.6793 - val_loss: 0.8480 - val_acc: 0.6780\n",
      "Epoch 1410/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8372 - acc: 0.6790 - val_loss: 0.8477 - val_acc: 0.6760\n",
      "Epoch 1411/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8380 - acc: 0.6787 - val_loss: 0.8379 - val_acc: 0.6850\n",
      "Epoch 1412/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8337 - acc: 0.6818 - val_loss: 0.8408 - val_acc: 0.6810\n",
      "Epoch 1413/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8350 - acc: 0.6846 - val_loss: 0.8417 - val_acc: 0.6810\n",
      "Epoch 1414/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8336 - acc: 0.6827 - val_loss: 0.8419 - val_acc: 0.6780\n",
      "Epoch 1415/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8353 - acc: 0.6836 - val_loss: 0.8427 - val_acc: 0.6800\n",
      "Epoch 1416/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8319 - acc: 0.6871 - val_loss: 0.8360 - val_acc: 0.6850\n",
      "Epoch 1417/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8330 - acc: 0.6833 - val_loss: 0.8387 - val_acc: 0.6800\n",
      "Epoch 1418/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8304 - acc: 0.6848 - val_loss: 0.8474 - val_acc: 0.6670\n",
      "Epoch 1419/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8354 - acc: 0.6829 - val_loss: 0.8464 - val_acc: 0.6770\n",
      "Epoch 1420/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8329 - acc: 0.6823 - val_loss: 0.8421 - val_acc: 0.6780\n",
      "Epoch 1421/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8366 - acc: 0.6800 - val_loss: 0.8418 - val_acc: 0.6690\n",
      "Epoch 1422/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8346 - acc: 0.6826 - val_loss: 0.8421 - val_acc: 0.6800\n",
      "Epoch 1423/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8360 - acc: 0.6809 - val_loss: 0.8427 - val_acc: 0.6720\n",
      "Epoch 1424/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8359 - acc: 0.6818 - val_loss: 0.8344 - val_acc: 0.6860\n",
      "Epoch 1425/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8342 - acc: 0.6816 - val_loss: 0.8426 - val_acc: 0.6780\n",
      "Epoch 1426/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8343 - acc: 0.6808 - val_loss: 0.8521 - val_acc: 0.6580\n",
      "Epoch 1427/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8322 - acc: 0.6816 - val_loss: 0.8373 - val_acc: 0.6810\n",
      "Epoch 1428/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8330 - acc: 0.6832 - val_loss: 0.8326 - val_acc: 0.6830\n",
      "Epoch 1429/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8360 - acc: 0.6792 - val_loss: 0.8448 - val_acc: 0.6670\n",
      "Epoch 1430/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8328 - acc: 0.6839 - val_loss: 0.8404 - val_acc: 0.6840\n",
      "Epoch 1431/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8342 - acc: 0.6859 - val_loss: 0.8329 - val_acc: 0.6800\n",
      "Epoch 1432/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8370 - acc: 0.6770 - val_loss: 0.8428 - val_acc: 0.6830\n",
      "Epoch 1433/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8391 - acc: 0.6783 - val_loss: 0.8464 - val_acc: 0.6770\n",
      "Epoch 1434/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8345 - acc: 0.6804 - val_loss: 0.8394 - val_acc: 0.6880\n",
      "Epoch 1435/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8317 - acc: 0.6825 - val_loss: 0.8368 - val_acc: 0.6790\n",
      "Epoch 1436/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8332 - acc: 0.6807 - val_loss: 0.8449 - val_acc: 0.6720\n",
      "Epoch 1437/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8365 - acc: 0.6770 - val_loss: 0.8416 - val_acc: 0.6770\n",
      "Epoch 1438/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8330 - acc: 0.6823 - val_loss: 0.8334 - val_acc: 0.6760\n",
      "Epoch 1439/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8326 - acc: 0.6832 - val_loss: 0.8376 - val_acc: 0.6770\n",
      "Epoch 1440/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8344 - acc: 0.6809 - val_loss: 0.8374 - val_acc: 0.6810\n",
      "Epoch 1441/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8344 - acc: 0.6811 - val_loss: 0.8392 - val_acc: 0.6820\n",
      "Epoch 1442/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8360 - acc: 0.6787 - val_loss: 0.8395 - val_acc: 0.6770\n",
      "Epoch 1443/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8306 - acc: 0.6832 - val_loss: 0.8350 - val_acc: 0.6790\n",
      "Epoch 1444/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8330 - acc: 0.6864 - val_loss: 0.8404 - val_acc: 0.6820\n",
      "Epoch 1445/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8342 - acc: 0.6830 - val_loss: 0.8422 - val_acc: 0.6820\n",
      "Epoch 1446/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8392 - acc: 0.6802 - val_loss: 0.8379 - val_acc: 0.6800\n",
      "Epoch 1447/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8351 - acc: 0.6803 - val_loss: 0.8415 - val_acc: 0.6760\n",
      "Epoch 1448/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8310 - acc: 0.6835 - val_loss: 0.8554 - val_acc: 0.6620\n",
      "Epoch 1449/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8401 - acc: 0.6810 - val_loss: 0.8419 - val_acc: 0.6820\n",
      "Epoch 1450/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8306 - acc: 0.6837 - val_loss: 0.8438 - val_acc: 0.6790\n",
      "Epoch 1451/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8347 - acc: 0.6833 - val_loss: 0.8625 - val_acc: 0.6690\n",
      "Epoch 1452/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8415 - acc: 0.6784 - val_loss: 0.8374 - val_acc: 0.6830\n",
      "Epoch 1453/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8312 - acc: 0.6826 - val_loss: 0.8421 - val_acc: 0.6780\n",
      "Epoch 1454/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8370 - acc: 0.6778 - val_loss: 0.8383 - val_acc: 0.6810\n",
      "Epoch 1455/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8342 - acc: 0.6830 - val_loss: 0.8385 - val_acc: 0.6780\n",
      "Epoch 1456/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8355 - acc: 0.6855 - val_loss: 0.8462 - val_acc: 0.6740\n",
      "Epoch 1457/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8340 - acc: 0.6802 - val_loss: 0.8388 - val_acc: 0.6810\n",
      "Epoch 1458/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8319 - acc: 0.6803 - val_loss: 0.8347 - val_acc: 0.6830\n",
      "Epoch 1459/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8317 - acc: 0.6825 - val_loss: 0.8377 - val_acc: 0.6760\n",
      "Epoch 1460/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8308 - acc: 0.6841 - val_loss: 0.8444 - val_acc: 0.6720\n",
      "Epoch 1461/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8368 - acc: 0.6804 - val_loss: 0.8479 - val_acc: 0.6760\n",
      "Epoch 1462/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8372 - acc: 0.6778 - val_loss: 0.8457 - val_acc: 0.6780\n",
      "Epoch 1463/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8338 - acc: 0.6807 - val_loss: 0.8337 - val_acc: 0.6820\n",
      "Epoch 1464/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8319 - acc: 0.6830 - val_loss: 0.8439 - val_acc: 0.6740\n",
      "Epoch 1465/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8321 - acc: 0.6812 - val_loss: 0.8412 - val_acc: 0.6760\n",
      "Epoch 1466/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8318 - acc: 0.6818 - val_loss: 0.8481 - val_acc: 0.6760\n",
      "Epoch 1467/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8321 - acc: 0.6842 - val_loss: 0.8325 - val_acc: 0.6780\n",
      "Epoch 1468/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8324 - acc: 0.6812 - val_loss: 0.8355 - val_acc: 0.6830\n",
      "Epoch 1469/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8326 - acc: 0.6837 - val_loss: 0.8389 - val_acc: 0.6740\n",
      "Epoch 1470/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8338 - acc: 0.6816 - val_loss: 0.8454 - val_acc: 0.6750\n",
      "Epoch 1471/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8386 - acc: 0.6826 - val_loss: 0.8416 - val_acc: 0.6760\n",
      "Epoch 1472/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8341 - acc: 0.6813 - val_loss: 0.8427 - val_acc: 0.6850\n",
      "Epoch 1473/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8355 - acc: 0.6792 - val_loss: 0.8434 - val_acc: 0.6730\n",
      "Epoch 1474/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8323 - acc: 0.6822 - val_loss: 0.8394 - val_acc: 0.6730\n",
      "Epoch 1475/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8303 - acc: 0.6819 - val_loss: 0.8312 - val_acc: 0.6850\n",
      "Epoch 1476/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8321 - acc: 0.6800 - val_loss: 0.8452 - val_acc: 0.6760\n",
      "Epoch 1477/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8335 - acc: 0.6843 - val_loss: 0.8394 - val_acc: 0.6760\n",
      "Epoch 1478/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8409 - acc: 0.6766 - val_loss: 0.8524 - val_acc: 0.6740\n",
      "Epoch 1479/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8342 - acc: 0.6800 - val_loss: 0.8449 - val_acc: 0.6750\n",
      "Epoch 1480/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8335 - acc: 0.6825 - val_loss: 0.8393 - val_acc: 0.6800\n",
      "Epoch 1481/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8330 - acc: 0.6813 - val_loss: 0.8515 - val_acc: 0.6640\n",
      "Epoch 1482/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8362 - acc: 0.6782 - val_loss: 0.8372 - val_acc: 0.6770\n",
      "Epoch 1483/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8314 - acc: 0.6845 - val_loss: 0.8349 - val_acc: 0.6830\n",
      "Epoch 1484/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8315 - acc: 0.6807 - val_loss: 0.8346 - val_acc: 0.6820\n",
      "Epoch 1485/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8325 - acc: 0.6816 - val_loss: 0.8405 - val_acc: 0.6820\n",
      "Epoch 1486/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8317 - acc: 0.6825 - val_loss: 0.8394 - val_acc: 0.6670\n",
      "Epoch 1487/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8313 - acc: 0.6818 - val_loss: 0.8294 - val_acc: 0.6830\n",
      "Epoch 1488/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8310 - acc: 0.6833 - val_loss: 0.8444 - val_acc: 0.6870\n",
      "Epoch 1489/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8317 - acc: 0.6820 - val_loss: 0.8364 - val_acc: 0.6810\n",
      "Epoch 1490/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8298 - acc: 0.6834 - val_loss: 0.8330 - val_acc: 0.6810\n",
      "Epoch 1491/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8298 - acc: 0.6823 - val_loss: 0.8389 - val_acc: 0.6750\n",
      "Epoch 1492/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8344 - acc: 0.6810 - val_loss: 0.8410 - val_acc: 0.6760\n",
      "Epoch 1493/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8325 - acc: 0.6827 - val_loss: 0.8396 - val_acc: 0.6800\n",
      "Epoch 1494/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8324 - acc: 0.6830 - val_loss: 0.8368 - val_acc: 0.6800\n",
      "Epoch 1495/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8303 - acc: 0.6877 - val_loss: 0.8346 - val_acc: 0.6730\n",
      "Epoch 1496/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8323 - acc: 0.6831 - val_loss: 0.8498 - val_acc: 0.6590\n",
      "Epoch 1497/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8342 - acc: 0.6821 - val_loss: 0.8438 - val_acc: 0.6720\n",
      "Epoch 1498/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8348 - acc: 0.6798 - val_loss: 0.8583 - val_acc: 0.6710\n",
      "Epoch 1499/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8391 - acc: 0.6795 - val_loss: 0.8399 - val_acc: 0.6840\n",
      "Epoch 1500/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8309 - acc: 0.6822 - val_loss: 0.8365 - val_acc: 0.6770\n",
      "Epoch 1501/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8319 - acc: 0.6841 - val_loss: 0.8422 - val_acc: 0.6760\n",
      "Epoch 1502/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8355 - acc: 0.6779 - val_loss: 0.8438 - val_acc: 0.6770\n",
      "Epoch 1503/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8344 - acc: 0.6774 - val_loss: 0.8344 - val_acc: 0.6810\n",
      "Epoch 1504/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8303 - acc: 0.6836 - val_loss: 0.8379 - val_acc: 0.6740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1505/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8291 - acc: 0.6831 - val_loss: 0.8428 - val_acc: 0.6690\n",
      "Epoch 1506/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8335 - acc: 0.6824 - val_loss: 0.8427 - val_acc: 0.6710\n",
      "Epoch 1507/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8378 - acc: 0.6792 - val_loss: 0.8372 - val_acc: 0.6840\n",
      "Epoch 1508/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8287 - acc: 0.6822 - val_loss: 0.8394 - val_acc: 0.6810\n",
      "Epoch 1509/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8300 - acc: 0.6841 - val_loss: 0.8384 - val_acc: 0.6780\n",
      "Epoch 1510/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8325 - acc: 0.6791 - val_loss: 0.8460 - val_acc: 0.6760\n",
      "Epoch 1511/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8320 - acc: 0.6834 - val_loss: 0.8361 - val_acc: 0.6800\n",
      "Epoch 1512/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8315 - acc: 0.6832 - val_loss: 0.8451 - val_acc: 0.6800\n",
      "Epoch 1513/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8319 - acc: 0.6827 - val_loss: 0.8441 - val_acc: 0.6750\n",
      "Epoch 1514/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8309 - acc: 0.6841 - val_loss: 0.8504 - val_acc: 0.6750\n",
      "Epoch 1515/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8368 - acc: 0.6814 - val_loss: 0.8331 - val_acc: 0.6820\n",
      "Epoch 1516/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8309 - acc: 0.6816 - val_loss: 0.8397 - val_acc: 0.6780\n",
      "Epoch 1517/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8320 - acc: 0.6816 - val_loss: 0.8360 - val_acc: 0.6840\n",
      "Epoch 1518/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8299 - acc: 0.6821 - val_loss: 0.8358 - val_acc: 0.6810\n",
      "Epoch 1519/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8297 - acc: 0.6834 - val_loss: 0.8497 - val_acc: 0.6660\n",
      "Epoch 1520/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8378 - acc: 0.6781 - val_loss: 0.8474 - val_acc: 0.6810\n",
      "Epoch 1521/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8341 - acc: 0.6822 - val_loss: 0.8406 - val_acc: 0.6750\n",
      "Epoch 1522/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8287 - acc: 0.6845 - val_loss: 0.8435 - val_acc: 0.6750\n",
      "Epoch 1523/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8334 - acc: 0.6843 - val_loss: 0.8518 - val_acc: 0.6610\n",
      "Epoch 1524/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8305 - acc: 0.6847 - val_loss: 0.8392 - val_acc: 0.6780\n",
      "Epoch 1525/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8310 - acc: 0.6812 - val_loss: 0.8339 - val_acc: 0.6840\n",
      "Epoch 1526/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8320 - acc: 0.6840 - val_loss: 0.8332 - val_acc: 0.6820\n",
      "Epoch 1527/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8307 - acc: 0.6802 - val_loss: 0.8502 - val_acc: 0.6630\n",
      "Epoch 1528/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8315 - acc: 0.6800 - val_loss: 0.8351 - val_acc: 0.6770\n",
      "Epoch 1529/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8302 - acc: 0.6810 - val_loss: 0.8321 - val_acc: 0.6850\n",
      "Epoch 1530/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8291 - acc: 0.6829 - val_loss: 0.8340 - val_acc: 0.6830\n",
      "Epoch 1531/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8294 - acc: 0.6830 - val_loss: 0.8438 - val_acc: 0.6790\n",
      "Epoch 1532/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8316 - acc: 0.6843 - val_loss: 0.8364 - val_acc: 0.6810\n",
      "Epoch 1533/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8331 - acc: 0.6836 - val_loss: 0.8422 - val_acc: 0.6750\n",
      "Epoch 1534/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8303 - acc: 0.6805 - val_loss: 0.8416 - val_acc: 0.6790\n",
      "Epoch 1535/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8312 - acc: 0.6830 - val_loss: 0.8454 - val_acc: 0.6650\n",
      "Epoch 1536/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8348 - acc: 0.6805 - val_loss: 0.8404 - val_acc: 0.6750\n",
      "Epoch 1537/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8309 - acc: 0.6809 - val_loss: 0.8337 - val_acc: 0.6820\n",
      "Epoch 1538/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8294 - acc: 0.6843 - val_loss: 0.8368 - val_acc: 0.6760\n",
      "Epoch 1539/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8303 - acc: 0.6829 - val_loss: 0.8501 - val_acc: 0.6690\n",
      "Epoch 1540/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8348 - acc: 0.6814 - val_loss: 0.8361 - val_acc: 0.6790\n",
      "Epoch 1541/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8349 - acc: 0.6795 - val_loss: 0.8463 - val_acc: 0.6770\n",
      "Epoch 1542/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8329 - acc: 0.6803 - val_loss: 0.8331 - val_acc: 0.6790\n",
      "Epoch 1543/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8290 - acc: 0.6844 - val_loss: 0.8382 - val_acc: 0.6810\n",
      "Epoch 1544/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8319 - acc: 0.6850 - val_loss: 0.8495 - val_acc: 0.6620\n",
      "Epoch 1545/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8298 - acc: 0.6842 - val_loss: 0.8397 - val_acc: 0.6740\n",
      "Epoch 1546/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8290 - acc: 0.6828 - val_loss: 0.8414 - val_acc: 0.6780\n",
      "Epoch 1547/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8319 - acc: 0.6809 - val_loss: 0.8383 - val_acc: 0.6750\n",
      "Epoch 1548/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8301 - acc: 0.6819 - val_loss: 0.8329 - val_acc: 0.6750\n",
      "Epoch 1549/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8316 - acc: 0.6834 - val_loss: 0.8740 - val_acc: 0.6520\n",
      "Epoch 1550/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8410 - acc: 0.6755 - val_loss: 0.8408 - val_acc: 0.6730\n",
      "Epoch 1551/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8302 - acc: 0.6812 - val_loss: 0.8410 - val_acc: 0.6770\n",
      "Epoch 1552/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8309 - acc: 0.6825 - val_loss: 0.8478 - val_acc: 0.6750\n",
      "Epoch 1553/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8370 - acc: 0.6770 - val_loss: 0.8333 - val_acc: 0.6710\n",
      "Epoch 1554/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8306 - acc: 0.6818 - val_loss: 0.8397 - val_acc: 0.6750\n",
      "Epoch 1555/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8291 - acc: 0.6851 - val_loss: 0.8382 - val_acc: 0.6790\n",
      "Epoch 1556/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8314 - acc: 0.6794 - val_loss: 0.8332 - val_acc: 0.6870\n",
      "Epoch 1557/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8307 - acc: 0.6829 - val_loss: 0.8306 - val_acc: 0.6780\n",
      "Epoch 1558/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8314 - acc: 0.6850 - val_loss: 0.8280 - val_acc: 0.6820\n",
      "Epoch 1559/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8297 - acc: 0.6830 - val_loss: 0.8324 - val_acc: 0.6840\n",
      "Epoch 1560/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8278 - acc: 0.6842 - val_loss: 0.8364 - val_acc: 0.6760\n",
      "Epoch 1561/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8306 - acc: 0.6839 - val_loss: 0.8515 - val_acc: 0.6670\n",
      "Epoch 1562/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8357 - acc: 0.6790 - val_loss: 0.8473 - val_acc: 0.6760\n",
      "Epoch 1563/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8341 - acc: 0.6810 - val_loss: 0.8600 - val_acc: 0.6560\n",
      "Epoch 1564/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8339 - acc: 0.6797 - val_loss: 0.8360 - val_acc: 0.6720\n",
      "Epoch 1565/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8291 - acc: 0.6859 - val_loss: 0.8363 - val_acc: 0.6800\n",
      "Epoch 1566/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8323 - acc: 0.6822 - val_loss: 0.8348 - val_acc: 0.6800\n",
      "Epoch 1567/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8283 - acc: 0.6839 - val_loss: 0.8373 - val_acc: 0.6790\n",
      "Epoch 1568/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8285 - acc: 0.6829 - val_loss: 0.8360 - val_acc: 0.6730\n",
      "Epoch 1569/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8262 - acc: 0.6839 - val_loss: 0.8364 - val_acc: 0.6760\n",
      "Epoch 1570/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8297 - acc: 0.6814 - val_loss: 0.8358 - val_acc: 0.6720\n",
      "Epoch 1571/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8277 - acc: 0.6843 - val_loss: 0.8374 - val_acc: 0.6790\n",
      "Epoch 1572/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8277 - acc: 0.6855 - val_loss: 0.8332 - val_acc: 0.6750\n",
      "Epoch 1573/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8315 - acc: 0.6819 - val_loss: 0.8353 - val_acc: 0.6730\n",
      "Epoch 1574/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8294 - acc: 0.6841 - val_loss: 0.8350 - val_acc: 0.6780\n",
      "Epoch 1575/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8296 - acc: 0.6793 - val_loss: 0.8444 - val_acc: 0.6680\n",
      "Epoch 1576/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8310 - acc: 0.6798 - val_loss: 0.8407 - val_acc: 0.6790\n",
      "Epoch 1577/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8309 - acc: 0.6846 - val_loss: 0.8398 - val_acc: 0.6800\n",
      "Epoch 1578/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8301 - acc: 0.6823 - val_loss: 0.8394 - val_acc: 0.6780\n",
      "Epoch 1579/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8280 - acc: 0.6850 - val_loss: 0.8361 - val_acc: 0.6750\n",
      "Epoch 1580/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8304 - acc: 0.6817 - val_loss: 0.8304 - val_acc: 0.6810\n",
      "Epoch 1581/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8295 - acc: 0.6806 - val_loss: 0.8419 - val_acc: 0.6690\n",
      "Epoch 1582/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8290 - acc: 0.6832 - val_loss: 0.8320 - val_acc: 0.6820\n",
      "Epoch 1583/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8287 - acc: 0.6832 - val_loss: 0.8417 - val_acc: 0.6760\n",
      "Epoch 1584/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8364 - acc: 0.6805 - val_loss: 0.8412 - val_acc: 0.6760\n",
      "Epoch 1585/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8310 - acc: 0.6825 - val_loss: 0.8343 - val_acc: 0.6740\n",
      "Epoch 1586/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8283 - acc: 0.6821 - val_loss: 0.8341 - val_acc: 0.6810\n",
      "Epoch 1587/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8356 - acc: 0.6788 - val_loss: 0.8364 - val_acc: 0.6700\n",
      "Epoch 1588/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8300 - acc: 0.6823 - val_loss: 0.8487 - val_acc: 0.6700\n",
      "Epoch 1589/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8307 - acc: 0.6846 - val_loss: 0.8482 - val_acc: 0.6720\n",
      "Epoch 1590/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8320 - acc: 0.6780 - val_loss: 0.8366 - val_acc: 0.6850\n",
      "Epoch 1591/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8343 - acc: 0.6792 - val_loss: 0.8382 - val_acc: 0.6750\n",
      "Epoch 1592/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8304 - acc: 0.6837 - val_loss: 0.8349 - val_acc: 0.6720\n",
      "Epoch 1593/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8293 - acc: 0.6837 - val_loss: 0.8451 - val_acc: 0.6740\n",
      "Epoch 1594/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8266 - acc: 0.6855 - val_loss: 0.8346 - val_acc: 0.6770\n",
      "Epoch 1595/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8290 - acc: 0.6820 - val_loss: 0.8348 - val_acc: 0.6750\n",
      "Epoch 1596/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8318 - acc: 0.6807 - val_loss: 0.8337 - val_acc: 0.6840\n",
      "Epoch 1597/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8339 - acc: 0.6789 - val_loss: 0.8335 - val_acc: 0.6860\n",
      "Epoch 1598/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8265 - acc: 0.6869 - val_loss: 0.8298 - val_acc: 0.6790\n",
      "Epoch 1599/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8299 - acc: 0.6836 - val_loss: 0.8303 - val_acc: 0.6780\n",
      "Epoch 1600/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8281 - acc: 0.6858 - val_loss: 0.8314 - val_acc: 0.6810\n",
      "Epoch 1601/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8270 - acc: 0.6857 - val_loss: 0.8335 - val_acc: 0.6800\n",
      "Epoch 1602/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8283 - acc: 0.6830 - val_loss: 0.8322 - val_acc: 0.6780\n",
      "Epoch 1603/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8290 - acc: 0.6805 - val_loss: 0.8341 - val_acc: 0.6830\n",
      "Epoch 1604/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8263 - acc: 0.6859 - val_loss: 0.8476 - val_acc: 0.6710\n",
      "Epoch 1605/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8312 - acc: 0.6841 - val_loss: 0.8513 - val_acc: 0.6730\n",
      "Epoch 1606/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8287 - acc: 0.6835 - val_loss: 0.8383 - val_acc: 0.6790\n",
      "Epoch 1607/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8276 - acc: 0.6828 - val_loss: 0.8379 - val_acc: 0.6760\n",
      "Epoch 1608/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8293 - acc: 0.6808 - val_loss: 0.8341 - val_acc: 0.6830\n",
      "Epoch 1609/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8285 - acc: 0.6818 - val_loss: 0.8303 - val_acc: 0.6750\n",
      "Epoch 1610/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8277 - acc: 0.6862 - val_loss: 0.8307 - val_acc: 0.6870\n",
      "Epoch 1611/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8284 - acc: 0.6859 - val_loss: 0.8359 - val_acc: 0.6760\n",
      "Epoch 1612/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8270 - acc: 0.6842 - val_loss: 0.8422 - val_acc: 0.6810\n",
      "Epoch 1613/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8318 - acc: 0.6797 - val_loss: 0.8357 - val_acc: 0.6810\n",
      "Epoch 1614/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8294 - acc: 0.6839 - val_loss: 0.8359 - val_acc: 0.6760\n",
      "Epoch 1615/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8271 - acc: 0.6837 - val_loss: 0.8345 - val_acc: 0.6720\n",
      "Epoch 1616/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8291 - acc: 0.6846 - val_loss: 0.8317 - val_acc: 0.6860\n",
      "Epoch 1617/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8288 - acc: 0.6837 - val_loss: 0.8339 - val_acc: 0.6780\n",
      "Epoch 1618/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8290 - acc: 0.6838 - val_loss: 0.8414 - val_acc: 0.6760\n",
      "Epoch 1619/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8313 - acc: 0.6835 - val_loss: 0.8340 - val_acc: 0.6790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1620/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8291 - acc: 0.6834 - val_loss: 0.8316 - val_acc: 0.6730\n",
      "Epoch 1621/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8291 - acc: 0.6830 - val_loss: 0.8416 - val_acc: 0.6710\n",
      "Epoch 1622/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8291 - acc: 0.6858 - val_loss: 0.8360 - val_acc: 0.6730\n",
      "Epoch 1623/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8293 - acc: 0.6835 - val_loss: 0.8407 - val_acc: 0.6690\n",
      "Epoch 1624/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8310 - acc: 0.6811 - val_loss: 0.8347 - val_acc: 0.6840\n",
      "Epoch 1625/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8282 - acc: 0.6842 - val_loss: 0.8387 - val_acc: 0.6760\n",
      "Epoch 1626/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8291 - acc: 0.6827 - val_loss: 0.8405 - val_acc: 0.6740\n",
      "Epoch 1627/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8272 - acc: 0.6861 - val_loss: 0.8383 - val_acc: 0.6760\n",
      "Epoch 1628/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8277 - acc: 0.6846 - val_loss: 0.8331 - val_acc: 0.6760\n",
      "Epoch 1629/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8272 - acc: 0.6833 - val_loss: 0.8321 - val_acc: 0.6750\n",
      "Epoch 1630/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8270 - acc: 0.6830 - val_loss: 0.8301 - val_acc: 0.6810\n",
      "Epoch 1631/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8309 - acc: 0.6828 - val_loss: 0.8327 - val_acc: 0.6770\n",
      "Epoch 1632/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8277 - acc: 0.6858 - val_loss: 0.8401 - val_acc: 0.6820\n",
      "Epoch 1633/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8294 - acc: 0.6815 - val_loss: 0.8320 - val_acc: 0.6860\n",
      "Epoch 1634/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8296 - acc: 0.6830 - val_loss: 0.8327 - val_acc: 0.6760\n",
      "Epoch 1635/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8257 - acc: 0.6867 - val_loss: 0.8387 - val_acc: 0.6780\n",
      "Epoch 1636/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8255 - acc: 0.6868 - val_loss: 0.8445 - val_acc: 0.6730\n",
      "Epoch 1637/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8282 - acc: 0.6842 - val_loss: 0.8360 - val_acc: 0.6790\n",
      "Epoch 1638/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8277 - acc: 0.6821 - val_loss: 0.8328 - val_acc: 0.6810\n",
      "Epoch 1639/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8255 - acc: 0.6871 - val_loss: 0.8429 - val_acc: 0.6810\n",
      "Epoch 1640/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8332 - acc: 0.6806 - val_loss: 0.8466 - val_acc: 0.6670\n",
      "Epoch 1641/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8369 - acc: 0.6811 - val_loss: 0.8489 - val_acc: 0.6720\n",
      "Epoch 1642/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8324 - acc: 0.6827 - val_loss: 0.8501 - val_acc: 0.6750\n",
      "Epoch 1643/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8313 - acc: 0.6814 - val_loss: 0.8385 - val_acc: 0.6760\n",
      "Epoch 1644/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8290 - acc: 0.6843 - val_loss: 0.8374 - val_acc: 0.6810\n",
      "Epoch 1645/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8274 - acc: 0.6823 - val_loss: 0.8404 - val_acc: 0.6800\n",
      "Epoch 1646/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8259 - acc: 0.6866 - val_loss: 0.8299 - val_acc: 0.6820\n",
      "Epoch 1647/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8315 - acc: 0.6814 - val_loss: 0.8393 - val_acc: 0.6810\n",
      "Epoch 1648/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8276 - acc: 0.6839 - val_loss: 0.8363 - val_acc: 0.6800\n",
      "Epoch 1649/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8291 - acc: 0.6848 - val_loss: 0.8322 - val_acc: 0.6830\n",
      "Epoch 1650/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8286 - acc: 0.6819 - val_loss: 0.8307 - val_acc: 0.6830\n",
      "Epoch 1651/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8269 - acc: 0.6823 - val_loss: 0.8367 - val_acc: 0.6710\n",
      "Epoch 1652/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8250 - acc: 0.6855 - val_loss: 0.8364 - val_acc: 0.6760\n",
      "Epoch 1653/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8275 - acc: 0.6845 - val_loss: 0.8372 - val_acc: 0.6820\n",
      "Epoch 1654/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8290 - acc: 0.6834 - val_loss: 0.8331 - val_acc: 0.6830\n",
      "Epoch 1655/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8291 - acc: 0.6830 - val_loss: 0.8336 - val_acc: 0.6790\n",
      "Epoch 1656/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8268 - acc: 0.6839 - val_loss: 0.8569 - val_acc: 0.6710\n",
      "Epoch 1657/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8334 - acc: 0.6861 - val_loss: 0.8399 - val_acc: 0.6720\n",
      "Epoch 1658/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8333 - acc: 0.6782 - val_loss: 0.8379 - val_acc: 0.6820\n",
      "Epoch 1659/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8313 - acc: 0.6804 - val_loss: 0.8342 - val_acc: 0.6870\n",
      "Epoch 1660/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8314 - acc: 0.6813 - val_loss: 0.8303 - val_acc: 0.6810\n",
      "Epoch 1661/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8291 - acc: 0.6807 - val_loss: 0.8469 - val_acc: 0.6650\n",
      "Epoch 1662/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8289 - acc: 0.6846 - val_loss: 0.8309 - val_acc: 0.6780\n",
      "Epoch 1663/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8296 - acc: 0.6833 - val_loss: 0.8494 - val_acc: 0.6730\n",
      "Epoch 1664/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8257 - acc: 0.6828 - val_loss: 0.8327 - val_acc: 0.6820\n",
      "Epoch 1665/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8261 - acc: 0.6855 - val_loss: 0.8341 - val_acc: 0.6780\n",
      "Epoch 1666/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8283 - acc: 0.6837 - val_loss: 0.8356 - val_acc: 0.6750\n",
      "Epoch 1667/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8290 - acc: 0.6837 - val_loss: 0.8362 - val_acc: 0.6780\n",
      "Epoch 1668/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8327 - acc: 0.6808 - val_loss: 0.8368 - val_acc: 0.6770\n",
      "Epoch 1669/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8320 - acc: 0.6791 - val_loss: 0.8287 - val_acc: 0.6800\n",
      "Epoch 1670/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8274 - acc: 0.6816 - val_loss: 0.8430 - val_acc: 0.6740\n",
      "Epoch 1671/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8275 - acc: 0.6852 - val_loss: 0.8268 - val_acc: 0.6810\n",
      "Epoch 1672/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8254 - acc: 0.6849 - val_loss: 0.8443 - val_acc: 0.6760\n",
      "Epoch 1673/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8284 - acc: 0.6839 - val_loss: 0.8307 - val_acc: 0.6790\n",
      "Epoch 1674/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8285 - acc: 0.6814 - val_loss: 0.8303 - val_acc: 0.6780\n",
      "Epoch 1675/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8292 - acc: 0.6832 - val_loss: 0.8357 - val_acc: 0.6740\n",
      "Epoch 1676/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8299 - acc: 0.6849 - val_loss: 0.8307 - val_acc: 0.6860\n",
      "Epoch 1677/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8262 - acc: 0.6839 - val_loss: 0.8325 - val_acc: 0.6810\n",
      "Epoch 1678/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8256 - acc: 0.6852 - val_loss: 0.8342 - val_acc: 0.6770\n",
      "Epoch 1679/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8268 - acc: 0.6843 - val_loss: 0.8374 - val_acc: 0.6720\n",
      "Epoch 1680/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8281 - acc: 0.6826 - val_loss: 0.8364 - val_acc: 0.6780\n",
      "Epoch 1681/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8272 - acc: 0.6874 - val_loss: 0.8527 - val_acc: 0.6720\n",
      "Epoch 1682/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8352 - acc: 0.6787 - val_loss: 0.8524 - val_acc: 0.6710\n",
      "Epoch 1683/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8330 - acc: 0.6786 - val_loss: 0.8444 - val_acc: 0.6720\n",
      "Epoch 1684/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8308 - acc: 0.6827 - val_loss: 0.8338 - val_acc: 0.6790\n",
      "Epoch 1685/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8249 - acc: 0.6847 - val_loss: 0.8385 - val_acc: 0.6730\n",
      "Epoch 1686/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8280 - acc: 0.6833 - val_loss: 0.8318 - val_acc: 0.6820\n",
      "Epoch 1687/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8255 - acc: 0.6852 - val_loss: 0.8338 - val_acc: 0.6810\n",
      "Epoch 1688/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8268 - acc: 0.6850 - val_loss: 0.8439 - val_acc: 0.6700\n",
      "Epoch 1689/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8272 - acc: 0.6836 - val_loss: 0.8328 - val_acc: 0.6820\n",
      "Epoch 1690/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8262 - acc: 0.6827 - val_loss: 0.8352 - val_acc: 0.6790\n",
      "Epoch 1691/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8275 - acc: 0.6846 - val_loss: 0.8342 - val_acc: 0.6680\n",
      "Epoch 1692/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8268 - acc: 0.6850 - val_loss: 0.8334 - val_acc: 0.6800\n",
      "Epoch 1693/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8279 - acc: 0.6833 - val_loss: 0.8315 - val_acc: 0.6790\n",
      "Epoch 1694/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8248 - acc: 0.6838 - val_loss: 0.8366 - val_acc: 0.6800\n",
      "Epoch 1695/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8317 - acc: 0.6791 - val_loss: 0.8329 - val_acc: 0.6810\n",
      "Epoch 1696/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8301 - acc: 0.6811 - val_loss: 0.8398 - val_acc: 0.6660\n",
      "Epoch 1697/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8279 - acc: 0.6838 - val_loss: 0.8387 - val_acc: 0.6780\n",
      "Epoch 1698/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8248 - acc: 0.6833 - val_loss: 0.8368 - val_acc: 0.6780\n",
      "Epoch 1699/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8251 - acc: 0.6864 - val_loss: 0.8344 - val_acc: 0.6770\n",
      "Epoch 1700/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8254 - acc: 0.6874 - val_loss: 0.8313 - val_acc: 0.6740\n",
      "Epoch 1701/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8277 - acc: 0.6838 - val_loss: 0.8440 - val_acc: 0.6740\n",
      "Epoch 1702/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8298 - acc: 0.6819 - val_loss: 0.8315 - val_acc: 0.6780\n",
      "Epoch 1703/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8278 - acc: 0.6834 - val_loss: 0.8335 - val_acc: 0.6810\n",
      "Epoch 1704/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8255 - acc: 0.6855 - val_loss: 0.8363 - val_acc: 0.6780\n",
      "Epoch 1705/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8304 - acc: 0.6818 - val_loss: 0.8404 - val_acc: 0.6840\n",
      "Epoch 1706/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8346 - acc: 0.6811 - val_loss: 0.8353 - val_acc: 0.6800\n",
      "Epoch 1707/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8290 - acc: 0.6832 - val_loss: 0.8343 - val_acc: 0.6740\n",
      "Epoch 1708/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8311 - acc: 0.6817 - val_loss: 0.8310 - val_acc: 0.6850\n",
      "Epoch 1709/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8316 - acc: 0.6769 - val_loss: 0.8559 - val_acc: 0.6620\n",
      "Epoch 1710/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8293 - acc: 0.6840 - val_loss: 0.8340 - val_acc: 0.6750\n",
      "Epoch 1711/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8292 - acc: 0.6816 - val_loss: 0.8282 - val_acc: 0.6800\n",
      "Epoch 1712/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8232 - acc: 0.6865 - val_loss: 0.8358 - val_acc: 0.6720\n",
      "Epoch 1713/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8240 - acc: 0.6832 - val_loss: 0.8319 - val_acc: 0.6810\n",
      "Epoch 1714/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8293 - acc: 0.6839 - val_loss: 0.8467 - val_acc: 0.6730\n",
      "Epoch 1715/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8256 - acc: 0.6861 - val_loss: 0.8365 - val_acc: 0.6720\n",
      "Epoch 1716/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8268 - acc: 0.6859 - val_loss: 0.8322 - val_acc: 0.6720\n",
      "Epoch 1717/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8261 - acc: 0.6838 - val_loss: 0.8295 - val_acc: 0.6790\n",
      "Epoch 1718/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8251 - acc: 0.6867 - val_loss: 0.8427 - val_acc: 0.6780\n",
      "Epoch 1719/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8269 - acc: 0.6816 - val_loss: 0.8407 - val_acc: 0.6690\n",
      "Epoch 1720/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8275 - acc: 0.6848 - val_loss: 0.8391 - val_acc: 0.6780\n",
      "Epoch 1721/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8251 - acc: 0.6840 - val_loss: 0.8394 - val_acc: 0.6720\n",
      "Epoch 1722/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8361 - acc: 0.6817 - val_loss: 0.8359 - val_acc: 0.6790\n",
      "Epoch 1723/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8331 - acc: 0.6815 - val_loss: 0.8423 - val_acc: 0.6690\n",
      "Epoch 1724/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8325 - acc: 0.6809 - val_loss: 0.8320 - val_acc: 0.6810\n",
      "Epoch 1725/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8278 - acc: 0.6808 - val_loss: 0.8376 - val_acc: 0.6780\n",
      "Epoch 1726/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8250 - acc: 0.6846 - val_loss: 0.8382 - val_acc: 0.6790\n",
      "Epoch 1727/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8260 - acc: 0.6869 - val_loss: 0.8352 - val_acc: 0.6860\n",
      "Epoch 1728/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8272 - acc: 0.6805 - val_loss: 0.8312 - val_acc: 0.6820\n",
      "Epoch 1729/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8257 - acc: 0.6858 - val_loss: 0.8413 - val_acc: 0.6710\n",
      "Epoch 1730/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8235 - acc: 0.6878 - val_loss: 0.8315 - val_acc: 0.6770\n",
      "Epoch 1731/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8253 - acc: 0.6818 - val_loss: 0.8382 - val_acc: 0.6800\n",
      "Epoch 1732/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8291 - acc: 0.6821 - val_loss: 0.8297 - val_acc: 0.6770\n",
      "Epoch 1733/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8270 - acc: 0.6841 - val_loss: 0.8307 - val_acc: 0.6770\n",
      "Epoch 1734/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8271 - acc: 0.6859 - val_loss: 0.8372 - val_acc: 0.6770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1735/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8252 - acc: 0.6831 - val_loss: 0.8516 - val_acc: 0.6680\n",
      "Epoch 1736/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8276 - acc: 0.6819 - val_loss: 0.8459 - val_acc: 0.6740\n",
      "Epoch 1737/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8260 - acc: 0.6839 - val_loss: 0.8384 - val_acc: 0.6820\n",
      "Epoch 1738/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8343 - acc: 0.6792 - val_loss: 0.8425 - val_acc: 0.6740\n",
      "Epoch 1739/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8265 - acc: 0.6834 - val_loss: 0.8384 - val_acc: 0.6870\n",
      "Epoch 1740/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8261 - acc: 0.6844 - val_loss: 0.8339 - val_acc: 0.6820\n",
      "Epoch 1741/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8260 - acc: 0.6812 - val_loss: 0.8452 - val_acc: 0.6750\n",
      "Epoch 1742/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8305 - acc: 0.6829 - val_loss: 0.8342 - val_acc: 0.6830\n",
      "Epoch 1743/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8326 - acc: 0.6802 - val_loss: 0.8549 - val_acc: 0.6660\n",
      "Epoch 1744/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8307 - acc: 0.6810 - val_loss: 0.8365 - val_acc: 0.6740\n",
      "Epoch 1745/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8264 - acc: 0.6825 - val_loss: 0.8272 - val_acc: 0.6840\n",
      "Epoch 1746/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8240 - acc: 0.6843 - val_loss: 0.8408 - val_acc: 0.6720\n",
      "Epoch 1747/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8345 - acc: 0.6781 - val_loss: 0.8338 - val_acc: 0.6730\n",
      "Epoch 1748/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8283 - acc: 0.6830 - val_loss: 0.8346 - val_acc: 0.6790\n",
      "Epoch 1749/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8257 - acc: 0.6828 - val_loss: 0.8265 - val_acc: 0.6820\n",
      "Epoch 1750/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8243 - acc: 0.6845 - val_loss: 0.8387 - val_acc: 0.6770\n",
      "Epoch 1751/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8277 - acc: 0.6820 - val_loss: 0.8347 - val_acc: 0.6820\n",
      "Epoch 1752/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8266 - acc: 0.6814 - val_loss: 0.8358 - val_acc: 0.6780\n",
      "Epoch 1753/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8282 - acc: 0.6832 - val_loss: 0.8311 - val_acc: 0.6780\n",
      "Epoch 1754/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8256 - acc: 0.6837 - val_loss: 0.8383 - val_acc: 0.6760\n",
      "Epoch 1755/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8268 - acc: 0.6841 - val_loss: 0.8332 - val_acc: 0.6760\n",
      "Epoch 1756/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8247 - acc: 0.6834 - val_loss: 0.8527 - val_acc: 0.6720\n",
      "Epoch 1757/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8315 - acc: 0.6815 - val_loss: 0.8437 - val_acc: 0.6710\n",
      "Epoch 1758/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8290 - acc: 0.6784 - val_loss: 0.8435 - val_acc: 0.6790\n",
      "Epoch 1759/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8335 - acc: 0.6803 - val_loss: 0.8290 - val_acc: 0.6850\n",
      "Epoch 1760/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8249 - acc: 0.6834 - val_loss: 0.8270 - val_acc: 0.6860\n",
      "Epoch 1761/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8270 - acc: 0.6819 - val_loss: 0.8467 - val_acc: 0.6650\n",
      "Epoch 1762/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8272 - acc: 0.6855 - val_loss: 0.8369 - val_acc: 0.6810\n",
      "Epoch 1763/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8282 - acc: 0.6820 - val_loss: 0.8385 - val_acc: 0.6730\n",
      "Epoch 1764/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8296 - acc: 0.6821 - val_loss: 0.8388 - val_acc: 0.6780\n",
      "Epoch 1765/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8290 - acc: 0.6830 - val_loss: 0.8312 - val_acc: 0.6760\n",
      "Epoch 1766/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8317 - acc: 0.6815 - val_loss: 0.8298 - val_acc: 0.6840\n",
      "Epoch 1767/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8266 - acc: 0.6830 - val_loss: 0.8409 - val_acc: 0.6830\n",
      "Epoch 1768/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8264 - acc: 0.6853 - val_loss: 0.8335 - val_acc: 0.6780\n",
      "Epoch 1769/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8290 - acc: 0.6784 - val_loss: 0.8287 - val_acc: 0.6830\n",
      "Epoch 1770/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8224 - acc: 0.6865 - val_loss: 0.8334 - val_acc: 0.6780\n",
      "Epoch 1771/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8269 - acc: 0.6837 - val_loss: 0.8425 - val_acc: 0.6680\n",
      "Epoch 1772/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8254 - acc: 0.6834 - val_loss: 0.8352 - val_acc: 0.6750\n",
      "Epoch 1773/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8237 - acc: 0.6873 - val_loss: 0.8338 - val_acc: 0.6790\n",
      "Epoch 1774/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8236 - acc: 0.6871 - val_loss: 0.8349 - val_acc: 0.6750\n",
      "Epoch 1775/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8231 - acc: 0.6864 - val_loss: 0.8349 - val_acc: 0.6750\n",
      "Epoch 1776/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8242 - acc: 0.6837 - val_loss: 0.8271 - val_acc: 0.6810\n",
      "Epoch 1777/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8267 - acc: 0.6847 - val_loss: 0.8269 - val_acc: 0.6800\n",
      "Epoch 1778/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8244 - acc: 0.6866 - val_loss: 0.8333 - val_acc: 0.6800\n",
      "Epoch 1779/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8238 - acc: 0.6862 - val_loss: 0.8330 - val_acc: 0.6760\n",
      "Epoch 1780/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8248 - acc: 0.6848 - val_loss: 0.8289 - val_acc: 0.6770\n",
      "Epoch 1781/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8240 - acc: 0.6835 - val_loss: 0.8483 - val_acc: 0.6700\n",
      "Epoch 1782/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8302 - acc: 0.6841 - val_loss: 0.8387 - val_acc: 0.6800\n",
      "Epoch 1783/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8232 - acc: 0.6846 - val_loss: 0.8269 - val_acc: 0.6850\n",
      "Epoch 1784/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8239 - acc: 0.6861 - val_loss: 0.8385 - val_acc: 0.6790\n",
      "Epoch 1785/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8333 - acc: 0.6772 - val_loss: 0.8326 - val_acc: 0.6800\n",
      "Epoch 1786/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8225 - acc: 0.6874 - val_loss: 0.8323 - val_acc: 0.6740\n",
      "Epoch 1787/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8257 - acc: 0.6825 - val_loss: 0.8341 - val_acc: 0.6780\n",
      "Epoch 1788/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8256 - acc: 0.6840 - val_loss: 0.8330 - val_acc: 0.6720\n",
      "Epoch 1789/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8237 - acc: 0.6866 - val_loss: 0.8314 - val_acc: 0.6770\n",
      "Epoch 1790/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8240 - acc: 0.6852 - val_loss: 0.8325 - val_acc: 0.6780\n",
      "Epoch 1791/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8269 - acc: 0.6800 - val_loss: 0.8292 - val_acc: 0.6790\n",
      "Epoch 1792/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8263 - acc: 0.6816 - val_loss: 0.8333 - val_acc: 0.6750\n",
      "Epoch 1793/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8269 - acc: 0.6848 - val_loss: 0.8492 - val_acc: 0.6760\n",
      "Epoch 1794/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8315 - acc: 0.6784 - val_loss: 0.8337 - val_acc: 0.6740\n",
      "Epoch 1795/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8256 - acc: 0.6817 - val_loss: 0.8267 - val_acc: 0.6810\n",
      "Epoch 1796/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8257 - acc: 0.6841 - val_loss: 0.8344 - val_acc: 0.6810\n",
      "Epoch 1797/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8252 - acc: 0.6864 - val_loss: 0.8424 - val_acc: 0.6740\n",
      "Epoch 1798/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8257 - acc: 0.6834 - val_loss: 0.8452 - val_acc: 0.6750\n",
      "Epoch 1799/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8344 - acc: 0.6772 - val_loss: 0.8435 - val_acc: 0.6690\n",
      "Epoch 1800/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8225 - acc: 0.6851 - val_loss: 0.8324 - val_acc: 0.6790\n",
      "Epoch 1801/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8242 - acc: 0.6860 - val_loss: 0.8306 - val_acc: 0.6750\n",
      "Epoch 1802/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8260 - acc: 0.6825 - val_loss: 0.8318 - val_acc: 0.6800\n",
      "Epoch 1803/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8278 - acc: 0.6793 - val_loss: 0.8366 - val_acc: 0.6820\n",
      "Epoch 1804/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8249 - acc: 0.6848 - val_loss: 0.8353 - val_acc: 0.6780\n",
      "Epoch 1805/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8254 - acc: 0.6832 - val_loss: 0.8340 - val_acc: 0.6740\n",
      "Epoch 1806/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8243 - acc: 0.6834 - val_loss: 0.8294 - val_acc: 0.6760\n",
      "Epoch 1807/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8216 - acc: 0.6850 - val_loss: 0.8286 - val_acc: 0.6770\n",
      "Epoch 1808/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8232 - acc: 0.6837 - val_loss: 0.8453 - val_acc: 0.6700\n",
      "Epoch 1809/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8300 - acc: 0.6802 - val_loss: 0.8317 - val_acc: 0.6780\n",
      "Epoch 1810/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8240 - acc: 0.6839 - val_loss: 0.8235 - val_acc: 0.6820\n",
      "Epoch 1811/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8315 - acc: 0.6791 - val_loss: 0.8411 - val_acc: 0.6740\n",
      "Epoch 1812/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8297 - acc: 0.6797 - val_loss: 0.8416 - val_acc: 0.6690\n",
      "Epoch 1813/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8247 - acc: 0.6829 - val_loss: 0.8445 - val_acc: 0.6710\n",
      "Epoch 1814/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8254 - acc: 0.6866 - val_loss: 0.8302 - val_acc: 0.6730\n",
      "Epoch 1815/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8242 - acc: 0.6831 - val_loss: 0.8286 - val_acc: 0.6780\n",
      "Epoch 1816/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8249 - acc: 0.6844 - val_loss: 0.8332 - val_acc: 0.6760\n",
      "Epoch 1817/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8256 - acc: 0.6834 - val_loss: 0.8354 - val_acc: 0.6800\n",
      "Epoch 1818/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8235 - acc: 0.6857 - val_loss: 0.8330 - val_acc: 0.6750\n",
      "Epoch 1819/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8247 - acc: 0.6849 - val_loss: 0.8265 - val_acc: 0.6810\n",
      "Epoch 1820/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8218 - acc: 0.6852 - val_loss: 0.8320 - val_acc: 0.6830\n",
      "Epoch 1821/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8263 - acc: 0.6835 - val_loss: 0.8464 - val_acc: 0.6750\n",
      "Epoch 1822/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8264 - acc: 0.6829 - val_loss: 0.8353 - val_acc: 0.6670\n",
      "Epoch 1823/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8229 - acc: 0.6864 - val_loss: 0.8337 - val_acc: 0.6770\n",
      "Epoch 1824/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8218 - acc: 0.6868 - val_loss: 0.8297 - val_acc: 0.6720\n",
      "Epoch 1825/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8227 - acc: 0.6844 - val_loss: 0.8328 - val_acc: 0.6780\n",
      "Epoch 1826/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8224 - acc: 0.6839 - val_loss: 0.8282 - val_acc: 0.6780\n",
      "Epoch 1827/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8223 - acc: 0.6858 - val_loss: 0.8328 - val_acc: 0.6800\n",
      "Epoch 1828/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8241 - acc: 0.6860 - val_loss: 0.8360 - val_acc: 0.6760\n",
      "Epoch 1829/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8230 - acc: 0.6880 - val_loss: 0.8286 - val_acc: 0.6740\n",
      "Epoch 1830/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8216 - acc: 0.6853 - val_loss: 0.8346 - val_acc: 0.6810\n",
      "Epoch 1831/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8221 - acc: 0.6871 - val_loss: 0.8382 - val_acc: 0.6700\n",
      "Epoch 1832/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8255 - acc: 0.6843 - val_loss: 0.8289 - val_acc: 0.6850\n",
      "Epoch 1833/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8222 - acc: 0.6864 - val_loss: 0.8298 - val_acc: 0.6760\n",
      "Epoch 1834/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8232 - acc: 0.6846 - val_loss: 0.8447 - val_acc: 0.6790\n",
      "Epoch 1835/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8273 - acc: 0.6832 - val_loss: 0.8466 - val_acc: 0.6690\n",
      "Epoch 1836/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8289 - acc: 0.6832 - val_loss: 0.8396 - val_acc: 0.6720\n",
      "Epoch 1837/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8253 - acc: 0.6827 - val_loss: 0.8313 - val_acc: 0.6760\n",
      "Epoch 1838/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8240 - acc: 0.6869 - val_loss: 0.8316 - val_acc: 0.6810\n",
      "Epoch 1839/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8238 - acc: 0.6823 - val_loss: 0.8406 - val_acc: 0.6710\n",
      "Epoch 1840/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8251 - acc: 0.6827 - val_loss: 0.8326 - val_acc: 0.6750\n",
      "Epoch 1841/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8233 - acc: 0.6834 - val_loss: 0.8399 - val_acc: 0.6710\n",
      "Epoch 1842/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8288 - acc: 0.6860 - val_loss: 0.8275 - val_acc: 0.6780\n",
      "Epoch 1843/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8250 - acc: 0.6846 - val_loss: 0.8282 - val_acc: 0.6840\n",
      "Epoch 1844/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8225 - acc: 0.6866 - val_loss: 0.8330 - val_acc: 0.6660\n",
      "Epoch 1845/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8230 - acc: 0.6854 - val_loss: 0.8446 - val_acc: 0.6790\n",
      "Epoch 1846/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8296 - acc: 0.6830 - val_loss: 0.8330 - val_acc: 0.6740\n",
      "Epoch 1847/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8212 - acc: 0.6866 - val_loss: 0.8298 - val_acc: 0.6700\n",
      "Epoch 1848/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8210 - acc: 0.6841 - val_loss: 0.8370 - val_acc: 0.6800\n",
      "Epoch 1849/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8246 - acc: 0.6836 - val_loss: 0.8348 - val_acc: 0.6790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1850/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8230 - acc: 0.6846 - val_loss: 0.8488 - val_acc: 0.6710\n",
      "Epoch 1851/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8260 - acc: 0.6869 - val_loss: 0.8325 - val_acc: 0.6800\n",
      "Epoch 1852/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8223 - acc: 0.6856 - val_loss: 0.8457 - val_acc: 0.6710\n",
      "Epoch 1853/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8268 - acc: 0.6843 - val_loss: 0.8373 - val_acc: 0.6870\n",
      "Epoch 1854/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8267 - acc: 0.6806 - val_loss: 0.8343 - val_acc: 0.6770\n",
      "Epoch 1855/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8238 - acc: 0.6840 - val_loss: 0.8474 - val_acc: 0.6720\n",
      "Epoch 1856/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8267 - acc: 0.6839 - val_loss: 0.8366 - val_acc: 0.6790\n",
      "Epoch 1857/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8237 - acc: 0.6866 - val_loss: 0.8345 - val_acc: 0.6800\n",
      "Epoch 1858/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8244 - acc: 0.6832 - val_loss: 0.8296 - val_acc: 0.6840\n",
      "Epoch 1859/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8250 - acc: 0.6834 - val_loss: 0.8308 - val_acc: 0.6790\n",
      "Epoch 1860/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8245 - acc: 0.6845 - val_loss: 0.8251 - val_acc: 0.6780\n",
      "Epoch 1861/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8209 - acc: 0.6851 - val_loss: 0.8337 - val_acc: 0.6720\n",
      "Epoch 1862/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8226 - acc: 0.6871 - val_loss: 0.8349 - val_acc: 0.6790\n",
      "Epoch 1863/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8252 - acc: 0.6822 - val_loss: 0.8327 - val_acc: 0.6810\n",
      "Epoch 1864/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8238 - acc: 0.6835 - val_loss: 0.8438 - val_acc: 0.6760\n",
      "Epoch 1865/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8257 - acc: 0.6848 - val_loss: 0.8254 - val_acc: 0.6790\n",
      "Epoch 1866/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8238 - acc: 0.6834 - val_loss: 0.8320 - val_acc: 0.6790\n",
      "Epoch 1867/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8261 - acc: 0.6837 - val_loss: 0.8245 - val_acc: 0.6820\n",
      "Epoch 1868/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8245 - acc: 0.6846 - val_loss: 0.8360 - val_acc: 0.6770\n",
      "Epoch 1869/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8226 - acc: 0.6837 - val_loss: 0.8357 - val_acc: 0.6730\n",
      "Epoch 1870/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8213 - acc: 0.6871 - val_loss: 0.8452 - val_acc: 0.6690\n",
      "Epoch 1871/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8298 - acc: 0.6804 - val_loss: 0.8312 - val_acc: 0.6760\n",
      "Epoch 1872/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8339 - acc: 0.6777 - val_loss: 0.8440 - val_acc: 0.6640\n",
      "Epoch 1873/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8273 - acc: 0.6808 - val_loss: 0.8489 - val_acc: 0.6700\n",
      "Epoch 1874/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8261 - acc: 0.6829 - val_loss: 0.8269 - val_acc: 0.6750\n",
      "Epoch 1875/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8217 - acc: 0.6842 - val_loss: 0.8358 - val_acc: 0.6720\n",
      "Epoch 1876/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8237 - acc: 0.6834 - val_loss: 0.8273 - val_acc: 0.6820\n",
      "Epoch 1877/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8280 - acc: 0.6840 - val_loss: 0.8320 - val_acc: 0.6750\n",
      "Epoch 1878/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8213 - acc: 0.6851 - val_loss: 0.8308 - val_acc: 0.6780\n",
      "Epoch 1879/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8231 - acc: 0.6857 - val_loss: 0.8361 - val_acc: 0.6850\n",
      "Epoch 1880/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8236 - acc: 0.6841 - val_loss: 0.8347 - val_acc: 0.6670\n",
      "Epoch 1881/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8233 - acc: 0.6849 - val_loss: 0.8342 - val_acc: 0.6790\n",
      "Epoch 1882/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8221 - acc: 0.6879 - val_loss: 0.8306 - val_acc: 0.6760\n",
      "Epoch 1883/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8265 - acc: 0.6800 - val_loss: 0.8366 - val_acc: 0.6780\n",
      "Epoch 1884/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8293 - acc: 0.6798 - val_loss: 0.8393 - val_acc: 0.6800\n",
      "Epoch 1885/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8224 - acc: 0.6826 - val_loss: 0.8313 - val_acc: 0.6780\n",
      "Epoch 1886/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8250 - acc: 0.6831 - val_loss: 0.8340 - val_acc: 0.6820\n",
      "Epoch 1887/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8250 - acc: 0.6825 - val_loss: 0.8269 - val_acc: 0.6740\n",
      "Epoch 1888/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8212 - acc: 0.6881 - val_loss: 0.8398 - val_acc: 0.6690\n",
      "Epoch 1889/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8195 - acc: 0.6862 - val_loss: 0.8291 - val_acc: 0.6760\n",
      "Epoch 1890/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8249 - acc: 0.6816 - val_loss: 0.8389 - val_acc: 0.6730\n",
      "Epoch 1891/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8205 - acc: 0.6851 - val_loss: 0.8325 - val_acc: 0.6690\n",
      "Epoch 1892/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8219 - acc: 0.6853 - val_loss: 0.8383 - val_acc: 0.6700\n",
      "Epoch 1893/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8233 - acc: 0.6832 - val_loss: 0.8295 - val_acc: 0.6780\n",
      "Epoch 1894/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8210 - acc: 0.6860 - val_loss: 0.8348 - val_acc: 0.6760\n",
      "Epoch 1895/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8223 - acc: 0.6844 - val_loss: 0.8366 - val_acc: 0.6780\n",
      "Epoch 1896/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8224 - acc: 0.6861 - val_loss: 0.8271 - val_acc: 0.6820\n",
      "Epoch 1897/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8211 - acc: 0.6848 - val_loss: 0.8281 - val_acc: 0.6800\n",
      "Epoch 1898/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8234 - acc: 0.6839 - val_loss: 0.8343 - val_acc: 0.6760\n",
      "Epoch 1899/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8293 - acc: 0.6788 - val_loss: 0.8350 - val_acc: 0.6710\n",
      "Epoch 1900/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8234 - acc: 0.6834 - val_loss: 0.8311 - val_acc: 0.6730\n",
      "Epoch 1901/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8234 - acc: 0.6865 - val_loss: 0.8302 - val_acc: 0.6730\n",
      "Epoch 1902/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8219 - acc: 0.6816 - val_loss: 0.8367 - val_acc: 0.6720\n",
      "Epoch 1903/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8239 - acc: 0.6829 - val_loss: 0.8295 - val_acc: 0.6720\n",
      "Epoch 1904/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8190 - acc: 0.6884 - val_loss: 0.8288 - val_acc: 0.6800\n",
      "Epoch 1905/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8218 - acc: 0.6842 - val_loss: 0.8349 - val_acc: 0.6690\n",
      "Epoch 1906/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8242 - acc: 0.6844 - val_loss: 0.8382 - val_acc: 0.6760\n",
      "Epoch 1907/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8254 - acc: 0.6816 - val_loss: 0.8288 - val_acc: 0.6710\n",
      "Epoch 1908/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8237 - acc: 0.6813 - val_loss: 0.8280 - val_acc: 0.6800\n",
      "Epoch 1909/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8221 - acc: 0.6824 - val_loss: 0.8277 - val_acc: 0.6790\n",
      "Epoch 1910/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8229 - acc: 0.6879 - val_loss: 0.8382 - val_acc: 0.6660\n",
      "Epoch 1911/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8293 - acc: 0.6792 - val_loss: 0.8275 - val_acc: 0.6770\n",
      "Epoch 1912/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8242 - acc: 0.6850 - val_loss: 0.8358 - val_acc: 0.6720\n",
      "Epoch 1913/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8210 - acc: 0.6866 - val_loss: 0.8382 - val_acc: 0.6720\n",
      "Epoch 1914/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8263 - acc: 0.6812 - val_loss: 0.8302 - val_acc: 0.6720\n",
      "Epoch 1915/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8264 - acc: 0.6834 - val_loss: 0.8466 - val_acc: 0.6680\n",
      "Epoch 1916/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8241 - acc: 0.6845 - val_loss: 0.8268 - val_acc: 0.6740\n",
      "Epoch 1917/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8257 - acc: 0.6843 - val_loss: 0.8297 - val_acc: 0.6760\n",
      "Epoch 1918/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8238 - acc: 0.6837 - val_loss: 0.8310 - val_acc: 0.6750\n",
      "Epoch 1919/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8208 - acc: 0.6856 - val_loss: 0.8308 - val_acc: 0.6710\n",
      "Epoch 1920/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8241 - acc: 0.6814 - val_loss: 0.8372 - val_acc: 0.6680\n",
      "Epoch 1921/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8264 - acc: 0.6841 - val_loss: 0.8301 - val_acc: 0.6830\n",
      "Epoch 1922/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8284 - acc: 0.6832 - val_loss: 0.8285 - val_acc: 0.6800\n",
      "Epoch 1923/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8227 - acc: 0.6845 - val_loss: 0.8264 - val_acc: 0.6810\n",
      "Epoch 1924/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8218 - acc: 0.6866 - val_loss: 0.8252 - val_acc: 0.6830\n",
      "Epoch 1925/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8226 - acc: 0.6849 - val_loss: 0.8412 - val_acc: 0.6680\n",
      "Epoch 1926/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8239 - acc: 0.6839 - val_loss: 0.8439 - val_acc: 0.6730\n",
      "Epoch 1927/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8264 - acc: 0.6822 - val_loss: 0.8369 - val_acc: 0.6720\n",
      "Epoch 1928/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8237 - acc: 0.6852 - val_loss: 0.8315 - val_acc: 0.6760\n",
      "Epoch 1929/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8230 - acc: 0.6858 - val_loss: 0.8281 - val_acc: 0.6800\n",
      "Epoch 1930/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8211 - acc: 0.6844 - val_loss: 0.8394 - val_acc: 0.6730\n",
      "Epoch 1931/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8216 - acc: 0.6855 - val_loss: 0.8325 - val_acc: 0.6740\n",
      "Epoch 1932/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8271 - acc: 0.6791 - val_loss: 0.8322 - val_acc: 0.6790\n",
      "Epoch 1933/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8233 - acc: 0.6829 - val_loss: 0.8294 - val_acc: 0.6770\n",
      "Epoch 1934/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8253 - acc: 0.6824 - val_loss: 0.8275 - val_acc: 0.6760\n",
      "Epoch 1935/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8227 - acc: 0.6848 - val_loss: 0.8277 - val_acc: 0.6800\n",
      "Epoch 1936/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8213 - acc: 0.6869 - val_loss: 0.8395 - val_acc: 0.6740\n",
      "Epoch 1937/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8269 - acc: 0.6822 - val_loss: 0.8338 - val_acc: 0.6750\n",
      "Epoch 1938/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8223 - acc: 0.6850 - val_loss: 0.8264 - val_acc: 0.6760\n",
      "Epoch 1939/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8231 - acc: 0.6823 - val_loss: 0.8352 - val_acc: 0.6760\n",
      "Epoch 1940/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8262 - acc: 0.6830 - val_loss: 0.8371 - val_acc: 0.6750\n",
      "Epoch 1941/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8273 - acc: 0.6844 - val_loss: 0.8385 - val_acc: 0.6720\n",
      "Epoch 1942/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8238 - acc: 0.6854 - val_loss: 0.8277 - val_acc: 0.6780\n",
      "Epoch 1943/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8206 - acc: 0.6864 - val_loss: 0.8229 - val_acc: 0.6820\n",
      "Epoch 1944/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8212 - acc: 0.6816 - val_loss: 0.8341 - val_acc: 0.6750\n",
      "Epoch 1945/2000\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8231 - acc: 0.6853 - val_loss: 0.8271 - val_acc: 0.6850\n",
      "Epoch 1946/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8210 - acc: 0.6841 - val_loss: 0.8274 - val_acc: 0.6780\n",
      "Epoch 1947/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8202 - acc: 0.6846 - val_loss: 0.8366 - val_acc: 0.6690\n",
      "Epoch 1948/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8241 - acc: 0.6848 - val_loss: 0.8455 - val_acc: 0.6730\n",
      "Epoch 1949/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8259 - acc: 0.6804 - val_loss: 0.8348 - val_acc: 0.6770\n",
      "Epoch 1950/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8260 - acc: 0.6818 - val_loss: 0.8351 - val_acc: 0.6720\n",
      "Epoch 1951/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8225 - acc: 0.6850 - val_loss: 0.8315 - val_acc: 0.6720\n",
      "Epoch 1952/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8195 - acc: 0.6857 - val_loss: 0.8445 - val_acc: 0.6710\n",
      "Epoch 1953/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8208 - acc: 0.6848 - val_loss: 0.8361 - val_acc: 0.6790\n",
      "Epoch 1954/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8220 - acc: 0.6850 - val_loss: 0.8344 - val_acc: 0.6720\n",
      "Epoch 1955/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8201 - acc: 0.6844 - val_loss: 0.8255 - val_acc: 0.6840\n",
      "Epoch 1956/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8196 - acc: 0.6861 - val_loss: 0.8377 - val_acc: 0.6680\n",
      "Epoch 1957/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8265 - acc: 0.6821 - val_loss: 0.8264 - val_acc: 0.6760\n",
      "Epoch 1958/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8221 - acc: 0.6851 - val_loss: 0.8315 - val_acc: 0.6840\n",
      "Epoch 1959/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8217 - acc: 0.6840 - val_loss: 0.8394 - val_acc: 0.6750\n",
      "Epoch 1960/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8213 - acc: 0.6855 - val_loss: 0.8363 - val_acc: 0.6670\n",
      "Epoch 1961/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8234 - acc: 0.6834 - val_loss: 0.8541 - val_acc: 0.6760\n",
      "Epoch 1962/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8264 - acc: 0.6851 - val_loss: 0.8301 - val_acc: 0.6720\n",
      "Epoch 1963/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8251 - acc: 0.6841 - val_loss: 0.8277 - val_acc: 0.6740\n",
      "Epoch 1964/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8255 - acc: 0.6841 - val_loss: 0.8415 - val_acc: 0.6660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1965/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8238 - acc: 0.6822 - val_loss: 0.8441 - val_acc: 0.6710\n",
      "Epoch 1966/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8248 - acc: 0.6859 - val_loss: 0.8309 - val_acc: 0.6710\n",
      "Epoch 1967/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8259 - acc: 0.6818 - val_loss: 0.8346 - val_acc: 0.6820\n",
      "Epoch 1968/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8233 - acc: 0.6832 - val_loss: 0.8306 - val_acc: 0.6710\n",
      "Epoch 1969/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8250 - acc: 0.6814 - val_loss: 0.8331 - val_acc: 0.6840\n",
      "Epoch 1970/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8225 - acc: 0.6843 - val_loss: 0.8389 - val_acc: 0.6810\n",
      "Epoch 1971/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8237 - acc: 0.6804 - val_loss: 0.8302 - val_acc: 0.6760\n",
      "Epoch 1972/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8177 - acc: 0.6866 - val_loss: 0.8305 - val_acc: 0.6720\n",
      "Epoch 1973/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8230 - acc: 0.6838 - val_loss: 0.8287 - val_acc: 0.6810\n",
      "Epoch 1974/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8237 - acc: 0.6838 - val_loss: 0.8394 - val_acc: 0.6750\n",
      "Epoch 1975/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8222 - acc: 0.6843 - val_loss: 0.8240 - val_acc: 0.6810\n",
      "Epoch 1976/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8215 - acc: 0.6823 - val_loss: 0.8338 - val_acc: 0.6750\n",
      "Epoch 1977/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8200 - acc: 0.6872 - val_loss: 0.8290 - val_acc: 0.6740\n",
      "Epoch 1978/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8220 - acc: 0.6831 - val_loss: 0.8324 - val_acc: 0.6830\n",
      "Epoch 1979/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8202 - acc: 0.6856 - val_loss: 0.8314 - val_acc: 0.6740\n",
      "Epoch 1980/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8229 - acc: 0.6839 - val_loss: 0.8279 - val_acc: 0.6710\n",
      "Epoch 1981/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8198 - acc: 0.6866 - val_loss: 0.8591 - val_acc: 0.6630\n",
      "Epoch 1982/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8242 - acc: 0.6846 - val_loss: 0.8273 - val_acc: 0.6760\n",
      "Epoch 1983/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8198 - acc: 0.6843 - val_loss: 0.8306 - val_acc: 0.6860\n",
      "Epoch 1984/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8272 - acc: 0.6851 - val_loss: 0.8331 - val_acc: 0.6760\n",
      "Epoch 1985/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8224 - acc: 0.6820 - val_loss: 0.8328 - val_acc: 0.6770\n",
      "Epoch 1986/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8191 - acc: 0.6869 - val_loss: 0.8246 - val_acc: 0.6790\n",
      "Epoch 1987/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8212 - acc: 0.6866 - val_loss: 0.8303 - val_acc: 0.6770\n",
      "Epoch 1988/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8219 - acc: 0.6850 - val_loss: 0.8294 - val_acc: 0.6710\n",
      "Epoch 1989/2000\n",
      "14367/14367 [==============================] - 0s 10us/step - loss: 0.8229 - acc: 0.6853 - val_loss: 0.8477 - val_acc: 0.6730\n",
      "Epoch 1990/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8245 - acc: 0.6827 - val_loss: 0.8451 - val_acc: 0.6740\n",
      "Epoch 1991/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8255 - acc: 0.6834 - val_loss: 0.8248 - val_acc: 0.6800\n",
      "Epoch 1992/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8215 - acc: 0.6829 - val_loss: 0.8303 - val_acc: 0.6790\n",
      "Epoch 1993/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8195 - acc: 0.6886 - val_loss: 0.8387 - val_acc: 0.6720\n",
      "Epoch 1994/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8223 - acc: 0.6849 - val_loss: 0.8288 - val_acc: 0.6710\n",
      "Epoch 1995/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8185 - acc: 0.6848 - val_loss: 0.8273 - val_acc: 0.6760\n",
      "Epoch 1996/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8204 - acc: 0.6857 - val_loss: 0.8315 - val_acc: 0.6800\n",
      "Epoch 1997/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8187 - acc: 0.6848 - val_loss: 0.8379 - val_acc: 0.6720\n",
      "Epoch 1998/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8234 - acc: 0.6787 - val_loss: 0.8284 - val_acc: 0.6830\n",
      "Epoch 1999/2000\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.8201 - acc: 0.6854 - val_loss: 0.8351 - val_acc: 0.6770\n",
      "Epoch 2000/2000\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.8221 - acc: 0.6850 - val_loss: 0.8317 - val_acc: 0.6700\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train.values,\n",
    "                    epochs=2000,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) +1)\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training Acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n",
    "plt.title('Training and Validation Acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 61us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8157173809051513, 0.6822]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "av_training_set_AFP    2317\n",
       "av_training_set_NTP     635\n",
       "av_training_set_PC      890\n",
       "av_training_set_UNK    1158\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4734"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test['av_training_set_AFP'].sum()/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so what have we learned here? well firstly, as might have been expected, this approach is better than guessing,\n",
    "but not by much, and we have also learned that there are just too many damn attributes to deal with reasonably\n",
    "so we will move to the autoencoder work to use in DEC - maybe we can handle the high dimesion data in that setting?\n",
    "if not well, then we have stuff to think through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## build a fun dumb autoencoder\n",
    "\n",
    "\n",
    "k_data = kepler_data.drop(['tce_rogue_flag', 'tce_delivname', 'rowupdate', \n",
    "                           'tce_datalink_dvs', 'tce_datalink_dvr', 'tce_steff_prov', \n",
    "                           'tce_slogg_prov', 'tce_smet_prov','tce_sradius_prov', 'tce_limbdark_mod',\n",
    "                           'tce_trans_mod', 'tce_eccen', 'tce_eccen_err', 'tce_longp', 'tce_longp_err'\n",
    "                          ],axis=1)\n",
    "# drop some columns that we will not be using\n",
    "# starting with all NA columns\n",
    "k_data.dropna(axis=1,how='any', inplace=True)\n",
    "\n",
    "# drop the Autovetter stuff\n",
    "autovetter_cols = ['av_vf_pc', 'av_vf_pc_err', 'av_vf_afp',\n",
    "                   'av_vf_afp_err', 'av_vf_ntp', 'av_vf_ntp_err', \n",
    "                   'av_pp_pc','av_pp_afp', 'av_pp_ntp', \n",
    "                   'av_training_set', 'av_pred_class']\n",
    "# drop all autovetter columns execpt the training set - as those are manually set - take as \"truth\"\n",
    "\n",
    "k_data = k_data.drop(['av_vf_pc', 'av_vf_pc_err', 'av_vf_afp','av_vf_afp_err',\n",
    "                           'av_vf_ntp', 'av_vf_ntp_err', 'av_pp_pc','av_pp_afp', \n",
    "                           'av_pp_ntp', 'av_pred_class'],axis=1)\n",
    "\n",
    "col_names = k_data.columns.values\n",
    "\n",
    "k_data = pd.get_dummies(k_data, columns=['av_training_set'])\n",
    "\n",
    "\n",
    "ae_data = k_data\n",
    "# update to be categorical\n",
    "ae_data_y = ae_data\n",
    "ae_data_y = ae_data[['av_training_set_AFP','av_training_set_NTP','av_training_set_PC', 'av_training_set_UNK']]\n",
    "ae_data = ae_data.drop(['av_training_set_AFP','av_training_set_NTP','av_training_set_PC', 'av_training_set_UNK'], axis=1)\n",
    "#x = k_data.drop(['rowid','kepid'], axis=1)\n",
    "#x = k_data[['tce_mesmad','tce_maxmes', 'tce_minmes', 'tce_maxmesd', 'tce_minmesd', 'tce_plnt_num']]\n",
    "ae_data = (ae_data - ae_data.mean()) / (ae_data.max() - ae_data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 93)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               9400      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 90)                9090      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 70)                6370      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 80)                5680      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 60)                4860      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 40)                2040      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 30)                1230      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 30)                630       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 40)                1240      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 50)                2050      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 60)                3060      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 80)                4880      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 70)                5670      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 90)                6390      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 100)               9100      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 93)                9393      \n",
      "=================================================================\n",
      "Total params: 85,183\n",
      "Trainable params: 85,183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "20367/20367 [==============================] - 2s 109us/step - loss: 0.0138 - acc: 0.0989\n",
      "Epoch 2/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0132 - acc: 0.1619\n",
      "Epoch 3/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0123 - acc: 0.2251\n",
      "Epoch 4/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0116 - acc: 0.2307\n",
      "Epoch 5/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0114 - acc: 0.3970\n",
      "Epoch 6/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0113 - acc: 0.4439\n",
      "Epoch 7/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0112 - acc: 0.4565\n",
      "Epoch 8/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0111 - acc: 0.4570\n",
      "Epoch 9/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0112 - acc: 0.4633\n",
      "Epoch 10/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0111 - acc: 0.4746\n",
      "Epoch 11/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0110 - acc: 0.4842\n",
      "Epoch 12/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0110 - acc: 0.4868\n",
      "Epoch 13/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0109 - acc: 0.4897\n",
      "Epoch 14/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0109 - acc: 0.4938\n",
      "Epoch 15/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0109 - acc: 0.5105\n",
      "Epoch 16/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0108 - acc: 0.5236\n",
      "Epoch 17/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0108 - acc: 0.5307\n",
      "Epoch 18/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0108 - acc: 0.5332\n",
      "Epoch 19/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0107 - acc: 0.5422\n",
      "Epoch 20/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0107 - acc: 0.5514\n",
      "Epoch 21/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0107 - acc: 0.5511\n",
      "Epoch 22/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0106 - acc: 0.5635\n",
      "Epoch 23/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0106 - acc: 0.5695\n",
      "Epoch 24/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0106 - acc: 0.5937\n",
      "Epoch 25/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0105 - acc: 0.5906\n",
      "Epoch 26/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0105 - acc: 0.5995\n",
      "Epoch 27/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0104 - acc: 0.6028\n",
      "Epoch 28/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0104 - acc: 0.6115\n",
      "Epoch 29/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0104 - acc: 0.6254\n",
      "Epoch 30/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0104 - acc: 0.6296\n",
      "Epoch 31/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0103 - acc: 0.6279\n",
      "Epoch 32/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0103 - acc: 0.6341\n",
      "Epoch 33/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0103 - acc: 0.6361\n",
      "Epoch 34/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0102 - acc: 0.6449\n",
      "Epoch 35/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0102 - acc: 0.6443\n",
      "Epoch 36/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0102 - acc: 0.6539\n",
      "Epoch 37/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0102 - acc: 0.6539\n",
      "Epoch 38/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0102 - acc: 0.6599\n",
      "Epoch 39/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0101 - acc: 0.6534\n",
      "Epoch 40/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0101 - acc: 0.6637\n",
      "Epoch 41/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0101 - acc: 0.6617\n",
      "Epoch 42/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0101 - acc: 0.6667\n",
      "Epoch 43/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0101 - acc: 0.6611\n",
      "Epoch 44/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0100 - acc: 0.6773\n",
      "Epoch 45/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0100 - acc: 0.6757\n",
      "Epoch 46/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0100 - acc: 0.6794\n",
      "Epoch 47/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0100 - acc: 0.6841\n",
      "Epoch 48/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0100 - acc: 0.6777\n",
      "Epoch 49/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0100 - acc: 0.6825\n",
      "Epoch 50/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0100 - acc: 0.6852\n",
      "Epoch 51/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0100 - acc: 0.6935\n",
      "Epoch 52/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0100 - acc: 0.6866\n",
      "Epoch 53/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0100 - acc: 0.6890\n",
      "Epoch 54/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0100 - acc: 0.6890\n",
      "Epoch 55/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0100 - acc: 0.6957\n",
      "Epoch 56/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0100 - acc: 0.6936\n",
      "Epoch 57/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0100 - acc: 0.6853\n",
      "Epoch 58/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0100 - acc: 0.6892\n",
      "Epoch 59/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0099 - acc: 0.7049\n",
      "Epoch 60/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0100 - acc: 0.7007\n",
      "Epoch 61/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0099 - acc: 0.6977\n",
      "Epoch 62/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0100 - acc: 0.6938\n",
      "Epoch 63/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0100 - acc: 0.6987\n",
      "Epoch 64/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0099 - acc: 0.6989\n",
      "Epoch 65/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0099 - acc: 0.7023\n",
      "Epoch 66/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0099 - acc: 0.7006\n",
      "Epoch 67/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0099 - acc: 0.6891\n",
      "Epoch 68/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0099 - acc: 0.6941\n",
      "Epoch 69/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0099 - acc: 0.6916\n",
      "Epoch 70/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0099 - acc: 0.7080\n",
      "Epoch 71/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0099 - acc: 0.6985\n",
      "Epoch 72/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0099 - acc: 0.7031\n",
      "Epoch 73/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0099 - acc: 0.6995\n",
      "Epoch 74/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0099 - acc: 0.6858\n",
      "Epoch 75/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0099 - acc: 0.7020\n",
      "Epoch 76/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0099 - acc: 0.6898\n",
      "Epoch 77/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0099 - acc: 0.6886\n",
      "Epoch 78/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0099 - acc: 0.7024\n",
      "Epoch 79/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0099 - acc: 0.7054\n",
      "Epoch 80/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0099 - acc: 0.6985\n",
      "Epoch 81/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0099 - acc: 0.6995\n",
      "Epoch 82/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7090\n",
      "Epoch 83/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6992\n",
      "Epoch 84/2000\n",
      "20367/20367 [==============================] - 0s 19us/step - loss: 0.0098 - acc: 0.7010\n",
      "Epoch 85/2000\n",
      "20367/20367 [==============================] - 0s 19us/step - loss: 0.0098 - acc: 0.6840\n",
      "Epoch 86/2000\n",
      "20367/20367 [==============================] - 0s 18us/step - loss: 0.0098 - acc: 0.6779\n",
      "Epoch 87/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.7128\n",
      "Epoch 88/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.7140\n",
      "Epoch 89/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.7008\n",
      "Epoch 90/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6980\n",
      "Epoch 91/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.7047\n",
      "Epoch 92/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7136\n",
      "Epoch 93/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.7048\n",
      "Epoch 94/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7165\n",
      "Epoch 95/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.7114\n",
      "Epoch 96/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.7086\n",
      "Epoch 97/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6923\n",
      "Epoch 98/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.7006\n",
      "Epoch 99/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0099 - acc: 0.6882\n",
      "Epoch 100/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.7020\n",
      "Epoch 101/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.7051\n",
      "Epoch 102/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7141\n",
      "Epoch 103/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.6985\n",
      "Epoch 104/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.6925\n",
      "Epoch 105/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7090\n",
      "Epoch 106/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0098 - acc: 0.7155\n",
      "Epoch 107/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0098 - acc: 0.7119\n",
      "Epoch 108/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0098 - acc: 0.7048\n",
      "Epoch 109/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7162\n",
      "Epoch 110/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7134\n",
      "Epoch 111/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7116\n",
      "Epoch 112/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6994\n",
      "Epoch 113/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7152\n",
      "Epoch 114/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0099 - acc: 0.7107\n",
      "Epoch 115/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7056\n",
      "Epoch 116/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6702\n",
      "Epoch 117/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6957\n",
      "Epoch 118/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7120\n",
      "Epoch 119/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6981\n",
      "Epoch 120/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0098 - acc: 0.6926\n",
      "Epoch 121/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7057\n",
      "Epoch 122/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7093\n",
      "Epoch 123/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7153\n",
      "Epoch 124/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7289\n",
      "Epoch 125/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7091\n",
      "Epoch 126/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7045\n",
      "Epoch 127/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7185\n",
      "Epoch 128/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7104\n",
      "Epoch 129/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7063\n",
      "Epoch 130/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7183\n",
      "Epoch 131/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7126\n",
      "Epoch 132/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7182\n",
      "Epoch 133/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7237\n",
      "Epoch 134/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7106\n",
      "Epoch 135/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7124\n",
      "Epoch 136/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6898\n",
      "Epoch 137/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7126\n",
      "Epoch 138/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.6989\n",
      "Epoch 139/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7093\n",
      "Epoch 140/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7008\n",
      "Epoch 141/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6890\n",
      "Epoch 142/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6931\n",
      "Epoch 143/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7060\n",
      "Epoch 144/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7072\n",
      "Epoch 145/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7097\n",
      "Epoch 146/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7232\n",
      "Epoch 147/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7208\n",
      "Epoch 148/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7060\n",
      "Epoch 149/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0098 - acc: 0.7166\n",
      "Epoch 150/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7111\n",
      "Epoch 151/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6947\n",
      "Epoch 152/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7001\n",
      "Epoch 153/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6948\n",
      "Epoch 154/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7106\n",
      "Epoch 155/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7092\n",
      "Epoch 156/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7165\n",
      "Epoch 157/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7006\n",
      "Epoch 158/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6902\n",
      "Epoch 159/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7183\n",
      "Epoch 160/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7116\n",
      "Epoch 161/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7211\n",
      "Epoch 162/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7206\n",
      "Epoch 163/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7233\n",
      "Epoch 164/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7219\n",
      "Epoch 165/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7124\n",
      "Epoch 166/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7225\n",
      "Epoch 167/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7324\n",
      "Epoch 168/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7270\n",
      "Epoch 169/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7312\n",
      "Epoch 170/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7284\n",
      "Epoch 171/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7298\n",
      "Epoch 172/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7016\n",
      "Epoch 173/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0098 - acc: 0.7139\n",
      "Epoch 174/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7296\n",
      "Epoch 175/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7283\n",
      "Epoch 176/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7289\n",
      "Epoch 177/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7246\n",
      "Epoch 178/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7254\n",
      "Epoch 179/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7294\n",
      "Epoch 180/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7110\n",
      "Epoch 181/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7085\n",
      "Epoch 182/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7223\n",
      "Epoch 183/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0098 - acc: 0.7065\n",
      "Epoch 184/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7111\n",
      "Epoch 185/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7209\n",
      "Epoch 186/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7302\n",
      "Epoch 187/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7272\n",
      "Epoch 188/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7294\n",
      "Epoch 189/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7331\n",
      "Epoch 190/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7253\n",
      "Epoch 191/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6473\n",
      "Epoch 192/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0099 - acc: 0.6449\n",
      "Epoch 193/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0098 - acc: 0.6678\n",
      "Epoch 194/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.6945\n",
      "Epoch 195/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7147\n",
      "Epoch 196/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7177\n",
      "Epoch 197/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7308\n",
      "Epoch 198/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7276\n",
      "Epoch 199/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7257\n",
      "Epoch 200/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7244\n",
      "Epoch 201/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7236\n",
      "Epoch 202/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7215\n",
      "Epoch 203/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7269\n",
      "Epoch 204/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7250\n",
      "Epoch 205/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6904\n",
      "Epoch 206/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7018\n",
      "Epoch 207/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7064\n",
      "Epoch 208/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6901\n",
      "Epoch 209/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7053\n",
      "Epoch 210/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7118\n",
      "Epoch 211/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7177\n",
      "Epoch 212/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7201\n",
      "Epoch 213/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7184\n",
      "Epoch 214/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7350\n",
      "Epoch 215/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7337\n",
      "Epoch 216/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7271\n",
      "Epoch 217/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7173\n",
      "Epoch 218/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7084\n",
      "Epoch 219/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0098 - acc: 0.6859\n",
      "Epoch 220/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0099 - acc: 0.6393\n",
      "Epoch 221/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6655\n",
      "Epoch 222/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6894\n",
      "Epoch 223/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6941\n",
      "Epoch 224/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7214\n",
      "Epoch 225/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7304\n",
      "Epoch 226/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7306\n",
      "Epoch 227/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7268\n",
      "Epoch 228/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7310\n",
      "Epoch 229/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7342\n",
      "Epoch 230/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7438\n",
      "Epoch 231/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7453\n",
      "Epoch 232/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7333\n",
      "Epoch 233/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7394\n",
      "Epoch 234/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7325\n",
      "Epoch 235/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7244\n",
      "Epoch 236/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7281\n",
      "Epoch 237/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7339\n",
      "Epoch 238/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7367\n",
      "Epoch 239/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7380\n",
      "Epoch 240/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7438\n",
      "Epoch 241/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7331\n",
      "Epoch 242/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7340\n",
      "Epoch 243/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7360\n",
      "Epoch 244/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7393\n",
      "Epoch 245/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7412\n",
      "Epoch 246/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7441\n",
      "Epoch 247/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7445\n",
      "Epoch 248/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7500\n",
      "Epoch 249/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7294\n",
      "Epoch 250/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6810\n",
      "Epoch 251/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0099 - acc: 0.6301\n",
      "Epoch 252/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6556\n",
      "Epoch 253/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0098 - acc: 0.6911\n",
      "Epoch 254/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0098 - acc: 0.6925\n",
      "Epoch 255/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7004\n",
      "Epoch 256/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7177\n",
      "Epoch 257/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7317\n",
      "Epoch 258/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7417\n",
      "Epoch 259/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.7208\n",
      "Epoch 260/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7119\n",
      "Epoch 261/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7278\n",
      "Epoch 262/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7200\n",
      "Epoch 263/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7282\n",
      "Epoch 264/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7424\n",
      "Epoch 265/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7381\n",
      "Epoch 266/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7367\n",
      "Epoch 267/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7372\n",
      "Epoch 268/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7365\n",
      "Epoch 269/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7395\n",
      "Epoch 270/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7380\n",
      "Epoch 271/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7333\n",
      "Epoch 272/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7354\n",
      "Epoch 273/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7465\n",
      "Epoch 274/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7419\n",
      "Epoch 275/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7364\n",
      "Epoch 276/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7410\n",
      "Epoch 277/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7357\n",
      "Epoch 278/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7383\n",
      "Epoch 279/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7391\n",
      "Epoch 280/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7391\n",
      "Epoch 281/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7265\n",
      "Epoch 282/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7271\n",
      "Epoch 283/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7203\n",
      "Epoch 284/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6897\n",
      "Epoch 285/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6807\n",
      "Epoch 286/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0098 - acc: 0.6897\n",
      "Epoch 287/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7290\n",
      "Epoch 288/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7362\n",
      "Epoch 289/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7395\n",
      "Epoch 290/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7389\n",
      "Epoch 291/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7430\n",
      "Epoch 292/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7281\n",
      "Epoch 293/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7222\n",
      "Epoch 294/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7295\n",
      "Epoch 295/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7395\n",
      "Epoch 296/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7437\n",
      "Epoch 297/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7378\n",
      "Epoch 298/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7342\n",
      "Epoch 299/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7182\n",
      "Epoch 300/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7133\n",
      "Epoch 301/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7332\n",
      "Epoch 302/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7388\n",
      "Epoch 303/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7423\n",
      "Epoch 304/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7332\n",
      "Epoch 305/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7288\n",
      "Epoch 306/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7330\n",
      "Epoch 307/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7365\n",
      "Epoch 308/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7387\n",
      "Epoch 309/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7254\n",
      "Epoch 310/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7202\n",
      "Epoch 311/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7357\n",
      "Epoch 312/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7403\n",
      "Epoch 313/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7492\n",
      "Epoch 314/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7439\n",
      "Epoch 315/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7400\n",
      "Epoch 316/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7327\n",
      "Epoch 317/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7306\n",
      "Epoch 318/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7397\n",
      "Epoch 319/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7396\n",
      "Epoch 320/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7433\n",
      "Epoch 321/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7441\n",
      "Epoch 322/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7446\n",
      "Epoch 323/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7564\n",
      "Epoch 324/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7522\n",
      "Epoch 325/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7494\n",
      "Epoch 326/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7481\n",
      "Epoch 327/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7438\n",
      "Epoch 328/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7496\n",
      "Epoch 329/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7562\n",
      "Epoch 330/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7445\n",
      "Epoch 331/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7422\n",
      "Epoch 332/2000\n",
      "20367/20367 [==============================] - 0s 19us/step - loss: 0.0097 - acc: 0.7493\n",
      "Epoch 333/2000\n",
      "20367/20367 [==============================] - 0s 18us/step - loss: 0.0097 - acc: 0.7546\n",
      "Epoch 334/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7415\n",
      "Epoch 335/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7390\n",
      "Epoch 336/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7434\n",
      "Epoch 337/2000\n",
      "20367/20367 [==============================] - 0s 15us/step - loss: 0.0097 - acc: 0.7468\n",
      "Epoch 338/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7389\n",
      "Epoch 339/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7505\n",
      "Epoch 340/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0096 - acc: 0.7569\n",
      "Epoch 341/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7535\n",
      "Epoch 342/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7531\n",
      "Epoch 343/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7464\n",
      "Epoch 344/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7563\n",
      "Epoch 345/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0096 - acc: 0.7581\n",
      "Epoch 346/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0096 - acc: 0.7508\n",
      "Epoch 347/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7547\n",
      "Epoch 348/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7523\n",
      "Epoch 349/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7478\n",
      "Epoch 350/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7409\n",
      "Epoch 351/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7488\n",
      "Epoch 352/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7521\n",
      "Epoch 353/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7515\n",
      "Epoch 354/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7464\n",
      "Epoch 355/2000\n",
      "20367/20367 [==============================] - 0s 17us/step - loss: 0.0097 - acc: 0.7407\n",
      "Epoch 356/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7406\n",
      "Epoch 357/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7470\n",
      "Epoch 358/2000\n",
      "20367/20367 [==============================] - 0s 16us/step - loss: 0.0097 - acc: 0.7568\n",
      "Epoch 359/2000\n",
      " 6656/20367 [========>.....................] - ETA: 0s - loss: 0.0098 - acc: 0.7509"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d1cd5c44d662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CADS/thesis/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/CADS/thesis/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CADS/thesis/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CADS/thesis/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CADS/thesis/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CADS/thesis/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CADS/thesis/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CADS/thesis/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dimensions = [100,90,70,80,60,50,40,30,20]\n",
    "\n",
    "x = Input(shape=(ae_data.shape[1],))\n",
    "h = x\n",
    "for i in range(len(dimensions)):\n",
    "    h = Dense(dimensions[i], activation='relu')(h)\n",
    "    \n",
    "encoding = Dense(10, activation='relu')(h)\n",
    "y = encoding\n",
    "\n",
    "for i in range(1,len(dimensions)+1):\n",
    "    y = Dense(dimensions[-i], activation='relu')(y)\n",
    "    \n",
    "y = Dense(ae_data.shape[1], activation='relu')(y)\n",
    "\n",
    "model = Model(inputs=x, outputs=y)\n",
    "encoder = Model(inputs=x, outputs=h)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(ae_data, ae_data, batch_size=512, epochs=2000)\n",
    "\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(1, len(loss)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_29/Relu:0' shape=(?, 20) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f,axs = plt.subplots(2,2,figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "\n",
    " \n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "acc = history.history['acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training Acc')\n",
    "plt.title('Training and Validation Acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Actual DEC work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an autoencoder \n",
    "### greedy layerwise train\n",
    "### then finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ae_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok - so we have a basic autoencoder\n",
    "# but we did not do layerwise greedy pre training\n",
    "# so we need to do that layerwise greedy pretrain\n",
    "\n",
    "layer_sizes = [ae_data.shape[1],100,90,80,70,60,50,40,30,20,10]\n",
    "\n",
    "# creates the layers to greedy layerwise pretrain\n",
    "# takes par\n",
    "def create_layers(layer_sizes):\n",
    "    autoencoders = []\n",
    "    encoders = []\n",
    "    decoders = []\n",
    "    for i in range(1, len(layer_sizes)):\n",
    "        if i == len(layer_sizes) - 1:\n",
    "            act = 'linear'\n",
    "        else:\n",
    "            act = 'relu'\n",
    "        h = Dense(layer_sizes[i], activation=act,\n",
    "                  input_shape=(layer_sizes[i-1],),\n",
    "                  name='encoder_%d'%i,\n",
    "                  bias_initializer='zeros',\n",
    "                  kernel_initializer=RandomNormal(mean=0.0, stddev=0.01, seed=None))\n",
    "        encoders.append(h)\n",
    "        \n",
    "        rev_i = len(layer_sizes) - i\n",
    "        if i == 1:\n",
    "            act = 'linear'\n",
    "        else:\n",
    "            act = 'relu'\n",
    "        d = Dense(layer_sizes[i-1], activation=act,\n",
    "                 name='decoder_%d'%rev_i,\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_initializer=RandomNormal(mean=0.0, stddev=0.01, seed=None))\n",
    "        decoders.append(d)\n",
    "        autoencoder = Sequential([\n",
    "            Dropout(0.005, input_shape=(layer_sizes[i-1],),name='encoder_dropout_%d'%i),\n",
    "            h,\n",
    "            Dropout(0.005, name='decoder_dropout_%d'%rev_i),\n",
    "            d\n",
    "        ])\n",
    "        autoencoder.compile(loss='mse', optimizer=SGD(lr=0.1, decay=0, momentum=0.9))\n",
    "        autoencoders.append(autoencoder)\n",
    "    return autoencoders, encoders, decoders\n",
    "\n",
    "# build up some models\n",
    "autoencoders, encoders, decoders = create_layers(layer_sizes)\n",
    "\n",
    "encoder = Sequential(encoders)\n",
    "encoder.compile(loss='mse', optimizer=SGD(lr=0.1, decay=0, momentum=0.9))\n",
    "decoders.reverse()\n",
    "autoencoder = Sequential(encoders + decoders)\n",
    "autoencoder.compile(loss='mse', optimizer=SGD(lr=0.1, decay=0, momentum=0.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20367/20367 [==============================] - 1s 32us/step - loss: 0.0138\n",
      "Epoch 2/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0137\n",
      "Epoch 3/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0137\n",
      "Epoch 4/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0136\n",
      "Epoch 5/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0136\n",
      "Epoch 6/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0134\n",
      "Epoch 7/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0132\n",
      "Epoch 8/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0129\n",
      "Epoch 9/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0125\n",
      "Epoch 10/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0120\n",
      "Epoch 11/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0113\n",
      "Epoch 12/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0105\n",
      "Epoch 13/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0097\n",
      "Epoch 14/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0089\n",
      "Epoch 15/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0083\n",
      "Epoch 16/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0078\n",
      "Epoch 17/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0073\n",
      "Epoch 18/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0070\n",
      "Epoch 19/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0068\n",
      "Epoch 20/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0066\n",
      "Epoch 21/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0064\n",
      "Epoch 22/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0063\n",
      "Epoch 23/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0061\n",
      "Epoch 24/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0060\n",
      "Epoch 25/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0058\n",
      "Epoch 26/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0056\n",
      "Epoch 27/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0054\n",
      "Epoch 28/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0052\n",
      "Epoch 29/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0050\n",
      "Epoch 30/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0048\n",
      "Epoch 31/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0046\n",
      "Epoch 32/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0044\n",
      "Epoch 33/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0042\n",
      "Epoch 34/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0040\n",
      "Epoch 35/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0038\n",
      "Epoch 36/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0036\n",
      "Epoch 37/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0035\n",
      "Epoch 38/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0034\n",
      "Epoch 39/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0032\n",
      "Epoch 40/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0031\n",
      "Epoch 41/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0030\n",
      "Epoch 42/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0029\n",
      "Epoch 43/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0028\n",
      "Epoch 44/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0027\n",
      "Epoch 45/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0027\n",
      "Epoch 46/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0026\n",
      "Epoch 47/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0025\n",
      "Epoch 48/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0025\n",
      "Epoch 49/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0024\n",
      "Epoch 50/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0024\n",
      "Epoch 51/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0023\n",
      "Epoch 52/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0023\n",
      "Epoch 53/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0023\n",
      "Epoch 54/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0022\n",
      "Epoch 55/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0022\n",
      "Epoch 56/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0022\n",
      "Epoch 57/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0021\n",
      "Epoch 58/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0021\n",
      "Epoch 59/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0021\n",
      "Epoch 60/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0021\n",
      "Epoch 61/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0021\n",
      "Epoch 62/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0020\n",
      "Epoch 63/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0020\n",
      "Epoch 64/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0020\n",
      "Epoch 65/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0020\n",
      "Epoch 66/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0020\n",
      "Epoch 67/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0020\n",
      "Epoch 68/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0019\n",
      "Epoch 69/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0019\n",
      "Epoch 70/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0019\n",
      "Epoch 71/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0019\n",
      "Epoch 72/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0019\n",
      "Epoch 73/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0019\n",
      "Epoch 74/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0019\n",
      "Epoch 75/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0018\n",
      "Epoch 76/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0018\n",
      "Epoch 77/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0018\n",
      "Epoch 78/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0018\n",
      "Epoch 79/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0018\n",
      "Epoch 80/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0018\n",
      "Epoch 81/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0017\n",
      "Epoch 82/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0017\n",
      "Epoch 83/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0017\n",
      "Epoch 84/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0017\n",
      "Epoch 85/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0017\n",
      "Epoch 86/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0017\n",
      "Epoch 87/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0017\n",
      "Epoch 88/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0017\n",
      "Epoch 89/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0016\n",
      "Epoch 90/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0016\n",
      "Epoch 91/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0016\n",
      "Epoch 92/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0016\n",
      "Epoch 93/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0016\n",
      "Epoch 94/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0016\n",
      "Epoch 95/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0016\n",
      "Epoch 96/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0016\n",
      "Epoch 97/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0016\n",
      "Epoch 98/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0016\n",
      "Epoch 99/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0015\n",
      "Epoch 100/100\n",
      "20367/20367 [==============================] - 0s 6us/step - loss: 0.0015\n",
      "Epoch 1/100\n",
      "20367/20367 [==============================] - 0s 21us/step - loss: 0.0213\n",
      "Epoch 2/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0130\n",
      "Epoch 3/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0111\n",
      "Epoch 4/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0108\n",
      "Epoch 5/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0107\n",
      "Epoch 6/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0106\n",
      "Epoch 7/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0104\n",
      "Epoch 8/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0102\n",
      "Epoch 9/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0099\n",
      "Epoch 10/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0095\n",
      "Epoch 11/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0090\n",
      "Epoch 12/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0086\n",
      "Epoch 13/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0082\n",
      "Epoch 14/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0077\n",
      "Epoch 15/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0073\n",
      "Epoch 16/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0070\n",
      "Epoch 17/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0066\n",
      "Epoch 18/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0061\n",
      "Epoch 19/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0057\n",
      "Epoch 20/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0054\n",
      "Epoch 21/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0052\n",
      "Epoch 22/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0050\n",
      "Epoch 23/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0048\n",
      "Epoch 24/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0047\n",
      "Epoch 25/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0046\n",
      "Epoch 26/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0045\n",
      "Epoch 27/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0044\n",
      "Epoch 28/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0042\n",
      "Epoch 29/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0041\n",
      "Epoch 30/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0040\n",
      "Epoch 31/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0039\n",
      "Epoch 32/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0038\n",
      "Epoch 33/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0036\n",
      "Epoch 34/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0035\n",
      "Epoch 35/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0034\n",
      "Epoch 36/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0032\n",
      "Epoch 37/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0031\n",
      "Epoch 38/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0029\n",
      "Epoch 39/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0028\n",
      "Epoch 40/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0027\n",
      "Epoch 41/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 42/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0024\n",
      "Epoch 43/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0023\n",
      "Epoch 44/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0022\n",
      "Epoch 45/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0021\n",
      "Epoch 46/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0021\n",
      "Epoch 47/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 48/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0019\n",
      "Epoch 49/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0019\n",
      "Epoch 50/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0018\n",
      "Epoch 51/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0017\n",
      "Epoch 52/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0017\n",
      "Epoch 53/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0017\n",
      "Epoch 54/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0016\n",
      "Epoch 55/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0016\n",
      "Epoch 56/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0015\n",
      "Epoch 57/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0015\n",
      "Epoch 58/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0015\n",
      "Epoch 59/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0015\n",
      "Epoch 60/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0014\n",
      "Epoch 61/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0014\n",
      "Epoch 62/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0014\n",
      "Epoch 63/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0014\n",
      "Epoch 64/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0014\n",
      "Epoch 65/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 66/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 67/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 68/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 69/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0013\n",
      "Epoch 70/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 71/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 72/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 73/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 74/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 75/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0012\n",
      "Epoch 76/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 77/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 78/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 79/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 80/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 81/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 82/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 83/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 84/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 85/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 86/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 87/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 88/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 90/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 91/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 92/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0011\n",
      "Epoch 93/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 94/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 95/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 96/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 97/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 98/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 99/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 100/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 1/100\n",
      "20367/20367 [==============================] - 0s 20us/step - loss: 0.0253\n",
      "Epoch 2/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0148\n",
      "Epoch 3/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0132\n",
      "Epoch 4/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0128\n",
      "Epoch 5/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0124\n",
      "Epoch 6/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0120\n",
      "Epoch 7/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0115\n",
      "Epoch 8/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0111\n",
      "Epoch 9/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0107\n",
      "Epoch 10/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0103\n",
      "Epoch 11/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0100\n",
      "Epoch 12/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0098\n",
      "Epoch 13/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0096\n",
      "Epoch 14/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0094\n",
      "Epoch 15/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0091\n",
      "Epoch 16/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0089\n",
      "Epoch 17/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0086\n",
      "Epoch 18/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0084\n",
      "Epoch 19/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0079\n",
      "Epoch 20/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0075\n",
      "Epoch 21/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0074\n",
      "Epoch 22/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0072\n",
      "Epoch 23/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0071\n",
      "Epoch 24/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0067\n",
      "Epoch 25/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0064\n",
      "Epoch 26/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0063\n",
      "Epoch 27/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0060\n",
      "Epoch 28/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0059\n",
      "Epoch 29/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0058\n",
      "Epoch 30/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0057\n",
      "Epoch 31/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0055\n",
      "Epoch 32/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0054\n",
      "Epoch 33/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0053\n",
      "Epoch 34/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0051\n",
      "Epoch 35/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0050\n",
      "Epoch 36/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0049\n",
      "Epoch 37/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0047\n",
      "Epoch 38/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0046\n",
      "Epoch 39/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0045\n",
      "Epoch 40/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0044\n",
      "Epoch 41/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0042\n",
      "Epoch 42/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0040\n",
      "Epoch 43/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0039\n",
      "Epoch 44/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0038\n",
      "Epoch 45/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0036\n",
      "Epoch 46/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0035\n",
      "Epoch 47/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0034\n",
      "Epoch 48/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0034\n",
      "Epoch 49/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0033\n",
      "Epoch 50/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0032\n",
      "Epoch 51/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0032\n",
      "Epoch 52/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0031\n",
      "Epoch 53/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0031\n",
      "Epoch 54/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0030\n",
      "Epoch 55/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0030\n",
      "Epoch 56/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0030\n",
      "Epoch 57/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0029\n",
      "Epoch 58/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0029\n",
      "Epoch 59/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0029\n",
      "Epoch 60/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0029\n",
      "Epoch 61/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0028\n",
      "Epoch 62/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0028\n",
      "Epoch 63/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0028\n",
      "Epoch 64/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0028\n",
      "Epoch 65/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0028\n",
      "Epoch 66/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 67/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 68/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 69/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 70/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 71/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 72/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0027\n",
      "Epoch 73/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 74/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 75/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 76/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 77/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 78/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 79/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 80/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 81/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 82/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 83/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 84/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 85/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 86/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 87/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 88/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 89/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 90/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 91/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 92/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 93/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 94/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 95/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 96/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 97/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 98/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 99/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0026\n",
      "Epoch 100/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 1/100\n",
      "20367/20367 [==============================] - 0s 21us/step - loss: 0.0275\n",
      "Epoch 2/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0161\n",
      "Epoch 3/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0143\n",
      "Epoch 4/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0135\n",
      "Epoch 5/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0126\n",
      "Epoch 6/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0116\n",
      "Epoch 7/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0109\n",
      "Epoch 8/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0104\n",
      "Epoch 9/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0100\n",
      "Epoch 10/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0098\n",
      "Epoch 11/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0097\n",
      "Epoch 12/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0096\n",
      "Epoch 13/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0095\n",
      "Epoch 14/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0093\n",
      "Epoch 15/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0092\n",
      "Epoch 16/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0090\n",
      "Epoch 17/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0088\n",
      "Epoch 18/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0086\n",
      "Epoch 19/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0084\n",
      "Epoch 20/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0081\n",
      "Epoch 21/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0076\n",
      "Epoch 22/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0073\n",
      "Epoch 23/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0071\n",
      "Epoch 24/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0070\n",
      "Epoch 25/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0068\n",
      "Epoch 26/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0066\n",
      "Epoch 27/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0058\n",
      "Epoch 28/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0051\n",
      "Epoch 29/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0049\n",
      "Epoch 30/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0047\n",
      "Epoch 31/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0046\n",
      "Epoch 32/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0044\n",
      "Epoch 33/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0043\n",
      "Epoch 34/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0041\n",
      "Epoch 35/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0039\n",
      "Epoch 36/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0038\n",
      "Epoch 37/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0036\n",
      "Epoch 38/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0035\n",
      "Epoch 39/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0034\n",
      "Epoch 40/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0032\n",
      "Epoch 41/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0031\n",
      "Epoch 42/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0030\n",
      "Epoch 43/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0029\n",
      "Epoch 44/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0028\n",
      "Epoch 45/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 46/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0026\n",
      "Epoch 47/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 48/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0025\n",
      "Epoch 49/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0024\n",
      "Epoch 50/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0024\n",
      "Epoch 51/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0023\n",
      "Epoch 52/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0023\n",
      "Epoch 53/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0022\n",
      "Epoch 54/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0022\n",
      "Epoch 55/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0022\n",
      "Epoch 56/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0021\n",
      "Epoch 57/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0021\n",
      "Epoch 58/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0021\n",
      "Epoch 59/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0021\n",
      "Epoch 60/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0021\n",
      "Epoch 61/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 62/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 63/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0020\n",
      "Epoch 64/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 65/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0020\n",
      "Epoch 66/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 67/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 68/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 69/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 70/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 71/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 72/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0020\n",
      "Epoch 73/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 74/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 75/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 76/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 78/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 79/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 80/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 81/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 82/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0020\n",
      "Epoch 83/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 84/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 85/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 86/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0020\n",
      "Epoch 87/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 88/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 89/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 90/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 91/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0020\n",
      "Epoch 92/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0020\n",
      "Epoch 93/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 94/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 95/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 96/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 97/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 98/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 99/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 100/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 1/100\n",
      "20367/20367 [==============================] - 0s 22us/step - loss: 0.0300\n",
      "Epoch 2/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0193\n",
      "Epoch 3/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0178\n",
      "Epoch 4/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0166\n",
      "Epoch 5/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0153\n",
      "Epoch 6/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0141\n",
      "Epoch 7/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0132\n",
      "Epoch 8/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0128\n",
      "Epoch 9/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0126\n",
      "Epoch 10/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0125\n",
      "Epoch 11/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0124\n",
      "Epoch 12/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0123\n",
      "Epoch 13/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0122\n",
      "Epoch 14/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0122\n",
      "Epoch 15/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0120\n",
      "Epoch 16/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0117\n",
      "Epoch 17/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0107\n",
      "Epoch 18/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0105\n",
      "Epoch 19/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0103\n",
      "Epoch 20/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0101\n",
      "Epoch 21/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0100\n",
      "Epoch 22/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0096\n",
      "Epoch 23/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0085\n",
      "Epoch 24/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0073\n",
      "Epoch 25/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0070\n",
      "Epoch 26/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0068\n",
      "Epoch 27/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0064\n",
      "Epoch 28/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0062\n",
      "Epoch 29/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0060\n",
      "Epoch 30/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0059\n",
      "Epoch 31/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0057\n",
      "Epoch 32/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0055\n",
      "Epoch 33/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0054\n",
      "Epoch 34/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0052\n",
      "Epoch 35/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0050\n",
      "Epoch 36/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0049\n",
      "Epoch 37/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0047\n",
      "Epoch 38/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0045\n",
      "Epoch 39/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0043\n",
      "Epoch 40/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0042\n",
      "Epoch 41/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0040\n",
      "Epoch 42/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0039\n",
      "Epoch 43/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0038\n",
      "Epoch 44/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0036\n",
      "Epoch 45/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0035\n",
      "Epoch 46/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0029\n",
      "Epoch 47/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 48/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0025\n",
      "Epoch 49/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0025\n",
      "Epoch 50/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0024\n",
      "Epoch 51/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0023\n",
      "Epoch 52/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0023\n",
      "Epoch 53/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0022\n",
      "Epoch 54/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0022\n",
      "Epoch 55/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0021\n",
      "Epoch 56/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0021\n",
      "Epoch 57/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0021\n",
      "Epoch 58/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0021\n",
      "Epoch 59/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 60/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 61/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0020\n",
      "Epoch 62/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 63/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 64/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 65/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 66/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 67/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 68/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 69/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 70/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0019\n",
      "Epoch 71/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0016\n",
      "Epoch 72/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0016\n",
      "Epoch 73/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0016\n",
      "Epoch 74/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0016\n",
      "Epoch 75/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 76/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 77/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 78/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 79/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0010\n",
      "Epoch 80/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0010\n",
      "Epoch 81/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 8.3669e-04\n",
      "Epoch 82/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3936e-04\n",
      "Epoch 83/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 1.8655e-04\n",
      "Epoch 84/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 1.6403e-04\n",
      "Epoch 85/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 1.4209e-04\n",
      "Epoch 86/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 1.2916e-04\n",
      "Epoch 87/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 1.2461e-04\n",
      "Epoch 88/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 1.1698e-04\n",
      "Epoch 89/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 1.0879e-04\n",
      "Epoch 90/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 1.0021e-04\n",
      "Epoch 91/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 1.0208e-04\n",
      "Epoch 92/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 9.2069e-05\n",
      "Epoch 93/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 8.8067e-05\n",
      "Epoch 94/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 8.6517e-05\n",
      "Epoch 95/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 8.6677e-05\n",
      "Epoch 96/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 8.7670e-05\n",
      "Epoch 97/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 8.2768e-05\n",
      "Epoch 98/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 8.4035e-05\n",
      "Epoch 99/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 8.3319e-05\n",
      "Epoch 100/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 8.1274e-05\n",
      "Epoch 1/100\n",
      "20367/20367 [==============================] - 0s 22us/step - loss: 0.0335\n",
      "Epoch 2/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0206\n",
      "Epoch 3/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0178\n",
      "Epoch 4/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0150\n",
      "Epoch 5/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0131\n",
      "Epoch 6/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0124\n",
      "Epoch 7/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0121\n",
      "Epoch 8/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0120\n",
      "Epoch 9/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0120\n",
      "Epoch 10/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0119\n",
      "Epoch 11/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0118\n",
      "Epoch 12/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0118\n",
      "Epoch 13/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0117\n",
      "Epoch 14/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0115\n",
      "Epoch 15/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0109\n",
      "Epoch 16/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0103\n",
      "Epoch 17/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0090\n",
      "Epoch 18/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0080\n",
      "Epoch 19/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0077\n",
      "Epoch 20/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0074\n",
      "Epoch 21/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0071\n",
      "Epoch 22/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0068\n",
      "Epoch 23/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0066\n",
      "Epoch 24/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0064\n",
      "Epoch 25/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0062\n",
      "Epoch 26/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0061\n",
      "Epoch 27/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0059\n",
      "Epoch 28/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0057\n",
      "Epoch 29/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0055\n",
      "Epoch 30/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0053\n",
      "Epoch 31/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0046\n",
      "Epoch 32/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0031\n",
      "Epoch 33/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0028\n",
      "Epoch 34/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0026\n",
      "Epoch 35/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0024\n",
      "Epoch 36/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0022\n",
      "Epoch 37/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 38/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0019\n",
      "Epoch 39/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0017\n",
      "Epoch 40/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0015\n",
      "Epoch 41/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0014\n",
      "Epoch 42/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 43/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 44/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 45/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 9.5552e-04\n",
      "Epoch 46/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 8.6272e-04\n",
      "Epoch 47/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 7.8822e-04\n",
      "Epoch 48/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 7.1199e-04\n",
      "Epoch 49/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 6.4797e-04\n",
      "Epoch 50/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 6.0043e-04\n",
      "Epoch 51/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 5.4779e-04\n",
      "Epoch 52/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 5.2434e-04\n",
      "Epoch 53/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.8790e-04\n",
      "Epoch 54/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.6621e-04\n",
      "Epoch 55/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.4268e-04\n",
      "Epoch 56/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.3281e-04\n",
      "Epoch 57/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.1729e-04\n",
      "Epoch 58/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.1419e-04\n",
      "Epoch 59/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.0983e-04\n",
      "Epoch 60/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.0439e-04\n",
      "Epoch 61/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.0074e-04\n",
      "Epoch 62/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9557e-04\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9757e-04\n",
      "Epoch 64/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9483e-04\n",
      "Epoch 65/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8937e-04\n",
      "Epoch 66/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8892e-04\n",
      "Epoch 67/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8926e-04\n",
      "Epoch 68/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8790e-04\n",
      "Epoch 69/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8331e-04\n",
      "Epoch 70/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9323e-04\n",
      "Epoch 71/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8743e-04\n",
      "Epoch 72/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8973e-04\n",
      "Epoch 73/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8596e-04\n",
      "Epoch 74/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8550e-04\n",
      "Epoch 75/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8520e-04\n",
      "Epoch 76/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9082e-04\n",
      "Epoch 77/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7951e-04\n",
      "Epoch 78/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 3.8088e-04\n",
      "Epoch 79/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8388e-04\n",
      "Epoch 80/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8219e-04\n",
      "Epoch 81/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 3.8508e-04\n",
      "Epoch 82/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8652e-04\n",
      "Epoch 83/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8186e-04\n",
      "Epoch 84/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8332e-04\n",
      "Epoch 85/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8572e-04\n",
      "Epoch 86/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8483e-04\n",
      "Epoch 87/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8235e-04\n",
      "Epoch 88/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8738e-04\n",
      "Epoch 89/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8452e-04\n",
      "Epoch 90/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8303e-04\n",
      "Epoch 91/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8574e-04\n",
      "Epoch 92/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7863e-04\n",
      "Epoch 93/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8914e-04\n",
      "Epoch 94/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8902e-04\n",
      "Epoch 95/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8516e-04\n",
      "Epoch 96/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8983e-04\n",
      "Epoch 97/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8275e-04\n",
      "Epoch 98/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8463e-04\n",
      "Epoch 99/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8120e-04\n",
      "Epoch 100/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8657e-04\n",
      "Epoch 1/100\n",
      "20367/20367 [==============================] - 0s 22us/step - loss: 0.0376\n",
      "Epoch 2/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0244\n",
      "Epoch 3/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0198\n",
      "Epoch 4/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0143\n",
      "Epoch 5/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0119\n",
      "Epoch 6/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0116\n",
      "Epoch 7/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0108\n",
      "Epoch 8/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0090\n",
      "Epoch 9/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0089\n",
      "Epoch 10/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0088\n",
      "Epoch 11/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0087\n",
      "Epoch 12/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0086\n",
      "Epoch 13/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0085\n",
      "Epoch 14/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0083\n",
      "Epoch 15/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0080\n",
      "Epoch 16/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0077\n",
      "Epoch 17/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0074\n",
      "Epoch 18/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0071\n",
      "Epoch 19/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0067\n",
      "Epoch 20/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0064\n",
      "Epoch 21/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0061\n",
      "Epoch 22/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0059\n",
      "Epoch 23/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0057\n",
      "Epoch 24/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0055\n",
      "Epoch 25/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0053\n",
      "Epoch 26/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0051\n",
      "Epoch 27/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0048\n",
      "Epoch 28/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0046\n",
      "Epoch 29/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0043\n",
      "Epoch 30/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0040\n",
      "Epoch 31/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0037\n",
      "Epoch 32/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0034\n",
      "Epoch 33/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0032\n",
      "Epoch 34/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0029\n",
      "Epoch 35/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0026\n",
      "Epoch 36/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0024\n",
      "Epoch 37/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0022\n",
      "Epoch 38/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 39/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0019\n",
      "Epoch 40/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0018\n",
      "Epoch 41/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0017\n",
      "Epoch 42/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0016\n",
      "Epoch 43/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0015\n",
      "Epoch 44/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0014\n",
      "Epoch 45/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0014\n",
      "Epoch 46/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 47/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 48/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 49/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 50/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 51/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 52/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 53/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 54/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 55/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 56/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 57/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 58/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 59/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 60/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 61/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 62/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 63/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 64/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 65/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 66/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 67/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 68/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 69/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 70/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 71/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 72/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 73/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 74/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 75/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 76/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 77/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 78/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 79/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 80/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 81/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 82/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 83/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 84/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 85/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 86/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 87/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 88/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 89/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 90/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0012\n",
      "Epoch 91/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 92/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 93/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 94/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 95/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 96/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0012\n",
      "Epoch 97/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 98/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 99/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 100/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 1/100\n",
      "20367/20367 [==============================] - 0s 23us/step - loss: 0.0480\n",
      "Epoch 2/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0312\n",
      "Epoch 3/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0217\n",
      "Epoch 4/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0165\n",
      "Epoch 5/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0150\n",
      "Epoch 6/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0120\n",
      "Epoch 7/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0116\n",
      "Epoch 8/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0116\n",
      "Epoch 9/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0115\n",
      "Epoch 10/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0114\n",
      "Epoch 11/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0113\n",
      "Epoch 12/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0110\n",
      "Epoch 13/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0107\n",
      "Epoch 14/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0084\n",
      "Epoch 15/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0078\n",
      "Epoch 16/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0073\n",
      "Epoch 17/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0069\n",
      "Epoch 18/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0064\n",
      "Epoch 19/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0061\n",
      "Epoch 20/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0058\n",
      "Epoch 21/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0055\n",
      "Epoch 22/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0052\n",
      "Epoch 23/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0049\n",
      "Epoch 24/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0046\n",
      "Epoch 25/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0043\n",
      "Epoch 26/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0040\n",
      "Epoch 27/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0036\n",
      "Epoch 28/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0033\n",
      "Epoch 29/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0030\n",
      "Epoch 30/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 31/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0023\n",
      "Epoch 32/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0021\n",
      "Epoch 33/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0019\n",
      "Epoch 34/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0017\n",
      "Epoch 35/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0015\n",
      "Epoch 36/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 37/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0011\n",
      "Epoch 38/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 9.8664e-04\n",
      "Epoch 39/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 8.9009e-04\n",
      "Epoch 40/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 7.7286e-04\n",
      "Epoch 41/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 6.9687e-04\n",
      "Epoch 42/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 6.0417e-04\n",
      "Epoch 43/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 5.3760e-04\n",
      "Epoch 44/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.7115e-04\n",
      "Epoch 45/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8512e-04\n",
      "Epoch 46/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7542e-04\n",
      "Epoch 47/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4454e-04\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.9334e-04\n",
      "Epoch 49/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.8370e-04\n",
      "Epoch 50/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.7950e-04\n",
      "Epoch 51/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.6956e-04\n",
      "Epoch 52/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 2.7080e-04\n",
      "Epoch 53/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.5913e-04\n",
      "Epoch 54/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.4664e-04\n",
      "Epoch 55/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.6229e-04\n",
      "Epoch 56/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.5570e-04\n",
      "Epoch 57/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 2.3702e-04\n",
      "Epoch 58/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.6593e-04\n",
      "Epoch 59/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.6354e-04\n",
      "Epoch 60/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 2.6082e-04\n",
      "Epoch 61/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3363e-04\n",
      "Epoch 62/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.8512e-04\n",
      "Epoch 63/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.5057e-04\n",
      "Epoch 64/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3591e-04\n",
      "Epoch 65/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3408e-04\n",
      "Epoch 66/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.2769e-04\n",
      "Epoch 67/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.5984e-04\n",
      "Epoch 68/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3722e-04\n",
      "Epoch 69/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.5091e-04\n",
      "Epoch 70/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.4562e-04\n",
      "Epoch 71/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.4730e-04\n",
      "Epoch 72/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.5445e-04\n",
      "Epoch 73/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.2701e-04\n",
      "Epoch 74/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 2.3927e-04\n",
      "Epoch 75/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.4784e-04\n",
      "Epoch 76/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3575e-04\n",
      "Epoch 77/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.5127e-04\n",
      "Epoch 78/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3424e-04\n",
      "Epoch 79/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.4694e-04\n",
      "Epoch 80/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3932e-04\n",
      "Epoch 81/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 2.5602e-04\n",
      "Epoch 82/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.4871e-04\n",
      "Epoch 83/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.5069e-04\n",
      "Epoch 84/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.4370e-04\n",
      "Epoch 85/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3577e-04\n",
      "Epoch 86/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.5957e-04\n",
      "Epoch 87/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3317e-04\n",
      "Epoch 88/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.4039e-04\n",
      "Epoch 89/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.1529e-04\n",
      "Epoch 90/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3254e-04\n",
      "Epoch 91/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3409e-04\n",
      "Epoch 92/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.6878e-04\n",
      "Epoch 93/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3306e-04\n",
      "Epoch 94/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.2695e-04\n",
      "Epoch 95/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.1971e-04\n",
      "Epoch 96/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3591e-04\n",
      "Epoch 97/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.3237e-04\n",
      "Epoch 98/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.2935e-04\n",
      "Epoch 99/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.4725e-04\n",
      "Epoch 100/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.4016e-04\n",
      "Epoch 1/100\n",
      "20367/20367 [==============================] - 0s 24us/step - loss: 0.0636\n",
      "Epoch 2/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0525\n",
      "Epoch 3/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0426\n",
      "Epoch 4/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0308\n",
      "Epoch 5/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0220\n",
      "Epoch 6/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0188\n",
      "Epoch 7/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0186\n",
      "Epoch 8/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0179\n",
      "Epoch 9/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0122\n",
      "Epoch 10/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0113\n",
      "Epoch 11/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0108\n",
      "Epoch 12/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0102\n",
      "Epoch 13/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0095\n",
      "Epoch 14/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0088\n",
      "Epoch 15/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0081\n",
      "Epoch 16/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0076\n",
      "Epoch 17/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0071\n",
      "Epoch 18/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0066\n",
      "Epoch 19/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0061\n",
      "Epoch 20/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0056\n",
      "Epoch 21/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0051\n",
      "Epoch 22/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0045\n",
      "Epoch 23/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0040\n",
      "Epoch 24/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0034\n",
      "Epoch 25/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0029\n",
      "Epoch 26/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0025\n",
      "Epoch 27/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0020\n",
      "Epoch 28/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0016\n",
      "Epoch 29/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0013\n",
      "Epoch 30/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0010\n",
      "Epoch 31/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 8.7482e-04\n",
      "Epoch 32/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 7.5827e-04\n",
      "Epoch 33/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 6.1318e-04\n",
      "Epoch 34/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 5.1518e-04\n",
      "Epoch 35/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.7162e-04\n",
      "Epoch 36/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.1894e-04\n",
      "Epoch 37/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8776e-04\n",
      "Epoch 38/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6815e-04\n",
      "Epoch 39/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9487e-04\n",
      "Epoch 40/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.5515e-04\n",
      "Epoch 41/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.1015e-04\n",
      "Epoch 42/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.0370e-04\n",
      "Epoch 43/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7412e-04\n",
      "Epoch 44/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8123e-04\n",
      "Epoch 45/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7821e-04\n",
      "Epoch 46/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6904e-04\n",
      "Epoch 47/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8899e-04\n",
      "Epoch 48/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4130e-04\n",
      "Epoch 49/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9719e-04\n",
      "Epoch 50/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4273e-04\n",
      "Epoch 51/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.3740e-04\n",
      "Epoch 52/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6956e-04\n",
      "Epoch 53/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4408e-04\n",
      "Epoch 54/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.3897e-04\n",
      "Epoch 55/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9874e-04\n",
      "Epoch 56/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6198e-04\n",
      "Epoch 57/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6754e-04\n",
      "Epoch 58/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.5780e-04\n",
      "Epoch 59/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6674e-04\n",
      "Epoch 60/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7436e-04\n",
      "Epoch 61/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4497e-04\n",
      "Epoch 62/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8627e-04\n",
      "Epoch 63/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.2467e-04\n",
      "Epoch 64/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4085e-04\n",
      "Epoch 65/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.5655e-04\n",
      "Epoch 66/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.3567e-04\n",
      "Epoch 67/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.5777e-04\n",
      "Epoch 68/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.5265e-04\n",
      "Epoch 69/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4756e-04\n",
      "Epoch 70/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.3397e-04\n",
      "Epoch 71/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6855e-04\n",
      "Epoch 72/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.5807e-04\n",
      "Epoch 73/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.3672e-04\n",
      "Epoch 74/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.2881e-04\n",
      "Epoch 75/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4674e-04\n",
      "Epoch 76/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.5306e-04\n",
      "Epoch 77/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.3731e-04\n",
      "Epoch 78/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.1915e-04\n",
      "Epoch 79/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.2842e-04\n",
      "Epoch 80/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.1258e-04\n",
      "Epoch 81/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.2343e-04\n",
      "Epoch 82/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7018e-04\n",
      "Epoch 83/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.0538e-04\n",
      "Epoch 84/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.2078e-04\n",
      "Epoch 85/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 2.8526e-04\n",
      "Epoch 86/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.3905e-04\n",
      "Epoch 87/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4108e-04\n",
      "Epoch 88/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.5822e-04\n",
      "Epoch 89/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6760e-04\n",
      "Epoch 90/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.3319e-04\n",
      "Epoch 91/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7535e-04\n",
      "Epoch 92/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4462e-04\n",
      "Epoch 93/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4817e-04\n",
      "Epoch 94/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.3155e-04\n",
      "Epoch 95/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.0885e-04\n",
      "Epoch 96/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.2657e-04\n",
      "Epoch 97/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.0305e-04\n",
      "Epoch 98/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.3852e-04\n",
      "Epoch 99/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 3.3985e-04\n",
      "Epoch 100/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.2521e-04\n",
      "Epoch 1/100\n",
      "20367/20367 [==============================] - 1s 25us/step - loss: 0.0711\n",
      "Epoch 2/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0481\n",
      "Epoch 3/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0230\n",
      "Epoch 4/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0175\n",
      "Epoch 5/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0172\n",
      "Epoch 6/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0170\n",
      "Epoch 7/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0165\n",
      "Epoch 8/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0158\n",
      "Epoch 9/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0146\n",
      "Epoch 10/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0132\n",
      "Epoch 11/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0119\n",
      "Epoch 12/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0109\n",
      "Epoch 13/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0101\n",
      "Epoch 14/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0091\n",
      "Epoch 15/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0081\n",
      "Epoch 16/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0070\n",
      "Epoch 17/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0060\n",
      "Epoch 18/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0051\n",
      "Epoch 19/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 0.0043\n",
      "Epoch 20/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0037\n",
      "Epoch 21/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0031\n",
      "Epoch 22/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0027\n",
      "Epoch 23/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0022\n",
      "Epoch 24/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0019\n",
      "Epoch 25/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0015\n",
      "Epoch 26/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 0.0012\n",
      "Epoch 27/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 9.7108e-04\n",
      "Epoch 28/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 7.6577e-04\n",
      "Epoch 29/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 6.6962e-04\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20367/20367 [==============================] - 0s 4us/step - loss: 5.7318e-04\n",
      "Epoch 31/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 5.7252e-04\n",
      "Epoch 32/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 5.0932e-04\n",
      "Epoch 33/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.4567e-04\n",
      "Epoch 34/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.1801e-04\n",
      "Epoch 35/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.5777e-04\n",
      "Epoch 36/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.0571e-04\n",
      "Epoch 37/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.4778e-04\n",
      "Epoch 38/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.6412e-04\n",
      "Epoch 39/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.9864e-04\n",
      "Epoch 40/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.3632e-04\n",
      "Epoch 41/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.3650e-04\n",
      "Epoch 42/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.8179e-04\n",
      "Epoch 43/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.5218e-04\n",
      "Epoch 44/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.2504e-04\n",
      "Epoch 45/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.3115e-04\n",
      "Epoch 46/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.4184e-04\n",
      "Epoch 47/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.4529e-04\n",
      "Epoch 48/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.3130e-04\n",
      "Epoch 49/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.2248e-04\n",
      "Epoch 50/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.5856e-04\n",
      "Epoch 51/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.4683e-04\n",
      "Epoch 52/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8124e-04\n",
      "Epoch 53/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.2814e-04\n",
      "Epoch 54/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.5214e-04\n",
      "Epoch 55/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.3852e-04\n",
      "Epoch 56/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.4642e-04\n",
      "Epoch 57/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.1916e-04\n",
      "Epoch 58/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.2817e-04\n",
      "Epoch 59/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.3757e-04\n",
      "Epoch 60/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.1564e-04\n",
      "Epoch 61/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.6047e-04\n",
      "Epoch 62/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9057e-04\n",
      "Epoch 63/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.1098e-04\n",
      "Epoch 64/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8163e-04\n",
      "Epoch 65/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9072e-04\n",
      "Epoch 66/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.1962e-04\n",
      "Epoch 67/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.3804e-04\n",
      "Epoch 68/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.3554e-04\n",
      "Epoch 69/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8124e-04\n",
      "Epoch 70/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.3926e-04\n",
      "Epoch 71/100\n",
      "20367/20367 [==============================] - 0s 5us/step - loss: 3.8765e-04\n",
      "Epoch 72/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7596e-04\n",
      "Epoch 73/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.0884e-04\n",
      "Epoch 74/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7499e-04\n",
      "Epoch 75/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 4.2393e-04\n",
      "Epoch 76/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9450e-04\n",
      "Epoch 77/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4282e-04\n",
      "Epoch 78/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9317e-04\n",
      "Epoch 79/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6610e-04\n",
      "Epoch 80/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7935e-04\n",
      "Epoch 81/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.2676e-04\n",
      "Epoch 82/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8970e-04\n",
      "Epoch 83/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4913e-04\n",
      "Epoch 84/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7237e-04\n",
      "Epoch 85/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.9251e-04\n",
      "Epoch 86/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6328e-04\n",
      "Epoch 87/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.3143e-04\n",
      "Epoch 88/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6873e-04\n",
      "Epoch 89/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4966e-04\n",
      "Epoch 90/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8901e-04\n",
      "Epoch 91/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8836e-04\n",
      "Epoch 92/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7135e-04\n",
      "Epoch 93/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6815e-04\n",
      "Epoch 94/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.5903e-04\n",
      "Epoch 95/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.8859e-04\n",
      "Epoch 96/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.4206e-04\n",
      "Epoch 97/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7382e-04\n",
      "Epoch 98/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.7773e-04\n",
      "Epoch 99/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.5792e-04\n",
      "Epoch 100/100\n",
      "20367/20367 [==============================] - 0s 4us/step - loss: 3.6092e-04\n"
     ]
    }
   ],
   "source": [
    "current_input = X\n",
    "histories =[]\n",
    "\n",
    "# Greedy Layer Wise Training\n",
    "for i, ae in enumerate(autoencoders):\n",
    "    if i > 0:\n",
    "        weights = encoders[i-1].get_weights()\n",
    "        dense_layer = Dense(layer_sizes[i], input_shape=(current_input.shape[1],),\n",
    "                            activation='relu', weights=weights,\n",
    "                            name='encoder_dense_copy_%d'%i)\n",
    "        encoder_model = Sequential([dense_layer])\n",
    "        encoder_model.compile(loss='mse', optimizer=SGD(lr=0.1, decay=0, momentum=0.9))\n",
    "        current_input = encoder_model.predict(current_input)\n",
    "    histories.append(ae.fit(current_input, current_input,\n",
    "                    batch_size=512, epochs=100))\n",
    "    \n",
    "    autoencoder.layers[i].set_weights(ae.layers[1].get_weights())\n",
    "    autoencoder.layers[len(autoencoder.layers)-i-1].set_weights(ae.layers[-1].get_weights())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "tot = len(histories)\n",
    "cols = 5\n",
    "rows = tot//cols\n",
    "rows += tot%cols\n",
    "\n",
    "pos = range(1,tot+1)\n",
    "print(tot)\n",
    "fig = plt.figure(1, )\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "for i,h in enumerate(histories):\n",
    "    ax = fig.add_subplot(rows,cols,pos[i])\n",
    "    loss = h.history['loss']\n",
    "    epochs = range(1, len(loss)+1)\n",
    "    ax.plot(epochs, loss)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "20367/20367 [==============================] - 1s 37us/step - loss: 0.0027\n",
      "Epoch 2/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0023\n",
      "Epoch 3/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0023\n",
      "Epoch 4/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0023\n",
      "Epoch 5/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0023\n",
      "Epoch 6/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0023\n",
      "Epoch 7/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0022\n",
      "Epoch 8/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0022\n",
      "Epoch 9/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0022\n",
      "Epoch 10/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0022\n",
      "Epoch 11/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0022\n",
      "Epoch 12/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0022\n",
      "Epoch 13/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0022\n",
      "Epoch 14/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0022\n",
      "Epoch 15/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0021\n",
      "Epoch 16/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0021\n",
      "Epoch 17/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0021\n",
      "Epoch 18/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0021\n",
      "Epoch 19/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0021\n",
      "Epoch 20/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0021\n",
      "Epoch 21/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0021\n",
      "Epoch 22/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0020\n",
      "Epoch 23/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0020\n",
      "Epoch 24/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0020\n",
      "Epoch 25/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0020\n",
      "Epoch 26/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0020\n",
      "Epoch 27/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0020\n",
      "Epoch 28/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0019\n",
      "Epoch 29/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0019\n",
      "Epoch 30/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0019\n",
      "Epoch 31/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0019\n",
      "Epoch 32/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0019\n",
      "Epoch 33/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0019\n",
      "Epoch 34/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0019\n",
      "Epoch 35/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0019\n",
      "Epoch 36/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0018\n",
      "Epoch 37/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0018\n",
      "Epoch 38/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0018\n",
      "Epoch 39/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0018\n",
      "Epoch 40/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0018\n",
      "Epoch 41/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0018\n",
      "Epoch 42/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0018\n",
      "Epoch 43/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0018\n",
      "Epoch 44/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0018\n",
      "Epoch 45/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0018\n",
      "Epoch 46/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0018\n",
      "Epoch 47/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0018\n",
      "Epoch 48/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0018\n",
      "Epoch 49/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0018\n",
      "Epoch 50/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0017\n",
      "Epoch 51/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 52/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0017\n",
      "Epoch 53/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 54/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 55/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0017\n",
      "Epoch 56/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0017\n",
      "Epoch 57/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0017\n",
      "Epoch 58/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 59/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 60/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 61/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0017\n",
      "Epoch 62/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0017\n",
      "Epoch 63/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0017\n",
      "Epoch 64/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 65/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0017\n",
      "Epoch 66/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0017\n",
      "Epoch 67/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 68/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0017\n",
      "Epoch 69/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 70/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 71/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 72/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 73/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0017\n",
      "Epoch 74/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0017\n",
      "Epoch 75/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0016\n",
      "Epoch 76/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 77/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 78/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 79/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 80/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 81/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 82/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 83/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 84/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 85/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 86/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0016\n",
      "Epoch 87/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 88/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 89/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0016\n",
      "Epoch 90/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 91/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 92/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 93/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0016\n",
      "Epoch 94/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 95/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0016\n",
      "Epoch 96/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 97/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 98/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 99/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 100/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0016\n",
      "Epoch 101/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 102/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0016\n",
      "Epoch 103/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 104/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 105/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 106/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 107/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 108/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 109/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 110/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 111/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 112/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 113/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 114/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 115/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 116/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 117/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 118/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 119/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 120/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 121/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 122/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 123/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 124/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 125/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 126/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 127/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 128/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 129/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 130/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 131/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 132/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 133/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 134/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 135/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 136/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 137/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 138/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 139/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 140/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 141/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 142/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 143/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 144/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 145/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 146/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 147/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 148/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 149/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 150/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 151/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0015\n",
      "Epoch 152/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 153/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0015\n",
      "Epoch 154/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 155/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 156/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0014\n",
      "Epoch 157/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 158/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 159/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 160/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 161/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0014\n",
      "Epoch 162/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 163/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 164/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 165/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 166/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 167/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 168/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 169/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 170/200\n",
      "20367/20367 [==============================] - 0s 10us/step - loss: 0.0014\n",
      "Epoch 171/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 172/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 173/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 174/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 175/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 176/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 177/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0014\n",
      "Epoch 178/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0014\n",
      "Epoch 179/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 180/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 181/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 182/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 183/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 184/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 185/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 186/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 187/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 188/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0014\n",
      "Epoch 189/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 190/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0014\n",
      "Epoch 191/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 192/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 193/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 194/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 195/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0014\n",
      "Epoch 196/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 197/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 198/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n",
      "Epoch 199/200\n",
      "20367/20367 [==============================] - 0s 11us/step - loss: 0.0014\n",
      "Epoch 200/200\n",
      "20367/20367 [==============================] - 0s 12us/step - loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(X,X, batch_size=512, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save_weights('autoencoder_smaller.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss)+1)\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks nice   so what we ought to do is to drop the decoder into the front of our basic neural net \n",
    "# and see what kinda improvements we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights('autoencoder_smaller.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,layer in enumerate(encoder.layers):\n",
    "    layer.set_weights(autoencoder.layers[i].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS IS THE ACTUAL DATA CLEAN CELL\n",
    "k_data = kepler_data.drop(['tce_rogue_flag', 'tce_delivname', 'rowupdate', \n",
    "                           'tce_datalink_dvs', 'tce_datalink_dvr', 'tce_steff_prov', \n",
    "                           'tce_slogg_prov', 'tce_smet_prov','tce_sradius_prov', 'tce_limbdark_mod',\n",
    "                           'tce_trans_mod', 'tce_eccen', 'tce_eccen_err', 'tce_longp', 'tce_longp_err'\n",
    "                          ],axis=1)\n",
    "# drop some columns that we will not be using\n",
    "# starting with all NA columns\n",
    "k_data.dropna(axis=1,how='any', inplace=True)\n",
    "\n",
    "# drop the Autovetter stuff\n",
    "autovetter_cols = ['av_vf_pc', 'av_vf_pc_err', 'av_vf_afp',\n",
    "                   'av_vf_afp_err', 'av_vf_ntp', 'av_vf_ntp_err', \n",
    "                   'av_pp_pc','av_pp_afp', 'av_pp_ntp', \n",
    "                   'av_training_set', 'av_pred_class']\n",
    "# drop all autovetter columns execpt the training set - as those are manually set - take as \"truth\"\n",
    "\n",
    "k_data = k_data.drop(['av_vf_pc', 'av_vf_pc_err', 'av_vf_afp','av_vf_afp_err',\n",
    "                           'av_vf_ntp', 'av_vf_ntp_err', 'av_pp_pc','av_pp_afp', \n",
    "                           'av_pp_ntp', 'av_pred_class'],axis=1)\n",
    "\n",
    "col_names = k_data.columns.values\n",
    "\n",
    "k_data = pd.get_dummies(k_data, columns=['av_training_set'])\n",
    "\n",
    "\n",
    "# train and test split\n",
    "y = k_data[['av_training_set_AFP','av_training_set_NTP','av_training_set_PC', 'av_training_set_UNK']]\n",
    "\n",
    "# update to be categorical\n",
    "x = k_data\n",
    "x = k_data.drop(['av_training_set_AFP','av_training_set_NTP','av_training_set_PC', 'av_training_set_UNK'], axis=1)\n",
    "#x = k_data.drop(['rowid','kepid'], axis=1)\n",
    "#x = k_data[['tce_mesmad','tce_maxmes', 'tce_minmes', 'tce_maxmesd', 'tce_minmesd', 'tce_plnt_num']]\n",
    "x = (x - x.mean()) / (x.max() - x.min())\n",
    "\n",
    "train_test_split_size = 5000\n",
    "x_test = x[:train_test_split_size]\n",
    "y_test = y[:train_test_split_size]\n",
    "\n",
    "x_train = x[train_test_split_size:]\n",
    "y_train = y[train_test_split_size:]\n",
    "\n",
    "train_val_split_size = 1000\n",
    "\n",
    "x_val = x_train[:train_val_split_size]\n",
    "y_val = y_train[:train_val_split_size]\n",
    "x_train = x_train[train_val_split_size:]\n",
    "y_train = y_train[train_val_split_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_13 (Sequential)   (None, 10)                42850     \n",
      "_________________________________________________________________\n",
      "sequential_24 (Sequential)   (None, 4)                 4054      \n",
      "=================================================================\n",
      "Total params: 46,904\n",
      "Trainable params: 46,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# be really dumb and just attempt to pass into a net\n",
    "deep_classifier = models.Sequential()\n",
    "deep_classifier.add(layers.Dense(10,kernel_initializer='random_normal',\n",
    "                       kernel_regularizer=regularizers.l2(0.01),\n",
    "                       activation='relu', \n",
    "                       input_shape=(10,)))\n",
    "deep_classifier.add(layers.Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "deep_classifier.add(layers.Dense(40, activation='relu', kernel_regularizer=regularizers.l2(0.01),))\n",
    "deep_classifier.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.l2(0.01),))\n",
    "deep_classifier.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model = Sequential([encoder,\n",
    "                   deep_classifier])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14367 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "14367/14367 [==============================] - 1s 48us/step - loss: 2.1766 - acc: 0.4184 - val_loss: 1.9651 - val_acc: 0.4720\n",
      "Epoch 2/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 1.8660 - acc: 0.4537 - val_loss: 1.7176 - val_acc: 0.4720\n",
      "Epoch 3/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.6298 - acc: 0.4537 - val_loss: 1.5253 - val_acc: 0.4720\n",
      "Epoch 4/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 1.4646 - acc: 0.4992 - val_loss: 1.3913 - val_acc: 0.5790\n",
      "Epoch 5/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 1.3189 - acc: 0.5992 - val_loss: 1.2560 - val_acc: 0.5940\n",
      "Epoch 6/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.1361 - acc: 0.6133 - val_loss: 1.1146 - val_acc: 0.5940\n",
      "Epoch 7/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 1.0384 - acc: 0.6163 - val_loss: 1.0555 - val_acc: 0.5930\n",
      "Epoch 8/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.9887 - acc: 0.6168 - val_loss: 1.0441 - val_acc: 0.5930\n",
      "Epoch 9/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.9564 - acc: 0.6184 - val_loss: 1.0247 - val_acc: 0.5750\n",
      "Epoch 10/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.9309 - acc: 0.6237 - val_loss: 1.0560 - val_acc: 0.5690\n",
      "Epoch 11/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.9274 - acc: 0.6251 - val_loss: 0.9668 - val_acc: 0.5990\n",
      "Epoch 12/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8981 - acc: 0.6419 - val_loss: 0.9695 - val_acc: 0.6110\n",
      "Epoch 13/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8877 - acc: 0.6507 - val_loss: 0.9450 - val_acc: 0.6150\n",
      "Epoch 14/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8727 - acc: 0.6499 - val_loss: 0.9247 - val_acc: 0.6260\n",
      "Epoch 15/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8559 - acc: 0.6519 - val_loss: 0.9384 - val_acc: 0.6070\n",
      "Epoch 16/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.8498 - acc: 0.6506 - val_loss: 0.9165 - val_acc: 0.6140\n",
      "Epoch 17/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8414 - acc: 0.6548 - val_loss: 0.9040 - val_acc: 0.6270\n",
      "Epoch 18/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8361 - acc: 0.6576 - val_loss: 0.9323 - val_acc: 0.6140\n",
      "Epoch 19/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.8260 - acc: 0.6583 - val_loss: 0.9028 - val_acc: 0.6150\n",
      "Epoch 20/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.8254 - acc: 0.6564 - val_loss: 0.9000 - val_acc: 0.6200\n",
      "Epoch 21/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.8162 - acc: 0.6589 - val_loss: 0.8926 - val_acc: 0.6170\n",
      "Epoch 22/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8103 - acc: 0.6620 - val_loss: 0.9131 - val_acc: 0.6080\n",
      "Epoch 23/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8171 - acc: 0.6527 - val_loss: 0.8918 - val_acc: 0.6190\n",
      "Epoch 24/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.8072 - acc: 0.6606 - val_loss: 0.8946 - val_acc: 0.6290\n",
      "Epoch 25/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.8082 - acc: 0.6637 - val_loss: 0.8694 - val_acc: 0.6220\n",
      "Epoch 26/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.8026 - acc: 0.6633 - val_loss: 0.8820 - val_acc: 0.6310\n",
      "Epoch 27/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7955 - acc: 0.6648 - val_loss: 0.8672 - val_acc: 0.6280\n",
      "Epoch 28/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7857 - acc: 0.6653 - val_loss: 0.9032 - val_acc: 0.6230\n",
      "Epoch 29/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.7845 - acc: 0.6661 - val_loss: 0.8652 - val_acc: 0.6210\n",
      "Epoch 30/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7839 - acc: 0.6658 - val_loss: 0.8753 - val_acc: 0.6230\n",
      "Epoch 31/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7801 - acc: 0.6651 - val_loss: 0.8619 - val_acc: 0.6170\n",
      "Epoch 32/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7703 - acc: 0.6701 - val_loss: 0.8668 - val_acc: 0.6270\n",
      "Epoch 33/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7701 - acc: 0.6720 - val_loss: 0.8535 - val_acc: 0.6350\n",
      "Epoch 34/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7687 - acc: 0.6719 - val_loss: 0.9061 - val_acc: 0.6030\n",
      "Epoch 35/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.7812 - acc: 0.6652 - val_loss: 0.8602 - val_acc: 0.6290\n",
      "Epoch 36/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7681 - acc: 0.6691 - val_loss: 0.8517 - val_acc: 0.6330\n",
      "Epoch 37/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7581 - acc: 0.6766 - val_loss: 0.8588 - val_acc: 0.6460\n",
      "Epoch 38/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.7531 - acc: 0.6768 - val_loss: 0.8498 - val_acc: 0.6400\n",
      "Epoch 39/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7472 - acc: 0.6788 - val_loss: 0.8385 - val_acc: 0.6520\n",
      "Epoch 40/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7478 - acc: 0.6800 - val_loss: 0.8448 - val_acc: 0.6270\n",
      "Epoch 41/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7478 - acc: 0.6732 - val_loss: 0.8383 - val_acc: 0.6360\n",
      "Epoch 42/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7425 - acc: 0.6811 - val_loss: 0.8680 - val_acc: 0.6310\n",
      "Epoch 43/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7464 - acc: 0.6789 - val_loss: 0.8365 - val_acc: 0.6580\n",
      "Epoch 44/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7404 - acc: 0.6843 - val_loss: 0.8227 - val_acc: 0.6480\n",
      "Epoch 45/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7307 - acc: 0.6849 - val_loss: 0.8348 - val_acc: 0.6370\n",
      "Epoch 46/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.7334 - acc: 0.6781 - val_loss: 0.8239 - val_acc: 0.6430\n",
      "Epoch 47/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7263 - acc: 0.6840 - val_loss: 0.8285 - val_acc: 0.6490\n",
      "Epoch 48/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7243 - acc: 0.6873 - val_loss: 0.8443 - val_acc: 0.6430\n",
      "Epoch 49/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7277 - acc: 0.6886 - val_loss: 0.8536 - val_acc: 0.6370\n",
      "Epoch 50/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7240 - acc: 0.6842 - val_loss: 0.8257 - val_acc: 0.6500\n",
      "Epoch 51/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7150 - acc: 0.6891 - val_loss: 0.8324 - val_acc: 0.6450\n",
      "Epoch 52/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7179 - acc: 0.6879 - val_loss: 0.8285 - val_acc: 0.6470\n",
      "Epoch 53/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7207 - acc: 0.6887 - val_loss: 0.8183 - val_acc: 0.6520\n",
      "Epoch 54/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7124 - acc: 0.6950 - val_loss: 0.8206 - val_acc: 0.6470\n",
      "Epoch 55/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7111 - acc: 0.6956 - val_loss: 0.8101 - val_acc: 0.6470\n",
      "Epoch 56/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.7153 - acc: 0.6889 - val_loss: 0.8913 - val_acc: 0.5920\n",
      "Epoch 57/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.7386 - acc: 0.6838 - val_loss: 0.8304 - val_acc: 0.6380\n",
      "Epoch 58/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.7050 - acc: 0.6937 - val_loss: 0.8565 - val_acc: 0.6420\n",
      "Epoch 59/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.7087 - acc: 0.6958 - val_loss: 0.8227 - val_acc: 0.6480\n",
      "Epoch 60/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7017 - acc: 0.6964 - val_loss: 0.8224 - val_acc: 0.6420\n",
      "Epoch 61/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6966 - acc: 0.7022 - val_loss: 0.8139 - val_acc: 0.6510\n",
      "Epoch 62/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6929 - acc: 0.7008 - val_loss: 0.8356 - val_acc: 0.6420\n",
      "Epoch 63/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7053 - acc: 0.6937 - val_loss: 0.8089 - val_acc: 0.6450\n",
      "Epoch 64/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7012 - acc: 0.6991 - val_loss: 0.8273 - val_acc: 0.6380\n",
      "Epoch 65/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6935 - acc: 0.7028 - val_loss: 0.8209 - val_acc: 0.6520\n",
      "Epoch 66/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6971 - acc: 0.6985 - val_loss: 0.8105 - val_acc: 0.6450\n",
      "Epoch 67/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6857 - acc: 0.7057 - val_loss: 0.8133 - val_acc: 0.6450\n",
      "Epoch 68/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6959 - acc: 0.6984 - val_loss: 0.8290 - val_acc: 0.6390\n",
      "Epoch 69/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6892 - acc: 0.7033 - val_loss: 0.8063 - val_acc: 0.6480\n",
      "Epoch 70/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6837 - acc: 0.7011 - val_loss: 0.8087 - val_acc: 0.6400\n",
      "Epoch 71/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6821 - acc: 0.7047 - val_loss: 0.8071 - val_acc: 0.6440\n",
      "Epoch 72/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6800 - acc: 0.7075 - val_loss: 0.8407 - val_acc: 0.6300\n",
      "Epoch 73/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6781 - acc: 0.7063 - val_loss: 0.8128 - val_acc: 0.6500\n",
      "Epoch 74/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6756 - acc: 0.7079 - val_loss: 0.8212 - val_acc: 0.6530\n",
      "Epoch 75/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6932 - acc: 0.6986 - val_loss: 0.8204 - val_acc: 0.6460\n",
      "Epoch 76/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6736 - acc: 0.7058 - val_loss: 0.7974 - val_acc: 0.6520\n",
      "Epoch 77/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6690 - acc: 0.7085 - val_loss: 0.8305 - val_acc: 0.6340\n",
      "Epoch 78/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6699 - acc: 0.7111 - val_loss: 0.8176 - val_acc: 0.6480\n",
      "Epoch 79/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6705 - acc: 0.7061 - val_loss: 0.8061 - val_acc: 0.6450\n",
      "Epoch 80/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6771 - acc: 0.7068 - val_loss: 0.8000 - val_acc: 0.6500\n",
      "Epoch 81/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6733 - acc: 0.7107 - val_loss: 0.8076 - val_acc: 0.6460\n",
      "Epoch 82/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6718 - acc: 0.7085 - val_loss: 0.7988 - val_acc: 0.6410\n",
      "Epoch 83/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6660 - acc: 0.7131 - val_loss: 0.8103 - val_acc: 0.6370\n",
      "Epoch 84/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6615 - acc: 0.7150 - val_loss: 0.8244 - val_acc: 0.6270\n",
      "Epoch 85/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6719 - acc: 0.7085 - val_loss: 0.8081 - val_acc: 0.6500\n",
      "Epoch 86/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6579 - acc: 0.7159 - val_loss: 0.8232 - val_acc: 0.6320\n",
      "Epoch 87/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6711 - acc: 0.7107 - val_loss: 0.8105 - val_acc: 0.6360\n",
      "Epoch 88/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6681 - acc: 0.7096 - val_loss: 0.7975 - val_acc: 0.6510\n",
      "Epoch 89/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6602 - acc: 0.7103 - val_loss: 0.8023 - val_acc: 0.6520\n",
      "Epoch 90/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6681 - acc: 0.7104 - val_loss: 0.8023 - val_acc: 0.6490\n",
      "Epoch 91/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6579 - acc: 0.7124 - val_loss: 0.8074 - val_acc: 0.6460\n",
      "Epoch 92/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6609 - acc: 0.7110 - val_loss: 0.8334 - val_acc: 0.6380\n",
      "Epoch 93/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6610 - acc: 0.7119 - val_loss: 0.8189 - val_acc: 0.6320\n",
      "Epoch 94/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6633 - acc: 0.7159 - val_loss: 0.7886 - val_acc: 0.6560\n",
      "Epoch 95/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6518 - acc: 0.7214 - val_loss: 0.8169 - val_acc: 0.6440\n",
      "Epoch 96/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6576 - acc: 0.7148 - val_loss: 0.8181 - val_acc: 0.6430\n",
      "Epoch 97/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6471 - acc: 0.7214 - val_loss: 0.7991 - val_acc: 0.6540\n",
      "Epoch 98/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6565 - acc: 0.7158 - val_loss: 0.8135 - val_acc: 0.6450\n",
      "Epoch 99/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6489 - acc: 0.7194 - val_loss: 0.8060 - val_acc: 0.6540\n",
      "Epoch 100/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6500 - acc: 0.7159 - val_loss: 0.7995 - val_acc: 0.6420\n",
      "Epoch 101/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6454 - acc: 0.7178 - val_loss: 0.8029 - val_acc: 0.6450\n",
      "Epoch 102/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6466 - acc: 0.7196 - val_loss: 0.8233 - val_acc: 0.6380\n",
      "Epoch 103/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6610 - acc: 0.7134 - val_loss: 0.8155 - val_acc: 0.6470\n",
      "Epoch 104/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6488 - acc: 0.7214 - val_loss: 0.8109 - val_acc: 0.6390\n",
      "Epoch 105/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6454 - acc: 0.7194 - val_loss: 0.8532 - val_acc: 0.6260\n",
      "Epoch 106/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6634 - acc: 0.7110 - val_loss: 0.7873 - val_acc: 0.6540\n",
      "Epoch 107/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6413 - acc: 0.7237 - val_loss: 0.8308 - val_acc: 0.6380\n",
      "Epoch 108/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6426 - acc: 0.7198 - val_loss: 0.8122 - val_acc: 0.6370\n",
      "Epoch 109/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6411 - acc: 0.7221 - val_loss: 0.8165 - val_acc: 0.6340\n",
      "Epoch 110/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6430 - acc: 0.7221 - val_loss: 0.8405 - val_acc: 0.6510\n",
      "Epoch 111/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6409 - acc: 0.7212 - val_loss: 0.8123 - val_acc: 0.6420\n",
      "Epoch 112/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6388 - acc: 0.7225 - val_loss: 0.8013 - val_acc: 0.6580\n",
      "Epoch 113/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6370 - acc: 0.7242 - val_loss: 0.8231 - val_acc: 0.6330\n",
      "Epoch 114/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6402 - acc: 0.7222 - val_loss: 0.8090 - val_acc: 0.6400\n",
      "Epoch 115/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6423 - acc: 0.7223 - val_loss: 0.8022 - val_acc: 0.6390\n",
      "Epoch 116/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6398 - acc: 0.7265 - val_loss: 0.8345 - val_acc: 0.6250\n",
      "Epoch 117/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6567 - acc: 0.7138 - val_loss: 0.8276 - val_acc: 0.6410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6446 - acc: 0.7182 - val_loss: 0.8244 - val_acc: 0.6340\n",
      "Epoch 119/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6380 - acc: 0.7256 - val_loss: 0.8082 - val_acc: 0.6450\n",
      "Epoch 120/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6309 - acc: 0.7283 - val_loss: 0.8244 - val_acc: 0.6330\n",
      "Epoch 121/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6290 - acc: 0.7294 - val_loss: 0.8849 - val_acc: 0.6380\n",
      "Epoch 122/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6608 - acc: 0.7109 - val_loss: 0.8070 - val_acc: 0.6370\n",
      "Epoch 123/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6369 - acc: 0.7247 - val_loss: 0.7968 - val_acc: 0.6460\n",
      "Epoch 124/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6299 - acc: 0.7287 - val_loss: 0.8012 - val_acc: 0.6430\n",
      "Epoch 125/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6226 - acc: 0.7337 - val_loss: 0.8622 - val_acc: 0.6220\n",
      "Epoch 126/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6307 - acc: 0.7265 - val_loss: 0.8250 - val_acc: 0.6500\n",
      "Epoch 127/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6342 - acc: 0.7269 - val_loss: 0.8169 - val_acc: 0.6330\n",
      "Epoch 128/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6225 - acc: 0.7295 - val_loss: 0.8091 - val_acc: 0.6520\n",
      "Epoch 129/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6266 - acc: 0.7299 - val_loss: 0.8282 - val_acc: 0.6420\n",
      "Epoch 130/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6234 - acc: 0.7309 - val_loss: 0.8208 - val_acc: 0.6550\n",
      "Epoch 131/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6248 - acc: 0.7285 - val_loss: 0.8415 - val_acc: 0.6480\n",
      "Epoch 132/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6322 - acc: 0.7248 - val_loss: 0.8234 - val_acc: 0.6370\n",
      "Epoch 133/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6173 - acc: 0.7364 - val_loss: 0.8069 - val_acc: 0.6460\n",
      "Epoch 134/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6171 - acc: 0.7327 - val_loss: 0.8133 - val_acc: 0.6290\n",
      "Epoch 135/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6123 - acc: 0.7372 - val_loss: 0.8427 - val_acc: 0.6270\n",
      "Epoch 136/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6348 - acc: 0.7258 - val_loss: 0.8016 - val_acc: 0.6480\n",
      "Epoch 137/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6247 - acc: 0.7320 - val_loss: 0.8410 - val_acc: 0.6350\n",
      "Epoch 138/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6465 - acc: 0.7214 - val_loss: 0.7896 - val_acc: 0.6510\n",
      "Epoch 139/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6194 - acc: 0.7306 - val_loss: 0.7914 - val_acc: 0.6460\n",
      "Epoch 140/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6111 - acc: 0.7385 - val_loss: 0.8067 - val_acc: 0.6360\n",
      "Epoch 141/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6151 - acc: 0.7361 - val_loss: 0.7957 - val_acc: 0.6530\n",
      "Epoch 142/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6139 - acc: 0.7381 - val_loss: 0.8186 - val_acc: 0.6490\n",
      "Epoch 143/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6171 - acc: 0.7358 - val_loss: 0.8105 - val_acc: 0.6590\n",
      "Epoch 144/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6118 - acc: 0.7347 - val_loss: 0.8164 - val_acc: 0.6520\n",
      "Epoch 145/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6132 - acc: 0.7384 - val_loss: 0.8283 - val_acc: 0.6390\n",
      "Epoch 146/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6135 - acc: 0.7377 - val_loss: 0.7963 - val_acc: 0.6480\n",
      "Epoch 147/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6103 - acc: 0.7400 - val_loss: 0.8134 - val_acc: 0.6530\n",
      "Epoch 148/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6229 - acc: 0.7316 - val_loss: 0.7918 - val_acc: 0.6490\n",
      "Epoch 149/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6169 - acc: 0.7347 - val_loss: 0.8020 - val_acc: 0.6510\n",
      "Epoch 150/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6155 - acc: 0.7386 - val_loss: 0.8165 - val_acc: 0.6380\n",
      "Epoch 151/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6052 - acc: 0.7421 - val_loss: 0.8040 - val_acc: 0.6530\n",
      "Epoch 152/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6010 - acc: 0.7452 - val_loss: 0.8532 - val_acc: 0.6500\n",
      "Epoch 153/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6097 - acc: 0.7384 - val_loss: 0.8160 - val_acc: 0.6560\n",
      "Epoch 154/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6028 - acc: 0.7441 - val_loss: 0.8279 - val_acc: 0.6450\n",
      "Epoch 155/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6010 - acc: 0.7434 - val_loss: 0.8220 - val_acc: 0.6400\n",
      "Epoch 156/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6345 - acc: 0.7312 - val_loss: 0.8053 - val_acc: 0.6600\n",
      "Epoch 157/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6201 - acc: 0.7358 - val_loss: 0.8105 - val_acc: 0.6610\n",
      "Epoch 158/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6038 - acc: 0.7463 - val_loss: 0.8250 - val_acc: 0.6430\n",
      "Epoch 159/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5976 - acc: 0.7468 - val_loss: 0.8283 - val_acc: 0.6400\n",
      "Epoch 160/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6009 - acc: 0.7434 - val_loss: 0.8136 - val_acc: 0.6580\n",
      "Epoch 161/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5948 - acc: 0.7487 - val_loss: 0.8180 - val_acc: 0.6520\n",
      "Epoch 162/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.6025 - acc: 0.7431 - val_loss: 0.8246 - val_acc: 0.6330\n",
      "Epoch 163/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5925 - acc: 0.7494 - val_loss: 0.8127 - val_acc: 0.6430\n",
      "Epoch 164/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6075 - acc: 0.7425 - val_loss: 0.8040 - val_acc: 0.6560\n",
      "Epoch 165/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6006 - acc: 0.7471 - val_loss: 0.8006 - val_acc: 0.6490\n",
      "Epoch 166/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6097 - acc: 0.7408 - val_loss: 0.8204 - val_acc: 0.6470\n",
      "Epoch 167/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5912 - acc: 0.7525 - val_loss: 0.8033 - val_acc: 0.6620\n",
      "Epoch 168/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5892 - acc: 0.7519 - val_loss: 0.7994 - val_acc: 0.6530\n",
      "Epoch 169/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.5930 - acc: 0.7506 - val_loss: 0.7837 - val_acc: 0.6640\n",
      "Epoch 170/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6020 - acc: 0.7440 - val_loss: 0.7998 - val_acc: 0.6530\n",
      "Epoch 171/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5834 - acc: 0.7566 - val_loss: 0.8296 - val_acc: 0.6460\n",
      "Epoch 172/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5845 - acc: 0.7514 - val_loss: 0.8752 - val_acc: 0.6540\n",
      "Epoch 173/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6056 - acc: 0.7426 - val_loss: 0.8108 - val_acc: 0.6600\n",
      "Epoch 174/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5820 - acc: 0.7549 - val_loss: 0.8152 - val_acc: 0.6600\n",
      "Epoch 175/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.5801 - acc: 0.7559 - val_loss: 0.8926 - val_acc: 0.6330\n",
      "Epoch 176/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.5958 - acc: 0.7463 - val_loss: 0.8114 - val_acc: 0.6580\n",
      "Epoch 177/200\n",
      "14367/14367 [==============================] - 0s 16us/step - loss: 0.5872 - acc: 0.7501 - val_loss: 0.8277 - val_acc: 0.6480\n",
      "Epoch 178/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5917 - acc: 0.7507 - val_loss: 0.8604 - val_acc: 0.6550\n",
      "Epoch 179/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5946 - acc: 0.7482 - val_loss: 0.8493 - val_acc: 0.6420\n",
      "Epoch 180/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.5796 - acc: 0.7553 - val_loss: 0.8553 - val_acc: 0.6420\n",
      "Epoch 181/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5830 - acc: 0.7539 - val_loss: 0.8256 - val_acc: 0.6640\n",
      "Epoch 182/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5995 - acc: 0.7454 - val_loss: 0.7986 - val_acc: 0.6610\n",
      "Epoch 183/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5865 - acc: 0.7537 - val_loss: 0.8127 - val_acc: 0.6560\n",
      "Epoch 184/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5858 - acc: 0.7532 - val_loss: 0.8092 - val_acc: 0.6580\n",
      "Epoch 185/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5804 - acc: 0.7575 - val_loss: 0.8202 - val_acc: 0.6630\n",
      "Epoch 186/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5756 - acc: 0.7614 - val_loss: 0.8098 - val_acc: 0.6610\n",
      "Epoch 187/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5751 - acc: 0.7575 - val_loss: 0.8671 - val_acc: 0.6520\n",
      "Epoch 188/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5903 - acc: 0.7512 - val_loss: 0.8117 - val_acc: 0.6650\n",
      "Epoch 189/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5842 - acc: 0.7523 - val_loss: 0.8161 - val_acc: 0.6590\n",
      "Epoch 190/200\n",
      "14367/14367 [==============================] - 0s 16us/step - loss: 0.5755 - acc: 0.7610 - val_loss: 0.8178 - val_acc: 0.6410\n",
      "Epoch 191/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5754 - acc: 0.7565 - val_loss: 0.8376 - val_acc: 0.6500\n",
      "Epoch 192/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5669 - acc: 0.7633 - val_loss: 0.8189 - val_acc: 0.6530\n",
      "Epoch 193/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5707 - acc: 0.7577 - val_loss: 0.9146 - val_acc: 0.6270\n",
      "Epoch 194/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5722 - acc: 0.7590 - val_loss: 0.8374 - val_acc: 0.6490\n",
      "Epoch 195/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5684 - acc: 0.7631 - val_loss: 0.8314 - val_acc: 0.6670\n",
      "Epoch 196/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5600 - acc: 0.7649 - val_loss: 0.8285 - val_acc: 0.6640\n",
      "Epoch 197/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.5648 - acc: 0.7617 - val_loss: 0.8489 - val_acc: 0.6400\n",
      "Epoch 198/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5737 - acc: 0.7555 - val_loss: 0.8388 - val_acc: 0.6440\n",
      "Epoch 199/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5774 - acc: 0.7549 - val_loss: 0.8395 - val_acc: 0.6660\n",
      "Epoch 200/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5658 - acc: 0.7640 - val_loss: 0.8390 - val_acc: 0.6340\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train.values,\n",
    "                    epochs=200,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) +1)\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training Acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n",
    "plt.title('Training and Validation Acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 48us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.852596959066391, 0.6582]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_12 (Sequential)   (None, 10)                42850     \n",
      "_________________________________________________________________\n",
      "sequential_14 (Sequential)   (None, 4)                 219       \n",
      "=================================================================\n",
      "Total params: 43,069\n",
      "Trainable params: 43,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_classifier = models.Sequential()\n",
    "deep_classifier.add(layers.Dense(10,kernel_initializer='random_normal',\n",
    "                       kernel_regularizer=regularizers.l2(0.01),\n",
    "                       activation='relu', \n",
    "                       input_shape=(10,)))\n",
    "deep_classifier.add(layers.Dense(7, activation='relu', kernel_regularizer=regularizers.l2(0.01),))\n",
    "deep_classifier.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model = Sequential([encoder,\n",
    "                   deep_classifier])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14367 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "14367/14367 [==============================] - 1s 48us/step - loss: 1.9893 - acc: 0.5481 - val_loss: 1.2757 - val_acc: 0.5560\n",
      "Epoch 2/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 1.1497 - acc: 0.6004 - val_loss: 1.1386 - val_acc: 0.5710\n",
      "Epoch 3/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9492 - acc: 0.6236 - val_loss: 1.0214 - val_acc: 0.5950\n",
      "Epoch 4/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.8379 - acc: 0.6460 - val_loss: 0.9437 - val_acc: 0.6130\n",
      "Epoch 5/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7878 - acc: 0.6755 - val_loss: 0.9792 - val_acc: 0.6190\n",
      "Epoch 6/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.7706 - acc: 0.6949 - val_loss: 0.9246 - val_acc: 0.6330\n",
      "Epoch 7/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7563 - acc: 0.7032 - val_loss: 0.9255 - val_acc: 0.6390\n",
      "Epoch 8/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7445 - acc: 0.7096 - val_loss: 0.9230 - val_acc: 0.6410\n",
      "Epoch 9/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.7304 - acc: 0.7127 - val_loss: 0.9251 - val_acc: 0.6450\n",
      "Epoch 10/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.7128 - acc: 0.7242 - val_loss: 0.9121 - val_acc: 0.6410\n",
      "Epoch 11/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6973 - acc: 0.7258 - val_loss: 0.9041 - val_acc: 0.6430\n",
      "Epoch 12/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6905 - acc: 0.7274 - val_loss: 0.9236 - val_acc: 0.6400\n",
      "Epoch 13/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6946 - acc: 0.7254 - val_loss: 0.9544 - val_acc: 0.6410\n",
      "Epoch 14/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6879 - acc: 0.7299 - val_loss: 0.9002 - val_acc: 0.6450\n",
      "Epoch 15/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6796 - acc: 0.7274 - val_loss: 0.9315 - val_acc: 0.6360\n",
      "Epoch 16/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6770 - acc: 0.7324 - val_loss: 0.9611 - val_acc: 0.6280\n",
      "Epoch 17/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6710 - acc: 0.7311 - val_loss: 0.8756 - val_acc: 0.6370\n",
      "Epoch 18/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6740 - acc: 0.7298 - val_loss: 0.9173 - val_acc: 0.6380\n",
      "Epoch 19/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6582 - acc: 0.7339 - val_loss: 0.8865 - val_acc: 0.6480\n",
      "Epoch 20/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6496 - acc: 0.7400 - val_loss: 0.8872 - val_acc: 0.6430\n",
      "Epoch 21/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6525 - acc: 0.7502 - val_loss: 0.9163 - val_acc: 0.6430\n",
      "Epoch 22/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6418 - acc: 0.7486 - val_loss: 0.8942 - val_acc: 0.6350\n",
      "Epoch 23/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6389 - acc: 0.7462 - val_loss: 0.9083 - val_acc: 0.6380\n",
      "Epoch 24/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6428 - acc: 0.7501 - val_loss: 0.8864 - val_acc: 0.6310\n",
      "Epoch 25/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6421 - acc: 0.7473 - val_loss: 0.9025 - val_acc: 0.6450\n",
      "Epoch 26/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6330 - acc: 0.7530 - val_loss: 0.8892 - val_acc: 0.6400\n",
      "Epoch 27/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6298 - acc: 0.7521 - val_loss: 0.9007 - val_acc: 0.6310\n",
      "Epoch 28/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6289 - acc: 0.7502 - val_loss: 0.8961 - val_acc: 0.6280\n",
      "Epoch 29/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6202 - acc: 0.7544 - val_loss: 0.8976 - val_acc: 0.6400\n",
      "Epoch 30/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6240 - acc: 0.7528 - val_loss: 0.8974 - val_acc: 0.6290\n",
      "Epoch 31/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6183 - acc: 0.7537 - val_loss: 0.8844 - val_acc: 0.6430\n",
      "Epoch 32/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6131 - acc: 0.7530 - val_loss: 0.9284 - val_acc: 0.6230\n",
      "Epoch 33/200\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.6183 - acc: 0.7539 - val_loss: 0.8919 - val_acc: 0.6530\n",
      "Epoch 34/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6047 - acc: 0.7597 - val_loss: 0.8724 - val_acc: 0.6390\n",
      "Epoch 35/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6120 - acc: 0.7536 - val_loss: 0.8717 - val_acc: 0.6480\n",
      "Epoch 36/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.6063 - acc: 0.7579 - val_loss: 0.8982 - val_acc: 0.6440\n",
      "Epoch 37/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6007 - acc: 0.7633 - val_loss: 0.8736 - val_acc: 0.6570\n",
      "Epoch 38/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5913 - acc: 0.7660 - val_loss: 0.8673 - val_acc: 0.6590\n",
      "Epoch 39/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6090 - acc: 0.7564 - val_loss: 0.8973 - val_acc: 0.6430\n",
      "Epoch 40/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6226 - acc: 0.7524 - val_loss: 0.9330 - val_acc: 0.6390\n",
      "Epoch 41/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6768 - acc: 0.7368 - val_loss: 0.8725 - val_acc: 0.6300\n",
      "Epoch 42/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6217 - acc: 0.7523 - val_loss: 0.8892 - val_acc: 0.6540\n",
      "Epoch 43/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6065 - acc: 0.7615 - val_loss: 0.8695 - val_acc: 0.6430\n",
      "Epoch 44/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5956 - acc: 0.7662 - val_loss: 0.8878 - val_acc: 0.6400\n",
      "Epoch 45/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5878 - acc: 0.7698 - val_loss: 0.8885 - val_acc: 0.6420\n",
      "Epoch 46/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5845 - acc: 0.7723 - val_loss: 0.9034 - val_acc: 0.6520\n",
      "Epoch 47/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5834 - acc: 0.7720 - val_loss: 0.8749 - val_acc: 0.6520\n",
      "Epoch 48/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5847 - acc: 0.7700 - val_loss: 0.8825 - val_acc: 0.6430\n",
      "Epoch 49/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5970 - acc: 0.7640 - val_loss: 0.9304 - val_acc: 0.6650\n",
      "Epoch 50/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5984 - acc: 0.7638 - val_loss: 0.8697 - val_acc: 0.6620\n",
      "Epoch 51/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5868 - acc: 0.7696 - val_loss: 0.8751 - val_acc: 0.6540\n",
      "Epoch 52/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5711 - acc: 0.7785 - val_loss: 0.8812 - val_acc: 0.6510\n",
      "Epoch 53/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5752 - acc: 0.7718 - val_loss: 0.9005 - val_acc: 0.6530\n",
      "Epoch 54/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5910 - acc: 0.7675 - val_loss: 0.8990 - val_acc: 0.6660\n",
      "Epoch 55/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5961 - acc: 0.7656 - val_loss: 0.8804 - val_acc: 0.6710\n",
      "Epoch 56/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5821 - acc: 0.7700 - val_loss: 0.9078 - val_acc: 0.6440\n",
      "Epoch 57/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5744 - acc: 0.7702 - val_loss: 0.8749 - val_acc: 0.6560\n",
      "Epoch 58/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5678 - acc: 0.7770 - val_loss: 0.8864 - val_acc: 0.6580\n",
      "Epoch 59/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5792 - acc: 0.7709 - val_loss: 0.8866 - val_acc: 0.6540\n",
      "Epoch 60/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5626 - acc: 0.7782 - val_loss: 0.8685 - val_acc: 0.6410\n",
      "Epoch 61/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5743 - acc: 0.7729 - val_loss: 0.8578 - val_acc: 0.6600\n",
      "Epoch 62/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5761 - acc: 0.7709 - val_loss: 0.8691 - val_acc: 0.6720\n",
      "Epoch 63/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5668 - acc: 0.7771 - val_loss: 0.8685 - val_acc: 0.6500\n",
      "Epoch 64/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5576 - acc: 0.7804 - val_loss: 0.8694 - val_acc: 0.6580\n",
      "Epoch 65/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5594 - acc: 0.7759 - val_loss: 0.9099 - val_acc: 0.6540\n",
      "Epoch 66/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5558 - acc: 0.7783 - val_loss: 0.8933 - val_acc: 0.6460\n",
      "Epoch 67/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5644 - acc: 0.7758 - val_loss: 0.9266 - val_acc: 0.6340\n",
      "Epoch 68/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5616 - acc: 0.7775 - val_loss: 0.8867 - val_acc: 0.6420\n",
      "Epoch 69/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5551 - acc: 0.7782 - val_loss: 0.9422 - val_acc: 0.6480\n",
      "Epoch 70/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.5606 - acc: 0.7728 - val_loss: 0.9109 - val_acc: 0.6380\n",
      "Epoch 71/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5594 - acc: 0.7724 - val_loss: 0.8811 - val_acc: 0.6590\n",
      "Epoch 72/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5449 - acc: 0.7772 - val_loss: 0.9252 - val_acc: 0.6480\n",
      "Epoch 73/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5401 - acc: 0.7844 - val_loss: 0.9134 - val_acc: 0.6480\n",
      "Epoch 74/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5415 - acc: 0.7783 - val_loss: 0.9585 - val_acc: 0.6490\n",
      "Epoch 75/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5375 - acc: 0.7810 - val_loss: 0.9380 - val_acc: 0.6600\n",
      "Epoch 76/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5436 - acc: 0.7781 - val_loss: 0.9249 - val_acc: 0.6600\n",
      "Epoch 77/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5334 - acc: 0.7817 - val_loss: 0.9729 - val_acc: 0.6540\n",
      "Epoch 78/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5499 - acc: 0.7771 - val_loss: 1.0038 - val_acc: 0.6110\n",
      "Epoch 79/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5543 - acc: 0.7700 - val_loss: 0.9690 - val_acc: 0.6500\n",
      "Epoch 80/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5889 - acc: 0.7571 - val_loss: 0.9317 - val_acc: 0.6460\n",
      "Epoch 81/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5388 - acc: 0.7797 - val_loss: 0.9327 - val_acc: 0.6550\n",
      "Epoch 82/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5382 - acc: 0.7815 - val_loss: 0.9773 - val_acc: 0.6430\n",
      "Epoch 83/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5423 - acc: 0.7768 - val_loss: 0.9873 - val_acc: 0.6250\n",
      "Epoch 84/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5303 - acc: 0.7844 - val_loss: 0.9938 - val_acc: 0.6710\n",
      "Epoch 85/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5566 - acc: 0.7732 - val_loss: 0.9398 - val_acc: 0.6460\n",
      "Epoch 86/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5430 - acc: 0.7769 - val_loss: 0.9103 - val_acc: 0.6490\n",
      "Epoch 87/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5268 - acc: 0.7850 - val_loss: 0.9179 - val_acc: 0.6520\n",
      "Epoch 88/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5148 - acc: 0.7899 - val_loss: 0.9260 - val_acc: 0.6460\n",
      "Epoch 89/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5235 - acc: 0.7839 - val_loss: 0.9279 - val_acc: 0.6600\n",
      "Epoch 90/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5207 - acc: 0.7842 - val_loss: 0.9564 - val_acc: 0.6450\n",
      "Epoch 91/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5169 - acc: 0.7894 - val_loss: 0.9450 - val_acc: 0.6480\n",
      "Epoch 92/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5100 - acc: 0.7916 - val_loss: 0.9497 - val_acc: 0.6490\n",
      "Epoch 93/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5111 - acc: 0.7921 - val_loss: 0.9661 - val_acc: 0.6590\n",
      "Epoch 94/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5612 - acc: 0.7752 - val_loss: 0.9349 - val_acc: 0.6490\n",
      "Epoch 95/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5255 - acc: 0.7872 - val_loss: 0.9339 - val_acc: 0.6580\n",
      "Epoch 96/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5132 - acc: 0.7902 - val_loss: 0.9836 - val_acc: 0.6480\n",
      "Epoch 97/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5189 - acc: 0.7885 - val_loss: 0.9570 - val_acc: 0.6320\n",
      "Epoch 98/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5227 - acc: 0.7858 - val_loss: 0.9651 - val_acc: 0.6260\n",
      "Epoch 99/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5135 - acc: 0.7903 - val_loss: 0.9380 - val_acc: 0.6370\n",
      "Epoch 100/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5133 - acc: 0.7917 - val_loss: 1.0006 - val_acc: 0.6470\n",
      "Epoch 101/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5163 - acc: 0.7883 - val_loss: 0.9685 - val_acc: 0.6530\n",
      "Epoch 102/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5152 - acc: 0.7899 - val_loss: 0.9578 - val_acc: 0.6420\n",
      "Epoch 103/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5145 - acc: 0.7917 - val_loss: 0.9537 - val_acc: 0.6320\n",
      "Epoch 104/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5145 - acc: 0.7913 - val_loss: 0.9863 - val_acc: 0.6500\n",
      "Epoch 105/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5358 - acc: 0.7781 - val_loss: 0.9303 - val_acc: 0.6480\n",
      "Epoch 106/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5139 - acc: 0.7866 - val_loss: 0.9637 - val_acc: 0.6560\n",
      "Epoch 107/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5122 - acc: 0.7910 - val_loss: 0.9730 - val_acc: 0.6550\n",
      "Epoch 108/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4966 - acc: 0.7987 - val_loss: 0.9846 - val_acc: 0.6420\n",
      "Epoch 109/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5021 - acc: 0.7928 - val_loss: 1.0279 - val_acc: 0.6550\n",
      "Epoch 110/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5135 - acc: 0.7874 - val_loss: 0.9853 - val_acc: 0.6490\n",
      "Epoch 111/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5027 - acc: 0.7955 - val_loss: 0.9862 - val_acc: 0.6560\n",
      "Epoch 112/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5138 - acc: 0.7911 - val_loss: 0.9792 - val_acc: 0.6560\n",
      "Epoch 113/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4968 - acc: 0.7996 - val_loss: 0.9694 - val_acc: 0.6490\n",
      "Epoch 114/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4908 - acc: 0.8001 - val_loss: 1.0301 - val_acc: 0.6280\n",
      "Epoch 115/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5015 - acc: 0.7936 - val_loss: 1.0019 - val_acc: 0.6290\n",
      "Epoch 116/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5387 - acc: 0.7754 - val_loss: 0.9790 - val_acc: 0.6510\n",
      "Epoch 117/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5040 - acc: 0.7959 - val_loss: 0.9814 - val_acc: 0.6550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4886 - acc: 0.8032 - val_loss: 0.9957 - val_acc: 0.6450\n",
      "Epoch 119/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5006 - acc: 0.7956 - val_loss: 1.0051 - val_acc: 0.6510\n",
      "Epoch 120/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4890 - acc: 0.7990 - val_loss: 1.0172 - val_acc: 0.6520\n",
      "Epoch 121/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4928 - acc: 0.7991 - val_loss: 0.9974 - val_acc: 0.6420\n",
      "Epoch 122/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5261 - acc: 0.7817 - val_loss: 0.9735 - val_acc: 0.6560\n",
      "Epoch 123/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4961 - acc: 0.7952 - val_loss: 1.0663 - val_acc: 0.6050\n",
      "Epoch 124/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5343 - acc: 0.7801 - val_loss: 0.9778 - val_acc: 0.6550\n",
      "Epoch 125/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5000 - acc: 0.7981 - val_loss: 1.0222 - val_acc: 0.6370\n",
      "Epoch 126/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4996 - acc: 0.7936 - val_loss: 0.9798 - val_acc: 0.6460\n",
      "Epoch 127/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4907 - acc: 0.7979 - val_loss: 0.9991 - val_acc: 0.6650\n",
      "Epoch 128/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4821 - acc: 0.8030 - val_loss: 0.9909 - val_acc: 0.6570\n",
      "Epoch 129/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4817 - acc: 0.8024 - val_loss: 1.0109 - val_acc: 0.6410\n",
      "Epoch 130/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4778 - acc: 0.8066 - val_loss: 0.9896 - val_acc: 0.6460\n",
      "Epoch 131/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4799 - acc: 0.8057 - val_loss: 1.0308 - val_acc: 0.6410\n",
      "Epoch 132/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4872 - acc: 0.7999 - val_loss: 1.0527 - val_acc: 0.6460\n",
      "Epoch 133/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4872 - acc: 0.8006 - val_loss: 0.9957 - val_acc: 0.6340\n",
      "Epoch 134/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4772 - acc: 0.8016 - val_loss: 1.0368 - val_acc: 0.6370\n",
      "Epoch 135/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5097 - acc: 0.7890 - val_loss: 1.0388 - val_acc: 0.6220\n",
      "Epoch 136/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4798 - acc: 0.8039 - val_loss: 0.9994 - val_acc: 0.6490\n",
      "Epoch 137/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4687 - acc: 0.8078 - val_loss: 1.0429 - val_acc: 0.6380\n",
      "Epoch 138/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4773 - acc: 0.8023 - val_loss: 1.0562 - val_acc: 0.6290\n",
      "Epoch 139/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5505 - acc: 0.7747 - val_loss: 0.9880 - val_acc: 0.6520\n",
      "Epoch 140/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4765 - acc: 0.8072 - val_loss: 1.0248 - val_acc: 0.6380\n",
      "Epoch 141/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4886 - acc: 0.8018 - val_loss: 1.0145 - val_acc: 0.6430\n",
      "Epoch 142/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4688 - acc: 0.8056 - val_loss: 1.1545 - val_acc: 0.6380\n",
      "Epoch 143/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5005 - acc: 0.7936 - val_loss: 1.0332 - val_acc: 0.6410\n",
      "Epoch 144/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4631 - acc: 0.8110 - val_loss: 1.0499 - val_acc: 0.6370\n",
      "Epoch 145/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4730 - acc: 0.8037 - val_loss: 1.0546 - val_acc: 0.6410\n",
      "Epoch 146/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4911 - acc: 0.7982 - val_loss: 1.0689 - val_acc: 0.6510\n",
      "Epoch 147/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4691 - acc: 0.8080 - val_loss: 1.0196 - val_acc: 0.6360\n",
      "Epoch 148/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4666 - acc: 0.8089 - val_loss: 1.1125 - val_acc: 0.6390\n",
      "Epoch 149/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4917 - acc: 0.7968 - val_loss: 1.0474 - val_acc: 0.6300\n",
      "Epoch 150/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4564 - acc: 0.8143 - val_loss: 1.0832 - val_acc: 0.6330\n",
      "Epoch 151/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4578 - acc: 0.8137 - val_loss: 1.0890 - val_acc: 0.6330\n",
      "Epoch 152/200\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.4647 - acc: 0.8066 - val_loss: 1.0196 - val_acc: 0.6400\n",
      "Epoch 153/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4523 - acc: 0.8161 - val_loss: 1.0720 - val_acc: 0.6300\n",
      "Epoch 154/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4629 - acc: 0.8102 - val_loss: 1.0353 - val_acc: 0.6350\n",
      "Epoch 155/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4503 - acc: 0.8163 - val_loss: 1.0907 - val_acc: 0.6300\n",
      "Epoch 156/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4761 - acc: 0.8049 - val_loss: 1.0481 - val_acc: 0.6400\n",
      "Epoch 157/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4602 - acc: 0.8110 - val_loss: 1.0672 - val_acc: 0.6350\n",
      "Epoch 158/200\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.4466 - acc: 0.8209 - val_loss: 1.0853 - val_acc: 0.6340\n",
      "Epoch 159/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4568 - acc: 0.8104 - val_loss: 1.0845 - val_acc: 0.6290\n",
      "Epoch 160/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4664 - acc: 0.8080 - val_loss: 1.1024 - val_acc: 0.6340\n",
      "Epoch 161/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4685 - acc: 0.8079 - val_loss: 1.0926 - val_acc: 0.6240\n",
      "Epoch 162/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4429 - acc: 0.8196 - val_loss: 1.1961 - val_acc: 0.6440\n",
      "Epoch 163/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4724 - acc: 0.8043 - val_loss: 1.0207 - val_acc: 0.6350\n",
      "Epoch 164/200\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.4495 - acc: 0.8126 - val_loss: 1.0648 - val_acc: 0.6430\n",
      "Epoch 165/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4392 - acc: 0.8183 - val_loss: 1.1112 - val_acc: 0.6320\n",
      "Epoch 166/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4733 - acc: 0.8088 - val_loss: 1.0652 - val_acc: 0.6230\n",
      "Epoch 167/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4382 - acc: 0.8233 - val_loss: 1.1290 - val_acc: 0.6350\n",
      "Epoch 168/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4373 - acc: 0.8210 - val_loss: 1.1350 - val_acc: 0.6380\n",
      "Epoch 169/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4462 - acc: 0.8144 - val_loss: 1.1412 - val_acc: 0.6380\n",
      "Epoch 170/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4579 - acc: 0.8130 - val_loss: 1.1243 - val_acc: 0.6270\n",
      "Epoch 171/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4705 - acc: 0.8021 - val_loss: 1.1302 - val_acc: 0.6400\n",
      "Epoch 172/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4635 - acc: 0.8103 - val_loss: 1.0808 - val_acc: 0.6350\n",
      "Epoch 173/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4421 - acc: 0.8197 - val_loss: 1.1434 - val_acc: 0.6300\n",
      "Epoch 174/200\n",
      "14367/14367 [==============================] - 0s 11us/step - loss: 0.4524 - acc: 0.8146 - val_loss: 1.0511 - val_acc: 0.6400\n",
      "Epoch 175/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4502 - acc: 0.8148 - val_loss: 1.1215 - val_acc: 0.6330\n",
      "Epoch 176/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4415 - acc: 0.8198 - val_loss: 1.1634 - val_acc: 0.6290\n",
      "Epoch 177/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4421 - acc: 0.8195 - val_loss: 1.1204 - val_acc: 0.6220\n",
      "Epoch 178/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4238 - acc: 0.8268 - val_loss: 1.1297 - val_acc: 0.6400\n",
      "Epoch 179/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4410 - acc: 0.8189 - val_loss: 1.1316 - val_acc: 0.6390\n",
      "Epoch 180/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4359 - acc: 0.8204 - val_loss: 1.1497 - val_acc: 0.6190\n",
      "Epoch 181/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4277 - acc: 0.8240 - val_loss: 1.2020 - val_acc: 0.6260\n",
      "Epoch 182/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4182 - acc: 0.8310 - val_loss: 1.2027 - val_acc: 0.6270\n",
      "Epoch 183/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4405 - acc: 0.8188 - val_loss: 1.1293 - val_acc: 0.6230\n",
      "Epoch 184/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4502 - acc: 0.8151 - val_loss: 1.1135 - val_acc: 0.6300\n",
      "Epoch 185/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4553 - acc: 0.8123 - val_loss: 1.1525 - val_acc: 0.6340\n",
      "Epoch 186/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4366 - acc: 0.8199 - val_loss: 1.1251 - val_acc: 0.6210\n",
      "Epoch 187/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4296 - acc: 0.8236 - val_loss: 1.1912 - val_acc: 0.6410\n",
      "Epoch 188/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4200 - acc: 0.8284 - val_loss: 1.1722 - val_acc: 0.6410\n",
      "Epoch 189/200\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4136 - acc: 0.8336 - val_loss: 1.2544 - val_acc: 0.6130\n",
      "Epoch 190/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4383 - acc: 0.8188 - val_loss: 1.1315 - val_acc: 0.6430\n",
      "Epoch 191/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4136 - acc: 0.8320 - val_loss: 1.2765 - val_acc: 0.6460\n",
      "Epoch 192/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4217 - acc: 0.8276 - val_loss: 1.1688 - val_acc: 0.6250\n",
      "Epoch 193/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4077 - acc: 0.8343 - val_loss: 1.1943 - val_acc: 0.6340\n",
      "Epoch 194/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4115 - acc: 0.8313 - val_loss: 1.1974 - val_acc: 0.6290\n",
      "Epoch 195/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4345 - acc: 0.8210 - val_loss: 1.1827 - val_acc: 0.6250\n",
      "Epoch 196/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4158 - acc: 0.8338 - val_loss: 1.1826 - val_acc: 0.6210\n",
      "Epoch 197/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4283 - acc: 0.8245 - val_loss: 1.1988 - val_acc: 0.6250\n",
      "Epoch 198/200\n",
      "14367/14367 [==============================] - 0s 15us/step - loss: 0.4294 - acc: 0.8226 - val_loss: 1.1820 - val_acc: 0.6210\n",
      "Epoch 199/200\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.4033 - acc: 0.8359 - val_loss: 1.2461 - val_acc: 0.6360\n",
      "Epoch 200/200\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4250 - acc: 0.8267 - val_loss: 1.2058 - val_acc: 0.6240\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train.values,\n",
    "                    epochs=200,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) +1)\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training Acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n",
    "plt.title('Training and Validation Acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 36us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.404834181022644, 0.581]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well clearly we've overfit the crap out of our encoder I think - because with it we are getting fuck all on the validation improvement - so maybe let's not do that and cap it earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_13 (Sequential)   (None, 10)                42850     \n",
      "_________________________________________________________________\n",
      "sequential_28 (Sequential)   (None, 4)                 109       \n",
      "=================================================================\n",
      "Total params: 42,959\n",
      "Trainable params: 42,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 14367 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "14367/14367 [==============================] - 1s 51us/step - loss: 1.5689 - acc: 0.4927 - val_loss: 1.1598 - val_acc: 0.5400\n",
      "Epoch 2/20\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.9018 - acc: 0.6150 - val_loss: 1.1086 - val_acc: 0.5590\n",
      "Epoch 3/20\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.7881 - acc: 0.6731 - val_loss: 1.0900 - val_acc: 0.6280\n",
      "Epoch 4/20\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.7085 - acc: 0.7342 - val_loss: 1.0914 - val_acc: 0.6320\n",
      "Epoch 5/20\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.6587 - acc: 0.7517 - val_loss: 1.0470 - val_acc: 0.6360\n",
      "Epoch 6/20\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.6093 - acc: 0.7636 - val_loss: 1.0663 - val_acc: 0.6110\n",
      "Epoch 7/20\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5717 - acc: 0.7711 - val_loss: 1.0733 - val_acc: 0.5930\n",
      "Epoch 8/20\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.5468 - acc: 0.7789 - val_loss: 1.0927 - val_acc: 0.6540\n",
      "Epoch 9/20\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.5251 - acc: 0.7828 - val_loss: 1.0339 - val_acc: 0.6270\n",
      "Epoch 10/20\n",
      "14367/14367 [==============================] - 0s 14us/step - loss: 0.5038 - acc: 0.7911 - val_loss: 1.0426 - val_acc: 0.6420\n",
      "Epoch 11/20\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4816 - acc: 0.7968 - val_loss: 1.1050 - val_acc: 0.6180\n",
      "Epoch 12/20\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4899 - acc: 0.7967 - val_loss: 1.0820 - val_acc: 0.6350\n",
      "Epoch 13/20\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4588 - acc: 0.8089 - val_loss: 1.0919 - val_acc: 0.6200\n",
      "Epoch 14/20\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4478 - acc: 0.8119 - val_loss: 1.1412 - val_acc: 0.6330\n",
      "Epoch 15/20\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4284 - acc: 0.8208 - val_loss: 1.2733 - val_acc: 0.6070\n",
      "Epoch 16/20\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4785 - acc: 0.7988 - val_loss: 1.1322 - val_acc: 0.6280\n",
      "Epoch 17/20\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4501 - acc: 0.8119 - val_loss: 1.1554 - val_acc: 0.6110\n",
      "Epoch 18/20\n",
      "14367/14367 [==============================] - 0s 12us/step - loss: 0.4254 - acc: 0.8209 - val_loss: 1.1806 - val_acc: 0.6260\n",
      "Epoch 19/20\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4176 - acc: 0.8230 - val_loss: 1.2456 - val_acc: 0.6230\n",
      "Epoch 20/20\n",
      "14367/14367 [==============================] - 0s 13us/step - loss: 0.4129 - acc: 0.8242 - val_loss: 1.2591 - val_acc: 0.6200\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deep_classifier = models.Sequential()\n",
    "deep_classifier.add(layers.Dense(7,kernel_initializer='random_normal',\n",
    "                       kernel_regularizer=regularizers.l2(0.01),\n",
    "                       activation='relu', \n",
    "                       input_shape=(10,)))\n",
    "deep_classifier.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model = Sequential([encoder,\n",
    "                   deep_classifier])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train.values,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) +1)\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training Acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n",
    "plt.title('Training and Validation Acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('acc')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer): \n",
    "    \n",
    "    def __init__(self, output_dim, input_dim=None, weights=None, alpha=1.0, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.alpha = alpha        \n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = [InputSpec(ndim=2)]\n",
    "        \n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_dim,)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = [InputSpec(dtype=K.floatx(),\n",
    "                                     shape=(None, input_dim))]\n",
    "        self.W = K.variable(self.initial_weights)\n",
    "        self.trainable_weights = [self.W]\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        q = 1.0/(1.0 + K.sqrt(K.sum(K.square(K.expand_dims(x, 1) - self.W), axis=2))**2/self.alpha)\n",
    "        q = q**((self.alpha+1.0)/2.0)\n",
    "        q = K.transpose(K.transpose(q)/K.sum(q, axis=1))\n",
    "        return q\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return (input_shape[0], self.output_dim)\n",
    "    \n",
    "    def get_output_shape(self, input_shape):\n",
    "        return self.get_output_shape(input_shape)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.output_dim\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'output_dim': self.output_dim,\n",
    "                  'input_dim': self.input_dim}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items() + list(config.items)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster centres with k-means.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.3192614 , -0.67387515, -1.0661435 ,  1.1519638 , -1.3874824 ,\n",
       "        -0.59217966,  0.04273814, -0.8631702 , -0.85108906,  1.3932242 ],\n",
       "       [-2.6727118 , -1.3539166 , -3.8449578 ,  3.337731  , -4.368597  ,\n",
       "        -2.1965017 , -0.07293184, -2.1344705 , -3.4495184 ,  2.6480927 ],\n",
       "       [-0.14163971,  0.17557585,  0.26744056, -0.11874795, -0.2751006 ,\n",
       "         0.08005375,  0.14786506, -0.0121094 ,  0.19194698, -0.03538728],\n",
       "       [-1.9628378 , -1.0120806 , -2.3214073 ,  2.1457813 , -2.7296338 ,\n",
       "        -1.3050716 , -0.03082382, -1.4509082 , -2.0170188 ,  2.0142813 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster_centers = None\n",
    "y_prediction = 0\n",
    "n_clusters=4\n",
    "# initialize cluster centres using k-means\n",
    "print('Initializing cluster centres with k-means.')\n",
    "if cluster_centers is None:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "    y_prediction = kmeans.fit_predict(encoder.predict(X))\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "cluster_centers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_11 (Sequential)   (None, 10)                42850     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 4)                 40        \n",
      "=================================================================\n",
      "Total params: 42,890\n",
      "Trainable params: 42,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 4\n",
    "model = Sequential([encoder,\n",
    "                    ClusteringLayer(n_clusters,\n",
    "                                    weights=cluster_centers,\n",
    "                                    name='clustering')])\n",
    "\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='kullback_leibler_divergence')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_mat(q):\n",
    "    weight = q**2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T\n",
    "\n",
    "labels = {\"av_training_set_AFP\": 0,\n",
    "          \"av_training_set_NTP\":1,\n",
    "          \"av_training_set_PC\":2,\n",
    "          \"av_training_set_UNK\":3}\n",
    "def cluster_acc(y_truth, y_pred):\n",
    "    yidx = y_truth.idxmax(1).values\n",
    "    y_true = np.vectorize(labels.get)(yidx)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max())+1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind])*1.0/y_pred.size, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03994622 0.78634316 0.01518675 0.15852384]\n",
      " [0.07820538 0.07552257 0.01839293 0.82787913]\n",
      " [0.17717837 0.05755479 0.0298038  0.735463  ]\n",
      " ...\n",
      " [0.02094117 0.90654707 0.00903262 0.0634792 ]\n",
      " [0.05905059 0.00886428 0.91173416 0.02035095]\n",
      " [0.0590625  0.00886436 0.9117213  0.02035187]]\n",
      "Iteration 0, Accuracy 0.38562\n",
      "95.91005%change in label assignment\n",
      "0.09967319\n",
      "[[0.02948083 0.8526732  0.01158308 0.10626293]\n",
      " [0.07728121 0.09487899 0.01902971 0.80881006]\n",
      " [0.13701274 0.05613574 0.02459089 0.78226066]\n",
      " ...\n",
      " [0.02355951 0.89860225 0.01036987 0.0674684 ]\n",
      " [0.05905515 0.00886198 0.91173935 0.0203435 ]\n",
      " [0.05906754 0.00886223 0.9117254  0.02034477]]\n",
      "Iteration 1, Accuracy 0.38911\n",
      "97.82491%change in label assignment\n",
      "0.097831935\n",
      "[[0.05044249 0.7139372  0.01851884 0.21710147]\n",
      " [0.07972737 0.06866463 0.01815165 0.8334564 ]\n",
      " [0.1997182  0.05813975 0.03218572 0.70995635]\n",
      " ...\n",
      " [0.02164274 0.8990746  0.00902496 0.07025772]\n",
      " [0.05888709 0.00884991 0.9119616  0.02030142]\n",
      " [0.05890264 0.00885018 0.9119442  0.02030298]]\n",
      "Iteration 2, Accuracy 0.39363\n",
      "95.92478%change in label assignment\n",
      "0.09998144\n",
      "[[0.02159693 0.90379554 0.00921809 0.06538944]\n",
      " [0.08599678 0.23842564 0.02505801 0.6505196 ]\n",
      " [0.0792597  0.06993333 0.01791468 0.8328923 ]\n",
      " ...\n",
      " [0.03384383 0.8626437  0.01569556 0.08781691]\n",
      " [0.05907077 0.00885804 0.91174364 0.02032754]\n",
      " [0.05909471 0.00885868 0.9117162  0.02033046]]\n",
      "Iteration 3, Accuracy 0.38474\n",
      "88.56484%change in label assignment\n",
      "0.10221727\n",
      "[[0.07243131 0.52892864 0.02477886 0.37386128]\n",
      " [0.10945169 0.05705743 0.02166248 0.81182843]\n",
      " [0.32318687 0.05885523 0.04366432 0.57429355]\n",
      " ...\n",
      " [0.03514902 0.8188383  0.01373046 0.13228223]\n",
      " [0.05870637 0.00883257 0.9122082  0.02025278]\n",
      " [0.05872053 0.00883282 0.91219246 0.0202542 ]]\n",
      "Iteration 4, Accuracy 0.40021\n",
      "83.75313%change in label assignment\n",
      "0.10466713\n",
      "[[0.04553862 0.8222764  0.02185098 0.110334  ]\n",
      " [0.06221563 0.62591875 0.0218049  0.29006073]\n",
      " [0.08577029 0.20859583 0.02434571 0.6812882 ]\n",
      " ...\n",
      " [0.06879169 0.74652064 0.03530844 0.14937927]\n",
      " [0.0591551  0.00886361 0.91162544 0.02035586]\n",
      " [0.059179   0.00886415 0.91159815 0.02035868]]\n",
      "Iteration 5, Accuracy 0.38557\n",
      "73.71238%change in label assignment\n",
      "0.10508941\n",
      "[[0.04089674 0.78035396 0.0155138  0.1632355 ]\n",
      " [0.07989598 0.06999702 0.0183127  0.8317943 ]\n",
      " [0.20083319 0.05829943 0.03241696 0.70845044]\n",
      " ...\n",
      " [0.02138376 0.9034753  0.00914336 0.06599757]\n",
      " [0.05870576 0.00883303 0.9122034  0.02025777]\n",
      " [0.05871441 0.00883305 0.9121943  0.02025833]]\n",
      "Iteration 6, Accuracy 0.41454\n",
      "77.95944%change in label assignment\n",
      "0.099273175\n",
      "[[0.03625591 0.8109144  0.01384011 0.13898958]\n",
      " [0.07783191 0.07470468 0.01796976 0.82949364]\n",
      " [0.19078791 0.05798535 0.03071194 0.7205148 ]\n",
      " ...\n",
      " [0.02187268 0.9040413  0.00946698 0.06461908]\n",
      " [0.05878132 0.00883219 0.91213334 0.0202532 ]\n",
      " [0.05879197 0.00883251 0.912121   0.02025445]]\n",
      "Iteration 7, Accuracy 0.38778\n",
      "98.13915%change in label assignment\n",
      "0.09960187\n",
      "[[0.04312866 0.764465   0.01621835 0.17618804]\n",
      " [0.07886942 0.06994207 0.01811201 0.8330765 ]\n",
      " [0.1913057  0.05794158 0.03129746 0.71945524]\n",
      " ...\n",
      " [0.02110833 0.9029335  0.00891071 0.06704747]\n",
      " [0.05874912 0.00882209 0.91220456 0.02022425]\n",
      " [0.05875824 0.00882208 0.91219485 0.02022478]]\n",
      "Iteration 8, Accuracy 0.39201\n",
      "97.95748%change in label assignment\n",
      "0.09872743\n",
      "[[0.03345752 0.8275964  0.01291978 0.12602632]\n",
      " [0.07684729 0.09543788 0.01896359 0.8087513 ]\n",
      " [0.1356175  0.05626022 0.02441834 0.7837039 ]\n",
      " ...\n",
      " [0.02146574 0.9053207  0.00928377 0.06392985]\n",
      " [0.0588151  0.00881964 0.9121539  0.02021136]\n",
      " [0.05883053 0.00881999 0.91213644 0.02021308]]\n",
      "Iteration 9, Accuracy 0.38793\n",
      "96.49924%change in label assignment\n",
      "0.09650352\n",
      "[[0.03762945 0.8002859  0.01441768 0.14766692]\n",
      " [0.07673351 0.09101722 0.01897717 0.8132721 ]\n",
      " [0.1481159  0.05663051 0.02633009 0.7689235 ]\n",
      " ...\n",
      " [0.02101439 0.90608853 0.00907954 0.06381752]\n",
      " [0.05865231 0.00881025 0.9123593  0.02017814]\n",
      " [0.05866482 0.00881029 0.91234577 0.02017909]]\n",
      "Iteration 10, Accuracy 0.39152\n",
      "98.7627%change in label assignment\n",
      "0.10095516\n",
      "[[0.04996264 0.7175616  0.01822854 0.21424723]\n",
      " [0.07827925 0.07173067 0.01786088 0.8321292 ]\n",
      " [0.19373816 0.0583483  0.03108599 0.7168275 ]\n",
      " ...\n",
      " [0.02135178 0.9011013  0.00888784 0.06865905]\n",
      " [0.05866629 0.00880539 0.91237056 0.02015775]\n",
      " [0.05868076 0.00880581 0.912354   0.02015951]]\n",
      "Iteration 11, Accuracy 0.3896\n",
      "97.51068%change in label assignment\n",
      "0.10290856\n",
      "[[0.06139519 0.62611854 0.02184439 0.29064187]\n",
      " [0.08145836 0.06563026 0.01829108 0.8346203 ]\n",
      " [0.22459681 0.05914895 0.03500588 0.6812483 ]\n",
      " ...\n",
      " [0.02318361 0.889517   0.00958761 0.07771175]\n",
      " [0.05863489 0.00879903 0.9124451  0.02012092]\n",
      " [0.058647   0.00879903 0.91243225 0.02012176]]\n",
      "Iteration 12, Accuracy 0.38503\n",
      "98.35027%change in label assignment\n",
      "0.10002375\n",
      "[[0.02513011 0.8791302  0.01015356 0.08558618]\n",
      " [0.08405281 0.1871157  0.02348581 0.7053457 ]\n",
      " [0.09084351 0.06050862 0.01894281 0.8297051 ]\n",
      " ...\n",
      " [0.03042561 0.87392926 0.01387481 0.08177033]\n",
      " [0.0588735  0.00880691 0.9121742  0.02014529]\n",
      " [0.0588957  0.00880748 0.9121487  0.02014798]]\n",
      "Iteration 13, Accuracy 0.38096\n",
      "89.11965%change in label assignment\n",
      "0.10432402\n",
      "[[0.08377766 0.18266805 0.0237746  0.70977974]\n",
      " [0.234693   0.05975613 0.03633398 0.66921693]\n",
      " [0.58149195 0.04625222 0.05476889 0.31748694]\n",
      " ...\n",
      " [0.06600781 0.5912573  0.02335528 0.3193797 ]\n",
      " [0.05833514 0.00877746 0.9128436  0.02004381]\n",
      " [0.05834431 0.00877761 0.91283345 0.02004468]]\n",
      "Iteration 14, Accuracy 0.3952\n",
      "75.71562%change in label assignment\n",
      "0.11092251\n",
      "[[0.04929482 0.8087393  0.0238699  0.11809599]\n",
      " [0.05309091 0.69634277 0.01919223 0.23137413]\n",
      " [0.08596075 0.22846678 0.02479809 0.6607744 ]\n",
      " ...\n",
      " [0.08635494 0.69157964 0.0462921  0.17577334]\n",
      " [0.05902623 0.00882544 0.91196287 0.02018546]\n",
      " [0.05904285 0.0088257  0.91194427 0.0201872 ]]\n",
      "Iteration 15, Accuracy 0.39019\n",
      "63.49487%change in label assignment\n",
      "0.108509555\n",
      "[[0.05153237 0.70343953 0.01886109 0.226167  ]\n",
      " [0.07746787 0.07166417 0.01798371 0.83288425]\n",
      " [0.198342   0.05825753 0.0320602  0.71134025]\n",
      " ...\n",
      " [0.02070756 0.90605426 0.00885479 0.06438338]\n",
      " [0.05864444 0.00878977 0.91244656 0.02011921]\n",
      " [0.05865653 0.00878991 0.91243327 0.02012029]]\n",
      "Iteration 16, Accuracy 0.41823\n",
      "77.61084%change in label assignment\n",
      "0.095055304\n",
      "[[0.08583044 0.27649254 0.02564276 0.6120343 ]\n",
      " [0.16866206 0.0574074  0.02828184 0.74564874]\n",
      " [0.47912446 0.0530693  0.05087164 0.41693464]\n",
      " ...\n",
      " [0.05002691 0.71582603 0.01826182 0.21588524]\n",
      " [0.05851395 0.00877627 0.9126147  0.02009508]\n",
      " [0.05852652 0.00877678 0.91259986 0.02009685]]\n",
      "Iteration 17, Accuracy 0.38454\n",
      "89.91997%change in label assignment\n",
      "0.102463774\n",
      "[[0.02082786 0.90740323 0.00898356 0.06278534]\n",
      " [0.08511058 0.2833804  0.02572306 0.60578597]\n",
      " [0.07710759 0.07043418 0.01769046 0.8347678 ]\n",
      " ...\n",
      " [0.04855094 0.81186455 0.02377949 0.11580501]\n",
      " [0.05875272 0.00879838 0.91230583 0.02014309]\n",
      " [0.05876742 0.00879847 0.91228986 0.02014434]]\n",
      "Iteration 18, Accuracy 0.38931\n",
      "78.35715%change in label assignment\n",
      "0.102972165\n",
      "[[0.05250443 0.6963628  0.01909736 0.23203543]\n",
      " [0.07779872 0.06881985 0.01777488 0.8356066 ]\n",
      " [0.20597534 0.05816901 0.03265688 0.70319873]\n",
      " ...\n",
      " [0.02038581 0.9074425  0.00868413 0.06348754]\n",
      " [0.0584559  0.00877482 0.9126698  0.02009946]\n",
      " [0.05846524 0.00877489 0.9126597  0.02010019]]\n",
      "Iteration 19, Accuracy 0.40315\n",
      "87.52394%change in label assignment\n",
      "0.09803805\n",
      "[[0.06566349 0.5891434  0.02274672 0.3224464 ]\n",
      " [0.08561    0.05998245 0.01818424 0.83622336]\n",
      " [0.2567506  0.0590648  0.03728102 0.64690363]\n",
      " ...\n",
      " [0.02264898 0.8915408  0.00923212 0.07657808]\n",
      " [0.0584223  0.00876757 0.9127291  0.0200811 ]\n",
      " [0.05843369 0.00876782 0.91271615 0.02008232]]\n",
      "Iteration 20, Accuracy 0.38376\n",
      "97.85928%change in label assignment\n",
      "0.09421751\n",
      "[[0.06796426 0.56642276 0.02346393 0.34214905]\n",
      " [0.0872765  0.05911398 0.01850817 0.8351013 ]\n",
      " [0.27460817 0.05902096 0.03917531 0.62719554]\n",
      " ...\n",
      " [0.022892   0.89050394 0.00938882 0.07721523]\n",
      " [0.05826574 0.00875842 0.9129189  0.02005694]\n",
      " [0.05827401 0.00875847 0.91291    0.02005757]]\n",
      "Iteration 21, Accuracy 0.38155\n",
      "98.95419%change in label assignment\n",
      "0.09618263\n",
      "[[0.03912539 0.79139274 0.01479133 0.15469053]\n",
      " [0.07633652 0.08943532 0.01855728 0.8156709 ]\n",
      " [0.16162331 0.05699892 0.02746904 0.75390875]\n",
      " ...\n",
      " [0.02284772 0.90118545 0.01004325 0.06592357]\n",
      " [0.0583175  0.00876022 0.91285527 0.02006708]\n",
      " [0.05832662 0.00876038 0.912845   0.02006798]]\n",
      "Iteration 22, Accuracy 0.38179\n",
      "94.69239%change in label assignment\n",
      "0.09745099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08329058 0.36626536 0.02646627 0.52397776]\n",
      " [0.12910469 0.05599522 0.0239225  0.79097754]\n",
      " [0.42676586 0.05553323 0.04955419 0.46814674]\n",
      " ...\n",
      " [0.03584418 0.81251776 0.01389123 0.13774683]\n",
      " [0.05805904 0.00874069 0.91319615 0.02000404]\n",
      " [0.05806452 0.00874068 0.9131904  0.02000432]]\n",
      "Iteration 23, Accuracy 0.39166\n",
      "89.93961%change in label assignment\n",
      "0.09890648\n",
      "[[0.02347559 0.8893694  0.00960241 0.07755255]\n",
      " [0.08328591 0.17171459 0.02289467 0.7221048 ]\n",
      " [0.10091737 0.05755255 0.02005029 0.82147986]\n",
      " ...\n",
      " [0.03962367 0.84220356 0.01872497 0.09944782]\n",
      " [0.05828227 0.00875684 0.912912   0.02004881]\n",
      " [0.05829059 0.00875695 0.91290295 0.02004956]]\n",
      "Iteration 24, Accuracy 0.38479\n",
      "85.75637%change in label assignment\n",
      "0.104344115\n",
      "[[0.05633186 0.6679373  0.02032999 0.2554008 ]\n",
      " [0.08124489 0.06445932 0.01811363 0.8361822 ]\n",
      " [0.2246288  0.05865462 0.0347692  0.68194735]\n",
      " ...\n",
      " [0.02119406 0.9023386  0.00895044 0.06751686]\n",
      " [0.05805692 0.00873506 0.91320026 0.0200077 ]\n",
      " [0.05806534 0.00873512 0.91319114 0.02000833]]\n",
      "Iteration 25, Accuracy 0.40026\n",
      "92.21289%change in label assignment\n",
      "0.098600715\n",
      "[[0.04374748 0.7644174  0.016322   0.17551304]\n",
      " [0.07889955 0.10030792 0.01948903 0.8013035 ]\n",
      " [0.1340725  0.05733041 0.0242387  0.78435844]\n",
      " ...\n",
      " [0.02350076 0.8975267  0.010202   0.06877056]\n",
      " [0.05830207 0.008741   0.9129235  0.02003342]\n",
      " [0.05833646 0.00874201 0.91288364 0.02003791]]\n",
      "Iteration 26, Accuracy 0.3842\n",
      "90.63681%change in label assignment\n",
      "0.10011258\n",
      "[[0.0839261  0.38050953 0.02716897 0.5083955 ]\n",
      " [0.14110355 0.05794632 0.02613057 0.77481955]\n",
      " [0.38814226 0.05754019 0.04864817 0.50566936]\n",
      " ...\n",
      " [0.04646908 0.74773824 0.0177027  0.18808995]\n",
      " [0.05793745 0.00871532 0.9133888  0.01995842]\n",
      " [0.05795121 0.00871547 0.9133737  0.01995965]]\n",
      "Iteration 27, Accuracy 0.38405\n",
      "85.60416%change in label assignment\n",
      "0.10681636\n",
      "[[0.02498457 0.8883977  0.01057906 0.0760387 ]\n",
      " [0.08770354 0.247993   0.02557458 0.63872886]\n",
      " [0.08486041 0.07017636 0.01874907 0.82621413]\n",
      " ...\n",
      " [0.04819369 0.8125322  0.02321602 0.11605808]\n",
      " [0.05838167 0.00874404 0.91284287 0.0200314 ]\n",
      " [0.05839182 0.00874413 0.9128318  0.02003222]]\n",
      "Iteration 28, Accuracy 0.38774\n",
      "82.75151%change in label assignment\n",
      "0.10614766\n",
      "[[0.05940244 0.64523244 0.0213775  0.27398762]\n",
      " [0.08337309 0.06494566 0.01864818 0.8330331 ]\n",
      " [0.22753215 0.05909534 0.03547755 0.67789495]\n",
      " ...\n",
      " [0.02284943 0.8932728  0.00959247 0.0742853 ]\n",
      " [0.05801621 0.00871572 0.91330034 0.0199678 ]\n",
      " [0.05802919 0.00871584 0.9132861  0.01996893]]\n",
      "Iteration 29, Accuracy 0.403\n",
      "88.48628%change in label assignment\n",
      "0.1026797\n",
      "[[0.02484926 0.888453   0.01047623 0.07622147]\n",
      " [0.08764093 0.24042399 0.02542062 0.6465145 ]\n",
      " [0.08356038 0.07189348 0.01866705 0.8258791 ]\n",
      " ...\n",
      " [0.04418042 0.8261289  0.02101381 0.10867684]\n",
      " [0.05838203 0.00873251 0.9128568  0.02002866]\n",
      " [0.05841322 0.00873355 0.9128203  0.02003293]]\n",
      "Iteration 30, Accuracy 0.38155\n",
      "85.16718%change in label assignment\n",
      "0.09895995\n",
      "[[0.04233496 0.76978475 0.01605478 0.17182545]\n",
      " [0.07712699 0.08187909 0.01868343 0.8223105 ]\n",
      " [0.15337506 0.05684723 0.02716035 0.76261735]\n",
      " ...\n",
      " [0.02161394 0.90369934 0.00939679 0.06528997]\n",
      " [0.05801611 0.00870582 0.9133161  0.01996204]\n",
      " [0.05804202 0.00870635 0.9132867  0.01996495]]\n",
      "Iteration 31, Accuracy 0.40021\n",
      "90.17037%change in label assignment\n",
      "0.0992952\n",
      "[[0.02679331 0.8693429  0.01070015 0.09316362]\n",
      " [0.0797186  0.12510125 0.0207162  0.77446395]\n",
      " [0.10399013 0.05689155 0.02042145 0.8186969 ]\n",
      " ...\n",
      " [0.02719715 0.8857182  0.01222299 0.07486162]\n",
      " [0.05811574 0.0087094  0.91320026 0.01997454]\n",
      " [0.05814347 0.00871022 0.91316813 0.01997809]]\n",
      "Iteration 32, Accuracy 0.38631\n",
      "96.01316%change in label assignment\n",
      "0.09580743\n",
      "[[0.08274791 0.38192517 0.02660368 0.5087233 ]\n",
      " [0.1215856  0.05589158 0.02310022 0.7994226 ]\n",
      " [0.35093692 0.05805739 0.04544303 0.5455627 ]\n",
      " ...\n",
      " [0.04269109 0.7686398  0.01623437 0.17243472]\n",
      " [0.05779718 0.0086839  0.91361433 0.01990462]\n",
      " [0.05783302 0.00868493 0.9135727  0.01990932]]\n",
      "Iteration 33, Accuracy 0.39319\n",
      "87.37173%change in label assignment\n",
      "0.10491823\n",
      "[[0.04283859 0.8313146  0.02040571 0.10544103]\n",
      " [0.07099073 0.54398954 0.02409283 0.36092684]\n",
      " [0.08284394 0.16163607 0.02253113 0.7329889 ]\n",
      " ...\n",
      " [0.07528599 0.72609264 0.03933757 0.15928382]\n",
      " [0.05809933 0.00871499 0.9131715  0.02001416]\n",
      " [0.05811582 0.00871515 0.9131533  0.02001567]]\n",
      "Iteration 34, Accuracy 0.38783\n",
      "75.804%change in label assignment\n",
      "0.10172604\n",
      "[[0.02800848 0.85967284 0.01108658 0.10123206]\n",
      " [0.07643557 0.097943   0.0191659  0.80645555]\n",
      " [0.1352425  0.05539761 0.02447364 0.78488624]\n",
      " ...\n",
      " [0.02318967 0.9005121  0.01031866 0.06597964]\n",
      " [0.05766679 0.00868729 0.91371393 0.01993197]\n",
      " [0.05767702 0.00868749 0.91370237 0.01993302]]\n",
      "Iteration 35, Accuracy 0.41587\n",
      "82.87426%change in label assignment\n",
      "0.09468186\n",
      "[[0.04021176 0.78445864 0.01513535 0.16019428]\n",
      " [0.07593338 0.07873918 0.01791598 0.8274115 ]\n",
      " [0.16244228 0.05633127 0.02745231 0.7537741 ]\n",
      " ...\n",
      " [0.02017286 0.90918785 0.00857532 0.06206395]\n",
      " [0.05766745 0.00867906 0.91372585 0.01992764]\n",
      " [0.05767898 0.00867934 0.9137128  0.01992894]]\n",
      "Iteration 36, Accuracy 0.39152\n",
      "98.45338%change in label assignment\n",
      "0.09422039\n",
      "[[0.08158356 0.3978419  0.02618192 0.49439263]\n",
      " [0.12552203 0.05501505 0.02317448 0.79628843]\n",
      " [0.38448325 0.05677918 0.04674848 0.51198906]\n",
      " ...\n",
      " [0.04456738 0.75376755 0.01664336 0.18502168]\n",
      " [0.05745737 0.00866224 0.9140266  0.01985385]\n",
      " [0.05746778 0.00866252 0.9140145  0.01985509]]\n",
      "Iteration 37, Accuracy 0.38705\n",
      "90.24402%change in label assignment\n",
      "0.10166803\n",
      "[[0.0221183  0.90306115 0.00958005 0.06524047]\n",
      " [0.08566561 0.29254913 0.02586553 0.5959197 ]\n",
      " [0.07714555 0.07297736 0.01774    0.8321371 ]\n",
      " ...\n",
      " [0.04933515 0.8095762  0.02410453 0.11698411]\n",
      " [0.05768764 0.00868175 0.91372246 0.01990816]\n",
      " [0.05770125 0.00868197 0.9137073  0.01990953]]\n",
      "Iteration 38, Accuracy 0.38823\n",
      "79.7221%change in label assignment\n",
      "0.103880085\n",
      "[[0.08207189 0.3920289  0.02629809 0.4996012 ]\n",
      " [0.1286539  0.05522008 0.02363682 0.7924892 ]\n",
      " [0.3925474  0.05648256 0.04730202 0.503668  ]\n",
      " ...\n",
      " [0.04396731 0.75864995 0.0164857  0.18089698]\n",
      " [0.05729366 0.0086533  0.9142175  0.01983554]\n",
      " [0.05730255 0.00865347 0.91420746 0.01983646]]\n",
      "Iteration 39, Accuracy 0.40242\n",
      "79.39805%change in label assignment\n",
      "[[0.02173145 0.90256494 0.00921643 0.06648723]\n",
      " [0.08611762 0.2601939  0.02541182 0.62827665]\n",
      " [0.08055667 0.06639125 0.01780277 0.8352493 ]\n",
      " ...\n",
      " [0.04773931 0.8148877  0.02315342 0.11421961]\n",
      " [0.05755907 0.00867173 0.9138858  0.01988347]\n",
      " [0.057571   0.00867199 0.9138723  0.01988477]]\n",
      "Iteration 40, Accuracy 0.38774\n",
      "81.44057%change in label assignment\n",
      "0.10313308\n",
      "[[0.07570023 0.48915803 0.02547372 0.40966803]\n",
      " [0.10057092 0.05657081 0.02033023 0.82252806]\n",
      " [0.34613097 0.05798662 0.04494466 0.5509377 ]\n",
      " ...\n",
      " [0.02276265 0.8930892  0.00947746 0.0746707 ]\n",
      " [0.05724638 0.00864715 0.9142853  0.01982114]\n",
      " [0.0572546  0.0086472  0.9142765  0.01982178]]\n",
      "Iteration 41, Accuracy 0.39952\n",
      "85.41268%change in label assignment\n",
      "0.09771184\n",
      "[[0.03741264 0.8035309  0.01421779 0.14483875]\n",
      " [0.07816904 0.11166853 0.01982863 0.79033375]\n",
      " [0.12617107 0.05603841 0.02307789 0.7947126 ]\n",
      " ...\n",
      " [0.02568214 0.89108044 0.01143574 0.07180168]\n",
      " [0.05753189 0.00865478 0.9139787  0.01983468]\n",
      " [0.0575464  0.0086552  0.91396195 0.01983646]]\n",
      "Iteration 42, Accuracy 0.3817\n",
      "90.26857%change in label assignment\n",
      "0.099725075\n",
      "[[0.07503145 0.4941354  0.02534222 0.405491  ]\n",
      " [0.11001581 0.05601277 0.02159345 0.8123779 ]\n",
      " [0.3347528  0.05839869 0.04427597 0.5625725 ]\n",
      " ...\n",
      " [0.02690166 0.86779296 0.0108847  0.09442069]\n",
      " [0.0572166  0.00863505 0.9143801  0.01976829]\n",
      " [0.05722644 0.00863513 0.9143693  0.01976911]]\n",
      "Iteration 43, Accuracy 0.3895\n",
      "90.27348%change in label assignment\n",
      "0.10188463\n",
      "[[0.02326966 0.89756644 0.01000917 0.06915475]\n",
      " [0.08593529 0.2998047  0.02594677 0.5883133 ]\n",
      " [0.07759401 0.08119594 0.01822576 0.8229843 ]\n",
      " ...\n",
      " [0.04785443 0.8140461  0.02314994 0.11494955]\n",
      " [0.057628   0.00865662 0.9138885  0.01982688]\n",
      " [0.05764923 0.00865716 0.91386425 0.01982938]]\n",
      "Iteration 44, Accuracy 0.38356\n",
      "81.52403%change in label assignment\n",
      "0.10844468\n",
      "[[0.08160654 0.40765128 0.02662504 0.48411724]\n",
      " [0.13432592 0.05675473 0.02492907 0.78399026]\n",
      " [0.3922245  0.05702038 0.04829409 0.5024611 ]\n",
      " ...\n",
      " [0.03131098 0.8421303  0.01247294 0.11408573]\n",
      " [0.05711807 0.00862623 0.9145192  0.01973642]\n",
      " [0.05712933 0.00862637 0.914507   0.01973745]]\n",
      "Iteration 45, Accuracy 0.40075\n",
      "79.42751%change in label assignment\n",
      "0.10076697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02162325 0.9026059  0.00915003 0.06662089]\n",
      " [0.08488634 0.22075696 0.02435646 0.6700002 ]\n",
      " [0.07969226 0.06599725 0.01761189 0.8366986 ]\n",
      " ...\n",
      " [0.04447736 0.82563114 0.02134941 0.10854208]\n",
      " [0.05748931 0.00864576 0.91406465 0.0198003 ]\n",
      " [0.05750524 0.00864612 0.9140465  0.01980209]]\n",
      "Iteration 46, Accuracy 0.3867\n",
      "83.55673%change in label assignment\n",
      "0.10408607\n",
      "[[0.04800155 0.72937346 0.01778286 0.20484217]\n",
      " [0.0791555  0.06616965 0.01790323 0.83677167]\n",
      " [0.18362635 0.05744903 0.0303922  0.72853243]\n",
      " ...\n",
      " [0.02074792 0.90606505 0.00888809 0.06429899]\n",
      " [0.05713503 0.00862287 0.9145079  0.01973414]\n",
      " [0.0571537  0.00862322 0.9144869  0.01973613]]\n",
      "Iteration 47, Accuracy 0.40109\n",
      "89.96907%change in label assignment\n",
      "0.099592395\n",
      "[[0.03294915 0.8309082  0.01272516 0.1234175 ]\n",
      " [0.07574106 0.08462172 0.01811834 0.82151896]\n",
      " [0.13044651 0.05580809 0.02356086 0.7901845 ]\n",
      " ...\n",
      " [0.02310886 0.89987797 0.01011892 0.06689423]\n",
      " [0.05731248 0.00862486 0.91431624 0.01974649]\n",
      " [0.05732867 0.00862524 0.9142977  0.01974837]]\n",
      "Iteration 48, Accuracy 0.38243\n",
      "96.80365%change in label assignment\n",
      "0.098859854\n",
      "[[0.06491468 0.5926386  0.02275271 0.31969404]\n",
      " [0.09006134 0.05775457 0.01890413 0.83327997]\n",
      " [0.23947978 0.05874182 0.03616788 0.66561055]\n",
      " ...\n",
      " [0.0233033  0.88782233 0.00956936 0.079305  ]\n",
      " [0.05711285 0.00860822 0.9145935  0.01968532]\n",
      " [0.05712982 0.00860844 0.91457474 0.01968698]]\n",
      "Iteration 49, Accuracy 0.39088\n",
      "94.6433%change in label assignment\n",
      "0.098254606\n",
      "[[0.0217949  0.89808035 0.00899899 0.07112575]\n",
      " [0.0832653  0.19501111 0.02345786 0.69826573]\n",
      " [0.08062473 0.06314351 0.01757051 0.83866125]\n",
      " ...\n",
      " [0.03677789 0.851986   0.0172547  0.09398134]\n",
      " [0.0572856  0.00861907 0.9143875  0.01970774]\n",
      " [0.05730872 0.00861955 0.91436136 0.01971038]]\n",
      "Iteration 50, Accuracy 0.38337\n",
      "87.94619%change in label assignment\n",
      "0.101707235\n",
      "[[0.08243177 0.37425017 0.0264014  0.51691663]\n",
      " [0.14100902 0.05629711 0.02550995 0.7771839 ]\n",
      " [0.4067518  0.05644349 0.0486949  0.48810974]\n",
      " ...\n",
      " [0.03334809 0.82735854 0.013079   0.12621439]\n",
      " [0.05692911 0.00859413 0.91485244 0.01962434]\n",
      " [0.05693842 0.00859421 0.9148422  0.01962515]]\n",
      "Iteration 51, Accuracy 0.39574\n",
      "82.12304%change in label assignment\n",
      "0.10522104\n",
      "[[0.04447233 0.8250536  0.02125324 0.10922084]\n",
      " [0.05998326 0.6402645  0.02115973 0.2785925 ]\n",
      " [0.08510245 0.22222881 0.02444407 0.6682247 ]\n",
      " ...\n",
      " [0.08071863 0.70864373 0.04270648 0.16793118]\n",
      " [0.05750396 0.00862757 0.91413116 0.01973727]\n",
      " [0.05752254 0.00862787 0.9141103  0.01973923]]\n",
      "Iteration 52, Accuracy 0.38798\n",
      "70.74189%change in label assignment\n",
      "0.10677916\n",
      "[[0.06774773 0.5653147  0.02348825 0.34344932]\n",
      " [0.0872125  0.05876658 0.01856215 0.8354588 ]\n",
      " [0.2623327  0.05906453 0.03830634 0.6402964 ]\n",
      " ...\n",
      " [0.02121882 0.9009318  0.00890706 0.06894241]\n",
      " [0.05697672 0.00859336 0.9147954  0.01963448]\n",
      " [0.05698848 0.00859353 0.9147824  0.01963565]]\n",
      "Iteration 53, Accuracy 0.41744\n",
      "75.75981%change in label assignment\n",
      "0.099586874\n",
      "[[0.03900946 0.791323   0.01473098 0.15493652]\n",
      " [0.07512643 0.08542285 0.01806656 0.82138413]\n",
      " [0.1426291  0.05621263 0.02507113 0.7760871 ]\n",
      " ...\n",
      " [0.02353892 0.8983873  0.01037082 0.06770297]\n",
      " [0.0571958  0.00859849 0.9145471  0.01965864]\n",
      " [0.05720799 0.00859875 0.9145332  0.01966   ]]\n",
      "Iteration 54, Accuracy 0.38238\n",
      "94.83478%change in label assignment\n",
      "0.09656078\n",
      "[[0.04919687 0.7212923  0.01830145 0.21120937]\n",
      " [0.0782317  0.07192332 0.01831067 0.8315344 ]\n",
      " [0.17652434 0.05771106 0.0299646  0.73579997]\n",
      " ...\n",
      " [0.02152758 0.90275556 0.00929828 0.06641855]\n",
      " [0.0570261  0.00858437 0.91474813 0.01964141]\n",
      " [0.05703674 0.00858441 0.9147366  0.01964223]]\n",
      "Iteration 55, Accuracy 0.38891\n",
      "98.21771%change in label assignment\n",
      "0.09029409\n",
      "[[0.07267834 0.53997123 0.02454264 0.36280778]\n",
      " [0.09609944 0.06148517 0.01964264 0.82277274]\n",
      " [0.26207566 0.06035613 0.03752384 0.64004433]\n",
      " ...\n",
      " [0.02855372 0.8620695  0.01139939 0.09797737]\n",
      " [0.05713189 0.00858093 0.91462743 0.01965977]\n",
      " [0.05714377 0.00858128 0.9146137  0.01966129]]\n",
      "Iteration 56, Accuracy 0.38484\n",
      "96.24392%change in label assignment\n",
      "0.09718107\n",
      "[[0.02573894 0.87834525 0.01078351 0.08513232]\n",
      " [0.08473441 0.1461711  0.0234088  0.7456857 ]\n",
      " [0.10682494 0.0607617  0.02223745 0.81017584]\n",
      " ...\n",
      " [0.03790219 0.8469226  0.01809148 0.09708373]\n",
      " [0.05696089 0.00858247 0.91479003 0.01966658]\n",
      " [0.05696841 0.00858223 0.9147828  0.01966664]]\n",
      "Iteration 57, Accuracy 0.38081\n",
      "91.20145%change in label assignment\n",
      "0.099666245\n",
      "[[0.07625416 0.4762277  0.02517746 0.42234063]\n",
      " [0.11825418 0.0546183  0.02198785 0.80513966]\n",
      " [0.35805064 0.05754686 0.04457163 0.5398308 ]\n",
      " ...\n",
      " [0.02756386 0.8621363  0.0108917  0.09940817]\n",
      " [0.05687268 0.00856581 0.9149265  0.01963502]\n",
      " [0.05688354 0.00856612 0.914914   0.01963636]]\n",
      "Iteration 58, Accuracy 0.3976\n",
      "88.27515%change in label assignment\n",
      "0.09602759\n",
      "[[0.0426391  0.76532453 0.01595969 0.17607674]\n",
      " [0.07561085 0.06845875 0.01731829 0.83861214]\n",
      " [0.19055371 0.0570819  0.03069876 0.7216657 ]\n",
      " ...\n",
      " [0.02160005 0.90598977 0.00951409 0.06289607]\n",
      " [0.05685374 0.00856888 0.9149486  0.01962878]\n",
      " [0.05686207 0.00856893 0.91493946 0.01962944]]\n",
      "Iteration 59, Accuracy 0.3844\n",
      "94.67275%change in label assignment\n",
      "0.09468803\n",
      "[[0.07561465 0.4794105  0.02510104 0.41987377]\n",
      " [0.11023953 0.05437417 0.02104656 0.81433976]\n",
      " [0.33838534 0.0579398  0.0434977  0.5601772 ]\n",
      " ...\n",
      " [0.02566405 0.8727375  0.01025278 0.09134565]\n",
      " [0.0567445  0.00855516 0.9151072  0.01959312]\n",
      " [0.05675517 0.00855539 0.91509503 0.01959433]]\n",
      "Iteration 60, Accuracy 0.38813\n",
      "94.68257%change in label assignment\n",
      "0.092730545\n",
      "[[0.03096554 0.840871   0.01205394 0.11610955]\n",
      " [0.07489198 0.09650088 0.01867444 0.8099327 ]\n",
      " [0.13228601 0.0548646  0.02381817 0.78903127]\n",
      " ...\n",
      " [0.02697593 0.88708806 0.0122569  0.07367913]\n",
      " [0.05677776 0.00855953 0.915056   0.01960673]\n",
      " [0.05678781 0.00855966 0.9150448  0.0196077 ]]\n",
      "Iteration 61, Accuracy 0.38435\n",
      "91.14744%change in label assignment\n",
      "0.09764107\n",
      "[[0.0848097  0.27510196 0.02557812 0.61451024]\n",
      " [0.2002405  0.05738265 0.03182732 0.7105495 ]\n",
      " [0.5196138  0.05003638 0.05208961 0.3782602 ]\n",
      " ...\n",
      " [0.04684867 0.7370842  0.01740316 0.19866389]\n",
      " [0.05648969 0.0085364  0.9154282  0.01954566]\n",
      " [0.05649942 0.00853666 0.9154171  0.01954681]]\n",
      "Iteration 62, Accuracy 0.39299\n",
      "85.1279%change in label assignment\n",
      "0.1029151\n",
      "[[0.02684029 0.8867347  0.01202243 0.0744025 ]\n",
      " [0.08410031 0.34206218 0.02607652 0.547761  ]\n",
      " [0.07578239 0.08765259 0.01829003 0.81827503]\n",
      " ...\n",
      " [0.06464403 0.75907934 0.03291024 0.14336644]\n",
      " [0.05694643 0.00856549 0.914874   0.019614  ]\n",
      " [0.05695726 0.00856561 0.9148621  0.01961499]]\n",
      "Iteration 63, Accuracy 0.38592\n",
      "75.68616%change in label assignment\n",
      "0.099631585\n",
      "[[0.04909436 0.72048396 0.018125   0.21229671]\n",
      " [0.07786685 0.06550181 0.01763119 0.83900017]\n",
      " [0.207981   0.05775503 0.03286207 0.7014019 ]\n",
      " ...\n",
      " [0.02104604 0.90700233 0.00920401 0.06274761]\n",
      " [0.05656983 0.00853749 0.9153435  0.0195492 ]\n",
      " [0.0565783  0.00853758 0.91533417 0.01954992]]\n",
      "Iteration 64, Accuracy 0.40998\n",
      "85.53052%change in label assignment\n",
      "0.09963333\n",
      "[[0.08224878 0.37940958 0.02601463 0.51232696]\n",
      " [0.12319725 0.05484315 0.02260194 0.7993577 ]\n",
      " [0.36534667 0.05751319 0.0449914  0.5321488 ]\n",
      " ...\n",
      " [0.03199815 0.8346438  0.01239844 0.1209596 ]\n",
      " [0.0565868  0.00852796 0.91535676 0.01952844]\n",
      " [0.05659968 0.00852834 0.9153419  0.01953009]]\n",
      "Iteration 65, Accuracy 0.38607\n",
      "93.42564%change in label assignment\n",
      "0.10012576\n",
      "[[0.02387152 0.8980376  0.01068039 0.06741048]\n",
      " [0.08263293 0.36295193 0.02614671 0.5282684 ]\n",
      " [0.07478455 0.09789483 0.01878366 0.80853695]\n",
      " ...\n",
      " [0.06280059 0.7650449  0.03208536 0.14006916]\n",
      " [0.05681844 0.008547   0.9150529  0.01958163]\n",
      " [0.05684508 0.00854762 0.9150225  0.01958488]]\n",
      "Iteration 66, Accuracy 0.38715\n",
      "77.10021%change in label assignment\n",
      "0.10411866\n",
      "[[0.0789461  0.43225387 0.02581724 0.4629827 ]\n",
      " [0.09254967 0.05547379 0.01890135 0.8330752 ]\n",
      " [0.27793565 0.05844026 0.03913483 0.6244893 ]\n",
      " ...\n",
      " [0.02350823 0.88570803 0.00956165 0.08122209]\n",
      " [0.05651614 0.0085192  0.9154506  0.01951409]\n",
      " [0.05653698 0.00851975 0.91542655 0.01951672]]\n",
      "Iteration 67, Accuracy 0.40777\n",
      "81.58786%change in label assignment\n",
      "0.10099514\n",
      "[[0.05836338 0.64920557 0.02069599 0.27173504]\n",
      " [0.07484889 0.07382738 0.01739669 0.83392704]\n",
      " [0.16858372 0.05669467 0.02806509 0.74665654]\n",
      " ...\n",
      " [0.02112843 0.9070982  0.00919368 0.06257966]\n",
      " [0.05662917 0.00852429 0.9153356  0.01951092]\n",
      " [0.05665118 0.00852488 0.9153103  0.01951369]]\n",
      "Iteration 68, Accuracy 0.38572\n",
      "95.10974%change in label assignment\n",
      "0.09972662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0844899  0.25073886 0.02524632 0.6395249 ]\n",
      " [0.14355676 0.05557566 0.02560294 0.7752647 ]\n",
      " [0.41511577 0.05572514 0.04868606 0.48047298]\n",
      " ...\n",
      " [0.0393125  0.788411   0.01505551 0.15722097]\n",
      " [0.0563884  0.00850526 0.91564804 0.01945833]\n",
      " [0.05640572 0.00850569 0.9156282  0.01946043]]\n",
      "Iteration 69, Accuracy 0.38017\n",
      "89.44371%change in label assignment\n",
      "0.10718087\n",
      "[[0.02849184 0.8805843  0.0128316  0.07809234]\n",
      " [0.07225339 0.5256539  0.02433799 0.37775466]\n",
      " [0.08158281 0.15463893 0.02201232 0.7417659 ]\n",
      " ...\n",
      " [0.07730333 0.71922785 0.04060501 0.16286385]\n",
      " [0.05694347 0.00853963 0.9149484  0.01956853]\n",
      " [0.05699027 0.00854144 0.9148927  0.01957559]]\n",
      "Iteration 70, Accuracy 0.3896\n",
      "70.55531%change in label assignment\n",
      "0.10085694\n",
      "[[0.04741136 0.73152953 0.01759989 0.20345926]\n",
      " [0.0748213  0.09476705 0.0188067  0.811605  ]\n",
      " [0.12671602 0.05478024 0.02343661 0.7950671 ]\n",
      " ...\n",
      " [0.02530177 0.8927274  0.0114705  0.07050034]\n",
      " [0.05653466 0.00850918 0.9154707  0.0194854 ]\n",
      " [0.05656721 0.00851013 0.91543293 0.01948973]]\n",
      "Iteration 71, Accuracy 0.4115\n",
      "83.8366%change in label assignment\n",
      "0.0990986\n",
      "[[0.05624161 0.6651403  0.02010355 0.25851458]\n",
      " [0.07530077 0.06756227 0.0171159  0.8400211 ]\n",
      " [0.17641567 0.05660799 0.0289564  0.73801994]\n",
      " ...\n",
      " [0.02011128 0.9105289  0.00868823 0.06067163]\n",
      " [0.05643046 0.00850104 0.9156041  0.01946438]\n",
      " [0.05644625 0.00850131 0.9155864  0.01946606]]\n",
      "Iteration 72, Accuracy 0.38381\n",
      "94.3487%change in label assignment\n",
      "0.09205367\n",
      "[[0.071307   0.5265594  0.02427802 0.37785554]\n",
      " [0.09139866 0.05546473 0.01875399 0.83438265]\n",
      " [0.2608405  0.05837214 0.03768973 0.6430976 ]\n",
      " ...\n",
      " [0.0222936  0.89254165 0.00913642 0.07602825]\n",
      " [0.05624991 0.00848643 0.91583437 0.01942936]\n",
      " [0.05626166 0.00848657 0.91582125 0.01943049]]\n",
      "Iteration 73, Accuracy 0.38214\n",
      "94.64821%change in label assignment\n",
      "0.092787735\n",
      "[[0.02225072 0.89376116 0.00906884 0.07491925]\n",
      " [0.07924075 0.14070575 0.0211561  0.7588974 ]\n",
      " [0.09128141 0.05596061 0.01855381 0.8342042 ]\n",
      " ...\n",
      " [0.03573339 0.8561245  0.01678055 0.09136153]\n",
      " [0.05623665 0.00849233 0.91580683 0.01946415]\n",
      " [0.05624806 0.00849246 0.9157942  0.01946524]]\n",
      "Iteration 74, Accuracy 0.38533\n",
      "91.18181%change in label assignment\n",
      "0.09564898\n",
      "[[0.07029068 0.5392513  0.02399193 0.36646605]\n",
      " [0.10204846 0.05422854 0.02003903 0.8236839 ]\n",
      " [0.28856915 0.058253   0.03987177 0.6133061 ]\n",
      " ...\n",
      " [0.02503121 0.8764374  0.01006089 0.08847049]\n",
      " [0.05594416 0.00847048 0.91618496 0.01940032]\n",
      " [0.05595532 0.00847061 0.9161727  0.01940138]]\n",
      "Iteration 75, Accuracy 0.39755\n",
      "89.16875%change in label assignment\n",
      "0.09268778\n",
      "[[0.02399554 0.8835184  0.00965691 0.08282917]\n",
      " [0.0771696  0.1181122  0.0199151  0.7848031 ]\n",
      " [0.10109236 0.05500864 0.01976541 0.8241336 ]\n",
      " ...\n",
      " [0.03240312 0.86775476 0.01500748 0.08483461]\n",
      " [0.05601174 0.00847752 0.91610277 0.01940789]\n",
      " [0.05602383 0.00847769 0.9160895  0.01940909]]\n",
      "Iteration 76, Accuracy 0.38572\n",
      "90.93632%change in label assignment\n",
      "0.094421595\n",
      "[[0.07903846 0.4282666  0.02576752 0.46692735]\n",
      " [0.12459046 0.05436224 0.02293664 0.79811066]\n",
      " [0.34155717 0.05779303 0.04382171 0.5568281 ]\n",
      " ...\n",
      " [0.03091571 0.84069896 0.01209992 0.11628542]\n",
      " [0.05576777 0.00845651 0.91644084 0.01933482]\n",
      " [0.05578037 0.00845673 0.9164267  0.01933615]]\n",
      "Iteration 77, Accuracy 0.39603\n",
      "89.41916%change in label assignment\n",
      "0.09916674\n",
      "[[0.0206677  0.90488523 0.00864221 0.06580484]\n",
      " [0.08315106 0.19674712 0.02346908 0.6966327 ]\n",
      " [0.0790417  0.0634488  0.01732958 0.8401799 ]\n",
      " ...\n",
      " [0.04609836 0.82018924 0.02232682 0.1113855 ]\n",
      " [0.05595111 0.00847087 0.916203   0.01937496]\n",
      " [0.05596676 0.00847114 0.91618544 0.01937664]]\n",
      "Iteration 78, Accuracy 0.38975\n",
      "83.33088%change in label assignment\n",
      "0.10044146\n",
      "[[0.08145501 0.39121246 0.02617846 0.5011541 ]\n",
      " [0.14541383 0.05528208 0.02565358 0.7736505 ]\n",
      " [0.38284856 0.05667199 0.04660655 0.5138729 ]\n",
      " ...\n",
      " [0.03766786 0.79894984 0.01443977 0.1489426 ]\n",
      " [0.05554004 0.00844332 0.91670775 0.01930893]\n",
      " [0.05555162 0.0084435  0.91669476 0.01931009]]\n",
      "Iteration 79, Accuracy 0.39981\n",
      "81.14106%change in label assignment\n",
      "[[0.02284374 0.9002134  0.00990368 0.06703921]\n",
      " [0.08548603 0.25526547 0.02511727 0.6341312 ]\n",
      " [0.07637748 0.07573646 0.01767309 0.83021295]\n",
      " ...\n",
      " [0.05698517 0.78403145 0.02836225 0.13062121]\n",
      " [0.05580254 0.00846203 0.9163698  0.01936561]\n",
      " [0.0558161  0.00846225 0.91635454 0.01936704]]\n",
      "Iteration 80, Accuracy 0.38857\n",
      "80.56169%change in label assignment\n",
      "0.1014794\n",
      "[[0.08420985 0.3394132  0.02651695 0.54986   ]\n",
      " [0.15722929 0.05620604 0.02735443 0.7592102 ]\n",
      " [0.45357883 0.05382825 0.05051357 0.44207937]\n",
      " ...\n",
      " [0.02390913 0.8853188  0.00983953 0.08093251]\n",
      " [0.05538524 0.00843416 0.9168934  0.01928715]\n",
      " [0.05539557 0.00843426 0.9168821  0.01928807]]\n",
      "Iteration 81, Accuracy 0.40477\n",
      "81.12142%change in label assignment\n",
      "0.09796607\n",
      "[[0.02279414 0.8926237  0.00932668 0.07525543]\n",
      " [0.08252023 0.17512411 0.02271778 0.7196378 ]\n",
      " [0.08546738 0.05998157 0.017954   0.8365971 ]\n",
      " ...\n",
      " [0.04715869 0.8164466  0.02280219 0.11359251]\n",
      " [0.05578484 0.00845203 0.916435   0.01932812]\n",
      " [0.0557984  0.00845231 0.91641957 0.01932965]]\n",
      "Iteration 82, Accuracy 0.38523\n",
      "84.1852%change in label assignment\n",
      "0.100800365\n",
      "[[0.08373129 0.34025976 0.02637123 0.5496377 ]\n",
      " [0.16604817 0.05651573 0.02835587 0.74908024]\n",
      " [0.45468298 0.05386091 0.05050769 0.44094846]\n",
      " ...\n",
      " [0.02458222 0.8804528  0.01003502 0.08493002]\n",
      " [0.05538267 0.00842614 0.9169435  0.01924764]\n",
      " [0.05539313 0.00842625 0.916932   0.01924859]]\n",
      "Iteration 83, Accuracy 0.40045\n",
      "84.09191%change in label assignment\n",
      "0.100450665\n",
      "[[0.02562992 0.8753144  0.01025585 0.08879974]\n",
      " [0.08068026 0.15036991 0.02161882 0.74733096]\n",
      " [0.09546033 0.05712992 0.01910625 0.82830346]\n",
      " ...\n",
      " [0.04750748 0.81508076 0.02297831 0.11443341]\n",
      " [0.05575289 0.00844445 0.91650945 0.01929324]\n",
      " [0.05576656 0.00844474 0.91649383 0.01929478]]\n",
      "Iteration 84, Accuracy 0.38543\n",
      "85.24083%change in label assignment\n",
      "0.10396734\n",
      "[[0.06213684 0.6174139  0.02203585 0.29841346]\n",
      " [0.08237083 0.06050455 0.01800977 0.8391149 ]\n",
      " [0.22949997 0.05823478 0.03511709 0.6771481 ]\n",
      " ...\n",
      " [0.02233436 0.9026489  0.00988951 0.06512716]\n",
      " [0.05541316 0.00842431 0.9169087  0.01925388]\n",
      " [0.05542456 0.00842442 0.9168961  0.01925494]]\n",
      "Iteration 85, Accuracy 0.39986\n",
      "91.78573%change in label assignment\n",
      "0.09566595\n",
      "[[0.04288242 0.76433474 0.01593756 0.17684528]\n",
      " [0.07386383 0.08030295 0.0175301  0.8283031 ]\n",
      " [0.14846341 0.05549052 0.02553036 0.7705156 ]\n",
      " ...\n",
      " [0.02774408 0.8841236  0.01256635 0.07556603]\n",
      " [0.05551596 0.00842541 0.9167951  0.0192636 ]\n",
      " [0.05552975 0.00842572 0.9167793  0.0192652 ]]\n",
      "Iteration 86, Accuracy 0.384\n",
      "96.33721%change in label assignment\n",
      "0.09636288\n",
      "[[0.05010503 0.7110675  0.01835027 0.2204772 ]\n",
      " [0.07376444 0.07261419 0.01731854 0.8363029 ]\n",
      " [0.15515378 0.05547205 0.0266928  0.76268137]\n",
      " ...\n",
      " [0.02163024 0.9056876  0.0095478  0.06313442]\n",
      " [0.05540096 0.00841536 0.916955   0.01922865]\n",
      " [0.05542072 0.0084158  0.9169325  0.01923099]]\n",
      "Iteration 87, Accuracy 0.39053\n",
      "97.73653%change in label assignment\n",
      "0.09648988\n",
      "[[0.04987119 0.7134634  0.01814343 0.218522  ]\n",
      " [0.07377176 0.07151605 0.01704872 0.8376635 ]\n",
      " [0.16024946 0.05581384 0.02695849 0.75697815]\n",
      " ...\n",
      " [0.02212388 0.9040681  0.0097373  0.06407075]\n",
      " [0.05541571 0.00841168 0.9169523  0.01922026]\n",
      " [0.05543048 0.00841194 0.9169356  0.01922186]]\n",
      "Iteration 88, Accuracy 0.38695\n",
      "98.76761%change in label assignment\n",
      "0.09537629\n",
      "[[0.05634464 0.6620776  0.02022268 0.26135507]\n",
      " [0.07651394 0.06266171 0.01708844 0.8437359 ]\n",
      " [0.18273973 0.05659452 0.02982021 0.7308455 ]\n",
      " ...\n",
      " [0.0200132  0.91067576 0.00867911 0.06063193]\n",
      " [0.05532081 0.00840183 0.91709113 0.01918621]\n",
      " [0.05533668 0.00840207 0.9170734  0.01918787]]\n",
      "Iteration 89, Accuracy 0.38793\n",
      "98.68906%change in label assignment\n",
      "0.09360606\n",
      "[[0.05230444 0.6940761  0.0188898  0.2347296 ]\n",
      " [0.07619178 0.06321903 0.01690333 0.8436859 ]\n",
      " [0.18592523 0.05689983 0.02986541 0.7273095 ]\n",
      " ...\n",
      " [0.02113646 0.907396   0.00924493 0.06222266]\n",
      " [0.05523961 0.00839856 0.91720116 0.01916073]\n",
      " [0.0552539  0.00839877 0.9171851  0.0191622 ]]\n",
      "Iteration 90, Accuracy 0.38518\n",
      "99.12113%change in label assignment\n",
      "0.10016774\n",
      "[[0.0488493  0.71899295 0.01792759 0.21423015]\n",
      " [0.07449421 0.06598039 0.01695486 0.84257054]\n",
      " [0.16733018 0.05606012 0.02801927 0.7485904 ]\n",
      " ...\n",
      " [0.02118031 0.90712714 0.00931291 0.06237965]\n",
      " [0.05520649 0.0083933  0.9172568  0.0191435 ]\n",
      " [0.05522298 0.00839359 0.9172381  0.01914529]]\n",
      "Iteration 91, Accuracy 0.38616\n",
      "98.64978%change in label assignment\n",
      "0.09898819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08126894 0.3786715  0.02577861 0.514281  ]\n",
      " [0.13363859 0.05483959 0.02387152 0.78765035]\n",
      " [0.36693013 0.05753737 0.04506089 0.53047156]\n",
      " ...\n",
      " [0.02572716 0.87129086 0.01024159 0.09274039]\n",
      " [0.05513526 0.00838349 0.9173749  0.01910632]\n",
      " [0.05514886 0.0083837  0.91735977 0.0191077 ]]\n",
      "Iteration 92, Accuracy 0.3868\n",
      "92.69897%change in label assignment\n",
      "0.09970122\n",
      "[[0.02114336 0.90701145 0.00923703 0.06260816]\n",
      " [0.08347156 0.29821372 0.02543367 0.5928811 ]\n",
      " [0.07275422 0.08148903 0.01748059 0.82827616]\n",
      " ...\n",
      " [0.06070485 0.7712594  0.03076581 0.13726993]\n",
      " [0.05540447 0.00840182 0.91704017 0.01915349]\n",
      " [0.0554217  0.00840206 0.91702104 0.01915527]]\n",
      "Iteration 93, Accuracy 0.38911\n",
      "80.62552%change in label assignment\n",
      "0.10521306\n",
      "[[0.07112116 0.5241083  0.02421222 0.3805583 ]\n",
      " [0.09216855 0.0555371  0.01887373 0.8334206 ]\n",
      " [0.25636122 0.05866827 0.03732759 0.64764285]\n",
      " ...\n",
      " [0.02048512 0.90369684 0.00856859 0.06724945]\n",
      " [0.05501663 0.00837647 0.91752756 0.01907933]\n",
      " [0.05504245 0.00837712 0.9174979  0.01908252]]\n",
      "Iteration 94, Accuracy 0.40693\n",
      "84.38651%change in label assignment\n",
      "0.09604474\n",
      "[[0.02092144 0.90500224 0.00883561 0.06524073]\n",
      " [0.08345283 0.21537758 0.02386903 0.6773005 ]\n",
      " [0.07593127 0.06910802 0.01716196 0.8377988 ]\n",
      " ...\n",
      " [0.04913351 0.80951965 0.02394659 0.11740031]\n",
      " [0.05521485 0.00838762 0.9172696  0.01912796]\n",
      " [0.05524854 0.00838867 0.91723025 0.01913252]]\n",
      "Iteration 95, Accuracy 0.38818\n",
      "86.38975%change in label assignment\n",
      "0.10404571\n",
      "[[0.08250076 0.18498082 0.02350595 0.7090125 ]\n",
      " [0.20407374 0.05780109 0.0326488  0.7054764 ]\n",
      " [0.47350302 0.05287329 0.05126408 0.42235962]\n",
      " ...\n",
      " [0.05844969 0.6503241  0.02113417 0.27009204]\n",
      " [0.05492521 0.00835582 0.91763186 0.0190871 ]\n",
      " [0.05494968 0.00835625 0.91760427 0.01908977]]\n",
      "Iteration 96, Accuracy 0.40045\n",
      "78.94142%change in label assignment\n",
      "0.113679186\n",
      "[[0.08966736 0.68189293 0.04837462 0.18006507]\n",
      " [0.02296948 0.89850336 0.00985478 0.0686724 ]\n",
      " [0.04492644 0.7541283  0.01662864 0.18431674]\n",
      " ...\n",
      " [0.12699175 0.5755556  0.07551151 0.22194113]\n",
      " [0.0556632  0.00841688 0.9166458  0.0192741 ]\n",
      " [0.05574578 0.0084205  0.9165462  0.01928754]]\n",
      "Iteration 97, Accuracy 0.39775\n",
      "54.77488%change in label assignment\n",
      "0.097069494\n",
      "[[0.03314674 0.86514974 0.01553438 0.08616918]\n",
      " [0.06599353 0.57982445 0.02297461 0.3312074 ]\n",
      " [0.08170919 0.1829121  0.023107   0.71227175]\n",
      " ...\n",
      " [0.08181033 0.70543766 0.0439398  0.16881216]\n",
      " [0.05532167 0.00838957 0.9170896  0.01919916]\n",
      " [0.05536627 0.00839091 0.9170374  0.01920533]]\n",
      "Iteration 98, Accuracy 0.42824\n",
      "82.95773%change in label assignment\n",
      "0.10475549\n",
      "[[0.02612122 0.8693088  0.01038477 0.09418522]\n",
      " [0.07806555 0.1370043  0.02093041 0.76399964]\n",
      " [0.09661136 0.05382563 0.01920982 0.8303532 ]\n",
      " ...\n",
      " [0.03825685 0.84735006 0.01822552 0.09616757]\n",
      " [0.05506855 0.00836954 0.9174037  0.01915825]\n",
      " [0.05508415 0.00836979 0.9173861  0.01915993]]\n",
      "Iteration 99, Accuracy 0.41302\n",
      "84.04281%change in label assignment\n",
      "0.10016857\n",
      "[[0.07478072 0.48659864 0.02490441 0.41371623]\n",
      " [0.08782703 0.05496389 0.01805594 0.8391532 ]\n",
      " [0.26278615 0.05786019 0.0373705  0.64198315]\n",
      " ...\n",
      " [0.02010819 0.90542936 0.0083363  0.06612612]\n",
      " [0.05494618 0.00835413 0.91757166 0.0191281 ]\n",
      " [0.0549676  0.0083546  0.91754717 0.01913064]]\n",
      "Iteration 100, Accuracy 0.39382\n",
      "91.01979%change in label assignment\n",
      "0.09278169\n",
      "[[0.06605449 0.57985556 0.02281322 0.3312767 ]\n",
      " [0.08092403 0.05746105 0.01723519 0.8443798 ]\n",
      " [0.23907405 0.0575639  0.03515241 0.6682096 ]\n",
      " ...\n",
      " [0.01915496 0.9131331  0.00814863 0.05956333]\n",
      " [0.05489477 0.00834992 0.91763395 0.01912127]\n",
      " [0.05490792 0.00835012 0.9176193  0.01912265]]\n",
      "Iteration 101, Accuracy 0.38828\n",
      "97.20627%change in label assignment\n",
      "0.09310377\n",
      "[[0.06215446 0.61513543 0.02180415 0.3009059 ]\n",
      " [0.08006024 0.05798016 0.01718274 0.8447769 ]\n",
      " [0.23563088 0.05747557 0.03489653 0.671997  ]\n",
      " ...\n",
      " [0.01915213 0.91350883 0.00818688 0.05915223]\n",
      " [0.05486679 0.00834468 0.9176758  0.01911278]\n",
      " [0.05487848 0.00834483 0.91766274 0.01911394]]\n",
      "Iteration 102, Accuracy 0.3841\n",
      "98.84617%change in label assignment\n",
      "0.09341597\n",
      "[[0.08230128 0.36120394 0.02594854 0.5305462 ]\n",
      " [0.11946132 0.05343756 0.0220648  0.80503637]\n",
      " [0.39580607 0.05595903 0.04665533 0.5015796 ]\n",
      " ...\n",
      " [0.02444265 0.8792415  0.0098116  0.08650427]\n",
      " [0.05479169 0.00833522 0.9178021  0.01907107]\n",
      " [0.05480242 0.00833536 0.91779006 0.01907212]]\n",
      "Iteration 103, Accuracy 0.38278\n",
      "94.27505%change in label assignment\n",
      "0.09376834\n",
      "[[0.05206705 0.6964369  0.01885052 0.23264554]\n",
      " [0.07300592 0.07177211 0.01698263 0.83823925]\n",
      " [0.1831675  0.05629031 0.02951372 0.7310285 ]\n",
      " ...\n",
      " [0.02307574 0.9010922  0.01028261 0.06554942]\n",
      " [0.05484629 0.00833851 0.9177406  0.01907459]\n",
      " [0.05485709 0.00833862 0.9177287  0.0190756 ]]\n",
      "Iteration 104, Accuracy 0.38425\n",
      "92.70388%change in label assignment\n",
      "0.09928596\n",
      "[[0.08390538 0.29099226 0.02558832 0.599514  ]\n",
      " [0.14006643 0.05433653 0.02476834 0.7808287 ]\n",
      " [0.41613713 0.05523101 0.04793893 0.48069292]\n",
      " ...\n",
      " [0.03159513 0.8360557  0.01233533 0.12001385]\n",
      " [0.05469473 0.00832238 0.9179518  0.01903109]\n",
      " [0.05470735 0.0083226  0.91793764 0.01903244]]\n",
      "Iteration 105, Accuracy 0.38621\n",
      "92.24235%change in label assignment\n",
      "0.09904759\n",
      "[[0.02088854 0.9067833  0.00895052 0.0633776 ]\n",
      " [0.0843478  0.24201453 0.02466149 0.6489762 ]\n",
      " [0.07591994 0.0670023  0.01706378 0.840014  ]\n",
      " ...\n",
      " [0.05711748 0.7834624  0.02855135 0.13086875]\n",
      " [0.05497302 0.00834408 0.91758966 0.01909312]\n",
      " [0.05498544 0.0083442  0.917576   0.01909424]]\n",
      "Iteration 106, Accuracy 0.38872\n",
      "83.14921%change in label assignment\n",
      "0.10089363\n",
      "[[0.08393527 0.31005597 0.02599193 0.58001685]\n",
      " [0.16547483 0.05574558 0.02804433 0.7507353 ]\n",
      " [0.4561157  0.0534688  0.05010333 0.44031218]\n",
      " ...\n",
      " [0.03304674 0.8277037  0.01290642 0.12634318]\n",
      " [0.05455982 0.00831292 0.91812116 0.01900604]\n",
      " [0.05457233 0.00831306 0.91810733 0.01900723]]\n",
      "Iteration 107, Accuracy 0.40512\n",
      "79.88904%change in label assignment\n",
      "0.096831955\n",
      "[[0.02892668 0.8544748  0.01135515 0.10524341]\n",
      " [0.07661274 0.1134694  0.01956883 0.790349  ]\n",
      " [0.10798284 0.05486705 0.02056442 0.81658566]\n",
      " ...\n",
      " [0.03602729 0.85466707 0.01685794 0.09244765]\n",
      " [0.05487317 0.00832827 0.91774625 0.01905239]\n",
      " [0.05488732 0.00832855 0.91773015 0.01905393]]\n",
      "Iteration 108, Accuracy 0.38484\n",
      "85.13281%change in label assignment\n",
      "0.10119111\n",
      "[[0.08404399 0.23966813 0.02500553 0.65128237]\n",
      " [0.1806056  0.05654946 0.02994398 0.732901  ]\n",
      " [0.47400904 0.05259847 0.05103069 0.42236182]\n",
      " ...\n",
      " [0.04001719 0.78327155 0.01531286 0.16139837]\n",
      " [0.05454474 0.0083029  0.91816896 0.01898346]\n",
      " [0.05455723 0.00830306 0.918155   0.01898467]]\n",
      "Iteration 109, Accuracy 0.3951\n",
      "85.48633%change in label assignment\n",
      "0.10243559\n",
      "[[0.0213843  0.9026211  0.0089982  0.06699636]\n",
      " [0.08459026 0.23296873 0.02451563 0.6579253 ]\n",
      " [0.0770854  0.06778795 0.01727572 0.83785087]\n",
      " ...\n",
      " [0.05545431 0.788782   0.02751718 0.12824643]\n",
      " [0.05493858 0.0083273  0.9176732  0.0190609 ]\n",
      " [0.05495501 0.00832761 0.9176547  0.0190627 ]]\n",
      "Iteration 110, Accuracy 0.38793\n",
      "79.39805%change in label assignment\n",
      "0.09399533\n",
      "[[0.0423465  0.765823   0.01593436 0.1758961 ]\n",
      " [0.0729188  0.08277056 0.01781597 0.82649463]\n",
      " [0.14074557 0.05436485 0.02495988 0.77992964]\n",
      " ...\n",
      " [0.02634558 0.8893535  0.01203826 0.0722627 ]\n",
      " [0.05460081 0.00830492 0.91808903 0.01900522]\n",
      " [0.0546153  0.00830514 0.9180729  0.01900673]]\n",
      "Iteration 111, Accuracy 0.40404\n",
      "92.43875%change in label assignment\n",
      "0.09454852\n",
      "[[0.05038835 0.70882803 0.0183422  0.22244142]\n",
      " [0.07427283 0.06450979 0.01673408 0.8444834 ]\n",
      " [0.18328252 0.0560993  0.02951117 0.731107  ]\n",
      " ...\n",
      " [0.02079874 0.9090441  0.00911924 0.06103793]\n",
      " [0.05452332 0.00829761 0.91819084 0.01898821]\n",
      " [0.05453793 0.0082979  0.91817445 0.01898983]]\n",
      "Iteration 112, Accuracy 0.3896\n",
      "96.75455%change in label assignment\n",
      "0.089030005\n",
      "[[0.06690059 0.5695714  0.02312636 0.34040168]\n",
      " [0.08311667 0.05586181 0.01752137 0.8435001 ]\n",
      " [0.22955604 0.05731161 0.03442446 0.67870796]\n",
      " ...\n",
      " [0.01930172 0.91031563 0.00810998 0.06227262]\n",
      " [0.05441621 0.00828515 0.9183335  0.01896515]\n",
      " [0.05443644 0.00828563 0.91831034 0.01896762]]\n",
      "Iteration 113, Accuracy 0.38636\n",
      "97.27009%change in label assignment\n",
      "0.09091069\n",
      "[[0.03131808 0.8381896  0.01214533 0.118347  ]\n",
      " [0.07329545 0.09092945 0.01807334 0.81770176]\n",
      " [0.11844871 0.0532011  0.02181088 0.80653924]\n",
      " ...\n",
      " [0.02681435 0.88802636 0.01219878 0.07296047]\n",
      " [0.05432639 0.00828538 0.91840357 0.01898469]\n",
      " [0.05434396 0.00828565 0.9183839  0.01898656]]\n",
      "Iteration 114, Accuracy 0.38307\n",
      "94.28978%change in label assignment\n",
      "0.0942291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06515101 0.5879935  0.02263115 0.3242244 ]\n",
      " [0.07927413 0.05790486 0.01707524 0.8457457 ]\n",
      " [0.2034326  0.05646308 0.03168355 0.70842075]\n",
      " ...\n",
      " [0.01898739 0.9129493  0.0080297  0.06003358]\n",
      " [0.0542012  0.00827311 0.9185724  0.01895326]\n",
      " [0.054251   0.00827485 0.91851383 0.01896038]]\n",
      "Iteration 115, Accuracy 0.39363\n",
      "94.05902%change in label assignment\n",
      "0.0931139\n",
      "[[0.03690871 0.8030194  0.01401955 0.14605232]\n",
      " [0.0746875  0.10473948 0.01894129 0.80163175]\n",
      " [0.10531124 0.05306683 0.02011811 0.8215039 ]\n",
      " ...\n",
      " [0.02821033 0.88305426 0.01290736 0.07582807]\n",
      " [0.0543031  0.00827713 0.9184468  0.01897292]\n",
      " [0.05461614 0.00829422 0.91805965 0.01903001]]\n",
      "Iteration 116, Accuracy 0.3813\n",
      "90.72028%change in label assignment\n",
      "0.09714528\n",
      "[[0.08251729 0.3600003  0.02608608 0.5313964 ]\n",
      " [0.12411996 0.05337338 0.02274482 0.7997619 ]\n",
      " [0.3521223  0.05693488 0.04421476 0.546728  ]\n",
      " ...\n",
      " [0.03095257 0.8403257  0.01210587 0.11661591]\n",
      " [0.05398719 0.00825748 0.918854   0.01890136]\n",
      " [0.05402459 0.00825841 0.918811   0.01890604]]\n",
      "Iteration 117, Accuracy 0.38572\n",
      "84.70074%change in label assignment\n",
      "0.10168183\n",
      "[[0.0238257  0.8844795  0.00957423 0.08212056]\n",
      " [0.07889374 0.13980193 0.02095222 0.76035213]\n",
      " [0.09024604 0.05537105 0.0182213  0.8361616 ]\n",
      " ...\n",
      " [0.03972286 0.84232384 0.01885333 0.09909993]\n",
      " [0.05413449 0.00827209 0.91865367 0.0189397 ]\n",
      " [0.05419097 0.00827401 0.91858727 0.01894773]]\n",
      "Iteration 118, Accuracy 0.39004\n",
      "85.89385%change in label assignment\n",
      "0.101696335\n",
      "[[0.08261883 0.36480603 0.02623259 0.52634263]\n",
      " [0.13400471 0.05400107 0.02411176 0.78788245]\n",
      " [0.37125236 0.05646703 0.04558877 0.5266918 ]\n",
      " ...\n",
      " [0.03217892 0.8334693  0.01257055 0.12178129]\n",
      " [0.05381318 0.0082493  0.91905254 0.01888505]\n",
      " [0.05385076 0.00825023 0.9190092  0.01888979]]\n",
      "Iteration 119, Accuracy 0.39314\n",
      "85.20646%change in label assignment\n",
      "[[0.02638118 0.8704448  0.01046462 0.09270946]\n",
      " [0.07701792 0.11359291 0.01958628 0.7898028 ]\n",
      " [0.10267424 0.05472649 0.01977368 0.8228256 ]\n",
      " ...\n",
      " [0.03539122 0.85727334 0.01648032 0.09085514]\n",
      " [0.05401065 0.00826338 0.9188037  0.01892225]\n",
      " [0.05404415 0.00826394 0.91876596 0.01892595]]\n",
      "Iteration 120, Accuracy 0.39093\n",
      "88.59429%change in label assignment\n",
      "0.09916167\n",
      "[[0.06335114 0.60832965 0.02241231 0.3059069 ]\n",
      " [0.09498336 0.05517592 0.01939558 0.8304452 ]\n",
      " [0.26936314 0.05801328 0.03859277 0.6340308 ]\n",
      " ...\n",
      " [0.02060531 0.9069374  0.00886882 0.06358846]\n",
      " [0.05373874 0.00824593 0.91914505 0.01887032]\n",
      " [0.05376404 0.0082463  0.9191166  0.01887305]]\n",
      "Iteration 121, Accuracy 0.39338\n",
      "91.60407%change in label assignment\n",
      "0.095429495\n",
      "[[0.03512565 0.8163188  0.01339621 0.13515933]\n",
      " [0.07407043 0.08340348 0.01765751 0.8248686 ]\n",
      " [0.12814991 0.05453469 0.02290644 0.794409  ]\n",
      " ...\n",
      " [0.02763412 0.8845122  0.01246488 0.07538881]\n",
      " [0.05398507 0.0082531  0.918877   0.01888482]\n",
      " [0.05402001 0.00825383 0.91883725 0.01888894]]\n",
      "Iteration 122, Accuracy 0.38513\n",
      "93.68095%change in label assignment\n",
      "0.09699822\n",
      "[[0.06318258 0.607202   0.02231054 0.30730483]\n",
      " [0.10002475 0.05412993 0.01992664 0.8259187 ]\n",
      " [0.27236485 0.0579967  0.03871885 0.63091964]\n",
      " ...\n",
      " [0.02019647 0.9071089  0.00858028 0.06411438]\n",
      " [0.05372789 0.00823771 0.9191996  0.01883473]\n",
      " [0.05375207 0.00823804 0.9191725  0.01883729]]\n",
      "Iteration 123, Accuracy 0.39068\n",
      "93.19978%change in label assignment\n",
      "0.0965365\n",
      "[[0.02515088 0.87753564 0.01005063 0.08726288]\n",
      " [0.07656673 0.1126443  0.01944192 0.791347  ]\n",
      " [0.09703408 0.05534957 0.01906775 0.82854855]\n",
      " ...\n",
      " [0.0337971  0.86254644 0.01563211 0.08802432]\n",
      " [0.05398364 0.00824876 0.9189061  0.01886146]\n",
      " [0.05402828 0.0082501  0.91885406 0.01886749]]\n",
      "Iteration 124, Accuracy 0.3843\n",
      "90.6319%change in label assignment\n",
      "0.10146709\n",
      "[[0.04714881 0.73458284 0.01758304 0.20068529]\n",
      " [0.078227   0.06348228 0.01762712 0.8406636 ]\n",
      " [0.17142476 0.05603661 0.02885273 0.7436859 ]\n",
      " ...\n",
      " [0.02099683 0.90688616 0.00918169 0.06293535]\n",
      " [0.05369333 0.00823243 0.9192462  0.01882805]\n",
      " [0.0537278  0.00823303 0.9192073  0.01883197]]\n",
      "Iteration 125, Accuracy 0.39407\n",
      "94.69239%change in label assignment\n",
      "0.09587566\n",
      "[[0.02796866 0.85974    0.01097086 0.10132046]\n",
      " [0.07480349 0.10195582 0.0186862  0.80455446]\n",
      " [0.10865003 0.05366447 0.02042193 0.8172636 ]\n",
      " ...\n",
      " [0.03129788 0.8716965  0.01437856 0.08262711]\n",
      " [0.05377382 0.00823586 0.9191483  0.01884206]\n",
      " [0.05379596 0.00823616 0.9191235  0.01884439]]\n",
      "Iteration 126, Accuracy 0.38341\n",
      "95.55163%change in label assignment\n",
      "0.0956816\n",
      "[[0.06484913 0.5912522  0.0226935  0.3212051 ]\n",
      " [0.1044867  0.05354387 0.02038126 0.82158816]\n",
      " [0.30543435 0.05779327 0.04119837 0.59557396]\n",
      " ...\n",
      " [0.01966952 0.9102818  0.00839111 0.06165758]\n",
      " [0.05349463 0.00821955 0.91949916 0.01878671]\n",
      " [0.05351177 0.00821963 0.91948044 0.01878822]]\n",
      "Iteration 127, Accuracy 0.39269\n",
      "90.44042%change in label assignment\n",
      "0.09593356\n",
      "[[0.02213712 0.8950839  0.0090244  0.07375459]\n",
      " [0.07792572 0.13077822 0.02039021 0.77090585]\n",
      " [0.08990484 0.05560636 0.01812852 0.8363603 ]\n",
      " ...\n",
      " [0.03918441 0.843941   0.0185065  0.09836809]\n",
      " [0.053732   0.00822966 0.91922134 0.01881701]\n",
      " [0.0537539  0.0082299  0.919197   0.01881921]]\n",
      "Iteration 128, Accuracy 0.38278\n",
      "88.90853%change in label assignment\n",
      "0.095464155\n",
      "[[0.06754141 0.56385255 0.02336039 0.3452457 ]\n",
      " [0.08922387 0.05470544 0.01837966 0.837691  ]\n",
      " [0.24205738 0.05761389 0.03577335 0.6645554 ]\n",
      " ...\n",
      " [0.02013038 0.90571284 0.00842464 0.06573207]\n",
      " [0.05345622 0.00820881 0.91958493 0.01875015]\n",
      " [0.05349268 0.00820957 0.9195433  0.0187545 ]]\n",
      "Iteration 129, Accuracy 0.39785\n",
      "90.71046%change in label assignment\n",
      "0.09553161\n",
      "[[0.02329094 0.8869797  0.00936734 0.08036201]\n",
      " [0.07520592 0.11141854 0.0191677  0.7942078 ]\n",
      " [0.09551797 0.05402116 0.01876439 0.83169645]\n",
      " ...\n",
      " [0.03528345 0.85755056 0.01648704 0.09067897]\n",
      " [0.05355532 0.00821683 0.9194649  0.01876301]\n",
      " [0.05358995 0.00821751 0.9194255  0.01876708]]\n",
      "Iteration 130, Accuracy 0.38464\n",
      "92.35528%change in label assignment\n",
      "0.09859013\n",
      "[[0.071389   0.52258337 0.02430053 0.3817271 ]\n",
      " [0.0946733  0.0539766  0.01906782 0.8322823 ]\n",
      " [0.25929445 0.05793663 0.03734889 0.64541996]\n",
      " ...\n",
      " [0.0210127  0.9000825  0.00871038 0.0701944 ]\n",
      " [0.05337489 0.00820037 0.91971534 0.01870943]\n",
      " [0.05342056 0.00820153 0.91966265 0.01871523]]\n",
      "Iteration 131, Accuracy 0.39544\n",
      "91.03943%change in label assignment\n",
      "0.102338836\n",
      "[[0.02111254 0.9032899  0.00882845 0.0667691 ]\n",
      " [0.08449584 0.25206408 0.0247354  0.63870466]\n",
      " [0.07435767 0.0776289  0.01734666 0.8306668 ]\n",
      " ...\n",
      " [0.04622314 0.8195112  0.02228759 0.11197814]\n",
      " [0.05376874 0.00821874 0.91924435 0.01876819]\n",
      " [0.05391946 0.00822599 0.9190608  0.01879369]]\n",
      "Iteration 132, Accuracy 0.38646\n",
      "82.5011%change in label assignment\n",
      "0.10567732\n",
      "[[0.08401763 0.33047596 0.0264831  0.55902326]\n",
      " [0.15090546 0.05606268 0.02676175 0.7662701 ]\n",
      " [0.40427157 0.05606891 0.04837991 0.49127963]\n",
      " ...\n",
      " [0.03243854 0.83312017 0.01283076 0.12161049]\n",
      " [0.05326739 0.0081899  0.9198644  0.0186783 ]\n",
      " [0.05328988 0.00819019 0.91983926 0.01868067]]\n",
      "Iteration 133, Accuracy 0.39377\n",
      "77.3899%change in label assignment\n",
      "0.10081737\n",
      "[[0.02528955 0.89199966 0.01117862 0.07153217]\n",
      " [0.08309922 0.35583434 0.02582023 0.53524613]\n",
      " [0.07529587 0.0996962  0.01862304 0.8063849 ]\n",
      " ...\n",
      " [0.06370396 0.76198006 0.03223063 0.14208543]\n",
      " [0.05373985 0.00821805 0.9192734  0.01876873]\n",
      " [0.05377369 0.00821856 0.9192353  0.01877239]]\n",
      "Iteration 134, Accuracy 0.39048\n",
      "78.3817%change in label assignment\n",
      "0.10871546\n",
      "[[0.08401454 0.33270153 0.02650412 0.55677974]\n",
      " [0.15696345 0.05621904 0.02749361 0.7593239 ]\n",
      " [0.42386755 0.05518856 0.04935357 0.47159037]\n",
      " ...\n",
      " [0.03074348 0.8435741  0.01223778 0.11344468]\n",
      " [0.05327832 0.00818593 0.9198497  0.01868601]\n",
      " [0.05329644 0.00818611 0.91982967 0.01868779]]\n",
      "Iteration 135, Accuracy 0.40404\n",
      "76.67796%change in label assignment\n",
      "0.09496353\n",
      "[[0.0453331  0.8215783  0.02137442 0.11171417]\n",
      " [0.07055255 0.5590535  0.02389453 0.3464994 ]\n",
      " [0.08613628 0.19083713 0.02374911 0.6992775 ]\n",
      " ...\n",
      " [0.08962917 0.6820618  0.04797612 0.18033285]\n",
      " [0.05374448 0.00821901 0.91924155 0.01879495]\n",
      " [0.05378226 0.00821988 0.9191982  0.01879956]]\n",
      "Iteration 136, Accuracy 0.3897\n",
      "72.57328%change in label assignment\n",
      "0.11086817\n",
      "[[0.04989549 0.71222806 0.01821792 0.21965854]\n",
      " [0.07315261 0.0663213  0.01674572 0.84378034]\n",
      " [0.16955702 0.05511084 0.02809093 0.7472412 ]\n",
      " ...\n",
      " [0.02119354 0.9080061  0.00935839 0.06144204]\n",
      " [0.05336149 0.00818974 0.9197086  0.01874004]\n",
      " [0.05340253 0.00819098 0.9196608  0.01874562]]\n",
      "Iteration 137, Accuracy 0.41268\n",
      "83.20322%change in label assignment\n",
      "0.09450321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08175838 0.37186888 0.02574081 0.5206319 ]\n",
      " [0.14267814 0.05371952 0.02465018 0.7789522 ]\n",
      " [0.38354394 0.05602298 0.04538308 0.51504993]\n",
      " ...\n",
      " [0.02748567 0.86093205 0.01079638 0.10078591]\n",
      " [0.05331593 0.00817988 0.919797   0.01870725]\n",
      " [0.05334265 0.00818036 0.9197667  0.01871027]]\n",
      "Iteration 138, Accuracy 0.38361\n",
      "90.91177%change in label assignment\n",
      "0.095933676\n",
      "[[0.04071219 0.7763218  0.01523676 0.16772927]\n",
      " [0.07141808 0.07421753 0.01682868 0.83753574]\n",
      " [0.14204265 0.05378658 0.02461879 0.779552  ]\n",
      " ...\n",
      " [0.02616677 0.890275   0.01187097 0.07168722]\n",
      " [0.05338742 0.0081875  0.91971076 0.01871432]\n",
      " [0.05342388 0.00818844 0.9196687  0.01871901]]\n",
      "Iteration 139, Accuracy 0.39161\n",
      "90.9756%change in label assignment\n",
      "0.09522333\n",
      "[[0.07770685 0.44091955 0.02537852 0.45599508]\n",
      " [0.11113255 0.05254843 0.0208761  0.8154429 ]\n",
      " [0.31455928 0.05747716 0.04124634 0.5867172 ]\n",
      " ...\n",
      " [0.02175543 0.8945955  0.00884526 0.07480381]\n",
      " [0.0532442  0.00817289 0.9199028  0.01868015]\n",
      " [0.05327676 0.00817369 0.91986525 0.01868428]]\n",
      "Iteration 140, Accuracy 0.38528\n",
      "92.96902%change in label assignment\n",
      "0.09075266\n",
      "[[0.03636674 0.80537695 0.01379677 0.14445955]\n",
      " [0.07162176 0.0823357  0.01727166 0.8287709 ]\n",
      " [0.12253021 0.05290594 0.02215312 0.8024107 ]\n",
      " ...\n",
      " [0.02693238 0.88751847 0.01224167 0.07330748]\n",
      " [0.0533109  0.00817722 0.9198142  0.01869769]\n",
      " [0.05336043 0.00817868 0.91975653 0.01870434]]\n",
      "Iteration 141, Accuracy 0.38945\n",
      "91.31438%change in label assignment\n",
      "0.0966\n",
      "[[0.08167789 0.37127703 0.02593461 0.5211104 ]\n",
      " [0.12853885 0.05326935 0.02320418 0.7949877 ]\n",
      " [0.3662521  0.05652361 0.04488596 0.5323383 ]\n",
      " ...\n",
      " [0.02631005 0.86783975 0.01046785 0.09538233]\n",
      " [0.05310155 0.00815966 0.9200846  0.01865412]\n",
      " [0.05312247 0.0081598  0.9200617  0.01865608]]\n",
      "Iteration 142, Accuracy 0.38661\n",
      "89.16384%change in label assignment\n",
      "0.096768714\n",
      "[[0.02558726 0.87298423 0.01014631 0.09128224]\n",
      " [0.0784401  0.14653018 0.02103428 0.7539954 ]\n",
      " [0.08801544 0.0547785  0.01780582 0.83940023]\n",
      " ...\n",
      " [0.04182616 0.83480585 0.01999204 0.10337597]\n",
      " [0.05337077 0.00817506 0.91977227 0.01868187]\n",
      " [0.05342091 0.0081767  0.91971344 0.01868884]]\n",
      "Iteration 143, Accuracy 0.38999\n",
      "85.57961%change in label assignment\n",
      "0.099040635\n",
      "[[0.07584552 0.4683628  0.02525358 0.43053812]\n",
      " [0.08359744 0.05584209 0.01766695 0.8428935 ]\n",
      " [0.24444242 0.05755472 0.0359295  0.6620734 ]\n",
      " ...\n",
      " [0.01932633 0.9105521  0.00816898 0.0619526 ]\n",
      " [0.05314567 0.00815491 0.92006636 0.01863304]\n",
      " [0.05319855 0.00815658 0.92000467 0.01864026]]\n",
      "Iteration 144, Accuracy 0.39225\n",
      "90.79884%change in label assignment\n",
      "0.09651929\n",
      "[[0.08051692 0.39886218 0.02561981 0.49500105]\n",
      " [0.09175713 0.0537404  0.01823375 0.8362687 ]\n",
      " [0.2913898  0.05801718 0.03907013 0.6115229 ]\n",
      " ...\n",
      " [0.02062075 0.9025586  0.00847134 0.06834924]\n",
      " [0.05316066 0.00815268 0.9200592  0.01862738]\n",
      " [0.05318927 0.00815327 0.92002666 0.01863078]]\n",
      "Iteration 145, Accuracy 0.38661\n",
      "97.10807%change in label assignment\n",
      "0.09680725\n",
      "[[0.05366232 0.6825821  0.01949865 0.24425693]\n",
      " [0.07236066 0.07229359 0.01710439 0.8382414 ]\n",
      " [0.16005652 0.05499991 0.02715794 0.7577857 ]\n",
      " ...\n",
      " [0.02308894 0.90082294 0.01036917 0.06571904]\n",
      " [0.05319031 0.00815428 0.92002887 0.0186265 ]\n",
      " [0.05320942 0.00815433 0.92000806 0.01862821]]\n",
      "Iteration 146, Accuracy 0.38616\n",
      "95.03609%change in label assignment\n",
      "0.09422772\n",
      "[[0.08320541 0.31557304 0.02549625 0.5757252 ]\n",
      " [0.1125571  0.0527609  0.02090144 0.81378055]\n",
      " [0.35079235 0.05716781 0.04336801 0.5486719 ]\n",
      " ...\n",
      " [0.02654667 0.8662964  0.01048844 0.0966685 ]\n",
      " [0.05310741 0.00814008 0.9201524  0.01860011]\n",
      " [0.05312692 0.00814032 0.92013067 0.01860213]]\n",
      "Iteration 147, Accuracy 0.38268\n",
      "93.34217%change in label assignment\n",
      "0.098354116\n",
      "[[0.04065412 0.7757127  0.01530198 0.16833122]\n",
      " [0.07211216 0.09354374 0.01804223 0.8163019 ]\n",
      " [0.12708008 0.05322281 0.02291295 0.7967841 ]\n",
      " ...\n",
      " [0.02838214 0.88201755 0.01306949 0.07653073]\n",
      " [0.0531906  0.00814895 0.92005277 0.01860767]\n",
      " [0.05320356 0.00814875 0.92003924 0.01860837]]\n",
      "Iteration 148, Accuracy 0.38744\n",
      "91.28001%change in label assignment\n",
      "0.099323094\n",
      "[[0.07997159 0.17712748 0.0224458  0.72045505]\n",
      " [0.18462493 0.05580599 0.02964669 0.7299224 ]\n",
      " [0.50737613 0.0504843  0.05066941 0.3914701 ]\n",
      " ...\n",
      " [0.04459808 0.7486951  0.01660101 0.19010574]\n",
      " [0.05297541 0.00812793 0.92034096 0.01855574]\n",
      " [0.0529892  0.00812797 0.9203259  0.01855688]]\n",
      "Iteration 149, Accuracy 0.38774\n",
      "85.70727%change in label assignment\n",
      "0.10246414\n",
      "[[0.02105975 0.9074619  0.00917526 0.06230316]\n",
      " [0.08269529 0.33951995 0.02566489 0.55211985]\n",
      " [0.0720854  0.08073694 0.01721764 0.82996   ]\n",
      " ...\n",
      " [0.06694681 0.75157374 0.03442692 0.14705253]\n",
      " [0.05332228 0.00815554 0.91988385 0.01863826]\n",
      " [0.05332911 0.0081547  0.919879   0.01863713]]\n",
      "Iteration 150, Accuracy 0.38847\n",
      "78.30805%change in label assignment\n",
      "0.094924726\n",
      "[[0.04218794 0.7652952  0.01584666 0.17667015]\n",
      " [0.07228218 0.09424398 0.01819484 0.815279  ]\n",
      " [0.12743014 0.05306389 0.02305553 0.79645044]\n",
      " ...\n",
      " [0.02875477 0.8807612  0.01330122 0.07718283]\n",
      " [0.05300675 0.00813037 0.920286   0.0185769 ]\n",
      " [0.05302139 0.00813038 0.9202702  0.01857805]]\n",
      "Iteration 151, Accuracy 0.40571\n",
      "90.88231%change in label assignment\n",
      "0.09435821\n",
      "[[0.05237196 0.69154805 0.01892049 0.23715948]\n",
      " [0.07083513 0.07386412 0.01671554 0.83858526]\n",
      " [0.15615681 0.05440935 0.02625742 0.76317644]\n",
      " ...\n",
      " [0.02276225 0.9024148  0.01016343 0.06465954]\n",
      " [0.0529319  0.00812167 0.9203862  0.01856024]\n",
      " [0.05294864 0.00812176 0.92036784 0.01856174]]\n",
      "Iteration 152, Accuracy 0.38818\n",
      "98.33554%change in label assignment\n",
      "0.09008896\n",
      "[[0.08172972 0.3588862  0.02584705 0.53353703]\n",
      " [0.11230792 0.05232858 0.02107373 0.81428975]\n",
      " [0.33499992 0.05716129 0.04280147 0.56503737]\n",
      " ...\n",
      " [0.02661908 0.86528057 0.01057594 0.09752446]\n",
      " [0.05275181 0.0081046  0.92062336 0.01852029]\n",
      " [0.0527695  0.0081047  0.9206039  0.01852189]]\n",
      "Iteration 153, Accuracy 0.38445\n",
      "92.88555%change in label assignment\n",
      "0.09361388\n",
      "[[0.01899394 0.9129349  0.00799948 0.06007164]\n",
      " [0.08138312 0.19057956 0.02292673 0.7051106 ]\n",
      " [0.07767639 0.0576194  0.01662306 0.8480812 ]\n",
      " ...\n",
      " [0.04932418 0.80958456 0.02422817 0.11686309]\n",
      " [0.05275558 0.00811666 0.92056304 0.01856477]\n",
      " [0.05276752 0.0081164  0.9205508  0.01856523]]\n",
      "Iteration 154, Accuracy 0.38842\n",
      "86.75308%change in label assignment\n",
      "0.093410626\n",
      "[[0.06787468 0.5582703  0.02333662 0.35051844]\n",
      " [0.09374215 0.05252345 0.01864831 0.83508605]\n",
      " [0.2868443  0.05740054 0.03915714 0.616598  ]\n",
      " ...\n",
      " [0.01957936 0.9076405  0.00813607 0.06464406]\n",
      " [0.05245356 0.00809508 0.9209459  0.01850549]\n",
      " [0.05246956 0.00809513 0.9209285  0.0185068 ]]\n",
      "Iteration 155, Accuracy 0.39957\n",
      "87.99529%change in label assignment\n",
      "0.08778887\n",
      "[[0.0581791  0.6469702  0.0205662  0.27428448]\n",
      " [0.08262043 0.05472201 0.0171009  0.84555674]\n",
      " [0.24422981 0.05729693 0.03511993 0.6633534 ]\n",
      " ...\n",
      " [0.01873717 0.9145583  0.00792754 0.05877709]\n",
      " [0.0524163  0.00809096 0.9210065  0.01848622]\n",
      " [0.05243085 0.0080909  0.920991   0.01848722]]\n",
      "Iteration 156, Accuracy 0.38194\n",
      "98.63996%change in label assignment\n",
      "0.08827608\n",
      "[[0.07214884 0.51005435 0.02429968 0.39349714]\n",
      " [0.09001174 0.05275739 0.01809088 0.83913994]\n",
      " [0.28888115 0.05755189 0.03918156 0.6143854 ]\n",
      " ...\n",
      " [0.01946902 0.9079492  0.00807655 0.0645052 ]\n",
      " [0.05234702 0.00808181 0.9211224  0.01844882]\n",
      " [0.05236274 0.00808183 0.92110527 0.01845007]]\n",
      "Iteration 157, Accuracy 0.38101\n",
      "98.04586%change in label assignment\n",
      "0.09245636\n",
      "[[0.06388728 0.5949155  0.02218748 0.31900972]\n",
      " [0.08159553 0.05474469 0.01700993 0.84664994]\n",
      " [0.24275552 0.05732014 0.03511501 0.6648092 ]\n",
      " ...\n",
      " [0.01851922 0.9149247  0.00782458 0.05873151]\n",
      " [0.05230618 0.00807782 0.9211824  0.0184336 ]\n",
      " [0.05232275 0.0080779  0.9211644  0.01843504]]\n",
      "Iteration 158, Accuracy 0.38253\n",
      "98.47302%change in label assignment\n",
      "0.085940756\n",
      "[[0.06690259 0.5655369  0.02304021 0.34452033]\n",
      " [0.08170317 0.05447911 0.01707046 0.8467472 ]\n",
      " [0.24294068 0.05721842 0.03525969 0.66458124]\n",
      " ...\n",
      " [0.01868848 0.9131172  0.00785066 0.06034378]\n",
      " [0.05224484 0.00807025 0.92126614 0.01841875]\n",
      " [0.0522598  0.0080702  0.9212502  0.01841984]]\n",
      "Iteration 159, Accuracy 0.38047\n",
      "99.67595%change in label assignment\n",
      "[[0.07914511 0.41192958 0.0255431  0.48338217]\n",
      " [0.09529131 0.05233214 0.01871648 0.83366007]\n",
      " [0.3068339  0.05756998 0.04046488 0.5951313 ]\n",
      " ...\n",
      " [0.02205948 0.8921838  0.00893667 0.07682007]\n",
      " [0.05217055 0.00806234 0.92137384 0.01839323]\n",
      " [0.05218616 0.00806247 0.9213566  0.01839469]]\n",
      "Iteration 160, Accuracy 0.38057\n",
      "97.60888%change in label assignment\n",
      "0.09713298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02095366 0.89885587 0.00855242 0.07163811]\n",
      " [0.08108821 0.20013829 0.02312802 0.6956455 ]\n",
      " [0.07433999 0.05934661 0.01628099 0.8500323 ]\n",
      " ...\n",
      " [0.03814121 0.847485   0.0181661  0.09620769]\n",
      " [0.05230573 0.00807143 0.92121726 0.01840559]\n",
      " [0.05232099 0.00807151 0.9212006  0.01840695]]\n",
      "Iteration 161, Accuracy 0.38459\n",
      "87.357%change in label assignment\n",
      "0.09663545\n",
      "[[0.0768889  0.44765684 0.02535948 0.4500948 ]\n",
      " [0.09380107 0.05268117 0.0187262  0.83479166]\n",
      " [0.2869373  0.0576741  0.03933178 0.6160568 ]\n",
      " ...\n",
      " [0.02107049 0.89846337 0.00866578 0.07180038]\n",
      " [0.05201596 0.00805304 0.92157805 0.01835301]\n",
      " [0.05203275 0.00805318 0.92155945 0.01835458]]\n",
      "Iteration 162, Accuracy 0.39539\n",
      "87.60249%change in label assignment\n",
      "0.09451477\n",
      "[[0.02409574 0.8809316  0.00962652 0.08534615]\n",
      " [0.07893907 0.16056688 0.02156108 0.73893297]\n",
      " [0.08233369 0.0555038  0.01706537 0.8450972 ]\n",
      " ...\n",
      " [0.03650914 0.8531361  0.01720472 0.09315005]\n",
      " [0.052182   0.00806171 0.92137915 0.01837723]\n",
      " [0.05219946 0.0080619  0.9213596  0.01837893]]\n",
      "Iteration 163, Accuracy 0.3842\n",
      "89.68429%change in label assignment\n",
      "0.09703522\n",
      "[[0.07479868 0.47716948 0.02505425 0.42297757]\n",
      " [0.08656657 0.05403252 0.01793194 0.84146893]\n",
      " [0.26066345 0.05766031 0.03732838 0.6443479 ]\n",
      " ...\n",
      " [0.02078852 0.90055716 0.00861583 0.0700385 ]\n",
      " [0.0519568  0.00804476 0.9216688  0.01832965]\n",
      " [0.05197291 0.00804486 0.92165107 0.0183311 ]]\n",
      "Iteration 164, Accuracy 0.3953\n",
      "90.74974%change in label assignment\n",
      "0.09503199\n",
      "[[0.02102632 0.89967924 0.00858819 0.07070623]\n",
      " [0.07967126 0.16807638 0.02188525 0.73036706]\n",
      " [0.08050915 0.05666874 0.01686852 0.8459536 ]\n",
      " ...\n",
      " [0.0398399  0.84153944 0.01895262 0.09966804]\n",
      " [0.05209761 0.00805607 0.9214835  0.01836285]\n",
      " [0.05211565 0.00805626 0.92146355 0.0183646 ]]\n",
      "Iteration 165, Accuracy 0.38371\n",
      "90.50425%change in label assignment\n",
      "0.09379189\n",
      "[[0.07931277 0.4103815  0.02578858 0.4845172 ]\n",
      " [0.11710887 0.05283306 0.02184357 0.80821455]\n",
      " [0.3741339  0.05643015 0.04549827 0.5239376 ]\n",
      " ...\n",
      " [0.02073316 0.9008796  0.00858583 0.06980147]\n",
      " [0.05176906 0.00803696 0.9218904  0.01830359]\n",
      " [0.05178147 0.00803676 0.9218776  0.01830421]]\n",
      "Iteration 166, Accuracy 0.39652\n",
      "86.16389%change in label assignment\n",
      "0.0948626\n",
      "[[0.02740298 0.86132264 0.01075685 0.10051758]\n",
      " [0.07665527 0.13436295 0.02024884 0.76873296]\n",
      " [0.09170219 0.05391128 0.01817866 0.83620787]\n",
      " ...\n",
      " [0.03335654 0.86410475 0.0155056  0.08703314]\n",
      " [0.05199725 0.00804435 0.92163885 0.0183195 ]\n",
      " [0.05201599 0.00804461 0.921618   0.01832144]]\n",
      "Iteration 167, Accuracy 0.37782\n",
      "87.81362%change in label assignment\n",
      "0.0965281\n",
      "[[0.0589486  0.6372004  0.02104666 0.2828043 ]\n",
      " [0.07227853 0.06629906 0.01672529 0.8446972 ]\n",
      " [0.1465966  0.05416554 0.02553089 0.773707  ]\n",
      " ...\n",
      " [0.01977742 0.9067663  0.00829329 0.06516299]\n",
      " [0.05186814 0.00802877 0.92182255 0.01828056]\n",
      " [0.05190571 0.00802962 0.92177933 0.01828522]]\n",
      "Iteration 168, Accuracy 0.39279\n",
      "92.7137%change in label assignment\n",
      "0.096077465\n",
      "[[0.02769242 0.85907346 0.01085818 0.10237589]\n",
      " [0.07614009 0.1331694  0.02014198 0.7705485 ]\n",
      " [0.08244865 0.05537729 0.01705858 0.8451154 ]\n",
      " ...\n",
      " [0.02607689 0.8900844  0.0117799  0.07205878]\n",
      " [0.05195704 0.00803244 0.9217224  0.01828815]\n",
      " [0.05199129 0.00803321 0.9216832  0.01829236]]\n",
      "Iteration 169, Accuracy 0.38302\n",
      "94.11794%change in label assignment\n",
      "0.09279661\n",
      "[[0.06579711 0.5747241  0.02294185 0.3365369 ]\n",
      " [0.07333461 0.06363113 0.01673023 0.846304  ]\n",
      " [0.16692293 0.05528415 0.02797962 0.7498133 ]\n",
      " ...\n",
      " [0.02047015 0.9024074  0.00852368 0.0685988 ]\n",
      " [0.05171205 0.00801789 0.9220313  0.01823877]\n",
      " [0.05174753 0.00801856 0.92199093 0.01824294]]\n",
      "Iteration 170, Accuracy 0.39098\n",
      "92.96902%change in label assignment\n",
      "0.099823385\n",
      "[[0.02095506 0.9005862  0.00858524 0.06987348]\n",
      " [0.08125792 0.19382653 0.02282782 0.70208776]\n",
      " [0.0746521  0.06309442 0.0164465  0.845807  ]\n",
      " ...\n",
      " [0.03630708 0.85362524 0.01703266 0.09303503]\n",
      " [0.05182407 0.00802624 0.92188114 0.01826857]\n",
      " [0.05185309 0.00802685 0.92184794 0.01827206]]\n",
      "Iteration 171, Accuracy 0.38651\n",
      "90.9265%change in label assignment\n",
      "0.09947463\n",
      "[[0.08166759 0.20431086 0.02370105 0.6903205 ]\n",
      " [0.20202269 0.057064   0.03211068 0.7088027 ]\n",
      " [0.48704588 0.05188511 0.0509723  0.4100967 ]\n",
      " ...\n",
      " [0.05200647 0.6951443  0.01911922 0.23373003]\n",
      " [0.051491   0.00800367 0.9223158  0.01818951]\n",
      " [0.05151033 0.00800369 0.92229486 0.01819113]]\n",
      "Iteration 172, Accuracy 0.39574\n",
      "80.61079%change in label assignment\n",
      "0.10427884\n",
      "[[0.03431942 0.8602867  0.01594    0.08945387]\n",
      " [0.06424527 0.59218025 0.02216103 0.32141346]\n",
      " [0.08199872 0.2048681  0.02321844 0.6899147 ]\n",
      " ...\n",
      " [0.07554878 0.7242152  0.03956594 0.16067004]\n",
      " [0.05201326 0.00803729 0.9216631  0.01828635]\n",
      " [0.05204967 0.00803814 0.9216213  0.0182909 ]]\n",
      "Iteration 173, Accuracy 0.39191\n",
      "70.12815%change in label assignment\n",
      "0.10479024\n",
      "[[0.03997227 0.77945465 0.01514745 0.16542567]\n",
      " [0.07417607 0.11460016 0.0194506  0.7917732 ]\n",
      " [0.10404354 0.0529797  0.0202216  0.82275516]\n",
      " ...\n",
      " [0.02489518 0.89414114 0.01130811 0.06965557]\n",
      " [0.05162941 0.00801522 0.92213875 0.01821661]\n",
      " [0.05166327 0.00801595 0.9221     0.01822076]]\n",
      "Iteration 174, Accuracy 0.41076\n",
      "85.51579%change in label assignment\n",
      "0.09505831\n",
      "[[0.0508414  0.7038304  0.01833203 0.22699612]\n",
      " [0.07202258 0.09498573 0.01782491 0.8151667 ]\n",
      " [0.11697871 0.05288251 0.02129827 0.8088405 ]\n",
      " ...\n",
      " [0.0200425  0.91083485 0.00862656 0.06049612]\n",
      " [0.05168812 0.00801078 0.9220666  0.01823455]\n",
      " [0.05172792 0.00801177 0.9220207  0.01823959]]\n",
      "Iteration 175, Accuracy 0.38346\n",
      "98.0164%change in label assignment\n",
      "0.08646994\n",
      "[[0.08401259 0.29907462 0.02612052 0.59079224]\n",
      " [0.1007953  0.05469029 0.02029299 0.8242214 ]\n",
      " [0.30497226 0.05823246 0.04178943 0.59500587]\n",
      " ...\n",
      " [0.03692647 0.8034414  0.01437756 0.1452546 ]\n",
      " [0.05150374 0.00799379 0.92229795 0.01820453]\n",
      " [0.05152984 0.0079942  0.92226857 0.01820744]]\n",
      "Iteration 176, Accuracy 0.38292\n",
      "91.3242%change in label assignment\n",
      "0.101154216\n",
      "[[0.05811851 0.7797032  0.02876582 0.13341251]\n",
      " [0.03205733 0.8374792  0.01240603 0.11805741]\n",
      " [0.08035731 0.42490712 0.02568312 0.46905246]\n",
      " ...\n",
      " [0.10156062 0.6468491  0.056376   0.1952143 ]\n",
      " [0.05194296 0.00803034 0.921714   0.01831268]\n",
      " [0.05197558 0.00803122 0.9216763  0.01831696]]\n",
      "Iteration 177, Accuracy 0.39156\n",
      "71.06103%change in label assignment\n",
      "0.101894215\n",
      "[[0.0190719  0.91024655 0.00795406 0.06272745]\n",
      " [0.08113578 0.19645755 0.02322386 0.69918287]\n",
      " [0.07784527 0.05647366 0.01677619 0.8489049 ]\n",
      " ...\n",
      " [0.0425441  0.8323205  0.02062465 0.10451081]\n",
      " [0.05151125 0.00800575 0.9222349  0.01824809]\n",
      " [0.05152693 0.00800564 0.92221826 0.01824918]]\n",
      "Iteration 178, Accuracy 0.41827\n",
      "80.47331%change in label assignment\n",
      "0.097200625\n",
      "[[0.04770948 0.72622424 0.01742657 0.20863964]\n",
      " [0.07010117 0.07207974 0.01642674 0.84139234]\n",
      " [0.1566471  0.05393392 0.02613004 0.763289  ]\n",
      " ...\n",
      " [0.01937407 0.91424495 0.00840369 0.05797729]\n",
      " [0.05143094 0.00799366 0.9223423  0.01823318]\n",
      " [0.05145086 0.00799389 0.92232    0.01823521]]\n",
      "Iteration 179, Accuracy 0.3949\n",
      "93.76442%change in label assignment\n",
      "0.0912263\n",
      "[[0.06349859 0.59707636 0.02205642 0.3173686 ]\n",
      " [0.07065708 0.06580154 0.01617237 0.847369  ]\n",
      " [0.17854872 0.05499358 0.02864381 0.73781383]\n",
      " ...\n",
      " [0.01832017 0.91537416 0.00771045 0.05859526]\n",
      " [0.05142422 0.0079871  0.92237234 0.01821627]\n",
      " [0.05144797 0.00798762 0.92234516 0.01821921]]\n",
      "Iteration 180, Accuracy 0.38312\n",
      "97.54996%change in label assignment\n",
      "0.08876486\n",
      "[[0.04734953 0.72849387 0.01734464 0.20681195]\n",
      " [0.07029896 0.08205102 0.01704648 0.8306035 ]\n",
      " [0.13581522 0.05280094 0.02369283 0.787691  ]\n",
      " ...\n",
      " [0.01927086 0.9146854  0.00837588 0.05766783]\n",
      " [0.05137745 0.00798375 0.9224246  0.01821418]\n",
      " [0.05140088 0.00798417 0.92239803 0.01821689]]\n",
      "Iteration 181, Accuracy 0.38597\n",
      "97.64325%change in label assignment\n",
      "0.09064897\n",
      "[[0.04866268 0.7188376  0.01775955 0.21474007]\n",
      " [0.0703029  0.0823699  0.01707052 0.8302567 ]\n",
      " [0.13091321 0.05251487 0.02309048 0.79348147]\n",
      " ...\n",
      " [0.01862069 0.9165602  0.00802037 0.05679879]\n",
      " [0.05137025 0.00797874 0.9224433  0.01820775]\n",
      " [0.05139766 0.00797932 0.9224119  0.0182111 ]]\n",
      "Iteration 182, Accuracy 0.38341\n",
      "98.83635%change in label assignment\n",
      "0.09219482\n",
      "[[0.076914   0.44452173 0.02516814 0.45339614]\n",
      " [0.08315047 0.05312044 0.01711885 0.8466102 ]\n",
      " [0.27765116 0.05734415 0.03805294 0.62695175]\n",
      " ...\n",
      " [0.02330224 0.884568   0.00936058 0.08276919]\n",
      " [0.05123513 0.0079699  0.922629   0.01816608]\n",
      " [0.0512518  0.00796998 0.9226107  0.01816759]]\n",
      "Iteration 183, Accuracy 0.38376\n",
      "92.94938%change in label assignment\n",
      "0.09333002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02921469 0.8491219  0.01137335 0.11029003]\n",
      " [0.07599029 0.13515572 0.02023339 0.7686206 ]\n",
      " [0.0945248  0.05197044 0.01843228 0.83507246]\n",
      " ...\n",
      " [0.0288228  0.88064456 0.01324976 0.07728288]\n",
      " [0.05131061 0.00797555 0.9225348  0.01817904]\n",
      " [0.05132864 0.00797565 0.92251503 0.01818071]]\n",
      "Iteration 184, Accuracy 0.38872\n",
      "91.5206%change in label assignment\n",
      "0.0992025\n",
      "[[0.08208403 0.33749968 0.02567059 0.5547457 ]\n",
      " [0.10326055 0.05169033 0.01978863 0.82526046]\n",
      " [0.34402314 0.05683734 0.04311751 0.55602205]\n",
      " ...\n",
      " [0.03129561 0.8362405  0.01218757 0.12027636]\n",
      " [0.05113105 0.00795861 0.9227722  0.01813812]\n",
      " [0.0511473  0.00795867 0.9227545  0.01813953]]\n",
      "Iteration 185, Accuracy 0.38803\n",
      "88.67776%change in label assignment\n",
      "0.09618498\n",
      "[[0.02413209 0.89706546 0.01074544 0.06805699]\n",
      " [0.08046097 0.39131877 0.02552394 0.5026963 ]\n",
      " [0.07274122 0.09668212 0.01801929 0.81255734]\n",
      " ...\n",
      " [0.06294546 0.7645181  0.03198319 0.14055319]\n",
      " [0.05137086 0.00797922 0.9224544  0.01819544]\n",
      " [0.05138605 0.00797902 0.9224385  0.01819634]]\n",
      "Iteration 186, Accuracy 0.38999\n",
      "82.54038%change in label assignment\n",
      "0.09838678\n",
      "[[0.07347038 0.4921551  0.02467653 0.40969798]\n",
      " [0.0922301  0.05226443 0.01845898 0.8370465 ]\n",
      " [0.29856545 0.05736874 0.04008075 0.60398513]\n",
      " ...\n",
      " [0.02277093 0.8880914  0.00924488 0.07989278]\n",
      " [0.05103071 0.00795378 0.92288965 0.01812581]\n",
      " [0.05104395 0.00795345 0.9228762  0.01812631]]\n",
      "Iteration 187, Accuracy 0.40453\n",
      "81.63205%change in label assignment\n",
      "0.092656076\n",
      "[[0.02795355 0.8580535  0.01093852 0.1030544 ]\n",
      " [0.07494648 0.11965644 0.01938517 0.78601193]\n",
      " [0.09476205 0.0530151  0.01846853 0.83375436]\n",
      " ...\n",
      " [0.02776637 0.8841607  0.01262036 0.0754526 ]\n",
      " [0.05123213 0.00795895 0.92266643 0.01814249]\n",
      " [0.05125245 0.00795915 0.9226439  0.0181445 ]]\n",
      "Iteration 188, Accuracy 0.38523\n",
      "89.09019%change in label assignment\n",
      "0.096719116\n",
      "[[0.08325933 0.28933793 0.02557447 0.6018282 ]\n",
      " [0.14241879 0.05389209 0.02514233 0.7785468 ]\n",
      " [0.41406843 0.05497395 0.04776516 0.4831925 ]\n",
      " ...\n",
      " [0.04323367 0.75902736 0.01632562 0.18141331]\n",
      " [0.05094686 0.00793963 0.92302555 0.01808803]\n",
      " [0.05095982 0.00793929 0.92301244 0.01808848]]\n",
      "Iteration 189, Accuracy 0.3897\n",
      "85.63853%change in label assignment\n",
      "0.09966779\n",
      "[[0.02201196 0.90409803 0.00961403 0.06427597]\n",
      " [0.08209107 0.35766286 0.02558844 0.53465766]\n",
      " [0.07240931 0.08755765 0.017509   0.8225241 ]\n",
      " ...\n",
      " [0.06032782 0.77290577 0.03039362 0.13637279]\n",
      " [0.05126471 0.00796161 0.92261773 0.01815598]\n",
      " [0.05128007 0.00796146 0.9226015  0.01815698]]\n",
      "Iteration 190, Accuracy 0.38592\n",
      "80.04615%change in label assignment\n",
      "0.09078769\n",
      "[[0.02882896 0.8511616  0.01132218 0.10868729]\n",
      " [0.0735801  0.11250481 0.01918244 0.79473263]\n",
      " [0.09943425 0.05155493 0.01929397 0.82971686]\n",
      " ...\n",
      " [0.02735187 0.8859242  0.01258992 0.07413394]\n",
      " [0.05097437 0.00794095 0.92297953 0.01810518]\n",
      " [0.05099091 0.00794082 0.92296195 0.01810632]]\n",
      "Iteration 191, Accuracy 0.40325\n",
      "92.44366%change in label assignment\n",
      "0.091561794\n",
      "[[0.03479853 0.8140353  0.01326682 0.13789931]\n",
      " [0.07274383 0.10729063 0.0185997  0.8013658 ]\n",
      " [0.09971266 0.05154659 0.01908127 0.82965946]\n",
      " ...\n",
      " [0.02471913 0.8955066  0.01115862 0.06861557]\n",
      " [0.05100017 0.00793507 0.9229665  0.01809824]\n",
      " [0.05102304 0.0079353  0.9229411  0.01810058]]\n",
      "Iteration 192, Accuracy 0.38911\n",
      "96.67109%change in label assignment\n",
      "0.088927746\n",
      "[[0.07699996 0.44300845 0.02530457 0.454687  ]\n",
      " [0.08969851 0.05192941 0.01803326 0.8403388 ]\n",
      " [0.25590122 0.05701846 0.03642413 0.65065616]\n",
      " ...\n",
      " [0.02625372 0.86698174 0.01043996 0.0963246 ]\n",
      " [0.05082331 0.00791893 0.9231985  0.01805934]\n",
      " [0.05085037 0.00791944 0.9231676  0.01806258]]\n",
      "Iteration 193, Accuracy 0.38729\n",
      "92.42402%change in label assignment\n",
      "0.09338996\n",
      "[[0.0189541  0.91522455 0.00813419 0.05768718]\n",
      " [0.08162245 0.20648669 0.02328953 0.6886013 ]\n",
      " [0.07213214 0.06198053 0.01603532 0.84985197]\n",
      " ...\n",
      " [0.04383072 0.82824534 0.0212003  0.10672355]\n",
      " [0.05076787 0.00792886 0.9232099  0.01809345]\n",
      " [0.05078549 0.00792867 0.9231912  0.0180946 ]]\n",
      "Iteration 194, Accuracy 0.38916\n",
      "87.71542%change in label assignment\n",
      "0.09225727\n",
      "[[0.05760895 0.64911455 0.02046663 0.27280986]\n",
      " [0.07746634 0.0549692  0.01647396 0.8510906 ]\n",
      " [0.20302553 0.05563799 0.03119096 0.71014553]\n",
      " ...\n",
      " [0.01880883 0.9116538  0.00783386 0.06170351]\n",
      " [0.05054067 0.00791008 0.9235071  0.01804212]\n",
      " [0.05056237 0.00791022 0.9234833  0.01804417]]\n",
      "Iteration 195, Accuracy 0.39883\n",
      "90.66136%change in label assignment\n",
      "0.08762449\n",
      "[[0.05553218 0.66644984 0.01976836 0.25824958]\n",
      " [0.07595401 0.05597861 0.01619739 0.85187006]\n",
      " [0.2044804  0.05586066 0.03109172 0.7085672 ]\n",
      " ...\n",
      " [0.01857605 0.9133725  0.00774139 0.06031002]\n",
      " [0.05047259 0.00790593 0.9236016  0.01801983]\n",
      " [0.05049293 0.00790603 0.9235793  0.01802168]]\n",
      "Iteration 196, Accuracy 0.3871\n",
      "98.5123%change in label assignment\n",
      "0.08830654\n",
      "[[0.05020373 0.7067637  0.01823183 0.22480078]\n",
      " [0.0698257  0.06794274 0.01614855 0.84608305]\n",
      " [0.15748836 0.05382548 0.02616856 0.76251763]\n",
      " ...\n",
      " [0.01866474 0.9166605  0.00807093 0.0566038 ]\n",
      " [0.0504764  0.00790178 0.9236199  0.01800193]\n",
      " [0.05049825 0.00790195 0.9235957  0.01800407]]\n",
      "Iteration 197, Accuracy 0.38577\n",
      "96.44523%change in label assignment\n",
      "0.09268307\n",
      "[[0.07968798 0.3957472  0.02551234 0.49905246]\n",
      " [0.10060346 0.05126986 0.01925918 0.82886755]\n",
      " [0.31847456 0.05708899 0.04099888 0.5834376 ]\n",
      " ...\n",
      " [0.02854803 0.85278654 0.01117941 0.10748599]\n",
      " [0.05033989 0.00789153 0.923802   0.01796655]\n",
      " [0.05035897 0.0078916  0.92378116 0.01796824]]\n",
      "Iteration 198, Accuracy 0.38361\n",
      "94.2014%change in label assignment\n",
      "0.08782743\n",
      "[[0.02287963 0.88732445 0.00917532 0.08062059]\n",
      " [0.07886944 0.16728613 0.02172607 0.73211837]\n",
      " [0.08093843 0.05415789 0.0167161  0.84818757]\n",
      " ...\n",
      " [0.03709635 0.8512975  0.01756657 0.09403964]\n",
      " [0.05046866 0.00790057 0.92364407 0.01798678]\n",
      " [0.05049212 0.00790079 0.9236179  0.01798916]]\n",
      "Iteration 199, Accuracy 0.39102\n",
      "87.63195%change in label assignment\n",
      "[[0.0826406  0.30672717 0.02545832 0.58517385]\n",
      " [0.11853117 0.05213369 0.02181187 0.8075233 ]\n",
      " [0.3687571  0.05626743 0.04470406 0.53027135]\n",
      " ...\n",
      " [0.03541981 0.8099153  0.01360636 0.14105855]\n",
      " [0.05020288 0.0078809  0.9239881  0.01792806]\n",
      " [0.05022317 0.00788095 0.923966   0.01792989]]\n",
      "Iteration 200, Accuracy 0.39328\n",
      "86.42412%change in label assignment\n",
      "0.09909169\n",
      "[[0.01947971 0.91278505 0.00834344 0.05939176]\n",
      " [0.08128613 0.36391136 0.02543949 0.52936304]\n",
      " [0.07161315 0.09223458 0.01758312 0.8185692 ]\n",
      " ...\n",
      " [0.04716846 0.8164846  0.02296594 0.11338102]\n",
      " [0.0504888  0.00789846 0.92363775 0.01797495]\n",
      " [0.05051565 0.00789906 0.92360693 0.01797831]]\n",
      "Iteration 201, Accuracy 0.39284\n",
      "81.64187%change in label assignment\n",
      "0.101172656\n",
      "[[0.07429674 0.48092225 0.02486587 0.41991514]\n",
      " [0.07244564 0.06191214 0.01637372 0.8492685 ]\n",
      " [0.20157114 0.05602936 0.03143058 0.710969  ]\n",
      " ...\n",
      " [0.02144027 0.8961882  0.0088043  0.07356723]\n",
      " [0.05019725 0.00787922 0.9240018  0.01792175]\n",
      " [0.05022468 0.0078798  0.92397046 0.01792513]]\n",
      "Iteration 202, Accuracy 0.39701\n",
      "87.17533%change in label assignment\n",
      "0.09396865\n",
      "[[0.03636558 0.80489266 0.01373247 0.1450093 ]\n",
      " [0.07835137 0.15781425 0.02123779 0.74259657]\n",
      " [0.08484989 0.05391587 0.01714422 0.84409   ]\n",
      " ...\n",
      " [0.02347397 0.8995505  0.0104251  0.06655043]\n",
      " [0.05036465 0.00788564 0.9238087  0.01794094]\n",
      " [0.05039967 0.00788671 0.9237678  0.01794579]]\n",
      "Iteration 203, Accuracy 0.39343\n",
      "92.48294%change in label assignment\n",
      "0.09685681\n",
      "[[0.07825436 0.42606032 0.02570576 0.4699796 ]\n",
      " [0.07516242 0.0595191  0.01672393 0.84859455]\n",
      " [0.22413379 0.05692182 0.03394304 0.6850014 ]\n",
      " ...\n",
      " [0.02454728 0.8781947  0.00995145 0.08730656]\n",
      " [0.05014099 0.00787095 0.92409223 0.0178958 ]\n",
      " [0.05016327 0.00787125 0.92406714 0.01789827]]\n",
      "Iteration 204, Accuracy 0.38474\n",
      "91.84956%change in label assignment\n",
      "0.09614034\n",
      "[[0.02005549 0.9103707  0.00856815 0.06100567]\n",
      " [0.07581546 0.4657953  0.02474948 0.4336397 ]\n",
      " [0.07523964 0.1208149  0.01937559 0.78456986]\n",
      " ...\n",
      " [0.04636417 0.8191858  0.02243732 0.11201269]\n",
      " [0.05042588 0.0078885  0.9237311  0.01795451]\n",
      " [0.05047461 0.00789024 0.9236735  0.01796159]]\n",
      "Iteration 205, Accuracy 0.39289\n",
      "82.38818%change in label assignment\n",
      "0.101197384\n",
      "[[0.06931778 0.5403474  0.02385815 0.36647668]\n",
      " [0.07144341 0.06920057 0.0167773  0.8425787 ]\n",
      " [0.17531033 0.05538936 0.02890875 0.7403915 ]\n",
      " ...\n",
      " [0.02082401 0.90065926 0.00866546 0.06985124]\n",
      " [0.0500902  0.00786744 0.9241555  0.0178869 ]\n",
      " [0.05011889 0.0078679  0.924123   0.0178902 ]]\n",
      "Iteration 206, Accuracy 0.3923\n",
      "83.96917%change in label assignment\n",
      "0.09919605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03950405 0.7847566  0.0147488  0.16099052]\n",
      " [0.0760656  0.13179776 0.01992933 0.7722074 ]\n",
      " [0.09261478 0.05334346 0.01810376 0.83593804]\n",
      " ...\n",
      " [0.02291253 0.90115786 0.01008869 0.06584094]\n",
      " [0.05030411 0.00787534 0.9239172  0.01790345]\n",
      " [0.05033171 0.00787572 0.9238861  0.01790647]]\n",
      "Iteration 207, Accuracy 0.39269\n",
      "95.17847%change in label assignment\n",
      "0.09642324\n",
      "[[0.05734747 0.64986086 0.02063921 0.2721525 ]\n",
      " [0.07242475 0.1009353  0.0186191  0.80802083]\n",
      " [0.10502619 0.05233198 0.02030681 0.82233495]\n",
      " ...\n",
      " [0.01962301 0.9080918  0.00829717 0.06398802]\n",
      " [0.05025322 0.00786524 0.9240015  0.01788001]\n",
      " [0.05031361 0.00786711 0.923931   0.01788831]]\n",
      "Iteration 208, Accuracy 0.38592\n",
      "95.6351%change in label assignment\n",
      "0.09742387\n",
      "[[0.04516427 0.7441218  0.01657277 0.1941412 ]\n",
      " [0.07216583 0.10195962 0.01811216 0.80776244]\n",
      " [0.10163438 0.05248763 0.01924715 0.82663083]\n",
      " ...\n",
      " [0.01965943 0.91222245 0.0084563  0.05966182]\n",
      " [0.05030748 0.00786603 0.9239505  0.01787594]\n",
      " [0.0503306  0.00786599 0.92392564 0.01787785]]\n",
      "Iteration 209, Accuracy 0.38877\n",
      "95.46325%change in label assignment\n",
      "0.09089589\n",
      "[[0.04741178 0.7258008  0.01756896 0.20921844]\n",
      " [0.07062694 0.08990283 0.01773887 0.8217314 ]\n",
      " [0.10930494 0.05204441 0.02074081 0.8179098 ]\n",
      " ...\n",
      " [0.01887795 0.9140919  0.00815376 0.05887642]\n",
      " [0.05014546 0.00785742 0.92415684 0.0178403 ]\n",
      " [0.05017477 0.00785748 0.9241248  0.01784297]]\n",
      "Iteration 210, Accuracy 0.3867\n",
      "98.88054%change in label assignment\n",
      "0.096741915\n",
      "[[0.03534428 0.8105285  0.01339925 0.14072795]\n",
      " [0.0722343  0.10358447 0.01818545 0.8059957 ]\n",
      " [0.09620012 0.05254854 0.01854321 0.8327082 ]\n",
      " ...\n",
      " [0.02045923 0.9098529  0.00889622 0.06079162]\n",
      " [0.05017243 0.00785697 0.9241243  0.01784633]\n",
      " [0.05019885 0.00785706 0.92409545 0.01784873]]\n",
      "Iteration 211, Accuracy 0.3897\n",
      "98.22262%change in label assignment\n",
      "0.096378475\n",
      "[[0.07506602 0.46643534 0.02516253 0.43333617]\n",
      " [0.08682649 0.05357519 0.01799891 0.8415994 ]\n",
      " [0.2314175  0.05730712 0.0347479  0.6765275 ]\n",
      " ...\n",
      " [0.02913765 0.8496852  0.0115822  0.10959493]\n",
      " [0.05003523 0.00784445 0.92431575 0.01780456]\n",
      " [0.05003899 0.00784446 0.9243116  0.01780495]]\n",
      "Iteration 212, Accuracy 0.38646\n",
      "93.17523%change in label assignment\n",
      "0.09847896\n",
      "[[0.02334444 0.8990294  0.01027956 0.06734663]\n",
      " [0.07642907 0.4521882  0.02485682 0.4465259 ]\n",
      " [0.07709551 0.14033489 0.02041754 0.762152  ]\n",
      " ...\n",
      " [0.05381598 0.7936162  0.02661074 0.12595709]\n",
      " [0.05035418 0.00786406 0.9239242  0.01785749]\n",
      " [0.05039515 0.0078648  0.9238777  0.01786227]]\n",
      "Iteration 213, Accuracy 0.39358\n",
      "81.71061%change in label assignment\n",
      "0.10356966\n",
      "[[0.06166074 0.61020416 0.02183721 0.3062978 ]\n",
      " [0.07106511 0.06589432 0.01651544 0.8465251 ]\n",
      " [0.1542647  0.05453793 0.0265205  0.7646769 ]\n",
      " ...\n",
      " [0.0200857  0.9042706  0.00839891 0.0672448 ]\n",
      " [0.04999822 0.00784223 0.9243633  0.01779619]\n",
      " [0.05001223 0.00784237 0.9243477  0.01779768]]\n",
      "Iteration 214, Accuracy 0.4003\n",
      "85.08371%change in label assignment\n",
      "0.09355962\n",
      "[[0.0210782  0.90151376 0.00869987 0.06870817]\n",
      " [0.08332291 0.265648   0.02456911 0.62646   ]\n",
      " [0.07166339 0.07678025 0.01672986 0.8348265 ]\n",
      " ...\n",
      " [0.03735939 0.8496115  0.01750008 0.09552897]\n",
      " [0.05017929 0.0078514  0.9241306  0.01783876]\n",
      " [0.05019967 0.00785144 0.92410827 0.01784057]]\n",
      "Iteration 215, Accuracy 0.39171\n",
      "90.45515%change in label assignment\n",
      "0.097835645\n",
      "[[0.07768287 0.14558865 0.02144499 0.7552835 ]\n",
      " [0.1777588  0.05593321 0.02961643 0.73669153]\n",
      " [0.44878346 0.0536731  0.04981949 0.44772395]\n",
      " ...\n",
      " [0.07086068 0.5277764  0.02453545 0.37682745]\n",
      " [0.04993095 0.00782272 0.9244515  0.01779485]\n",
      " [0.04993191 0.00782287 0.92445016 0.01779501]]\n",
      "Iteration 216, Accuracy 0.39255\n",
      "78.44552%change in label assignment\n",
      "0.11148624\n",
      "[[0.06818372 0.7474558  0.03496592 0.1493946 ]\n",
      " [0.02155808 0.897541   0.00880787 0.07209303]\n",
      " [0.06262717 0.60867184 0.02170642 0.30699453]\n",
      " ...\n",
      " [0.10559772 0.6349063  0.05959804 0.19989793]\n",
      " [0.0504911  0.00787012 0.92370707 0.01793169]\n",
      " [0.05049207 0.00787022 0.92370564 0.01793202]]\n",
      "Iteration 217, Accuracy 0.40251\n",
      "62.59636%change in label assignment\n",
      "0.10002119\n",
      "[[0.02003801 0.9121642  0.00885345 0.0589443 ]\n",
      " [0.08054005 0.3719136  0.02571457 0.5218318 ]\n",
      " [0.07089597 0.09496157 0.01794117 0.8162013 ]\n",
      " ...\n",
      " [0.05283212 0.7973455  0.02645139 0.12337098]\n",
      " [0.05014247 0.00784765 0.9241336  0.01787626]\n",
      " [0.05014418 0.00784784 0.9241314  0.01787659]]\n",
      "Iteration 218, Accuracy 0.42083\n",
      "82.85462%change in label assignment\n",
      "0.097786695\n",
      "[[0.02869292 0.85150385 0.01120364 0.1085996 ]\n",
      " [0.07236114 0.10957396 0.01868312 0.79938185]\n",
      " [0.09943822 0.05068261 0.01904101 0.83083814]\n",
      " ...\n",
      " [0.02337078 0.90051913 0.01052677 0.06558336]\n",
      " [0.04998916 0.00783343 0.9243189  0.01785839]\n",
      " [0.04999213 0.00783375 0.9243151  0.01785906]]\n",
      "Iteration 219, Accuracy 0.39903\n",
      "90.92159%change in label assignment\n",
      "0.092012614\n",
      "[[0.05966135 0.6291974  0.02098179 0.29015946]\n",
      " [0.06957471 0.06265944 0.0157545  0.85201126]\n",
      " [0.16640376 0.05390525 0.02710161 0.75258934]\n",
      " ...\n",
      " [0.01894147 0.9100536  0.00782876 0.06317624]\n",
      " [0.04995305 0.00782402 0.9243855  0.01783743]\n",
      " [0.04996011 0.00782469 0.9243761  0.01783901]]\n",
      "Iteration 220, Accuracy 0.38646\n",
      "95.28158%change in label assignment\n",
      "0.08724253\n",
      "[[0.03647656 0.8022118  0.01383559 0.14747602]\n",
      " [0.07182537 0.10588014 0.01840061 0.8038938 ]\n",
      " [0.09748966 0.05060717 0.01871634 0.8331868 ]\n",
      " ...\n",
      " [0.01915485 0.91543216 0.0083673  0.05704564]\n",
      " [0.04996926 0.00782279 0.92436534 0.01784271]\n",
      " [0.0499723  0.00782305 0.92436135 0.01784331]]\n",
      "Iteration 221, Accuracy 0.38965\n",
      "95.05082%change in label assignment\n",
      "0.08990752\n",
      "[[0.04697599 0.72959524 0.01725571 0.2061731 ]\n",
      " [0.06984685 0.08948554 0.01735378 0.8233138 ]\n",
      " [0.10852513 0.05075156 0.02017639 0.820547  ]\n",
      " ...\n",
      " [0.01780716 0.91799396 0.00754411 0.05665472]\n",
      " [0.04993279 0.00781625 0.92441976 0.01783115]\n",
      " [0.04996297 0.00781694 0.92438513 0.01783495]]\n",
      "Iteration 222, Accuracy 0.38621\n",
      "97.79054%change in label assignment\n",
      "0.09040308\n",
      "[[0.05803887 0.6423848  0.02054358 0.2790327 ]\n",
      " [0.06912852 0.08436023 0.01693424 0.82957697]\n",
      " [0.11735297 0.05123203 0.0212585  0.81015646]\n",
      " ...\n",
      " [0.01828116 0.9141442  0.0076294  0.05994525]\n",
      " [0.04994483 0.00781339 0.92442924 0.01781261]\n",
      " [0.05000893 0.00781607 0.9243522  0.01782274]]\n",
      "Iteration 223, Accuracy 0.38847\n",
      "97.09825%change in label assignment\n",
      "0.0933752\n",
      "[[0.04925904 0.7119028  0.01795639 0.22088179]\n",
      " [0.07303343 0.1176266  0.01910551 0.7902344 ]\n",
      " [0.09427514 0.05073104 0.01832386 0.83667   ]\n",
      " ...\n",
      " [0.01822959 0.91813004 0.00787804 0.05576227]\n",
      " [0.04993256 0.00781137 0.9244552  0.0178009 ]\n",
      " [0.04998769 0.00781343 0.92438984 0.01780911]]\n",
      "Iteration 224, Accuracy 0.38882\n",
      "97.92802%change in label assignment\n",
      "0.09587812\n",
      "[[0.07123311 0.5123775  0.02404023 0.39234915]\n",
      " [0.0684115  0.07686727 0.01647616 0.8382451 ]\n",
      " [0.13636455 0.0522711  0.02369651 0.7876679 ]\n",
      " ...\n",
      " [0.0202959  0.90162456 0.00831485 0.06976475]\n",
      " [0.0498891  0.00780448 0.924519   0.01778742]\n",
      " [0.04994148 0.00780642 0.92445683 0.01779522]]\n",
      "Iteration 225, Accuracy 0.38587\n",
      "96.56798%change in label assignment\n",
      "0.095237225\n",
      "[[0.025535   0.87091    0.0101014  0.09345359]\n",
      " [0.08235268 0.26975396 0.02462257 0.62327075]\n",
      " [0.06906712 0.06986014 0.01604089 0.84503186]\n",
      " ...\n",
      " [0.02993222 0.8765616  0.01384501 0.07966119]\n",
      " [0.05009481 0.00781622 0.9242685  0.01782049]\n",
      " [0.05021666 0.00782239 0.92411894 0.01784205]]\n",
      "Iteration 226, Accuracy 0.3899\n",
      "88.38317%change in label assignment\n",
      "0.09799432\n",
      "[[0.08176691 0.33383164 0.02572784 0.5586736 ]\n",
      " [0.08095436 0.05325709 0.01704085 0.84874773]\n",
      " [0.23603263 0.05671623 0.03486514 0.67238593]\n",
      " ...\n",
      " [0.03825748 0.79089975 0.01464574 0.15619704]\n",
      " [0.04979412 0.00779328 0.9246583  0.0177543 ]\n",
      " [0.04981853 0.00779409 0.92462975 0.01775771]]\n",
      "Iteration 227, Accuracy 0.37728\n",
      "83.50272%change in label assignment\n",
      "0.101906225\n",
      "[[0.02036696 0.90938175 0.00877124 0.06147997]\n",
      " [0.07496159 0.47414345 0.02465382 0.42624113]\n",
      " [0.07648195 0.13530292 0.02017772 0.76803744]\n",
      " ...\n",
      " [0.05133838 0.8022234  0.02527548 0.12116281]\n",
      " [0.0501307  0.0078161  0.9242358  0.0178173 ]\n",
      " [0.05020844 0.00781918 0.9241431  0.01782937]]\n",
      "Iteration 228, Accuracy 0.39687\n",
      "79.94305%change in label assignment\n",
      "0.10730363\n",
      "[[0.07939152 0.40547964 0.02603631 0.4890925 ]\n",
      " [0.07573624 0.05907274 0.01692971 0.8482613 ]\n",
      " [0.20854858 0.05659342 0.03277012 0.7020879 ]\n",
      " ...\n",
      " [0.03008625 0.844681   0.01197619 0.11325652]\n",
      " [0.04977398 0.00779022 0.9246889  0.01774698]\n",
      " [0.04977596 0.00779043 0.9246863  0.01774728]]\n",
      "Iteration 229, Accuracy 0.38906\n",
      "79.83503%change in label assignment\n",
      "0.10154154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02223014 0.90239435 0.00962882 0.06574669]\n",
      " [0.07501183 0.4804614  0.02466509 0.41986173]\n",
      " [0.07726225 0.1346139  0.02026959 0.7678542 ]\n",
      " ...\n",
      " [0.05369883 0.7943264  0.02649156 0.12548323]\n",
      " [0.05013686 0.00781343 0.9242287  0.017821  ]\n",
      " [0.05013897 0.00781354 0.92422616 0.01782127]]\n",
      "Iteration 230, Accuracy 0.39657\n",
      "83.03137%change in label assignment\n",
      "0.094882704\n",
      "[[0.02779046 0.8568646  0.01101507 0.1043299 ]\n",
      " [0.07881081 0.17270233 0.02228517 0.7262017 ]\n",
      " [0.0781895  0.05407769 0.01672276 0.85101   ]\n",
      " ...\n",
      " [0.02381589 0.89849114 0.01083456 0.06685839]\n",
      " [0.04984935 0.00779204 0.9245886  0.01776992]\n",
      " [0.04984776 0.00779194 0.9245908  0.01776943]]\n",
      "Iteration 231, Accuracy 0.39431\n",
      "88.58938%change in label assignment\n",
      "0.09037966\n",
      "[[0.02890806 0.85012025 0.0112648  0.10970682]\n",
      " [0.07672468 0.15268783 0.02093251 0.74965495]\n",
      " [0.08061846 0.05218165 0.01655512 0.8506448 ]\n",
      " ...\n",
      " [0.02112498 0.90842456 0.00936571 0.06108474]\n",
      " [0.04985476 0.00778706 0.9245961  0.01776221]\n",
      " [0.04985324 0.00778697 0.924598   0.01776176]]\n",
      "Iteration 232, Accuracy 0.38548\n",
      "98.91982%change in label assignment\n",
      "0.08705386\n",
      "[[0.04814788 0.71973836 0.01771264 0.21440105]\n",
      " [0.06894858 0.08457055 0.01708157 0.82939935]\n",
      " [0.11360107 0.05089173 0.020994   0.8145132 ]\n",
      " ...\n",
      " [0.01878767 0.91062766 0.00783101 0.06275367]\n",
      " [0.04976535 0.00777448 0.92472196 0.01773824]\n",
      " [0.04976403 0.00777441 0.92472374 0.01773785]]\n",
      "Iteration 233, Accuracy 0.38548\n",
      "96.8282%change in label assignment\n",
      "0.087685965\n",
      "[[0.0192787  0.90820885 0.00794182 0.06457062]\n",
      " [0.08121678 0.21892665 0.02350473 0.6763519 ]\n",
      " [0.06938013 0.0629004  0.01568937 0.8520301 ]\n",
      " ...\n",
      " [0.02650916 0.8891471  0.01211218 0.07223152]\n",
      " [0.04972224 0.00777591 0.9247467  0.01775509]\n",
      " [0.04972065 0.00777582 0.9247489  0.01775462]]\n",
      "Iteration 234, Accuracy 0.38749\n",
      "94.20631%change in label assignment\n",
      "0.08895783\n",
      "[[0.05485493 0.6686337  0.01972594 0.25678536]\n",
      " [0.06839272 0.06948133 0.01612898 0.8459969 ]\n",
      " [0.14230324 0.05237176 0.02450103 0.78082395]\n",
      " ...\n",
      " [0.02172026 0.8931855  0.00882639 0.07626785]\n",
      " [0.04948776 0.00775942 0.92504555 0.01770722]\n",
      " [0.04948667 0.00775939 0.92504704 0.01770691]]\n",
      "Iteration 235, Accuracy 0.38999\n",
      "91.88884%change in label assignment\n",
      "0.089055784\n",
      "[[0.02301189 0.88599247 0.00921549 0.08178015]\n",
      " [0.07608288 0.14389566 0.02050932 0.7595121 ]\n",
      " [0.07909027 0.05285268 0.016358   0.851699  ]\n",
      " ...\n",
      " [0.02064927 0.9101781  0.00911542 0.06005717]\n",
      " [0.0495067  0.00776178 0.9250291  0.0177024 ]\n",
      " [0.04950521 0.00776169 0.9250311  0.01770196]]\n",
      "Iteration 236, Accuracy 0.38931\n",
      "95.18338%change in label assignment\n",
      "0.08709941\n",
      "[[0.05492761 0.6669367  0.01973856 0.2583972 ]\n",
      " [0.06879123 0.0633136  0.01579147 0.8521037 ]\n",
      " [0.15112072 0.05299906 0.02553734 0.7703429 ]\n",
      " ...\n",
      " [0.02309715 0.88475496 0.00930943 0.08283846]\n",
      " [0.04937026 0.0077479  0.92522764 0.01765421]\n",
      " [0.04936913 0.00774786 0.9252291  0.01765389]]\n",
      "Iteration 237, Accuracy 0.38626\n",
      "94.74149%change in label assignment\n",
      "0.09136473\n",
      "[[0.02248664 0.88889813 0.00903741 0.07957782]\n",
      " [0.07712267 0.15698498 0.02111329 0.7447791 ]\n",
      " [0.07529312 0.05470786 0.01596633 0.85403275]\n",
      " ...\n",
      " [0.02158423 0.9067569  0.00959834 0.06206058]\n",
      " [0.04942307 0.00775143 0.92516553 0.01765992]\n",
      " [0.04942164 0.00775136 0.9251675  0.01765951]]\n",
      "Iteration 238, Accuracy 0.3898\n",
      "93.2587%change in label assignment\n",
      "0.08721573\n",
      "[[0.05337498 0.67967725 0.01933129 0.2476165 ]\n",
      " [0.06993725 0.06057712 0.01584749 0.8536381 ]\n",
      " [0.15438381 0.05321626 0.02602322 0.7663767 ]\n",
      " ...\n",
      " [0.02619287 0.8664048  0.01043356 0.09696878]\n",
      " [0.04924711 0.00773638 0.9253878  0.01762877]\n",
      " [0.04924598 0.00773634 0.92538923 0.01762844]]\n",
      "Iteration 239, Accuracy 0.38543\n",
      "93.05249%change in label assignment\n",
      "[[0.01856582 0.9153591  0.00787877 0.05819629]\n",
      " [0.08223163 0.24884805 0.02422084 0.6446995 ]\n",
      " [0.06926262 0.07177124 0.01616396 0.8428022 ]\n",
      " ...\n",
      " [0.03023727 0.87549996 0.01398841 0.08027435]\n",
      " [0.04935487 0.00774311 0.92524976 0.01765233]\n",
      " [0.04935325 0.00774301 0.92525196 0.01765184]]\n",
      "Iteration 240, Accuracy 0.38906\n",
      "90.14582%change in label assignment\n",
      "0.0945097\n",
      "[[0.06406974 0.590051   0.02257437 0.32330495]\n",
      " [0.08287591 0.05297025 0.0173926  0.8467612 ]\n",
      " [0.23237628 0.0565429  0.03470613 0.6763747 ]\n",
      " ...\n",
      " [0.02532642 0.8728856  0.01023902 0.09154893]\n",
      " [0.04911847 0.00772611 0.92554724 0.01760824]\n",
      " [0.04911711 0.00772603 0.925549   0.01760782]]\n",
      "Iteration 241, Accuracy 0.39171\n",
      "88.21132%change in label assignment\n",
      "0.091733746\n",
      "[[0.01990448 0.91112995 0.00855043 0.06041513]\n",
      " [0.08271816 0.30482057 0.02510932 0.5873519 ]\n",
      " [0.07096603 0.08831257 0.01727539 0.82344604]\n",
      " ...\n",
      " [0.03345134 0.8638439  0.01558626 0.08711848]\n",
      " [0.04933907 0.00773835 0.9252865  0.01763606]\n",
      " [0.04933879 0.00773835 0.92528695 0.01763586]]\n",
      "Iteration 242, Accuracy 0.38597\n",
      "85.23101%change in label assignment\n",
      "0.09725653\n",
      "[[0.05078924 0.70156    0.0187226  0.2289281 ]\n",
      " [0.07114302 0.06331889 0.0164396  0.84909844]\n",
      " [0.14899087 0.05347797 0.02585965 0.7716715 ]\n",
      " ...\n",
      " [0.0232084  0.88550365 0.00950161 0.08178636]\n",
      " [0.04909789 0.00772099 0.92559    0.01759114]\n",
      " [0.04909676 0.00772095 0.9255915  0.01759081]]\n",
      "Iteration 243, Accuracy 0.39407\n",
      "89.37988%change in label assignment\n",
      "0.09224639\n",
      "[[0.02074877 0.9016301  0.00850753 0.06911361]\n",
      " [0.07734919 0.14940031 0.02080147 0.75244904]\n",
      " [0.07637913 0.05681608 0.01618046 0.8506244 ]\n",
      " ...\n",
      " [0.02471638 0.8949218  0.01105611 0.06930582]\n",
      " [0.04923487 0.00772897 0.92542726 0.01760896]\n",
      " [0.04923332 0.00772889 0.9254292  0.01760851]]\n",
      "Iteration 244, Accuracy 0.38945\n",
      "93.98046%change in label assignment\n",
      "0.09691316\n",
      "[[0.02934396 0.84768826 0.01160336 0.11136446]\n",
      " [0.07027411 0.08654022 0.01761589 0.8255698 ]\n",
      " [0.10201453 0.05120154 0.0198316  0.82695234]\n",
      " ...\n",
      " [0.0185979  0.9145505  0.00799335 0.05885829]\n",
      " [0.04909078 0.00771812 0.92559415 0.01759689]\n",
      " [0.04908929 0.00771803 0.92559624 0.01759644]]\n",
      "Iteration 245, Accuracy 0.38778\n",
      "96.69564%change in label assignment\n",
      "0.09067668\n",
      "[[0.01911889 0.9104868  0.0079345  0.06245974]\n",
      " [0.07905208 0.17599459 0.0219495  0.72300386]\n",
      " [0.07097899 0.06057323 0.01569142 0.8527564 ]\n",
      " ...\n",
      " [0.02285776 0.902077   0.0101854  0.06487982]\n",
      " [0.04913831 0.00771925 0.9255351  0.01760733]\n",
      " [0.04913682 0.00771918 0.92553705 0.0176069 ]]\n",
      "Iteration 246, Accuracy 0.38386\n",
      "95.07046%change in label assignment\n",
      "0.09294595\n",
      "[[0.03437374 0.8153941  0.0132756  0.13695654]\n",
      " [0.06908902 0.08186929 0.01705646 0.8319852 ]\n",
      " [0.10521889 0.05076428 0.02006269 0.8239542 ]\n",
      " ...\n",
      " [0.01921432 0.9086432  0.00803513 0.06410731]\n",
      " [0.04896257 0.00770792 0.925764   0.01756551]\n",
      " [0.04896142 0.00770789 0.9257655  0.01756519]]\n",
      "Iteration 247, Accuracy 0.38774\n",
      "94.65312%change in label assignment\n",
      "0.09397249\n",
      "[[0.01877513 0.91308904 0.00786155 0.06027424]\n",
      " [0.08051295 0.20148131 0.02286352 0.69514215]\n",
      " [0.06924708 0.06732468 0.0158607  0.8475675 ]\n",
      " ...\n",
      " [0.02357502 0.8994401  0.01054995 0.06643493]\n",
      " [0.04908207 0.00771244 0.9256244  0.01758119]\n",
      " [0.04908177 0.00771244 0.92562485 0.01758099]]\n",
      "Iteration 248, Accuracy 0.38901\n",
      "94.27996%change in label assignment\n",
      "0.0924846\n",
      "[[0.05743813 0.64624393 0.02061542 0.2757025 ]\n",
      " [0.07312952 0.05639145 0.01614117 0.8543378 ]\n",
      " [0.17481421 0.05437375 0.02855737 0.74225473]\n",
      " ...\n",
      " [0.02727279 0.8600099  0.01086042 0.10185693]\n",
      " [0.04890248 0.00769716 0.9258702  0.01753016]\n",
      " [0.04890136 0.0076971  0.92587173 0.01752983]]\n",
      "Iteration 249, Accuracy 0.38793\n",
      "89.81195%change in label assignment\n",
      "0.09182509\n",
      "[[0.02614703 0.88997304 0.01183448 0.07204544]\n",
      " [0.07620519 0.4499972  0.02486339 0.44893423]\n",
      " [0.07625534 0.1440952  0.0204552  0.75919425]\n",
      " ...\n",
      " [0.04324839 0.8297328  0.02087675 0.10614207]\n",
      " [0.04906182 0.00771021 0.9256658  0.01756223]\n",
      " [0.04906027 0.00771011 0.9256678  0.01756179]]\n",
      "Iteration 250, Accuracy 0.39053\n",
      "83.75313%change in label assignment\n",
      "0.099544674\n",
      "[[0.05012649 0.70542073 0.01847849 0.22597426]\n",
      " [0.06909136 0.07082863 0.01651995 0.84356   ]\n",
      " [0.13463837 0.0526422  0.02404433 0.78867507]\n",
      " ...\n",
      " [0.02267509 0.88812166 0.00928167 0.07992154]\n",
      " [0.04881458 0.00769157 0.92598796 0.01750592]\n",
      " [0.04881372 0.00769155 0.9259891  0.01750568]]\n",
      "Iteration 251, Accuracy 0.39682\n",
      "87.13114%change in label assignment\n",
      "0.09819966\n",
      "[[0.02032935 0.9091961  0.00871903 0.0617555 ]\n",
      " [0.08267977 0.31271726 0.02516862 0.57943434]\n",
      " [0.07099986 0.08840202 0.01726381 0.8233343 ]\n",
      " ...\n",
      " [0.03289389 0.8656205  0.015254   0.08623162]\n",
      " [0.04905175 0.00770302 0.9256992  0.01754605]\n",
      " [0.04905118 0.00770299 0.92570007 0.01754581]]\n",
      "Iteration 252, Accuracy 0.3925\n",
      "90.19983%change in label assignment\n",
      "0.095935345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05182023 0.69294924 0.01906702 0.23616359]\n",
      " [0.07032499 0.06539994 0.01646066 0.8478144 ]\n",
      " [0.14434883 0.05341568 0.02540047 0.776835  ]\n",
      " ...\n",
      " [0.02599837 0.86857593 0.01050165 0.09492407]\n",
      " [0.04881249 0.00768538 0.9260044  0.01749761]\n",
      " [0.0488116  0.00768536 0.92600566 0.01749736]]\n",
      "Iteration 253, Accuracy 0.39152\n",
      "89.89542%change in label assignment\n",
      "0.094130546\n",
      "[[0.01939004 0.9102446  0.00810948 0.06225595]\n",
      " [0.08204585 0.23761262 0.02388956 0.656452  ]\n",
      " [0.06983625 0.067767   0.01595484 0.8464419 ]\n",
      " ...\n",
      " [0.02763051 0.8844986  0.01256996 0.07530094]\n",
      " [0.04901714 0.00769625 0.9257535  0.01753314]\n",
      " [0.04901568 0.00769615 0.92575544 0.0175327 ]]\n",
      "Iteration 254, Accuracy 0.39264\n",
      "92.00668%change in label assignment\n",
      "0.0942563\n",
      "[[0.05221245 0.6919411  0.01930646 0.23654005]\n",
      " [0.07079947 0.07245896 0.0171009  0.83964074]\n",
      " [0.1416711  0.05356468 0.02528551 0.7794787 ]\n",
      " ...\n",
      " [0.02497125 0.8757619  0.01021739 0.08904938]\n",
      " [0.04883423 0.00767975 0.92597884 0.01750717]\n",
      " [0.04883318 0.00767969 0.9259803  0.01750685]]\n",
      "Iteration 255, Accuracy 0.38828\n",
      "91.94285%change in label assignment\n",
      "0.08652103\n",
      "[[0.02662748 0.8845661  0.01151156 0.07729482]\n",
      " [0.08353475 0.38159645 0.02608456 0.50878423]\n",
      " [0.07881971 0.11559351 0.01978761 0.78579915]\n",
      " ...\n",
      " [0.04183226 0.8335746  0.01960974 0.10498329]\n",
      " [0.04899561 0.00769178 0.92576605 0.01754664]\n",
      " [0.04899745 0.00769194 0.9257637  0.01754691]]\n",
      "Iteration 256, Accuracy 0.38896\n",
      "86.87092%change in label assignment\n",
      "0.103347465\n",
      "[[0.08333685 0.29286656 0.02592815 0.59786844]\n",
      " [0.11147005 0.05248245 0.02149041 0.8145571 ]\n",
      " [0.32852796 0.0571695  0.04342864 0.5708739 ]\n",
      " ...\n",
      " [0.06067049 0.62619096 0.02190286 0.2912357 ]\n",
      " [0.04864702 0.00766573 0.9261936  0.01749363]\n",
      " [0.04864654 0.00766574 0.9261942  0.01749351]]\n",
      "Iteration 257, Accuracy 0.39156\n",
      "81.29818%change in label assignment\n",
      "0.103242755\n",
      "[[0.03371242 0.86288965 0.01567278 0.08772516]\n",
      " [0.05986563 0.6312277  0.02096818 0.28793854]\n",
      " [0.08162955 0.21196452 0.02328797 0.68311805]\n",
      " ...\n",
      " [0.06089326 0.77108216 0.03077832 0.13724624]\n",
      " [0.04909018 0.0076978  0.9256324  0.01757955]\n",
      " [0.04908868 0.00769768 0.92563456 0.01757911]]\n",
      "Iteration 258, Accuracy 0.39844\n",
      "77.30152%change in label assignment\n",
      "0.102001466\n",
      "[[0.01934687 0.90725255 0.00803035 0.06537025]\n",
      " [0.08170424 0.23272653 0.02423428 0.661335  ]\n",
      " [0.06931275 0.0629223  0.01599853 0.85176647]\n",
      " ...\n",
      " [0.02501904 0.8944726  0.01147278 0.06903561]\n",
      " [0.04882589 0.00767899 0.92595315 0.01754191]\n",
      " [0.04882494 0.00767894 0.92595446 0.01754162]]\n",
      "Iteration 259, Accuracy 0.39829\n",
      "91.90357%change in label assignment\n",
      "0.093345955\n",
      "[[0.05347621 0.6804081  0.01918497 0.24693067]\n",
      " [0.06852951 0.08374921 0.01675473 0.83096653]\n",
      " [0.1156319  0.05031147 0.02090671 0.8131499 ]\n",
      " ...\n",
      " [0.0209276  0.89792407 0.00848631 0.07266199]\n",
      " [0.04875794 0.00766801 0.92605144 0.01752264]\n",
      " [0.04876048 0.00766826 0.9260481  0.01752311]]\n",
      "Iteration 260, Accuracy 0.38587\n",
      "93.17523%change in label assignment\n",
      "0.08853629\n",
      "[[0.02724252 0.8598727  0.01075021 0.10213456]\n",
      " [0.07387992 0.12672928 0.01973471 0.7796561 ]\n",
      " [0.08536132 0.0501226  0.01722557 0.84729046]\n",
      " ...\n",
      " [0.01809888 0.91918975 0.00789002 0.05482137]\n",
      " [0.04877022 0.00766717 0.92603356 0.01752898]\n",
      " [0.0487691  0.00766709 0.92603517 0.01752864]]\n",
      "Iteration 261, Accuracy 0.39314\n",
      "94.58438%change in label assignment\n",
      "0.08932948\n",
      "[[0.03037943 0.841135   0.01177402 0.11671147]\n",
      " [0.07499027 0.13589127 0.02007933 0.76903915]\n",
      " [0.07623582 0.05230267 0.0159467  0.8555148 ]\n",
      " ...\n",
      " [0.01761634 0.9196023  0.00749933 0.05528207]\n",
      " [0.04880123 0.00766291 0.9260112  0.01752467]\n",
      " [0.04880538 0.00766331 0.9260059  0.01752541]]\n",
      "Iteration 262, Accuracy 0.38744\n",
      "94.53037%change in label assignment\n",
      "0.09153334\n",
      "[[0.05332728 0.68023807 0.01935495 0.24707972]\n",
      " [0.06813911 0.07976718 0.01673329 0.83536047]\n",
      " [0.10975578 0.05014532 0.02048535 0.8196136 ]\n",
      " ...\n",
      " [0.02261589 0.8877354  0.00917736 0.08047128]\n",
      " [0.04871143 0.00765451 0.92614484 0.0174893 ]\n",
      " [0.04871622 0.00765497 0.92613876 0.01749012]]\n",
      "Iteration 263, Accuracy 0.38719\n",
      "95.84131%change in label assignment\n",
      "0.09359327\n",
      "[[0.0191285  0.90950847 0.00791034 0.06345265]\n",
      " [0.08232647 0.29917902 0.02504312 0.59345144]\n",
      " [0.06944788 0.08743344 0.01704844 0.82607025]\n",
      " ...\n",
      " [0.02739248 0.8858665  0.01254803 0.07419292]\n",
      " [0.0488565  0.00766395 0.9259681  0.0175115 ]\n",
      " [0.04887183 0.00766468 0.9259493  0.01751423]]\n",
      "Iteration 264, Accuracy 0.39255\n",
      "88.35371%change in label assignment\n",
      "0.10199049\n",
      "[[0.05737182 0.64943653 0.02074772 0.27244395]\n",
      " [0.06923403 0.07642838 0.01694178 0.83739585]\n",
      " [0.12395412 0.051522   0.0226871  0.8018368 ]\n",
      " ...\n",
      " [0.02501863 0.8745512  0.0101455  0.09028476]\n",
      " [0.04865804 0.00764561 0.92622066 0.01747571]\n",
      " [0.04865712 0.00764557 0.92622185 0.01747543]]\n",
      "Iteration 265, Accuracy 0.38047\n",
      "85.49124%change in label assignment\n",
      "0.097449936\n",
      "[[0.02779042 0.8834494  0.01251729 0.07624286]\n",
      " [0.06994094 0.5414878  0.02361524 0.36495602]\n",
      " [0.08193004 0.19247502 0.02289201 0.702703  ]\n",
      " ...\n",
      " [0.05088165 0.80390334 0.02493482 0.12028022]\n",
      " [0.04894878 0.00766553 0.9258565  0.01752919]\n",
      " [0.04895902 0.0076665  0.92584324 0.01753129]]\n",
      "Iteration 266, Accuracy 0.39245\n",
      "81.71061%change in label assignment\n",
      "0.101336695\n",
      "[[0.05999205 0.62610793 0.02147808 0.2924219 ]\n",
      " [0.068933   0.07850558 0.01697486 0.83558655]\n",
      " [0.11635693 0.05110515 0.02170436 0.8108335 ]\n",
      " ...\n",
      " [0.02401775 0.8801625  0.00976988 0.08604992]\n",
      " [0.04867577 0.00764291 0.9262124  0.01746901]\n",
      " [0.04868187 0.00764351 0.92620444 0.01747016]]\n",
      "Iteration 267, Accuracy 0.39083\n",
      "84.60254%change in label assignment\n",
      "0.09710783\n",
      "[[0.02050152 0.9030801  0.00843818 0.06798016]\n",
      " [0.08294573 0.29423776 0.02503846 0.5977781 ]\n",
      " [0.07052308 0.08724271 0.0171656  0.82506865]\n",
      " ...\n",
      " [0.02969361 0.87719744 0.0136306  0.07947839]\n",
      " [0.0489311  0.00765889 0.9259073  0.01750263]\n",
      " [0.04893873 0.00765905 0.9258985  0.01750371]]\n",
      "Iteration 268, Accuracy 0.39495\n",
      "88.6483%change in label assignment\n",
      "0.10196839\n",
      "[[0.07241782 0.5045127  0.02490112 0.39816836]\n",
      " [0.07192248 0.06249541 0.01666182 0.8489203 ]\n",
      " [0.15142253 0.05376445 0.02650361 0.76830935]\n",
      " ...\n",
      " [0.03389433 0.82062817 0.01335768 0.13211988]\n",
      " [0.04867787 0.00763795 0.92623395 0.01745029]\n",
      " [0.04868668 0.00763878 0.9262223  0.01745223]]\n",
      "Iteration 269, Accuracy 0.38145\n",
      "87.83326%change in label assignment\n",
      "0.10293519\n",
      "[[0.03643902 0.8531143  0.01707383 0.09337286]\n",
      " [0.04870599 0.7214341  0.01773507 0.2121248 ]\n",
      " [0.08318033 0.31685334 0.02538547 0.57458085]\n",
      " ...\n",
      " [0.06826816 0.74740547 0.03514249 0.14918387]\n",
      " [0.04906992 0.00766577 0.92572486 0.01753943]\n",
      " [0.04907591 0.00766608 0.9257173  0.0175407 ]]\n",
      "Iteration 270, Accuracy 0.39824\n",
      "77.68449%change in label assignment\n",
      "0.10116436\n",
      "[[0.02288241 0.8860658  0.00930656 0.08174527]\n",
      " [0.08068703 0.21013813 0.02367212 0.68550265]\n",
      " [0.06936141 0.0616494  0.01600851 0.8529807 ]\n",
      " ...\n",
      " [0.02262938 0.90300477 0.01028494 0.06408088]\n",
      " [0.04875728 0.0076415  0.92612416 0.01747706]\n",
      " [0.04875778 0.00764158 0.9261236  0.01747694]]\n",
      "Iteration 271, Accuracy 0.38818\n",
      "83.8857%change in label assignment\n",
      "0.09210623\n",
      "[[0.02273967 0.88723063 0.00913018 0.08089948]\n",
      " [0.08030392 0.20698258 0.02307178 0.6896417 ]\n",
      " [0.06840374 0.06015986 0.01538053 0.8560559 ]\n",
      " ...\n",
      " [0.02156295 0.9071007  0.0096203  0.06171605]\n",
      " [0.04879063 0.0076389  0.9260921  0.0174783 ]\n",
      " [0.04879405 0.00763919 0.926088   0.01747883]]\n",
      "Iteration 272, Accuracy 0.38533\n",
      "99.2046%change in label assignment\n",
      "0.08961899\n",
      "[[0.06690943 0.5579536  0.02322641 0.3519105 ]\n",
      " [0.07113691 0.05567783 0.01578391 0.8574014 ]\n",
      " [0.1804977  0.05394995 0.02908604 0.7364663 ]\n",
      " ...\n",
      " [0.03637053 0.8020277  0.01399788 0.14760393]\n",
      " [0.0486475  0.00762219 0.92629296 0.01743742]\n",
      " [0.04864653 0.00762214 0.9262941  0.01743714]]\n",
      "Iteration 273, Accuracy 0.38376\n",
      "86.13443%change in label assignment\n",
      "0.09298162\n",
      "[[0.03376702 0.8631406  0.01586859 0.08722381]\n",
      " [0.06101063 0.6173811  0.02139239 0.30021584]\n",
      " [0.08084784 0.21183379 0.02327498 0.6840434 ]\n",
      " ...\n",
      " [0.05804263 0.7805468  0.02935154 0.132059  ]\n",
      " [0.04881679 0.00764036 0.9260444  0.0174985 ]\n",
      " [0.04881518 0.00764023 0.92604655 0.01749802]]\n",
      "Iteration 274, Accuracy 0.39417\n",
      "80.8121%change in label assignment\n",
      "0.09716171\n",
      "[[0.01872413 0.9104018  0.00775978 0.06311429]\n",
      " [0.08085803 0.22108531 0.02369762 0.67435896]\n",
      " [0.06704898 0.06986771 0.0159462  0.84713715]\n",
      " ...\n",
      " [0.02303731 0.90199995 0.01046118 0.06450156]\n",
      " [0.04860025 0.00762361 0.92632157 0.0174546 ]\n",
      " [0.04861421 0.00762492 0.926303   0.01745784]]\n",
      "Iteration 275, Accuracy 0.39859\n",
      "89.05583%change in label assignment\n",
      "0.09362993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02794705 0.85551554 0.01097645 0.10556099]\n",
      " [0.07528839 0.14204505 0.02047156 0.762195  ]\n",
      " [0.0730162  0.05274931 0.015628   0.85860646]\n",
      " ...\n",
      " [0.01724712 0.9215241  0.00740186 0.05382687]\n",
      " [0.04859474 0.0076166  0.9263361  0.01745253]\n",
      " [0.04859936 0.00761701 0.9263303  0.01745326]]\n",
      "Iteration 276, Accuracy 0.3814\n",
      "93.91663%change in label assignment\n",
      "0.08937154\n",
      "[[0.05694034 0.6498573  0.0203552  0.2728471 ]\n",
      " [0.06683549 0.06547075 0.01562695 0.8520668 ]\n",
      " [0.13217804 0.0509419  0.02313539 0.7937447 ]\n",
      " ...\n",
      " [0.02877669 0.85002166 0.01130189 0.10989968]\n",
      " [0.0485218  0.00760688 0.9264603  0.01741098]\n",
      " [0.04852104 0.00760687 0.9264613  0.01741078]]\n",
      "Iteration 277, Accuracy 0.38646\n",
      "91.19654%change in label assignment\n",
      "0.092757516\n",
      "[[0.01745727 0.9192139  0.00739148 0.0559374 ]\n",
      " [0.0815928  0.25499815 0.02437964 0.6390294 ]\n",
      " [0.06715338 0.07547067 0.01616089 0.841215  ]\n",
      " ...\n",
      " [0.02505872 0.89452326 0.01145965 0.06895836]\n",
      " [0.04860028 0.00761371 0.9263631  0.01742294]\n",
      " [0.04859879 0.00761363 0.926365   0.01742253]]\n",
      "Iteration 278, Accuracy 0.39274\n",
      "89.92979%change in label assignment\n",
      "0.092006736\n",
      "[[0.05376838 0.6759658  0.01949961 0.2507662 ]\n",
      " [0.06709364 0.0670139  0.01585439 0.8500381 ]\n",
      " [0.12556334 0.05060547 0.02246104 0.80137014]\n",
      " ...\n",
      " [0.02874186 0.85041004 0.01134057 0.10950753]\n",
      " [0.0484196  0.00759756 0.92659265 0.01739019]\n",
      " [0.04841894 0.00759756 0.9265934  0.01739003]]\n",
      "Iteration 279, Accuracy 0.3844\n",
      "89.95925%change in label assignment\n",
      "[[0.01909451 0.91546804 0.0083346  0.05710287]\n",
      " [0.08107296 0.34942156 0.02545322 0.54405224]\n",
      " [0.07126587 0.10572892 0.01826674 0.8047384 ]\n",
      " ...\n",
      " [0.03244542 0.8678228  0.01523534 0.08449639]\n",
      " [0.04857367 0.00760683 0.92640346 0.01741607]\n",
      " [0.04857223 0.00760674 0.9264055  0.01741566]]\n",
      "Iteration 280, Accuracy 0.39156\n",
      "87.78416%change in label assignment\n",
      "0.097653754\n",
      "[[0.0334093  0.8215929  0.01303265 0.13196513]\n",
      " [0.07309945 0.11878937 0.01957235 0.7885388 ]\n",
      " [0.07952408 0.05135364 0.01676351 0.8523587 ]\n",
      " ...\n",
      " [0.01858053 0.91265136 0.00788129 0.06088685]\n",
      " [0.04841609 0.00759317 0.9266031  0.01738773]\n",
      " [0.04841506 0.00759316 0.9266044  0.01738747]]\n",
      "Iteration 281, Accuracy 0.38808\n",
      "92.30618%change in label assignment\n",
      "0.09237449\n",
      "[[0.01925724 0.9086781  0.00796561 0.06409902]\n",
      " [0.07976349 0.19382647 0.02265459 0.70375544]\n",
      " [0.06881752 0.06185625 0.0155201  0.85380614]\n",
      " ...\n",
      " [0.02235885 0.9040729  0.01000517 0.06356312]\n",
      " [0.04853302 0.00759791 0.92646956 0.01739956]\n",
      " [0.04853289 0.00759794 0.9264696  0.01739961]]\n",
      "Iteration 282, Accuracy 0.38847\n",
      "94.52055%change in label assignment\n",
      "0.091413535\n",
      "[[0.02956039 0.84547377 0.01168368 0.11328213]\n",
      " [0.07202962 0.11209933 0.0190938  0.79677725]\n",
      " [0.07881885 0.05127916 0.01664263 0.8532593 ]\n",
      " ...\n",
      " [0.01824919 0.91428924 0.00774825 0.05971333]\n",
      " [0.04839918 0.00758648 0.9266437  0.01737062]\n",
      " [0.04839949 0.00758659 0.9266431  0.01737086]]\n",
      "Iteration 283, Accuracy 0.38528\n",
      "96.12118%change in label assignment\n",
      "0.09148269\n",
      "[[0.02096408 0.908744   0.00924755 0.06104441]\n",
      " [0.07895856 0.4051317  0.02537313 0.49053663]\n",
      " [0.07664812 0.15039206 0.02083034 0.7521295 ]\n",
      " ...\n",
      " [0.02852217 0.88175684 0.01314249 0.07657844]\n",
      " [0.0485113  0.00759344 0.92650455 0.01739073]\n",
      " [0.04852427 0.00759463 0.9264872  0.01739385]]\n",
      "Iteration 284, Accuracy 0.38754\n",
      "86.77272%change in label assignment\n",
      "0.09935276\n",
      "[[0.04101783 0.7728897  0.01574481 0.17034769]\n",
      " [0.07004309 0.07921939 0.01741852 0.833319  ]\n",
      " [0.10386363 0.05129581 0.0204087  0.82443184]\n",
      " ...\n",
      " [0.02404488 0.88094497 0.0098919  0.08511828]\n",
      " [0.04827068 0.00757662 0.92679924 0.01735339]\n",
      " [0.04827107 0.00757674 0.9267986  0.01735364]]\n",
      "Iteration 285, Accuracy 0.37963\n",
      "83.84642%change in label assignment\n",
      "0.09596341\n",
      "[[0.0345582  0.86021066 0.0162121  0.08901907]\n",
      " [0.07130551 0.5178528  0.02397731 0.3868644 ]\n",
      " [0.08087233 0.20170368 0.02300026 0.69442374]\n",
      " ...\n",
      " [0.04697988 0.81735206 0.02296794 0.11270013]\n",
      " [0.0484762  0.00759165 0.9265303  0.01740179]\n",
      " [0.04847596 0.00759167 0.92653054 0.01740183]]\n",
      "Iteration 286, Accuracy 0.39048\n",
      "87.25389%change in label assignment\n",
      "0.09788592\n",
      "[[0.02860011 0.8519715  0.01136976 0.1080586 ]\n",
      " [0.06889196 0.08127131 0.01716816 0.83266854]\n",
      " [0.10191449 0.05013291 0.01980428 0.8281483 ]\n",
      " ...\n",
      " [0.01849177 0.91337335 0.00786272 0.06027213]\n",
      " [0.04821008 0.00757288 0.9268647  0.01735232]\n",
      " [0.04821017 0.00757293 0.9268643  0.01735245]]\n",
      "Iteration 287, Accuracy 0.39805\n",
      "85.38322%change in label assignment\n",
      "0.09249606\n",
      "[[0.02055902 0.90150726 0.00841178 0.06952193]\n",
      " [0.07365756 0.12248714 0.01931403 0.7845413 ]\n",
      " [0.0779013  0.0522626  0.01611654 0.85371953]\n",
      " ...\n",
      " [0.01996487 0.91229    0.00873052 0.05901458]\n",
      " [0.04834142 0.00757617 0.92671716 0.01736532]\n",
      " [0.04834121 0.00757618 0.9267172  0.01736534]]\n",
      "Iteration 288, Accuracy 0.3867\n",
      "97.29955%change in label assignment\n",
      "0.089946285\n",
      "[[0.03117252 0.83497524 0.01221257 0.12163962]\n",
      " [0.06726068 0.07652345 0.01652938 0.8396865 ]\n",
      " [0.1016511  0.04927088 0.01948945 0.82958853]\n",
      " ...\n",
      " [0.01826396 0.913355   0.00769046 0.06069057]\n",
      " [0.04825487 0.00756507 0.92685    0.01732997]\n",
      " [0.04825486 0.00756512 0.92684996 0.01733007]]\n",
      "Iteration 289, Accuracy 0.38685\n",
      "97.01969%change in label assignment\n",
      "0.08795836\n",
      "[[0.01771257 0.9175673  0.0074607  0.05725943]\n",
      " [0.07417201 0.13435757 0.0199357  0.77153474]\n",
      " [0.07341971 0.05265949 0.01559883 0.858322  ]\n",
      " ...\n",
      " [0.02246855 0.9039165  0.01012557 0.06348933]\n",
      " [0.0482664  0.00756597 0.92683965 0.01732794]\n",
      " [0.04826611 0.00756597 0.92684    0.01732794]]\n",
      "Iteration 290, Accuracy 0.38675\n",
      "95.71856%change in label assignment\n",
      "0.09189306\n",
      "[[0.04688989 0.7279657  0.0174518  0.20769268]\n",
      " [0.07802889 0.05071932 0.01642877 0.854823  ]\n",
      " [0.2019106  0.05460839 0.03132123 0.71215975]\n",
      " ...\n",
      " [0.02329189 0.88317823 0.00945525 0.08407465]\n",
      " [0.0481268  0.00755292 0.92703253 0.0172878 ]\n",
      " [0.04812668 0.00755294 0.9270325  0.01728786]]\n",
      "Iteration 291, Accuracy 0.38936\n",
      "92.01159%change in label assignment\n",
      "0.093458585\n",
      "[[0.03415964 0.8616106  0.01611613 0.08811361]\n",
      " [0.07661252 0.44031528 0.02509273 0.45797947]\n",
      " [0.0768044  0.15501602 0.0210434  0.7471362 ]\n",
      " ...\n",
      " [0.05046852 0.8054368  0.02506567 0.11902905]\n",
      " [0.04835188 0.00756662 0.92674536 0.01733615]\n",
      " [0.04835156 0.00756662 0.9267457  0.01733614]]\n",
      "Iteration 292, Accuracy 0.387\n",
      "82.24579%change in label assignment\n",
      "0.0988496\n",
      "[[0.05568572 0.660851   0.02027315 0.2631901 ]\n",
      " [0.09194629 0.05007688 0.01850005 0.8394768 ]\n",
      " [0.23184396 0.05577071 0.03469348 0.6776919 ]\n",
      " ...\n",
      " [0.03133519 0.8348159  0.01237379 0.12147515]\n",
      " [0.04801283 0.00754175 0.9271783  0.01726699]\n",
      " [0.04801294 0.00754181 0.9271781  0.01726713]]\n",
      "Iteration 293, Accuracy 0.40463\n",
      "81.33255%change in label assignment\n",
      "0.09229993\n",
      "[[0.02748657 0.88542855 0.01261029 0.07447454]\n",
      " [0.08212514 0.2848977  0.02487878 0.6080984 ]\n",
      " [0.06941318 0.09068485 0.01724202 0.82266   ]\n",
      " ...\n",
      " [0.04221177 0.83350635 0.0204379  0.10384393]\n",
      " [0.04826687 0.00755925 0.9268553  0.01731858]\n",
      " [0.04826655 0.00755925 0.9268557  0.01731857]]\n",
      "Iteration 294, Accuracy 0.38857\n",
      "86.63524%change in label assignment\n",
      "0.097777724\n",
      "[[0.04729988 0.7266424  0.01771867 0.20833905]\n",
      " [0.0745123  0.05394991 0.01637969 0.8551581 ]\n",
      " [0.16789699 0.05332004 0.02804843 0.75073457]\n",
      " ...\n",
      " [0.02624531 0.86647576 0.01060278 0.09667611]\n",
      " [0.04804188 0.00753829 0.9271397  0.01728013]\n",
      " [0.04804207 0.00753837 0.9271392  0.0172803 ]]\n",
      "Iteration 295, Accuracy 0.39991\n",
      "89.24731%change in label assignment\n",
      "0.08407992\n",
      "[[0.03147279 0.870473   0.01443384 0.08362034]\n",
      " [0.08372164 0.31591105 0.02554954 0.5748177 ]\n",
      " [0.07352463 0.10149258 0.01837061 0.80661213]\n",
      " ...\n",
      " [0.04778861 0.8143507  0.02324136 0.11461934]\n",
      " [0.04816953 0.00755159 0.9269601  0.01731873]\n",
      " [0.04816926 0.00755159 0.9269604  0.01731875]]\n",
      "Iteration 296, Accuracy 0.38685\n",
      "88.69249%change in label assignment\n",
      "0.101424694\n",
      "[[0.08008236 0.38163185 0.02600824 0.5122776 ]\n",
      " [0.17809966 0.05372238 0.02919518 0.73898286]\n",
      " [0.42267647 0.05349242 0.04795611 0.47587502]\n",
      " ...\n",
      " [0.05830506 0.6416365  0.02109064 0.2789678 ]\n",
      " [0.04783604 0.00752478 0.9273753  0.01726386]\n",
      " [0.04783635 0.00752487 0.92737466 0.01726406]]\n",
      "Iteration 297, Accuracy 0.39962\n",
      "79.92341%change in label assignment\n",
      "0.09674219\n",
      "[[0.03653003 0.8534268  0.01733671 0.09270647]\n",
      " [0.07696683 0.44086838 0.0251551  0.45700967]\n",
      " [0.07527135 0.13722056 0.02017746 0.7673306 ]\n",
      " ...\n",
      " [0.05857833 0.77886736 0.02968254 0.13287175]\n",
      " [0.04819585 0.00755194 0.926919   0.01733319]\n",
      " [0.04819567 0.00755196 0.9269191  0.01733323]]\n",
      "Iteration 298, Accuracy 0.39068\n",
      "77.82197%change in label assignment\n",
      "0.096893325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01876801 0.91005737 0.00782076 0.06335393]\n",
      " [0.07181292 0.11214605 0.01899546 0.7970455 ]\n",
      " [0.08317176 0.04941466 0.01704823 0.8503654 ]\n",
      " ...\n",
      " [0.02073662 0.9104102  0.00935234 0.05950083]\n",
      " [0.04794007 0.00753313 0.9272307  0.01729607]\n",
      " [0.04794027 0.0075332  0.9272303  0.01729623]]\n",
      "Iteration 299, Accuracy 0.40251\n",
      "91.55988%change in label assignment\n",
      "0.0889566\n",
      "[[0.05260119 0.6868976  0.0190297  0.24147154]\n",
      " [0.07517563 0.05055057 0.01575595 0.8585178 ]\n",
      " [0.19345535 0.05371836 0.02983127 0.72299504]\n",
      " ...\n",
      " [0.02525047 0.8719207  0.01003873 0.09279013]\n",
      " [0.04787226 0.00752262 0.927331   0.0172742 ]\n",
      " [0.0478725  0.0075227  0.9273304  0.01727438]]\n",
      "Iteration 300, Accuracy 0.38744\n",
      "93.96573%change in label assignment\n",
      "0.08418223\n",
      "[[0.01866842 0.9104919  0.00775124 0.06308842]\n",
      " [0.07031289 0.10316562 0.01825178 0.8082697 ]\n",
      " [0.08503122 0.04861209 0.01708332 0.8492734 ]\n",
      " ...\n",
      " [0.02000204 0.9132508  0.00896552 0.05778159]\n",
      " [0.04788146 0.00752376 0.9273121  0.01728274]\n",
      " [0.04788151 0.00752381 0.92731184 0.01728285]]\n",
      "Iteration 301, Accuracy 0.39019\n",
      "94.43708%change in label assignment\n",
      "0.0883566\n",
      "[[0.04692344 0.729415   0.01732844 0.20633316]\n",
      " [0.06886937 0.05512619 0.01523542 0.86076903]\n",
      " [0.15661071 0.05174403 0.02588538 0.7657598 ]\n",
      " ...\n",
      " [0.02203756 0.89079344 0.00892367 0.07824533]\n",
      " [0.04781438 0.00751296 0.92740905 0.01726365]\n",
      " [0.04781469 0.00751304 0.92740846 0.01726384]]\n",
      "Iteration 302, Accuracy 0.3871\n",
      "95.00172%change in label assignment\n",
      "0.087838165\n",
      "[[0.02266962 0.886803   0.00915606 0.08137129]\n",
      " [0.07106996 0.11051046 0.01864221 0.7997774 ]\n",
      " [0.08269249 0.0488229  0.01671582 0.85176873]\n",
      " ...\n",
      " [0.01923031 0.9159271  0.00856013 0.05628243]\n",
      " [0.04784267 0.00751552 0.92739004 0.01725177]\n",
      " [0.04784282 0.00751557 0.9273898  0.0172519 ]]\n",
      "Iteration 303, Accuracy 0.38911\n",
      "94.0983%change in label assignment\n",
      "0.08857697\n",
      "[[0.05937824 0.62914306 0.02115271 0.29032603]\n",
      " [0.07421214 0.05088158 0.01577554 0.85913074]\n",
      " [0.20304292 0.05412498 0.0310751  0.711757  ]\n",
      " ...\n",
      " [0.02539447 0.87058836 0.01015325 0.09386388]\n",
      " [0.04770681 0.00750244 0.9275733  0.01721741]\n",
      " [0.04770707 0.00750251 0.92757285 0.01721759]]\n",
      "Iteration 304, Accuracy 0.38503\n",
      "92.27672%change in label assignment\n",
      "0.08943203\n",
      "[[0.03635178 0.802089   0.01386627 0.14769298]\n",
      " [0.06704143 0.079661   0.01639772 0.8368999 ]\n",
      " [0.11215853 0.04939358 0.02043746 0.81801045]\n",
      " ...\n",
      " [0.01720282 0.92183733 0.00740456 0.05355525]\n",
      " [0.04774742 0.00750687 0.927528   0.01721765]\n",
      " [0.04774753 0.00750691 0.9275278  0.01721776]]\n",
      "Iteration 305, Accuracy 0.39058\n",
      "95.26685%change in label assignment\n",
      "0.089660615\n",
      "[[0.07578813 0.4497998  0.02527842 0.44913363]\n",
      " [0.09234496 0.04875809 0.01819226 0.8407046 ]\n",
      " [0.27240613 0.05588813 0.03766751 0.6340382 ]\n",
      " ...\n",
      " [0.03673896 0.79927826 0.01415634 0.14982645]\n",
      " [0.04767639 0.00749707 0.927634   0.01719246]\n",
      " [0.0476767  0.00749715 0.9276336  0.01719264]]\n",
      "Iteration 306, Accuracy 0.38621\n",
      "93.87244%change in label assignment\n",
      "0.08963668\n",
      "[[0.01950374 0.9145024  0.00861197 0.05738191]\n",
      " [0.0812998  0.33867636 0.02547766 0.5545461 ]\n",
      " [0.06943262 0.09386005 0.01746167 0.81924564]\n",
      " ...\n",
      " [0.04515281 0.8236247  0.02217087 0.10905166]\n",
      " [0.04785078 0.00751055 0.9274013  0.01723742]\n",
      " [0.04785078 0.00751062 0.92740095 0.01723756]]\n",
      "Iteration 307, Accuracy 0.39456\n",
      "81.27363%change in label assignment\n",
      "0.09984034\n",
      "[[0.07571052 0.45429522 0.02542102 0.44457325]\n",
      " [0.10451665 0.04968655 0.02011555 0.8256812 ]\n",
      " [0.32840657 0.05594209 0.04248254 0.5731688 ]\n",
      " ...\n",
      " [0.0377394  0.7935485  0.01457789 0.15413415]\n",
      " [0.04749031 0.00748757 0.9278567  0.01716548]\n",
      " [0.04749055 0.00748764 0.92785627 0.01716564]]\n",
      "Iteration 308, Accuracy 0.38975\n",
      "77.55192%change in label assignment\n",
      "0.09408978\n",
      "[[0.02803366 0.8835232  0.01286241 0.07558072]\n",
      " [0.07999496 0.38949156 0.02552151 0.504992  ]\n",
      " [0.0722035  0.10799233 0.01848298 0.8013212 ]\n",
      " ...\n",
      " [0.05432237 0.7928286  0.02718451 0.12566452]\n",
      " [0.04778654 0.00750833 0.92747337 0.01723179]\n",
      " [0.04778643 0.00750836 0.9274734  0.01723185]]\n",
      "Iteration 309, Accuracy 0.39098\n",
      "80.62552%change in label assignment\n",
      "0.10205044\n",
      "[[0.06166703 0.60954684 0.02201588 0.3067703 ]\n",
      " [0.07385714 0.05293914 0.01614099 0.85706276]\n",
      " [0.20803604 0.05474764 0.03227833 0.704938  ]\n",
      " ...\n",
      " [0.02329108 0.88377696 0.0095171  0.0834149 ]\n",
      " [0.04751751 0.00748685 0.92782575 0.01716982]\n",
      " [0.04751787 0.00748693 0.9278253  0.01717002]]\n",
      "Iteration 310, Accuracy 0.39775\n",
      "86.46831%change in label assignment\n",
      "0.08623257\n",
      "[[0.04740098 0.72566104 0.01741821 0.20951976]\n",
      " [0.06653523 0.06247922 0.01528607 0.8556994 ]\n",
      " [0.16104306 0.05246689 0.02628557 0.7602045 ]\n",
      " ...\n",
      " [0.01782707 0.9162608  0.00746949 0.05844264]\n",
      " [0.04758659 0.00748698 0.9277601  0.01716626]\n",
      " [0.04758667 0.00748703 0.9277599  0.01716637]]\n",
      "Iteration 311, Accuracy 0.39156\n",
      "97.54505%change in label assignment\n",
      "0.08574793\n",
      "[[0.04407883 0.74765486 0.01649421 0.19177209]\n",
      " [0.06582119 0.06275723 0.01537951 0.856042  ]\n",
      " [0.15642028 0.0519813  0.02608171 0.7655167 ]\n",
      " ...\n",
      " [0.01717832 0.91931975 0.0072773  0.0562246 ]\n",
      " [0.04754549 0.00748247 0.9278258  0.01714626]\n",
      " [0.04754556 0.00748251 0.9278255  0.01714635]]\n",
      "Iteration 312, Accuracy 0.38656\n",
      "99.46973%change in label assignment\n",
      "0.0816129\n",
      "[[0.0393866  0.7807455  0.01491794 0.1649499 ]\n",
      " [0.06554574 0.06594405 0.01546005 0.85305023]\n",
      " [0.13741301 0.05077462 0.02369303 0.7881193 ]\n",
      " ...\n",
      " [0.01721097 0.9191559  0.00726771 0.05636537]\n",
      " [0.04753974 0.00747634 0.927842   0.01714191]\n",
      " [0.04753978 0.00747638 0.92784184 0.017142  ]]\n",
      "Iteration 313, Accuracy 0.38656\n",
      "98.92964%change in label assignment\n",
      "0.083556056\n",
      "[[0.02759238 0.8568476  0.01093238 0.10462768]\n",
      " [0.06827217 0.09225518 0.01743193 0.82204074]\n",
      " [0.10027697 0.04838972 0.01909596 0.83223736]\n",
      " ...\n",
      " [0.01774748 0.9209344  0.0078192  0.05349896]\n",
      " [0.04742779 0.00747091 0.92796075 0.01714053]\n",
      " [0.04742787 0.00747095 0.92796063 0.01714062]]\n",
      "Iteration 314, Accuracy 0.38459\n",
      "96.89694%change in label assignment\n",
      "0.08432826\n",
      "[[0.04818817 0.718511   0.01776138 0.21553947]\n",
      " [0.06655315 0.05882403 0.01517168 0.8594512 ]\n",
      " [0.16480349 0.052294   0.02688112 0.75602144]\n",
      " ...\n",
      " [0.01895639 0.9084207  0.00784412 0.06477879]\n",
      " [0.04730678 0.00746171 0.9281164  0.01711519]\n",
      " [0.04730691 0.00746175 0.9281161  0.0171153 ]]\n",
      "Iteration 315, Accuracy 0.38322\n",
      "96.51397%change in label assignment\n",
      "0.085241824\n",
      "[[0.04195572 0.7630827  0.01578197 0.17917964]\n",
      " [0.06559517 0.06537043 0.01545558 0.8535788 ]\n",
      " [0.14309968 0.05103958 0.02439396 0.7814668 ]\n",
      " ...\n",
      " [0.01711131 0.9197641  0.00724173 0.05588279]\n",
      " [0.04725125 0.00745823 0.92819554 0.01709506]\n",
      " [0.04725132 0.00745826 0.92819524 0.01709514]]\n",
      "Iteration 316, Accuracy 0.38739\n",
      "99.15059%change in label assignment\n",
      "0.08381725\n",
      "[[0.06108544 0.6114126  0.02160704 0.30589494]\n",
      " [0.07373464 0.05053678 0.01563218 0.86009634]\n",
      " [0.22552632 0.05502639 0.03320022 0.68624705]\n",
      " ...\n",
      " [0.02431228 0.8764405  0.00975668 0.08949053]\n",
      " [0.0471691  0.00745036 0.9283198  0.0170607 ]\n",
      " [0.04716916 0.00745039 0.92831963 0.01706078]]\n",
      "Iteration 317, Accuracy 0.38685\n",
      "95.62528%change in label assignment\n",
      "0.087571375\n",
      "[[0.02487213 0.8730671  0.00994268 0.09211809]\n",
      " [0.07010742 0.10779062 0.01834352 0.8037584 ]\n",
      " [0.09509356 0.04843845 0.01831853 0.8381495 ]\n",
      " ...\n",
      " [0.02012526 0.9125973  0.00903755 0.05823986]\n",
      " [0.04719196 0.00745242 0.92829216 0.01706353]\n",
      " [0.04719197 0.00745245 0.928292   0.0170636 ]]\n",
      "Iteration 318, Accuracy 0.38994\n",
      "92.53695%change in label assignment\n",
      "0.08607365\n",
      "[[0.05654508 0.6512272  0.02036335 0.27186435]\n",
      " [0.06782416 0.05543216 0.01519708 0.8615467 ]\n",
      " [0.17245384 0.05281108 0.02788505 0.7468501 ]\n",
      " ...\n",
      " [0.02009971 0.9014607  0.00827286 0.0701668 ]\n",
      " [0.04707059 0.00744085 0.92844534 0.01704327]\n",
      " [0.04707094 0.00744094 0.92844474 0.01704347]]\n",
      "Iteration 319, Accuracy 0.38415\n",
      "95.00172%change in label assignment\n",
      "[[0.04555218 0.7369904  0.01692996 0.20052747]\n",
      " [0.06541564 0.06570476 0.0154128  0.8534668 ]\n",
      " [0.13689758 0.05078862 0.02361827 0.7886955 ]\n",
      " ...\n",
      " [0.01707658 0.919914   0.00722624 0.05578316]\n",
      " [0.04706892 0.00743922 0.92845887 0.01703294]\n",
      " [0.0470691  0.00743928 0.9284585  0.01703308]]\n",
      "Iteration 320, Accuracy 0.38882\n",
      "98.44847%change in label assignment\n",
      "0.09089889\n",
      "[[0.02014671 0.90101695 0.0082878  0.07054859]\n",
      " [0.07350896 0.13595474 0.02011855 0.77041775]\n",
      " [0.0731971  0.05053396 0.01558946 0.8606795 ]\n",
      " ...\n",
      " [0.02038317 0.911493   0.00918963 0.05893425]\n",
      " [0.04706716 0.00744039 0.9284651  0.01702739]\n",
      " [0.04706767 0.00744052 0.9284641  0.01702769]]\n",
      "Iteration 321, Accuracy 0.38778\n",
      "93.6024%change in label assignment\n",
      "0.091262475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05576379 0.65683585 0.02011379 0.26728657]\n",
      " [0.07561345 0.04973124 0.01587537 0.85878   ]\n",
      " [0.21156523 0.05461972 0.03195589 0.7018592 ]\n",
      " ...\n",
      " [0.02163653 0.89222187 0.00881526 0.07732629]\n",
      " [0.04695695 0.0074302  0.9286136  0.01699929]\n",
      " [0.04695714 0.00743025 0.92861325 0.01699942]]\n",
      "Iteration 322, Accuracy 0.3841\n",
      "90.03781%change in label assignment\n",
      "0.091081515\n",
      "[[0.01691244 0.9226097  0.00728206 0.05319582]\n",
      " [0.07811632 0.18839401 0.02240103 0.71108866]\n",
      " [0.06835155 0.05445228 0.01506972 0.86212647]\n",
      " ...\n",
      " [0.0336259  0.86351883 0.01601436 0.08684088]\n",
      " [0.04705924 0.00743858 0.9284813  0.01702089]\n",
      " [0.04705922 0.0074386  0.9284812  0.01702095]]\n",
      "Iteration 323, Accuracy 0.39053\n",
      "91.08362%change in label assignment\n",
      "0.09342003\n",
      "[[0.06384226 0.5846695  0.02251953 0.32896876]\n",
      " [0.09175517 0.04848586 0.01813787 0.8416211 ]\n",
      " [0.28732356 0.05604067 0.03901783 0.617618  ]\n",
      " ...\n",
      " [0.02340681 0.8819417  0.00950344 0.08514804]\n",
      " [0.04687333 0.00742214 0.92872286 0.01698177]\n",
      " [0.04687344 0.00742218 0.9287226  0.01698186]]\n",
      "Iteration 324, Accuracy 0.38985\n",
      "88.48628%change in label assignment\n",
      "0.09115046\n",
      "[[0.01895089 0.916429   0.00835576 0.05626438]\n",
      " [0.07756881 0.17615612 0.0218718  0.7244032 ]\n",
      " [0.07020801 0.05381551 0.01521922 0.86075723]\n",
      " ...\n",
      " [0.04134206 0.8364631  0.02010105 0.10209382]\n",
      " [0.04702346 0.0074361  0.92852056 0.01701984]\n",
      " [0.0470233  0.00743611 0.92852074 0.01701985]]\n",
      "Iteration 325, Accuracy 0.3896\n",
      "89.87087%change in label assignment\n",
      "0.091418594\n",
      "[[0.0741239  0.46736205 0.02495691 0.43355715]\n",
      " [0.14030138 0.05105601 0.02438083 0.78426176]\n",
      " [0.42311698 0.05346573 0.04726289 0.47615436]\n",
      " ...\n",
      " [0.02448127 0.87546796 0.00988216 0.09016861]\n",
      " [0.04677137 0.00741648 0.9288495  0.0169626 ]\n",
      " [0.04677146 0.00741651 0.92884934 0.0169627 ]]\n",
      "Iteration 326, Accuracy 0.39485\n",
      "86.58123%change in label assignment\n",
      "0.09026949\n",
      "[[0.02481311 0.8735928  0.00989165 0.09170242]\n",
      " [0.06571137 0.07142857 0.01569228 0.8471678 ]\n",
      " [0.1374771  0.05125096 0.02356565 0.78770626]\n",
      " ...\n",
      " [0.02654821 0.88893217 0.0122606  0.07225899]\n",
      " [0.04693255 0.00742574 0.9286637  0.01697809]\n",
      " [0.04693245 0.00742575 0.92866373 0.01697811]]\n",
      "Iteration 327, Accuracy 0.38744\n",
      "92.33073%change in label assignment\n",
      "0.09034749\n",
      "[[0.07627982 0.43039683 0.02530635 0.46801698]\n",
      " [0.13965474 0.05106408 0.02428504 0.78499615]\n",
      " [0.38291955 0.05491786 0.04526548 0.5168971 ]\n",
      " ...\n",
      " [0.0311721  0.83371073 0.01224971 0.12286754]\n",
      " [0.04678392 0.00740995 0.92887264 0.01693348]\n",
      " [0.04678408 0.00741    0.9288723  0.01693359]]\n",
      "Iteration 328, Accuracy 0.38469\n",
      "91.85938%change in label assignment\n",
      "0.08823553\n",
      "[[0.01710108 0.9204659  0.00726917 0.05516383]\n",
      " [0.07453843 0.14675537 0.02045464 0.7582515 ]\n",
      " [0.07280774 0.05173357 0.01545709 0.8600016 ]\n",
      " ...\n",
      " [0.03453985 0.8600966  0.01647086 0.08889274]\n",
      " [0.04692649 0.00742102 0.9286917  0.01696084]\n",
      " [0.04692651 0.00742105 0.9286916  0.01696091]]\n",
      "Iteration 329, Accuracy 0.39112\n",
      "86.28173%change in label assignment\n",
      "0.093476415\n",
      "[[0.06295314 0.5912718  0.02231457 0.3234605 ]\n",
      " [0.072903   0.05121447 0.01578056 0.86010194]\n",
      " [0.19528559 0.05418994 0.03067907 0.71984535]\n",
      " ...\n",
      " [0.02147872 0.89314777 0.00883205 0.07654148]\n",
      " [0.04671093 0.00740467 0.9289678  0.01691666]\n",
      " [0.04671194 0.00740488 0.928966   0.01691717]]\n",
      "Iteration 330, Accuracy 0.38832\n",
      "89.27677%change in label assignment\n",
      "0.094855264\n",
      "[[0.01804569 0.91938645 0.00789369 0.0546741 ]\n",
      " [0.08108013 0.2810483  0.02471909 0.6131525 ]\n",
      " [0.0661639  0.07697605 0.01607315 0.84078693]\n",
      " ...\n",
      " [0.03869417 0.84553754 0.01868391 0.09708434]\n",
      " [0.04686394 0.00741482 0.9287612  0.01696001]\n",
      " [0.04686436 0.00741494 0.9287604  0.0169603 ]]\n",
      "Iteration 331, Accuracy 0.39127\n",
      "86.97894%change in label assignment\n",
      "0.10009336\n",
      "[[0.07941468 0.37065    0.02579797 0.5241374 ]\n",
      " [0.09786414 0.04920062 0.01921549 0.8337198 ]\n",
      " [0.29722217 0.05634636 0.04027204 0.60615945]\n",
      " ...\n",
      " [0.03922659 0.7814126  0.01509483 0.164266  ]\n",
      " [0.04660895 0.00739611 0.9290964  0.01689852]\n",
      " [0.04660991 0.0073963  0.9290948  0.01689897]]\n",
      "Iteration 332, Accuracy 0.38351\n",
      "84.49453%change in label assignment\n",
      "0.098493285\n",
      "[[0.03599777 0.8548142  0.01714045 0.09204763]\n",
      " [0.05753042 0.643167   0.02047965 0.27882293]\n",
      " [0.08034117 0.22402172 0.02347392 0.6721632 ]\n",
      " ...\n",
      " [0.06864132 0.74574465 0.03584038 0.14977367]\n",
      " [0.04697269 0.00742354 0.9286288  0.01697501]\n",
      " [0.04697335 0.00742372 0.92862755 0.01697543]]\n",
      "Iteration 333, Accuracy 0.39471\n",
      "74.80238%change in label assignment\n",
      "0.107785694\n",
      "[[0.0407064  0.77109474 0.01559088 0.17260794]\n",
      " [0.06773607 0.08647023 0.01729594 0.82849777]\n",
      " [0.11644971 0.05041879 0.0218292  0.8113023 ]\n",
      " ...\n",
      " [0.01833886 0.9176518  0.0081248  0.05588457]\n",
      " [0.04663283 0.0074035  0.9290536  0.01691006]\n",
      " [0.04663352 0.00740364 0.9290525  0.0169104 ]]\n",
      "Iteration 334, Accuracy 0.38494\n",
      "81.47003%change in label assignment\n",
      "0.091457635\n",
      "[[0.04159924 0.76821154 0.01554444 0.17464477]\n",
      " [0.06771519 0.08197754 0.01650631 0.83380103]\n",
      " [0.11524294 0.05025714 0.02076455 0.8137354 ]\n",
      " ...\n",
      " [0.01857227 0.9153651  0.00792414 0.05813852]\n",
      " [0.04673357 0.00740308 0.9289314  0.01693202]\n",
      " [0.04673416 0.00740321 0.9289302  0.01693234]]\n",
      "Iteration 335, Accuracy 0.38484\n",
      "98.88545%change in label assignment\n",
      "0.08393403\n",
      "[[0.08280952 0.22963709 0.02537027 0.66218317]\n",
      " [0.16607237 0.05506968 0.02917558 0.7496824 ]\n",
      " [0.4382872  0.05366161 0.05071405 0.45733708]\n",
      " ...\n",
      " [0.06231589 0.6129452  0.02291473 0.3018242 ]\n",
      " [0.04658556 0.0073857  0.9291278  0.01690099]\n",
      " [0.04658585 0.00738576 0.9291273  0.01690114]]\n",
      "Iteration 336, Accuracy 0.38651\n",
      "87.36682%change in label assignment\n",
      "0.10728166\n",
      "[[0.0775068  0.7182526  0.04112774 0.16311285]\n",
      " [0.02012659 0.9046715  0.00833909 0.06686287]\n",
      " [0.06150684 0.6142664  0.02156907 0.30265772]\n",
      " ...\n",
      " [0.10879863 0.62548363 0.06246711 0.20325063]\n",
      " [0.04710735 0.00743119 0.9284311  0.01703036]\n",
      " [0.0471075  0.00743128 0.9284307  0.01703056]]\n",
      "Iteration 337, Accuracy 0.39736\n",
      "63.6078%change in label assignment\n",
      "0.10507358\n",
      "[[0.01968011 0.9141289  0.00885301 0.05733798]\n",
      " [0.07991522 0.3575101  0.02568148 0.5368932 ]\n",
      " [0.06605606 0.07833171 0.01643655 0.8391757 ]\n",
      " ...\n",
      " [0.05109334 0.8030617  0.0257807  0.12006424]\n",
      " [0.04675116 0.0074091  0.9288748  0.01696495]\n",
      " [0.04675146 0.00740917 0.92887425 0.01696512]]\n",
      "Iteration 338, Accuracy 0.40615\n",
      "82.42746%change in label assignment\n",
      "0.10080601\n",
      "[[0.01699881 0.92036045 0.00718118 0.05545946]\n",
      " [0.08053338 0.23863983 0.02397091 0.65685594]\n",
      " [0.06537366 0.05920063 0.01499902 0.8604266 ]\n",
      " ...\n",
      " [0.03278694 0.86674327 0.01556438 0.08490533]\n",
      " [0.0467505  0.00740263 0.92887414 0.01697285]\n",
      " [0.04675126 0.0074028  0.92887276 0.01697323]]\n",
      "Iteration 339, Accuracy 0.38886\n",
      "95.81676%change in label assignment\n",
      "0.09731829\n",
      "[[0.05052218 0.6995644  0.0185493  0.23136418]\n",
      " [0.06491552 0.0680896  0.01557932 0.85141563]\n",
      " [0.1371135  0.05027963 0.02380289 0.78880394]\n",
      " ...\n",
      " [0.01749044 0.9168788  0.00735317 0.05827757]\n",
      " [0.04668903 0.00739081 0.9289589  0.0169613 ]\n",
      " [0.04668949 0.0073909  0.928958   0.01696154]]\n",
      "Iteration 340, Accuracy 0.37841\n",
      "90.27348%change in label assignment\n",
      "0.088708885\n",
      "[[0.04723016 0.7248088  0.01746255 0.21049847]\n",
      " [0.06488597 0.0696613  0.01558553 0.8498672 ]\n",
      " [0.13249552 0.04995312 0.02304577 0.7945056 ]\n",
      " ...\n",
      " [0.0172318  0.9186348  0.00725314 0.05688029]\n",
      " [0.04674752 0.00738928 0.9288983  0.01696494]\n",
      " [0.04674777 0.00738934 0.92889774 0.01696509]]\n",
      "Iteration 341, Accuracy 0.38582\n",
      "98.69888%change in label assignment\n",
      "0.08738225\n",
      "[[0.0293649  0.84527093 0.01157089 0.11379326]\n",
      " [0.07044471 0.11284024 0.01873574 0.7979794 ]\n",
      " [0.08573216 0.04762987 0.01712718 0.8495108 ]\n",
      " ...\n",
      " [0.01767457 0.9213976  0.00782408 0.05310372]\n",
      " [0.0468091  0.00738998 0.92882955 0.01697148]\n",
      " [0.04680933 0.00739004 0.92882895 0.01697163]]\n",
      "Iteration 342, Accuracy 0.38695\n",
      "95.77748%change in label assignment\n",
      "0.09013311\n",
      "[[0.07519419 0.45041794 0.02502948 0.4493584 ]\n",
      " [0.07734109 0.04842944 0.01599853 0.858231  ]\n",
      " [0.23955432 0.05507786 0.03449701 0.6708708 ]\n",
      " ...\n",
      " [0.03383908 0.81671023 0.01310497 0.13634568]\n",
      " [0.04674727 0.00738053 0.9289342  0.01693794]\n",
      " [0.04674763 0.00738061 0.92893356 0.01693813]]\n",
      "Iteration 343, Accuracy 0.38238\n",
      "92.42893%change in label assignment\n",
      "0.0944298\n",
      "[[0.01790668 0.91399205 0.00749375 0.06060752]\n",
      " [0.07905948 0.20950009 0.02322642 0.688214  ]\n",
      " [0.06651612 0.05493625 0.01491333 0.86363435]\n",
      " ...\n",
      " [0.02952247 0.8783079  0.01393874 0.07823089]\n",
      " [0.04689732 0.00739178 0.9287455  0.01696534]\n",
      " [0.04689734 0.00739181 0.9287455  0.0169654 ]]\n",
      "Iteration 344, Accuracy 0.3924\n",
      "89.00673%change in label assignment\n",
      "0.09967839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05657232 0.64974    0.02051816 0.27316952]\n",
      " [0.06480939 0.06362295 0.01540651 0.8561611 ]\n",
      " [0.14926547 0.05109392 0.0254029  0.7742377 ]\n",
      " ...\n",
      " [0.01893307 0.90807205 0.00790542 0.06508949]\n",
      " [0.04677801 0.00737846 0.92889684 0.01694661]\n",
      " [0.04677812 0.00737848 0.9288968  0.01694669]]\n",
      "Iteration 345, Accuracy 0.38086\n",
      "91.66789%change in label assignment\n",
      "0.08956878\n",
      "[[0.05145774 0.6924554  0.01877154 0.23731533]\n",
      " [0.06504998 0.06664    0.01536881 0.85294116]\n",
      " [0.14566684 0.05122957 0.024561   0.7785426 ]\n",
      " ...\n",
      " [0.01770033 0.9158878  0.00741953 0.05899242]\n",
      " [0.04685534 0.00738087 0.928814   0.01694987]\n",
      " [0.04685531 0.00738088 0.9288139  0.0169499 ]]\n",
      "Iteration 346, Accuracy 0.38557\n",
      "98.70379%change in label assignment\n",
      "0.08578394\n",
      "[[0.05083095 0.6965302  0.01887362 0.23376526]\n",
      " [0.06523229 0.06273887 0.01551293 0.85651594]\n",
      " [0.1464405  0.0510395  0.02524133 0.77727866]\n",
      " ...\n",
      " [0.01861924 0.91025984 0.00784972 0.06327119]\n",
      " [0.04677144 0.00737305 0.92892134 0.01693417]\n",
      " [0.04677152 0.00737307 0.9289211  0.01693423]]\n",
      "Iteration 347, Accuracy 0.38739\n",
      "99.37153%change in label assignment\n",
      "0.08988358\n",
      "[[0.0569531  0.647283   0.02041236 0.27535155]\n",
      " [0.06701242 0.05586591 0.01496814 0.8621535 ]\n",
      " [0.17602892 0.05311966 0.02801057 0.7428409 ]\n",
      " ...\n",
      " [0.02080972 0.8971804  0.0085015  0.0735084 ]\n",
      " [0.04680381 0.00737181 0.92890215 0.01692219]\n",
      " [0.04680382 0.00737182 0.92890215 0.01692223]]\n",
      "Iteration 348, Accuracy 0.38847\n",
      "98.49266%change in label assignment\n",
      "0.08780503\n",
      "[[0.0431289  0.75344837 0.01642885 0.18699391]\n",
      " [0.06547014 0.07460236 0.01629451 0.843633  ]\n",
      " [0.11965271 0.0493392  0.02194272 0.80906534]\n",
      " ...\n",
      " [0.01706293 0.9207529  0.00739921 0.054785  ]\n",
      " [0.04675275 0.00736936 0.9289628  0.01691502]\n",
      " [0.04675288 0.0073694  0.92896265 0.01691512]]\n",
      "Iteration 349, Accuracy 0.38926\n",
      "95.67438%change in label assignment\n",
      "0.0904028\n",
      "[[0.04179971 0.7636653  0.01572333 0.17881161]\n",
      " [0.0654947  0.07515768 0.01593665 0.84341097]\n",
      " [0.12144017 0.04966332 0.02162699 0.8072695 ]\n",
      " ...\n",
      " [0.01690034 0.92153937 0.00720748 0.05435277]\n",
      " [0.04675306 0.00736737 0.9289605  0.01691915]\n",
      " [0.04675309 0.00736739 0.9289603  0.0169192 ]]\n",
      "Iteration 350, Accuracy 0.3869\n",
      "99.06712%change in label assignment\n",
      "0.08403921\n",
      "[[0.04000438 0.77495676 0.01533477 0.169704  ]\n",
      " [0.06459816 0.06764227 0.01568351 0.85207605]\n",
      " [0.13643418 0.050327   0.02397206 0.7892667 ]\n",
      " ...\n",
      " [0.01697559 0.9202052  0.00728138 0.05553781]\n",
      " [0.04661935 0.00735693 0.9291366  0.01688719]\n",
      " [0.04661936 0.00735695 0.92913646 0.01688722]]\n",
      "Iteration 351, Accuracy 0.3867\n",
      "97.00005%change in label assignment\n",
      "0.08527443\n",
      "[[0.02721209 0.85830003 0.01079132 0.10369655]\n",
      " [0.06706287 0.09171627 0.01712194 0.8240989 ]\n",
      " [0.09357067 0.04782856 0.01808622 0.84051454]\n",
      " ...\n",
      " [0.01744282 0.92201257 0.00768559 0.05285903]\n",
      " [0.04661054 0.00735534 0.9291533  0.0168808 ]\n",
      " [0.04661062 0.00735537 0.9291532  0.01688087]]\n",
      "Iteration 352, Accuracy 0.38891\n",
      "95.47307%change in label assignment\n",
      "0.08572715\n",
      "[[0.05482836 0.66283983 0.02000839 0.2623234 ]\n",
      " [0.06712585 0.05340424 0.01508364 0.8643862 ]\n",
      " [0.1512791  0.05123229 0.02566635 0.7718222 ]\n",
      " ...\n",
      " [0.0217628  0.89093286 0.00892386 0.07838046]\n",
      " [0.04649699 0.00734496 0.92930305 0.01685491]\n",
      " [0.0465072  0.00734585 0.92928964 0.01685729]]\n",
      "Iteration 353, Accuracy 0.38572\n",
      "92.2178%change in label assignment\n",
      "0.09114936\n",
      "[[0.0209211  0.90978324 0.00948304 0.05981262]\n",
      " [0.08090107 0.2820227  0.02492635 0.6121499 ]\n",
      " [0.06715687 0.09132177 0.01712424 0.82439715]\n",
      " ...\n",
      " [0.04085706 0.83817464 0.02001818 0.10095017]\n",
      " [0.04660184 0.00735528 0.9291431  0.01689983]\n",
      " [0.04661935 0.00735682 0.9291199  0.01690395]]\n",
      "Iteration 354, Accuracy 0.39137\n",
      "86.06078%change in label assignment\n",
      "0.0989665\n",
      "[[0.03048592 0.8383984  0.01208356 0.11903205]\n",
      " [0.06547923 0.06709537 0.01589748 0.851528  ]\n",
      " [0.10980002 0.04866809 0.02073067 0.8208012 ]\n",
      " ...\n",
      " [0.01757737 0.91671216 0.00749463 0.05821585]\n",
      " [0.04635628 0.00733707 0.929445   0.01686154]\n",
      " [0.04635681 0.00733719 0.92944425 0.01686183]]\n",
      "Iteration 355, Accuracy 0.38012\n",
      "84.7793%change in label assignment\n",
      "0.087735906\n",
      "[[0.02212451 0.8901274  0.00896835 0.07877976]\n",
      " [0.06793522 0.09320806 0.01727463 0.8215821 ]\n",
      " [0.08528604 0.04810557 0.01693407 0.84967434]\n",
      " ...\n",
      " [0.01755918 0.9212004  0.00765852 0.05358188]\n",
      " [0.04642849 0.00733865 0.92936134 0.01687148]\n",
      " [0.04642888 0.00733875 0.9293606  0.01687174]]\n",
      "Iteration 356, Accuracy 0.38911\n",
      "97.74635%change in label assignment\n",
      "0.087471604\n",
      "[[0.06183943 0.602096   0.02210025 0.3139643 ]\n",
      " [0.07751363 0.04860736 0.01633275 0.85754627]\n",
      " [0.20305718 0.0539036  0.03160055 0.71143866]\n",
      " ...\n",
      " [0.03118188 0.83361804 0.01232135 0.12287877]\n",
      " [0.04629535 0.00732671 0.92955387 0.01682403]\n",
      " [0.04629654 0.00732695 0.929552   0.0168246 ]]\n",
      "Iteration 357, Accuracy 0.38538\n",
      "92.07542%change in label assignment\n",
      "0.09386273\n",
      "[[0.01854671 0.9105967  0.00770881 0.06314778]\n",
      " [0.06802127 0.09712028 0.01751656 0.8173419 ]\n",
      " [0.09190306 0.04780375 0.01784118 0.842452  ]\n",
      " ...\n",
      " [0.01826083 0.919239   0.00809596 0.05440426]\n",
      " [0.04635118 0.00733266 0.92948073 0.01683544]\n",
      " [0.04635112 0.00733267 0.9294808  0.01683546]]\n",
      "Iteration 358, Accuracy 0.3952\n",
      "90.89704%change in label assignment\n",
      "0.08518025\n",
      "[[0.03981461 0.7762534  0.01522176 0.1687102 ]\n",
      " [0.0690734  0.05126386 0.01522161 0.8644411 ]\n",
      " [0.16998442 0.05221435 0.02783216 0.749969  ]\n",
      " ...\n",
      " [0.02356063 0.8803061  0.00957361 0.08655963]\n",
      " [0.04624159 0.00732042 0.9296254  0.01681254]\n",
      " [0.04624173 0.00732046 0.92962515 0.01681265]]\n",
      "Iteration 359, Accuracy 0.38828\n",
      "95.77257%change in label assignment\n",
      "[[0.02444758 0.8754078  0.00981091 0.09033369]\n",
      " [0.06595016 0.08266013 0.01647356 0.8349162 ]\n",
      " [0.09442113 0.04788073 0.01818323 0.839515  ]\n",
      " ...\n",
      " [0.0168527  0.9235775  0.00733796 0.05223192]\n",
      " [0.04628799 0.00732134 0.92957914 0.01681156]\n",
      " [0.04628852 0.00732146 0.9295782  0.01681186]]\n",
      "Iteration 360, Accuracy 0.39235\n",
      "92.22762%change in label assignment\n",
      "0.08936935\n",
      "[[0.02862453 0.849775   0.0114639  0.11013658]\n",
      " [0.06726199 0.05492601 0.01538321 0.86242884]\n",
      " [0.15169129 0.05137407 0.02603766 0.7708969 ]\n",
      " ...\n",
      " [0.0170748  0.92140377 0.00746096 0.05406052]\n",
      " [0.04618443 0.00731498 0.9297079  0.01679267]\n",
      " [0.04618462 0.00731503 0.9297076  0.01679279]]\n",
      "Iteration 361, Accuracy 0.38877\n",
      "94.38798%change in label assignment\n",
      "0.08507833\n",
      "[[0.02425415 0.87710863 0.00973182 0.0889054 ]\n",
      " [0.06557173 0.06065067 0.01507052 0.8587071 ]\n",
      " [0.12865055 0.05026258 0.02252315 0.7985637 ]\n",
      " ...\n",
      " [0.01735252 0.9214458  0.00753791 0.05366379]\n",
      " [0.04624467 0.00731568 0.92964965 0.01679009]\n",
      " [0.04624492 0.00731575 0.929649   0.01679027]]\n",
      "Iteration 362, Accuracy 0.38955\n",
      "98.19316%change in label assignment\n",
      "0.08808177\n",
      "[[0.02485053 0.8727084  0.01011732 0.09232377]\n",
      " [0.06878801 0.05291455 0.0154892  0.8628082 ]\n",
      " [0.15249154 0.05148791 0.02617614 0.7698444 ]\n",
      " ...\n",
      " [0.01706287 0.9216468  0.00748073 0.05380962]\n",
      " [0.04614091 0.00730842 0.9297819  0.01676874]\n",
      " [0.04614106 0.00730847 0.9297816  0.01676886]]\n",
      "Iteration 363, Accuracy 0.38999\n",
      "97.95748%change in label assignment\n",
      "0.086010955\n",
      "[[0.01816257 0.91371727 0.00760716 0.06051302]\n",
      " [0.06517489 0.06529677 0.01530292 0.8542254 ]\n",
      " [0.10991853 0.04910115 0.02016904 0.8208113 ]\n",
      " ...\n",
      " [0.01972956 0.9136796  0.00880035 0.0577905 ]\n",
      " [0.0461824  0.00730891 0.9297356  0.01677318]\n",
      " [0.04618258 0.00730897 0.92973506 0.01677332]]\n",
      "Iteration 364, Accuracy 0.39048\n",
      "97.56469%change in label assignment\n",
      "0.090171814\n",
      "[[0.019909   0.9024528  0.00834267 0.06929553]\n",
      " [0.06819131 0.0543511  0.01555813 0.86189944]\n",
      " [0.13075906 0.05006869 0.023577   0.7955952 ]\n",
      " ...\n",
      " [0.01750639 0.9208031  0.00776407 0.05392637]\n",
      " [0.04607391 0.00730079 0.9298592  0.01676615]\n",
      " [0.04607398 0.00730082 0.929859   0.01676624]]\n",
      "Iteration 365, Accuracy 0.38729\n",
      "97.82982%change in label assignment\n",
      "0.08697881\n",
      "[[0.02473095 0.8741032  0.00990514 0.0912607 ]\n",
      " [0.07528421 0.0492488  0.01568999 0.859777  ]\n",
      " [0.19127126 0.05358069 0.02962458 0.7255235 ]\n",
      " ...\n",
      " [0.01721147 0.9221867  0.00748915 0.05311276]\n",
      " [0.0460595  0.0072974  0.9298811  0.01676201]\n",
      " [0.0460596  0.00729744 0.9298808  0.0167621 ]]\n",
      "Iteration 366, Accuracy 0.38621\n",
      "96.84784%change in label assignment\n",
      "0.085391715\n",
      "[[0.02042768 0.8986855  0.00844755 0.07243923]\n",
      " [0.06533408 0.05545424 0.01499298 0.8642187 ]\n",
      " [0.12779237 0.04942325 0.02282663 0.79995775]\n",
      " ...\n",
      " [0.01701803 0.9232916  0.00752977 0.05216061]\n",
      " [0.04598616 0.00729226 0.9299822  0.01673938]\n",
      " [0.04598638 0.00729232 0.9299818  0.01673952]]\n",
      "Iteration 367, Accuracy 0.38572\n",
      "96.07208%change in label assignment\n",
      "0.08730571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04414975 0.74560726 0.01655543 0.19368751]\n",
      " [0.09430256 0.04750843 0.01820241 0.83998656]\n",
      " [0.24442923 0.05519849 0.03493193 0.6654404 ]\n",
      " ...\n",
      " [0.02060403 0.8977958  0.00845117 0.07314905]\n",
      " [0.04595403 0.00728599 0.93003994 0.01671996]\n",
      " [0.04595442 0.00728608 0.93003935 0.01672017]]\n",
      "Iteration 368, Accuracy 0.38803\n",
      "95.92969%change in label assignment\n",
      "0.08725339\n",
      "[[0.01635838 0.92499924 0.00713672 0.05150564]\n",
      " [0.06525281 0.08314212 0.01657145 0.83503366]\n",
      " [0.08729871 0.04714351 0.01739249 0.84816533]\n",
      " ...\n",
      " [0.02580261 0.8916933  0.01208456 0.07041951]\n",
      " [0.04598248 0.00728831 0.93000543 0.01672374]\n",
      " [0.04598252 0.00728834 0.9300054  0.01672382]]\n",
      "Iteration 369, Accuracy 0.39284\n",
      "93.0083%change in label assignment\n",
      "0.08701807\n",
      "[[0.03510619 0.80744874 0.01360573 0.14383936]\n",
      " [0.07076836 0.04951803 0.01526773 0.8644458 ]\n",
      " [0.17357557 0.05240046 0.02809816 0.74592584]\n",
      " ...\n",
      " [0.01736396 0.9168813  0.00733083 0.05842389]\n",
      " [0.04583539 0.00727615 0.93019414 0.01669435]\n",
      " [0.04583615 0.00727631 0.93019277 0.01669473]]\n",
      "Iteration 370, Accuracy 0.39191\n",
      "93.51402%change in label assignment\n",
      "0.090762325\n",
      "[[0.0165439  0.9247389  0.00723685 0.05148039]\n",
      " [0.06951044 0.11185087 0.0185716  0.80006707]\n",
      " [0.07359561 0.04845518 0.01554572 0.8624035 ]\n",
      " ...\n",
      " [0.02656285 0.88900113 0.01245233 0.07198371]\n",
      " [0.04586478 0.00727906 0.93014854 0.01670761]\n",
      " [0.04586503 0.00727913 0.93014807 0.01670779]]\n",
      "Iteration 371, Accuracy 0.39176\n",
      "93.86753%change in label assignment\n",
      "0.09272854\n",
      "[[0.08050664 0.29383782 0.02520229 0.60045326]\n",
      " [0.25401363 0.05537417 0.03644129 0.6541709 ]\n",
      " [0.54882115 0.04661027 0.0509092  0.35365948]\n",
      " ...\n",
      " [0.05714165 0.6428178  0.02076101 0.2792795 ]\n",
      " [0.04567397 0.00726151 0.9304156  0.01664888]\n",
      " [0.04567445 0.00726161 0.9304148  0.01664911]]\n",
      "Iteration 372, Accuracy 0.39039\n",
      "81.80881%change in label assignment\n",
      "0.09760063\n",
      "[[0.03253838 0.8672449  0.01548394 0.08473282]\n",
      " [0.08009743 0.33805895 0.02536453 0.5564791 ]\n",
      " [0.06743448 0.09301154 0.01718558 0.8223684 ]\n",
      " ...\n",
      " [0.05927291 0.7758395  0.03049617 0.1343914 ]\n",
      " [0.04599509 0.00728864 0.9299929  0.01672334]\n",
      " [0.04599523 0.00728869 0.9299926  0.01672348]]\n",
      "Iteration 373, Accuracy 0.39746\n",
      "75.3572%change in label assignment\n",
      "0.10211232\n",
      "[[0.03816894 0.78723496 0.01479149 0.15980461]\n",
      " [0.06897009 0.05292773 0.01555018 0.862552  ]\n",
      " [0.19188796 0.05368824 0.03074378 0.72367996]\n",
      " ...\n",
      " [0.01682195 0.92261595 0.00736278 0.05319941]\n",
      " [0.04566627 0.00726895 0.9304035  0.01666135]\n",
      " [0.04566674 0.00726904 0.93040264 0.01666157]]\n",
      "Iteration 374, Accuracy 0.40212\n",
      "86.78745%change in label assignment\n",
      "0.087099835\n",
      "[[0.02510238 0.87252957 0.01002997 0.09233808]\n",
      " [0.06514728 0.07041482 0.01558303 0.8488549 ]\n",
      " [0.1251782  0.04993779 0.0220435  0.80284053]\n",
      " ...\n",
      " [0.0198337  0.913077   0.00880792 0.05828144]\n",
      " [0.04574634 0.00727117 0.93030024 0.01668231]\n",
      " [0.04574689 0.00727128 0.93029934 0.01668259]]\n",
      "Iteration 375, Accuracy 0.39156\n",
      "96.92149%change in label assignment\n",
      "0.083811305\n",
      "[[0.07918152 0.19559748 0.02362492 0.7015961 ]\n",
      " [0.25073004 0.05586644 0.03731447 0.65608907]\n",
      " [0.556107   0.04618431 0.05259088 0.34511787]\n",
      " ...\n",
      " [0.06941305 0.53085893 0.02458597 0.37514207]\n",
      " [0.04557355 0.00725107 0.9305332  0.01664223]\n",
      " [0.04557445 0.00725123 0.9305317  0.01664263]]\n",
      "Iteration 376, Accuracy 0.38877\n",
      "85.44705%change in label assignment\n",
      "0.10970417\n",
      "[[0.06560852 0.75558764 0.03423242 0.14457144]\n",
      " [0.04009159 0.7753051  0.01520535 0.16939797]\n",
      " [0.08117127 0.3038614  0.02515278 0.58981454]\n",
      " ...\n",
      " [0.09697855 0.6593603  0.05466669 0.18899453]\n",
      " [0.04602658 0.00729341 0.92991585 0.01676423]\n",
      " [0.04602669 0.00729345 0.9299155  0.01676433]]\n",
      "Iteration 377, Accuracy 0.40369\n",
      "68.12491%change in label assignment\n",
      "0.10183367\n",
      "[[0.0187785  0.91756207 0.0084714  0.05518801]\n",
      " [0.07573793 0.16138685 0.02164385 0.7412314 ]\n",
      " [0.07222625 0.04929404 0.01560132 0.8628784 ]\n",
      " ...\n",
      " [0.04213624 0.83360666 0.02088532 0.1033718 ]\n",
      " [0.04569608 0.00727291 0.9303236  0.01670742]\n",
      " [0.04569648 0.00727299 0.9303229  0.01670764]]\n",
      "Iteration 378, Accuracy 0.41116\n",
      "85.36358%change in label assignment\n",
      "0.09453864\n",
      "[[0.02118905 0.8943886  0.00865786 0.07576445]\n",
      " [0.06360777 0.06330526 0.01510632 0.8579806 ]\n",
      " [0.13768174 0.04976213 0.02376953 0.78878665]\n",
      " ...\n",
      " [0.01927925 0.9159913  0.00868342 0.05604601]\n",
      " [0.04564557 0.00726377 0.93039036 0.01670029]\n",
      " [0.04564608 0.00726387 0.9303894  0.01670053]]\n",
      "Iteration 379, Accuracy 0.39422\n",
      "95.03609%change in label assignment\n",
      "0.08582405\n",
      "[[0.0461708  0.7311609  0.01724872 0.2054196 ]\n",
      " [0.07554527 0.04758317 0.01575781 0.8611137 ]\n",
      " [0.2244173  0.05417979 0.03317301 0.68822986]\n",
      " ...\n",
      " [0.01815073 0.9122228  0.00758171 0.06204477]\n",
      " [0.04561206 0.00725625 0.9304475  0.01668409]\n",
      " [0.04561308 0.00725644 0.93044597 0.01668455]]\n",
      "Iteration 380, Accuracy 0.38985\n",
      "95.27667%change in label assignment\n",
      "0.08432399\n",
      "[[0.02300096 0.8835685  0.0093189  0.08411159]\n",
      " [0.06474442 0.07829034 0.01614551 0.8408197 ]\n",
      " [0.10386366 0.04740722 0.01944027 0.82928884]\n",
      " ...\n",
      " [0.01852935 0.91869164 0.00831078 0.0544682 ]\n",
      " [0.04566845 0.00725893 0.930377   0.0166956 ]\n",
      " [0.04567224 0.00725946 0.9303714  0.0166969 ]]\n",
      "Iteration 381, Accuracy 0.3949\n",
      "91.92812%change in label assignment\n",
      "0.08800048\n",
      "[[0.04273565 0.75573945 0.01620245 0.18532251]\n",
      " [0.06498511 0.05459462 0.01482345 0.86559683]\n",
      " [0.1544903  0.05084301 0.02590397 0.7687627 ]\n",
      " ...\n",
      " [0.01700846 0.91909933 0.00721695 0.05667528]\n",
      " [0.04563596 0.00725321 0.9304257  0.01668509]\n",
      " [0.04565445 0.00725502 0.93040067 0.0166898 ]]\n",
      "Iteration 382, Accuracy 0.39098\n",
      "95.72838%change in label assignment\n",
      "0.08893162\n",
      "[[0.0355083  0.8050861  0.01371462 0.14569099]\n",
      " [0.0636543  0.05921618 0.01481865 0.86231095]\n",
      " [0.14554527 0.05042323 0.02466806 0.77936345]\n",
      " ...\n",
      " [0.01622257 0.9257107  0.00704734 0.0510194 ]\n",
      " [0.04565458 0.00725348 0.9304179  0.01667407]\n",
      " [0.04565812 0.00725396 0.9304127  0.01667527]]\n",
      "Iteration 383, Accuracy 0.39584\n",
      "98.04095%change in label assignment\n",
      "0.0872416\n",
      "[[0.04491124 0.73938435 0.01694477 0.19875959]\n",
      " [0.06897803 0.04990259 0.01510043 0.866019  ]\n",
      " [0.20908423 0.05377694 0.03192635 0.70521253]\n",
      " ...\n",
      " [0.01702712 0.91884434 0.00723641 0.05689214]\n",
      " [0.04559864 0.00724695 0.9305008  0.01665358]\n",
      " [0.04559933 0.00724708 0.93049973 0.0166539 ]]\n",
      "Iteration 384, Accuracy 0.39417\n",
      "94.42235%change in label assignment\n",
      "0.08751011\n",
      "[[0.04835515 0.71401477 0.01792474 0.21970531]\n",
      " [0.06966338 0.04962359 0.01500696 0.86570615]\n",
      " [0.20714737 0.05384221 0.03135914 0.70765126]\n",
      " ...\n",
      " [0.01749163 0.91606617 0.00734922 0.05909292]\n",
      " [0.04560961 0.00724629 0.930495   0.01664902]\n",
      " [0.04561063 0.00724649 0.9304934  0.01664949]]\n",
      "Iteration 385, Accuracy 0.39579\n",
      "98.79707%change in label assignment\n",
      "0.08685899\n",
      "[[0.04005549 0.7735937  0.01537062 0.17098016]\n",
      " [0.06496058 0.05433923 0.01484728 0.86585283]\n",
      " [0.17960072 0.05254992 0.02889374 0.73895556]\n",
      " ...\n",
      " [0.01623977 0.92518467 0.00708758 0.05148799]\n",
      " [0.04561057 0.00724679 0.93049717 0.01664543]\n",
      " [0.04561115 0.00724691 0.9304963  0.01664572]]\n",
      "Iteration 386, Accuracy 0.39628\n",
      "98.24716%change in label assignment\n",
      "0.0847812\n",
      "[[0.0416909  0.76244015 0.01580975 0.18005912]\n",
      " [0.06881135 0.04978758 0.01494086 0.8664602 ]\n",
      " [0.220807   0.05425332 0.03280545 0.6921342 ]\n",
      " ...\n",
      " [0.01652738 0.9220249  0.0070434  0.05440435]\n",
      " [0.04557983 0.00724138 0.9305374  0.01664138]\n",
      " [0.04557993 0.00724141 0.93053716 0.01664147]]\n",
      "Iteration 387, Accuracy 0.39427\n",
      "95.9837%change in label assignment\n",
      "0.08570138\n",
      "[[0.04468964 0.7402833  0.01686882 0.19815819]\n",
      " [0.07448508 0.04774787 0.01571492 0.8620521 ]\n",
      " [0.25440806 0.05513733 0.03619941 0.65425515]\n",
      " ...\n",
      " [0.01746211 0.9158981  0.00737782 0.05926192]\n",
      " [0.04554088 0.00723656 0.93060267 0.01661989]\n",
      " [0.04554099 0.0072366  0.9306024  0.01661998]]\n",
      "Iteration 388, Accuracy 0.39392\n",
      "97.54014%change in label assignment\n",
      "0.08456794\n",
      "[[0.03836479 0.7851956  0.01471144 0.16172823]\n",
      " [0.06666288 0.05139939 0.01474997 0.86718774]\n",
      " [0.22388905 0.05443321 0.03313264 0.68854517]\n",
      " ...\n",
      " [0.01613984 0.92519987 0.00697565 0.05168464]\n",
      " [0.04556176 0.00723634 0.9305785  0.01662339]\n",
      " [0.04556172 0.00723634 0.93057853 0.01662341]]\n",
      "Iteration 389, Accuracy 0.39093\n",
      "98.33063%change in label assignment\n",
      "0.08475461\n",
      "[[0.02589367 0.86551076 0.01043218 0.0981634 ]\n",
      " [0.06334307 0.0691425  0.01553782 0.8519766 ]\n",
      " [0.14577518 0.05045064 0.02498638 0.7787878 ]\n",
      " ...\n",
      " [0.01799479 0.9203785  0.00809405 0.05353269]\n",
      " [0.04553378 0.00723282 0.93061006 0.01662326]\n",
      " [0.04553391 0.00723285 0.9306099  0.01662336]]\n",
      "Iteration 390, Accuracy 0.38739\n",
      "95.89532%change in label assignment\n",
      "0.080524854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03516481 0.8066444  0.01365242 0.1445384 ]\n",
      " [0.06368101 0.05634399 0.0147116  0.86526346]\n",
      " [0.18421805 0.05269949 0.02923135 0.7338511 ]\n",
      " ...\n",
      " [0.01610164 0.92603576 0.00701678 0.05084582]\n",
      " [0.04541379 0.00722285 0.93076813 0.01659521]\n",
      " [0.04541399 0.0072229  0.9307678  0.01659533]]\n",
      "Iteration 391, Accuracy 0.38675\n",
      "98.19316%change in label assignment\n",
      "0.08287053\n",
      "[[0.02980687 0.84114707 0.01179917 0.11724691]\n",
      " [0.06298705 0.06149204 0.01496821 0.86055267]\n",
      " [0.16900909 0.05191368 0.0276166  0.7514607 ]\n",
      " ...\n",
      " [0.01650636 0.925345   0.00728593 0.05086272]\n",
      " [0.04535192 0.00721865 0.9308496  0.01657986]\n",
      " [0.04535212 0.0072187  0.9308492  0.01657999]]\n",
      "Iteration 392, Accuracy 0.3894\n",
      "99.15059%change in label assignment\n",
      "0.080871314\n",
      "[[0.03991576 0.7744139  0.01527889 0.17039144]\n",
      " [0.06344517 0.05704749 0.01474639 0.8647609 ]\n",
      " [0.18171605 0.05253776 0.02900439 0.7367417 ]\n",
      " ...\n",
      " [0.01609254 0.9250122  0.00694742 0.05194785]\n",
      " [0.04530802 0.00721182 0.9309095  0.01657075]\n",
      " [0.04530833 0.00721189 0.93090886 0.01657091]]\n",
      "Iteration 393, Accuracy 0.38886\n",
      "97.34374%change in label assignment\n",
      "0.08349998\n",
      "[[0.02327893 0.8814729  0.00947524 0.0857729 ]\n",
      " [0.0677063  0.10065028 0.01787922 0.8137642 ]\n",
      " [0.11134821 0.04777719 0.02059577 0.8202788 ]\n",
      " ...\n",
      " [0.02111658 0.9091938  0.00969727 0.05999236]\n",
      " [0.04525275 0.00720917 0.9309588  0.01657927]\n",
      " [0.04525296 0.00720921 0.93095845 0.01657938]]\n",
      "Iteration 394, Accuracy 0.38999\n",
      "96.4305%change in label assignment\n",
      "0.08495344\n",
      "[[0.05834254 0.63219357 0.0210939  0.28836992]\n",
      " [0.06846938 0.04965389 0.01500832 0.86686844]\n",
      " [0.25342035 0.05483633 0.03608945 0.65565383]\n",
      " ...\n",
      " [0.01901403 0.9067654  0.00792188 0.06629868]\n",
      " [0.04510688 0.00719884 0.93114346 0.01655074]\n",
      " [0.04510719 0.0071989  0.93114305 0.01655089]]\n",
      "Iteration 395, Accuracy 0.38577\n",
      "94.78568%change in label assignment\n",
      "0.084471904\n",
      "[[0.0275158  0.8558827  0.01095963 0.10564185]\n",
      " [0.06484173 0.08144223 0.01636166 0.83735436]\n",
      " [0.14274405 0.05011466 0.02441367 0.78272766]\n",
      " ...\n",
      " [0.0189153  0.91726714 0.00853432 0.05528327]\n",
      " [0.04511284 0.00720072 0.9311355  0.01655096]\n",
      " [0.04511302 0.00720076 0.9311352  0.01655106]]\n",
      "Iteration 396, Accuracy 0.39264\n",
      "95.86095%change in label assignment\n",
      "0.08302751\n",
      "[[0.07532677 0.43965757 0.02531737 0.45969826]\n",
      " [0.09457986 0.04685525 0.01844754 0.8401174 ]\n",
      " [0.39037806 0.05403956 0.04578437 0.50979805]\n",
      " ...\n",
      " [0.03048518 0.8369865  0.01206712 0.12046114]\n",
      " [0.04497126 0.00718819 0.9313369  0.01650367]\n",
      " [0.04497148 0.00718823 0.9313364  0.01650378]]\n",
      "Iteration 397, Accuracy 0.38715\n",
      "92.47312%change in label assignment\n",
      "0.089301005\n",
      "[[0.01801739 0.9129121  0.00753741 0.06153313]\n",
      " [0.07330812 0.14295861 0.0203281  0.76340514]\n",
      " [0.09144575 0.04712839 0.0177817  0.8436442 ]\n",
      " ...\n",
      " [0.03145773 0.87129796 0.01499943 0.08224487]\n",
      " [0.04508282 0.00719866 0.9311886  0.01652986]\n",
      " [0.04508303 0.0071987  0.9311882  0.01652998]]\n",
      "Iteration 398, Accuracy 0.39608\n",
      "87.8529%change in label assignment\n",
      "0.09068569\n",
      "[[0.07374611 0.4660674  0.0252006  0.43498588]\n",
      " [0.1012324  0.04758843 0.01960327 0.8315759 ]\n",
      " [0.3978565  0.05387131 0.04675379 0.5015183 ]\n",
      " ...\n",
      " [0.03248527 0.824662   0.01285004 0.13000263]\n",
      " [0.04485012 0.00718103 0.9314807  0.01648823]\n",
      " [0.0448504  0.00718108 0.9314801  0.01648836]]\n",
      "Iteration 399, Accuracy 0.38705\n",
      "86.71871%change in label assignment\n",
      "[[0.01674804 0.92253894 0.00719168 0.05352139]\n",
      " [0.0770455  0.1787858  0.02203343 0.72213525]\n",
      " [0.08392092 0.04758029 0.01675637 0.85174245]\n",
      " ...\n",
      " [0.03725218 0.8507152  0.01803581 0.09399687]\n",
      " [0.0450198  0.00719391 0.93125755 0.01652884]\n",
      " [0.04501987 0.00719393 0.9312573  0.0165289 ]]\n",
      "Iteration 400, Accuracy 0.395\n",
      "88.85943%change in label assignment\n",
      "0.090567045\n",
      "[[0.03980004 0.7757955  0.01536271 0.16904177]\n",
      " [0.06363004 0.06359534 0.01541232 0.8573623 ]\n",
      " [0.19243938 0.05307385 0.03060199 0.72388476]\n",
      " ...\n",
      " [0.01652426 0.9244824  0.00727881 0.05171452]\n",
      " [0.04487149 0.00718185 0.93144745 0.01649927]\n",
      " [0.04487173 0.00718189 0.93144697 0.0164994 ]]\n",
      "Iteration 401, Accuracy 0.39304\n",
      "93.77915%change in label assignment\n",
      "0.08417475\n",
      "[[0.01928241 0.90594697 0.00797962 0.06679101]\n",
      " [0.0740692  0.14786465 0.02056217 0.75750405]\n",
      " [0.08623244 0.04729526 0.01703573 0.8494365 ]\n",
      " ...\n",
      " [0.02218543 0.90507525 0.0101315  0.06260781]\n",
      " [0.04496054 0.00718551 0.9313462  0.01650772]\n",
      " [0.0449608  0.00718556 0.9313458  0.01650785]]\n",
      "Iteration 402, Accuracy 0.39063\n",
      "92.99357%change in label assignment\n",
      "0.08924403\n",
      "[[0.03946167 0.77795655 0.01526333 0.16731845]\n",
      " [0.06382419 0.06110904 0.01531396 0.8597528 ]\n",
      " [0.17098112 0.05204757 0.02828767 0.7486837 ]\n",
      " ...\n",
      " [0.01789546 0.9138208  0.00761144 0.06067231]\n",
      " [0.04482543 0.00717566 0.9315202  0.01647862]\n",
      " [0.0448259  0.00717575 0.9315195  0.01647884]]\n",
      "Iteration 403, Accuracy 0.3871\n",
      "95.40924%change in label assignment\n",
      "0.088572994\n",
      "[[0.01696892 0.92299706 0.00738705 0.05264699]\n",
      " [0.07978745 0.23337016 0.02380943 0.663033  ]\n",
      " [0.06661062 0.05369771 0.01477753 0.8649141 ]\n",
      " ...\n",
      " [0.02858839 0.8816445  0.01342921 0.07633795]\n",
      " [0.04493943 0.00718453 0.93137234 0.01650365]\n",
      " [0.04493987 0.00718461 0.9313717  0.01650387]]\n",
      "Iteration 404, Accuracy 0.39323\n",
      "90.9265%change in label assignment\n",
      "0.09809548\n",
      "[[0.02778923 0.85492647 0.0112605  0.10602382]\n",
      " [0.06523295 0.06998569 0.01626796 0.8485134 ]\n",
      " [0.15014747 0.0511918  0.02625134 0.7724094 ]\n",
      " ...\n",
      " [0.01706922 0.92089176 0.00748043 0.05455863]\n",
      " [0.04477173 0.00717143 0.9315743  0.01648257]\n",
      " [0.044772   0.00717148 0.93157387 0.01648271]]\n",
      "Iteration 405, Accuracy 0.38371\n",
      "89.33078%change in label assignment\n",
      "0.08809521\n",
      "[[0.01767761 0.91672444 0.00745776 0.05814015]\n",
      " [0.07285064 0.13283639 0.0197786  0.7745344 ]\n",
      " [0.08840221 0.04755007 0.01729243 0.8467552 ]\n",
      " ...\n",
      " [0.02283871 0.9026163  0.01041456 0.06413042]\n",
      " [0.04486552 0.00717564 0.9314574  0.01650142]\n",
      " [0.04486571 0.00717568 0.93145704 0.01650154]]\n",
      "Iteration 406, Accuracy 0.3899\n",
      "96.65636%change in label assignment\n",
      "0.08793227\n",
      "[[0.02341072 0.88080144 0.00958775 0.08620009]\n",
      " [0.06783246 0.09970942 0.01805843 0.81439966]\n",
      " [0.10362051 0.04739702 0.01985497 0.8291275 ]\n",
      " ...\n",
      " [0.01650827 0.9247316  0.00728873 0.0514714 ]\n",
      " [0.04474879 0.00716744 0.93160945 0.01647438]\n",
      " [0.04474916 0.0071675  0.9316088  0.01647455]]\n",
      "Iteration 407, Accuracy 0.38675\n",
      "96.12609%change in label assignment\n",
      "0.09145212\n",
      "[[0.02414028 0.87701756 0.00971673 0.08912545]\n",
      " [0.06648732 0.09141468 0.01701814 0.82507986]\n",
      " [0.11016065 0.04799059 0.02017031 0.82167846]\n",
      " ...\n",
      " [0.01648722 0.92424154 0.00712421 0.05214705]\n",
      " [0.04483749 0.00716848 0.93151283 0.01648124]\n",
      " [0.04483785 0.00716855 0.93151224 0.01648142]]\n",
      "Iteration 408, Accuracy 0.38911\n",
      "99.37153%change in label assignment\n",
      "0.08896038\n",
      "[[0.02634196 0.86268735 0.01065263 0.10031803]\n",
      " [0.0655551  0.0868711  0.01704211 0.8305317 ]\n",
      " [0.12241898 0.04863848 0.02230331 0.80663925]\n",
      " ...\n",
      " [0.01625742 0.92529625 0.00714805 0.05129834]\n",
      " [0.04480394 0.00716427 0.93156934 0.01646244]\n",
      " [0.04480411 0.00716431 0.931569   0.01646254]]\n",
      "Iteration 409, Accuracy 0.38926\n",
      "97.42721%change in label assignment\n",
      "0.086431414\n",
      "[[0.0248933  0.8719187  0.01000059 0.09318738]\n",
      " [0.06640769 0.09329028 0.01713669 0.82316536]\n",
      " [0.12267967 0.04877542 0.02182927 0.80671567]\n",
      " ...\n",
      " [0.01680785 0.92426807 0.00739699 0.05152712]\n",
      " [0.04479277 0.00716227 0.9315874  0.01645756]\n",
      " [0.04479294 0.0071623  0.9315871  0.01645766]]\n",
      "Iteration 410, Accuracy 0.38911\n",
      "99.34698%change in label assignment\n",
      "0.08854292\n",
      "[[0.02296638 0.88312644 0.00943062 0.08447659]\n",
      " [0.07038862 0.12040927 0.01937943 0.78982264]\n",
      " [0.10297734 0.04725817 0.0197266  0.83003783]\n",
      " ...\n",
      " [0.01768239 0.921281   0.00796393 0.05307272]\n",
      " [0.04473102 0.00715835 0.9316657  0.01644502]\n",
      " [0.04473116 0.00715838 0.93166536 0.0164451 ]]\n",
      "Iteration 411, Accuracy 0.38886\n",
      "97.51559%change in label assignment\n",
      "0.090163335\n",
      "[[0.06621847 0.555367   0.02311623 0.3552983 ]\n",
      " [0.06852318 0.04961772 0.01485101 0.86700803]\n",
      " [0.2671196  0.05532505 0.03684417 0.6407111 ]\n",
      " ...\n",
      " [0.02882652 0.8475355  0.01140736 0.1122307 ]\n",
      " [0.04469937 0.00715193 0.93172663 0.01642203]\n",
      " [0.04469967 0.00715198 0.9317261  0.01642217]]\n",
      "Iteration 412, Accuracy 0.38661\n",
      "92.92974%change in label assignment\n",
      "0.091413274\n",
      "[[0.02674989 0.8882241  0.01260109 0.07242493]\n",
      " [0.06921141 0.5192119  0.0240061  0.38757056]\n",
      " [0.06721289 0.10161047 0.01773415 0.81344247]\n",
      " ...\n",
      " [0.05079538 0.8037506  0.0258023  0.11965174]\n",
      " [0.04485114 0.00716871 0.93151593 0.01646425]\n",
      " [0.04485136 0.00716876 0.9315155  0.01646439]]\n",
      "Iteration 413, Accuracy 0.39485\n",
      "82.13286%change in label assignment\n",
      "0.1012208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02609088 0.8642861  0.0105757  0.09904733]\n",
      " [0.06671374 0.09331966 0.01760038 0.82236624]\n",
      " [0.1271854  0.04931837 0.02310306 0.80039316]\n",
      " ...\n",
      " [0.01679357 0.92390645 0.00746627 0.05183372]\n",
      " [0.04459628 0.00715212 0.9318322  0.01641943]\n",
      " [0.04459655 0.00715217 0.9318317  0.01641956]]\n",
      "Iteration 414, Accuracy 0.38636\n",
      "84.97079%change in label assignment\n",
      "0.08469251\n",
      "[[0.01788099 0.91515255 0.00752046 0.05944601]\n",
      " [0.07627191 0.17101242 0.02164707 0.73106855]\n",
      " [0.07797673 0.04768056 0.01593544 0.8584073 ]\n",
      " ...\n",
      " [0.02248565 0.90379924 0.01024237 0.06347278]\n",
      " [0.04467751 0.00715455 0.9317222  0.01644577]\n",
      " [0.04467772 0.0071546  0.9317218  0.01644589]]\n",
      "Iteration 415, Accuracy 0.38931\n",
      "96.03771%change in label assignment\n",
      "0.08451262\n",
      "[[0.08070922 0.330409   0.026297   0.56258476]\n",
      " [0.1191731  0.04941308 0.02251644 0.80889744]\n",
      " [0.43355477 0.0526751  0.0494245  0.46434566]\n",
      " ...\n",
      " [0.05918429 0.6284872  0.0218173  0.29051116]\n",
      " [0.04451725 0.00713441 0.93193597 0.01641239]\n",
      " [0.04451752 0.00713446 0.9319355  0.01641251]]\n",
      "Iteration 416, Accuracy 0.38572\n",
      "84.90696%change in label assignment\n",
      "0.105658524\n",
      "[[0.06543182 0.7561959  0.03419168 0.1441806 ]\n",
      " [0.02678344 0.8614167  0.01067    0.10112979]\n",
      " [0.08067083 0.33218476 0.02544579 0.5616987 ]\n",
      " ...\n",
      " [0.09038986 0.6789308  0.05023437 0.18044496]\n",
      " [0.04493202 0.00717465 0.931367   0.01652639]\n",
      " [0.044932   0.00717466 0.9313669  0.01652642]]\n",
      "Iteration 417, Accuracy 0.39918\n",
      "70.96283%change in label assignment\n",
      "0.101080164\n",
      "[[0.02080197 0.91039556 0.0095734  0.05922906]\n",
      " [0.08042007 0.31621516 0.0257146  0.57765025]\n",
      " [0.06360289 0.05698127 0.01493417 0.8644817 ]\n",
      " ...\n",
      " [0.04000565 0.841068   0.01979954 0.09912676]\n",
      " [0.04463987 0.00715565 0.9317283  0.01647611]\n",
      " [0.04464008 0.00715569 0.93172795 0.01647622]]\n",
      "Iteration 418, Accuracy 0.40791\n",
      "88.02475%change in label assignment\n",
      "0.093015894\n",
      "[[0.0178514  0.91363066 0.00747802 0.06103985]\n",
      " [0.07165888 0.12975924 0.01971996 0.7788619 ]\n",
      " [0.09460659 0.04613322 0.01824685 0.8410134 ]\n",
      " ...\n",
      " [0.01873128 0.91819    0.00846273 0.05461604]\n",
      " [0.04459443 0.00714686 0.9317874  0.01647141]\n",
      " [0.04459468 0.00714691 0.9317869  0.01647154]]\n",
      "Iteration 419, Accuracy 0.39206\n",
      "95.12447%change in label assignment\n",
      "0.08547216\n",
      "[[0.03565554 0.80397195 0.01383613 0.14653637]\n",
      " [0.06271726 0.05826324 0.01474942 0.8642701 ]\n",
      " [0.18666428 0.05215065 0.02947998 0.73170507]\n",
      " ...\n",
      " [0.01843766 0.91010225 0.0077059  0.06375425]\n",
      " [0.04454577 0.00713827 0.93185997 0.01645593]\n",
      " [0.04454603 0.00713832 0.9318596  0.01645607]]\n",
      "Iteration 420, Accuracy 0.38724\n",
      "94.31433%change in label assignment\n",
      "0.08255216\n",
      "[[0.0208031  0.8963183  0.00854147 0.0743371 ]\n",
      " [0.06836624 0.10605063 0.01814406 0.807439  ]\n",
      " [0.10548817 0.0468289  0.01962044 0.8280625 ]\n",
      " ...\n",
      " [0.01664112 0.9254457  0.00736729 0.05054589]\n",
      " [0.04458111 0.00713944 0.9318184  0.01646099]\n",
      " [0.04458138 0.00713949 0.931818   0.01646113]]\n",
      "Iteration 421, Accuracy 0.39269\n",
      "94.89861%change in label assignment\n",
      "0.08434974\n",
      "[[0.0371853  0.79356563 0.01441676 0.15483232]\n",
      " [0.06265424 0.05866513 0.01481992 0.8638607 ]\n",
      " [0.18447137 0.0520478  0.0293517  0.73412913]\n",
      " ...\n",
      " [0.01867102 0.90868413 0.00782127 0.06482354]\n",
      " [0.04452673 0.00713188 0.93189776 0.01644366]\n",
      " [0.04452696 0.00713193 0.93189734 0.01644378]]\n",
      "Iteration 422, Accuracy 0.38896\n",
      "95.56636%change in label assignment\n",
      "0.08426284\n",
      "[[0.02804279 0.852582   0.01114885 0.10822638]\n",
      " [0.06258939 0.0647257  0.01503015 0.8576547 ]\n",
      " [0.17552613 0.05178869 0.02805714 0.7446281 ]\n",
      " ...\n",
      " [0.01601043 0.9252389  0.00688186 0.05186881]\n",
      " [0.04456262 0.00713278 0.9318689  0.01643562]\n",
      " [0.04456272 0.0071328  0.93186873 0.01643569]]\n",
      "Iteration 423, Accuracy 0.39299\n",
      "97.32901%change in label assignment\n",
      "0.081548914\n",
      "[[0.03005384 0.8395966  0.01194379 0.11840574]\n",
      " [0.06248554 0.06693161 0.01531895 0.8552639 ]\n",
      " [0.16532959 0.05108517 0.02725347 0.7563318 ]\n",
      " ...\n",
      " [0.01609038 0.92430776 0.00693562 0.05266628]\n",
      " [0.04450457 0.00712588 0.93195266 0.01641693]\n",
      " [0.0445047  0.0071259  0.93195236 0.01641701]]\n",
      "Iteration 424, Accuracy 0.3898\n",
      "98.8118%change in label assignment\n",
      "0.0856073\n",
      "[[0.05364073 0.6713765  0.01968135 0.25530136]\n",
      " [0.07558969 0.04637819 0.01569748 0.86233467]\n",
      " [0.3074503  0.05502098 0.0401249  0.5974038 ]\n",
      " ...\n",
      " [0.02662783 0.8609534  0.01067559 0.10174318]\n",
      " [0.04445932 0.00712069 0.9320215  0.01639855]\n",
      " [0.04445942 0.00712071 0.9320212  0.01639862]]\n",
      "Iteration 425, Accuracy 0.39068\n",
      "94.24068%change in label assignment\n",
      "0.08562235\n",
      "[[0.01615366 0.9267823  0.00712726 0.04993677]\n",
      " [0.07414788 0.15514676 0.02103937 0.74966604]\n",
      " [0.08128715 0.04605703 0.01644494 0.8562108 ]\n",
      " ...\n",
      " [0.02739472 0.886139   0.01297529 0.07349095]\n",
      " [0.04455268 0.00712907 0.9318991  0.01641918]\n",
      " [0.04455265 0.00712908 0.931899   0.01641923]]\n",
      "Iteration 426, Accuracy 0.39299\n",
      "92.08524%change in label assignment\n",
      "0.087506294\n",
      "[[0.05178153 0.6860407  0.01925382 0.24292397]\n",
      " [0.07822221 0.04618968 0.01625902 0.85932904]\n",
      " [0.29738775 0.05500123 0.03994706 0.6076639 ]\n",
      " ...\n",
      " [0.02733243 0.8565656  0.01100906 0.10509291]\n",
      " [0.04437759 0.00711164 0.93213314 0.01637758]\n",
      " [0.04437778 0.00711168 0.93213284 0.01637769]]\n",
      "Iteration 427, Accuracy 0.39122\n",
      "92.08524%change in label assignment\n",
      "0.08525191\n",
      "[[0.01612005 0.9249984  0.0069455  0.05193606]\n",
      " [0.06938449 0.11400171 0.01861198 0.7980019 ]\n",
      " [0.09778655 0.04675031 0.01859949 0.83686364]\n",
      " ...\n",
      " [0.02205513 0.90576255 0.01015768 0.06202463]\n",
      " [0.04447724 0.00711946 0.9320057  0.01639768]\n",
      " [0.04447728 0.00711948 0.93200547 0.01639775]]\n",
      "Iteration 428, Accuracy 0.39323\n",
      "93.51893%change in label assignment\n",
      "0.09002663\n",
      "[[0.05759026 0.6375735  0.02110791 0.28372836]\n",
      " [0.08615269 0.04627258 0.01750251 0.85007215]\n",
      " [0.33643532 0.05487386 0.04312405 0.5655668 ]\n",
      " ...\n",
      " [0.03071567 0.83549505 0.01227183 0.12151745]\n",
      " [0.04431666 0.00710444 0.9322196  0.01635933]\n",
      " [0.0443168  0.00710447 0.9322194  0.01635943]]\n",
      "Iteration 429, Accuracy 0.38808\n",
      "91.43713%change in label assignment\n",
      "0.08865581\n",
      "[[0.01904923 0.9166057  0.00857059 0.05577449]\n",
      " [0.07927363 0.22324619 0.02362352 0.6738567 ]\n",
      " [0.06796349 0.05055526 0.01481322 0.866668  ]\n",
      " ...\n",
      " [0.03614067 0.85469145 0.01752629 0.09164157]\n",
      " [0.0444826  0.00711866 0.9319956  0.01640309]\n",
      " [0.04448273 0.00711869 0.9319953  0.01640318]]\n",
      "Iteration 430, Accuracy 0.39142\n",
      "86.88074%change in label assignment\n",
      "0.083436795\n",
      "[[0.02947342 0.84314674 0.01178671 0.11559319]\n",
      " [0.06250893 0.06161444 0.01512767 0.8607489 ]\n",
      " [0.18333788 0.05217333 0.02956454 0.73492426]\n",
      " ...\n",
      " [0.01592311 0.92596585 0.00695028 0.0511608 ]\n",
      " [0.04425164 0.00710073 0.9322933  0.01635438]\n",
      " [0.04425186 0.00710077 0.9322929  0.0163545 ]]\n",
      "Iteration 431, Accuracy 0.39456\n",
      "91.97722%change in label assignment\n",
      "0.080984004\n",
      "[[0.02088078 0.8955722  0.00858314 0.07496388]\n",
      " [0.06388377 0.07959753 0.01613459 0.8403841 ]\n",
      " [0.13126747 0.0488923  0.02297554 0.7968647 ]\n",
      " ...\n",
      " [0.01682292 0.92469954 0.00747795 0.0509996 ]\n",
      " [0.04426663 0.00710022 0.93228376 0.01634936]\n",
      " [0.04426702 0.0071003  0.9322832  0.01634955]]\n",
      "Iteration 432, Accuracy 0.38823\n",
      "95.98861%change in label assignment\n",
      "0.08025918\n",
      "[[0.03169421 0.82890314 0.01256868 0.12683398]\n",
      " [0.06223697 0.06276809 0.01513498 0.85986   ]\n",
      " [0.162218   0.05091264 0.02711533 0.75975406]\n",
      " ...\n",
      " [0.01623107 0.92323995 0.00699977 0.05352923]\n",
      " [0.04418364 0.00709065 0.9323939  0.01633181]\n",
      " [0.04418408 0.00709073 0.93239313 0.01633201]]\n",
      "Iteration 433, Accuracy 0.38994\n",
      "98.32081%change in label assignment\n",
      "0.08168653\n",
      "[[0.01708257 0.91807276 0.00722386 0.05762081]\n",
      " [0.07174573 0.132506   0.01985329 0.775895  ]\n",
      " [0.08987455 0.04588589 0.01758848 0.8466511 ]\n",
      " ...\n",
      " [0.02142954 0.90824133 0.00987204 0.06045712]\n",
      " [0.04416345 0.00709119 0.93239754 0.01634777]\n",
      " [0.04416373 0.00709124 0.9323971  0.01634791]]\n",
      "Iteration 434, Accuracy 0.39102\n",
      "96.19973%change in label assignment\n",
      "0.082476944\n",
      "[[0.03857254 0.7837383  0.01492186 0.16276726]\n",
      " [0.06230675 0.06425904 0.01519754 0.8582366 ]\n",
      " [0.17025201 0.05124114 0.02792158 0.75058526]\n",
      " ...\n",
      " [0.01628376 0.92286825 0.00698993 0.05385803]\n",
      " [0.0440128  0.00708073 0.93258864 0.01631779]\n",
      " [0.04401319 0.0070808  0.9325881  0.01631796]]\n",
      "Iteration 435, Accuracy 0.38661\n",
      "95.27176%change in label assignment\n",
      "0.08283832\n",
      "[[0.03376946 0.8160109  0.01319232 0.13702738]\n",
      " [0.06435972 0.08252253 0.01640895 0.8367088 ]\n",
      " [0.15132864 0.05010465 0.02543452 0.77313215]\n",
      " ...\n",
      " [0.01594499 0.9274169  0.00700098 0.04963711]\n",
      " [0.04400524 0.00707986 0.9326022  0.01631265]\n",
      " [0.04400544 0.0070799  0.9326018  0.01631274]]\n",
      "Iteration 436, Accuracy 0.39127\n",
      "98.59577%change in label assignment\n",
      "0.0807264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06303256 0.5866863  0.02253572 0.32774544]\n",
      " [0.06465614 0.05096232 0.01462664 0.8697549 ]\n",
      " [0.26417366 0.05468521 0.03715103 0.64399016]\n",
      " ...\n",
      " [0.02110816 0.8939213  0.00871601 0.0762545 ]\n",
      " [0.04391048 0.00707185 0.9327413  0.0162764 ]\n",
      " [0.04391071 0.00707189 0.9327409  0.0162765 ]]\n",
      "Iteration 437, Accuracy 0.39073\n",
      "95.59582%change in label assignment\n",
      "0.08811176\n",
      "[[0.02458727 0.87329996 0.00993381 0.09217893]\n",
      " [0.06955246 0.11757888 0.01885919 0.79400945]\n",
      " [0.10509031 0.04691695 0.01961744 0.82837534]\n",
      " ...\n",
      " [0.02042269 0.91182196 0.00934877 0.0584066 ]\n",
      " [0.04397679 0.00707745 0.9326587  0.01628711]\n",
      " [0.04397745 0.00707756 0.9326576  0.01628739]]\n",
      "Iteration 438, Accuracy 0.39377\n",
      "91.26037%change in label assignment\n",
      "0.08550537\n",
      "[[0.06120791 0.60470295 0.0221737  0.31191543]\n",
      " [0.06491502 0.05246695 0.01494648 0.8676716 ]\n",
      " [0.24855928 0.05455673 0.03627215 0.66061187]\n",
      " ...\n",
      " [0.02078313 0.8960831  0.0086568  0.07447696]\n",
      " [0.04381382 0.00706599 0.93285984 0.01626032]\n",
      " [0.04381414 0.00706604 0.93285936 0.01626046]]\n",
      "Iteration 439, Accuracy 0.38729\n",
      "93.11141%change in label assignment\n",
      "[[0.02676223 0.8607359  0.01068559 0.1018163 ]\n",
      " [0.06910942 0.11169244 0.0184442  0.80075395]\n",
      " [0.12294108 0.04852285 0.02183612 0.80669993]\n",
      " ...\n",
      " [0.01942886 0.9153055  0.00877893 0.05648671]\n",
      " [0.0438954  0.00707121 0.9327592  0.01627411]\n",
      " [0.04389553 0.00707123 0.932759   0.01627417]]\n",
      "Iteration 440, Accuracy 0.39382\n",
      "94.78077%change in label assignment\n",
      "0.085659266\n",
      "[[0.03121392 0.8319855  0.01245102 0.12434956]\n",
      " [0.06713139 0.09973383 0.01801862 0.81511617]\n",
      " [0.13623871 0.04927337 0.02413101 0.79035693]\n",
      " ...\n",
      " [0.01787508 0.9207726  0.00810906 0.05324322]\n",
      " [0.04382428 0.00706597 0.93285495 0.0162548 ]\n",
      " [0.04382438 0.00706599 0.9328548  0.01625484]]\n",
      "Iteration 441, Accuracy 0.3895\n",
      "98.66451%change in label assignment\n",
      "0.08310397\n",
      "[[0.02023351 0.89956784 0.00833972 0.07185894]\n",
      " [0.07512608 0.16532357 0.02142544 0.7381249 ]\n",
      " [0.0891571  0.04630584 0.01742671 0.84711033]\n",
      " ...\n",
      " [0.02269951 0.90331763 0.01047362 0.06350924]\n",
      " [0.04387177 0.00706743 0.93279946 0.01626131]\n",
      " [0.04387188 0.00706745 0.9327993  0.01626135]]\n",
      "Iteration 442, Accuracy 0.39127\n",
      "96.47469%change in label assignment\n",
      "0.08679603\n",
      "[[0.03619477 0.79920644 0.01419221 0.15040661]\n",
      " [0.0689977  0.11335662 0.01891168 0.798734  ]\n",
      " [0.12879756 0.04869523 0.02317207 0.7993352 ]\n",
      " ...\n",
      " [0.01668703 0.9248454  0.00747662 0.0509909 ]\n",
      " [0.04377162 0.00705958 0.93293077 0.01623797]\n",
      " [0.04377159 0.00705957 0.9329309  0.01623796]]\n",
      "Iteration 443, Accuracy 0.38837\n",
      "97.07861%change in label assignment\n",
      "0.08678801\n",
      "[[0.02599696 0.8649392  0.01041992 0.09864386]\n",
      " [0.07172877 0.13389753 0.0197852  0.77458847]\n",
      " [0.11825171 0.04805765 0.02126196 0.8124287 ]\n",
      " ...\n",
      " [0.02187008 0.9063541  0.01004923 0.06172656]\n",
      " [0.04380468 0.00706169 0.93289006 0.01624363]\n",
      " [0.04380469 0.00706169 0.93289    0.01624364]]\n",
      "Iteration 444, Accuracy 0.39039\n",
      "97.19645%change in label assignment\n",
      "0.08841671\n",
      "[[0.01905904 0.9060737  0.00801622 0.06685109]\n",
      " [0.07722684 0.19268177 0.02301735 0.70707405]\n",
      " [0.084073   0.04582522 0.01712401 0.8529778 ]\n",
      " ...\n",
      " [0.02519122 0.8940383  0.0119128  0.06885765]\n",
      " [0.04376571 0.00705884 0.9329255  0.0162499 ]\n",
      " [0.04376575 0.00705884 0.9329255  0.01624991]]\n",
      "Iteration 445, Accuracy 0.39068\n",
      "97.53523%change in label assignment\n",
      "0.08559669\n",
      "[[0.02642361 0.8621539  0.01059453 0.10082794]\n",
      " [0.07387774 0.15360455 0.02092745 0.75159025]\n",
      " [0.10611059 0.0467945  0.01973974 0.8273552 ]\n",
      " ...\n",
      " [0.02124968 0.90881157 0.00976921 0.06016957]\n",
      " [0.0437364  0.0070542  0.93296474 0.01624465]\n",
      " [0.04373644 0.0070542  0.9329646  0.01624466]]\n",
      "Iteration 446, Accuracy 0.38513\n",
      "98.17843%change in label assignment\n",
      "0.08630336\n",
      "[[0.04010572 0.7722895  0.0154766  0.17212814]\n",
      " [0.06540874 0.09159707 0.01721125 0.8257829 ]\n",
      " [0.187733   0.05228432 0.02990052 0.7300822 ]\n",
      " ...\n",
      " [0.01846006 0.91896105 0.00840541 0.05417356]\n",
      " [0.04368532 0.00705014 0.9330393  0.01622523]\n",
      " [0.04368532 0.00705015 0.9330393  0.01622524]]\n",
      "Iteration 447, Accuracy 0.38808\n",
      "94.98699%change in label assignment\n",
      "0.08491725\n",
      "[[0.02904924 0.84565896 0.01154056 0.11375127]\n",
      " [0.0715074  0.13334535 0.01980454 0.7753427 ]\n",
      " [0.1359897  0.04916132 0.0235637  0.7912853 ]\n",
      " ...\n",
      " [0.02330044 0.9011687  0.01083163 0.06469931]\n",
      " [0.04372111 0.00705016 0.9329994  0.01622936]\n",
      " [0.04372111 0.00705016 0.9329994  0.01622936]]\n",
      "Iteration 448, Accuracy 0.38842\n",
      "98.0655%change in label assignment\n",
      "0.08358591\n",
      "[[0.03792807 0.7870279  0.01475013 0.16029388]\n",
      " [0.06892026 0.11541808 0.01890466 0.796757  ]\n",
      " [0.14992103 0.05004521 0.02562653 0.77440727]\n",
      " ...\n",
      " [0.0190173  0.9168972  0.00870386 0.05538171]\n",
      " [0.0436678  0.00704303 0.9330809  0.01620824]\n",
      " [0.04366781 0.00704303 0.9330809  0.01620824]]\n",
      "Iteration 449, Accuracy 0.3867\n",
      "98.88545%change in label assignment\n",
      "0.08196781\n",
      "[[0.03628547 0.7984279  0.01408117 0.1512055 ]\n",
      " [0.06696963 0.10214229 0.0177888  0.8130993 ]\n",
      " [0.17045249 0.05146923 0.02767077 0.7504075 ]\n",
      " ...\n",
      " [0.01988995 0.9137317  0.00909332 0.05728498]\n",
      " [0.04363469 0.00704019 0.93312716 0.0161979 ]\n",
      " [0.04363471 0.00704019 0.9331272  0.0161979 ]]\n",
      "Iteration 450, Accuracy 0.38607\n",
      "98.20298%change in label assignment\n",
      "0.085689686\n",
      "[[0.02866927 0.84758943 0.01149954 0.11224177]\n",
      " [0.06952415 0.12007319 0.01918335 0.7912193 ]\n",
      " [0.14682108 0.0498166  0.02522207 0.7781403 ]\n",
      " ...\n",
      " [0.02251535 0.90394676 0.01050912 0.06302877]\n",
      " [0.04360219 0.0070373  0.9331677  0.01619283]\n",
      " [0.04360219 0.0070373  0.93316764 0.01619283]]\n",
      "Iteration 451, Accuracy 0.38764\n",
      "98.96401%change in label assignment\n",
      "0.08555366\n",
      "[[0.06409167 0.57410884 0.02278777 0.33901173]\n",
      " [0.06600384 0.04901032 0.01460506 0.8703808 ]\n",
      " [0.33951133 0.05484452 0.04259225 0.56305194]\n",
      " ...\n",
      " [0.01694964 0.9183739  0.00719533 0.05748113]\n",
      " [0.04355224 0.00702966 0.93324983 0.01616823]\n",
      " [0.04355227 0.00702966 0.9332499  0.01616824]]\n",
      "Iteration 452, Accuracy 0.38592\n",
      "94.39289%change in label assignment\n",
      "0.08506477\n",
      "[[0.01620196 0.9265611  0.00718178 0.05005517]\n",
      " [0.07955313 0.31474546 0.02536026 0.58034116]\n",
      " [0.0682626  0.04788814 0.01478655 0.8690627 ]\n",
      " ...\n",
      " [0.0461964  0.8193727  0.02326318 0.11116777]\n",
      " [0.04366891 0.00704011 0.93309546 0.01619553]\n",
      " [0.04366891 0.00704011 0.93309546 0.01619553]]\n",
      "Iteration 453, Accuracy 0.3899\n",
      "88.23587%change in label assignment\n",
      "0.09200356\n",
      "[[0.05383152 0.6675019  0.02001017 0.25865638]\n",
      " [0.06192358 0.06385759 0.01524215 0.8589766 ]\n",
      " [0.24146688 0.05444954 0.03566307 0.66842055]\n",
      " ...\n",
      " [0.01577869 0.92664057 0.00692116 0.05065956]\n",
      " [0.04346612 0.00702533 0.93335366 0.01615487]\n",
      " [0.04346611 0.00702533 0.9333537  0.01615487]]\n",
      "Iteration 454, Accuracy 0.39358\n",
      "91.68262%change in label assignment\n",
      "0.084941335\n",
      "[[0.01741901 0.9223155  0.00775351 0.05251195]\n",
      " [0.07974893 0.3283447  0.02541905 0.5664874 ]\n",
      " [0.06731489 0.04950345 0.01468046 0.86850125]\n",
      " ...\n",
      " [0.04681718 0.81747115 0.02347177 0.11223988]\n",
      " [0.04359129 0.00703509 0.9331778  0.01619579]\n",
      " [0.04359129 0.00703509 0.9331778  0.01619579]]\n",
      "Iteration 455, Accuracy 0.38975\n",
      "91.08853%change in label assignment\n",
      "0.0933593\n",
      "[[0.0795295  0.34353998 0.02609376 0.5508368 ]\n",
      " [0.089568   0.04620202 0.01814034 0.8460896 ]\n",
      " [0.4570781  0.0512047  0.04979577 0.44192135]\n",
      " ...\n",
      " [0.03371264 0.81609637 0.01340327 0.13678773]\n",
      " [0.04341829 0.00701405 0.93340296 0.01616462]\n",
      " [0.04341828 0.00701405 0.93340296 0.01616462]]\n",
      "Iteration 456, Accuracy 0.39373\n",
      "85.25065%change in label assignment\n",
      "0.09508812\n",
      "[[0.04479649 0.8245132  0.02225184 0.1084385 ]\n",
      " [0.03867877 0.78462154 0.01483457 0.1618651 ]\n",
      " [0.07524998 0.15526035 0.02106515 0.7484246 ]\n",
      " ...\n",
      " [0.08785554 0.6864485  0.04867712 0.17701879]\n",
      " [0.04372481 0.0070448  0.93297976 0.01625062]\n",
      " [0.0437248  0.00704479 0.9329798  0.01625061]]\n",
      "Iteration 457, Accuracy 0.39088\n",
      "76.98237%change in label assignment\n",
      "0.097587846\n",
      "[[0.01833009 0.9103443  0.00775372 0.06357188]\n",
      " [0.07444923 0.15697601 0.02155645 0.74701834]\n",
      " [0.13660453 0.04899959 0.02423908 0.79015684]\n",
      " ...\n",
      " [0.03442617 0.8607183  0.01686075 0.08799474]\n",
      " [0.04343344 0.00702465 0.9333432  0.01619877]\n",
      " [0.04343344 0.00702465 0.9333431  0.01619877]]\n",
      "Iteration 458, Accuracy 0.40649\n",
      "83.61074%change in label assignment\n",
      "0.08537489\n",
      "[[0.05317913 0.67499435 0.01962936 0.25219718]\n",
      " [0.06294189 0.05201477 0.01440459 0.8706387 ]\n",
      " [0.3676051  0.05391095 0.04433231 0.5341517 ]\n",
      " ...\n",
      " [0.01590224 0.9279324  0.00702747 0.04913792]\n",
      " [0.04337401 0.00701519 0.93342733 0.01618357]\n",
      " [0.04337401 0.00701519 0.9334273  0.01618357]]\n",
      "Iteration 459, Accuracy 0.38872\n",
      "91.96249%change in label assignment\n",
      "0.0768639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05170181 0.6865989  0.01917552 0.24252377]\n",
      " [0.06159743 0.05686766 0.01451289 0.867022  ]\n",
      " [0.31904247 0.05461648 0.04114141 0.58519965]\n",
      " ...\n",
      " [0.01616018 0.9272106  0.00717984 0.04944938]\n",
      " [0.04336905 0.00701181 0.9334467  0.01617238]\n",
      " [0.04336905 0.0070118  0.93344676 0.01617238]]\n",
      "Iteration 460, Accuracy 0.38057\n",
      "97.23572%change in label assignment\n",
      "0.077858366\n",
      "[[0.05451993 0.6632884  0.02010636 0.26208532]\n",
      " [0.06245469 0.05286382 0.014422   0.8702595 ]\n",
      " [0.34694678 0.05430577 0.04314853 0.5555989 ]\n",
      " ...\n",
      " [0.01561491 0.92866474 0.00688772 0.04883266]\n",
      " [0.04333173 0.0070073  0.93349785 0.01616319]\n",
      " [0.04333171 0.0070073  0.93349785 0.01616318]]\n",
      "Iteration 461, Accuracy 0.38263\n",
      "99.26842%change in label assignment\n",
      "0.07839405\n",
      "[[0.03177204 0.8284098  0.01255341 0.12726475]\n",
      " [0.06811819 0.10908061 0.01836617 0.804435  ]\n",
      " [0.18205819 0.05168298 0.02896021 0.73729867]\n",
      " ...\n",
      " [0.02356633 0.9003707  0.01102579 0.06503722]\n",
      " [0.04336761 0.00700773 0.9334551  0.01616953]\n",
      " [0.0433675  0.00700771 0.9334553  0.01616948]]\n",
      "Iteration 462, Accuracy 0.38189\n",
      "93.97555%change in label assignment\n",
      "0.08328555\n",
      "[[0.07612678 0.42209738 0.02573586 0.47603992]\n",
      " [0.08087525 0.04529665 0.01665765 0.8571705 ]\n",
      " [0.4745322  0.05018825 0.04959453 0.42568502]\n",
      " ...\n",
      " [0.02172843 0.89008737 0.00899816 0.07918605]\n",
      " [0.04325375 0.00699573 0.9336185  0.01613205]\n",
      " [0.04325366 0.00699572 0.9336186  0.01613202]]\n",
      "Iteration 463, Accuracy 0.38503\n",
      "91.84465%change in label assignment\n",
      "0.085713714\n",
      "[[0.01999958 0.9007219  0.00828566 0.07099281]\n",
      " [0.07547522 0.16960026 0.02172534 0.7331992 ]\n",
      " [0.11713209 0.04756834 0.02115418 0.8141454 ]\n",
      " ...\n",
      " [0.03446992 0.8606893  0.01673539 0.08810538]\n",
      " [0.04334817 0.00700441 0.9334892  0.01615818]\n",
      " [0.0433481  0.0070044  0.9334894  0.01615815]]\n",
      "Iteration 464, Accuracy 0.38602\n",
      "88.61884%change in label assignment\n",
      "0.09209515\n",
      "[[0.06530767 0.56415474 0.02345452 0.347083  ]\n",
      " [0.06371717 0.05278862 0.01488435 0.8686099 ]\n",
      " [0.31969732 0.05471071 0.04225719 0.5833348 ]\n",
      " ...\n",
      " [0.01619961 0.92342144 0.00703905 0.05333988]\n",
      " [0.04320593 0.00699118 0.9336747  0.01612813]\n",
      " [0.04320584 0.00699117 0.93367493 0.01612809]]\n",
      "Iteration 465, Accuracy 0.38852\n",
      "93.71532%change in label assignment\n",
      "0.0834879\n",
      "[[0.03646912 0.7989937  0.01411368 0.15042357]\n",
      " [0.06860974 0.10843223 0.01828357 0.80467445]\n",
      " [0.18007766 0.05192171 0.02851517 0.73948544]\n",
      " ...\n",
      " [0.02483906 0.89551836 0.01158084 0.06806173]\n",
      " [0.04328494 0.00699676 0.9335724  0.01614577]\n",
      " [0.04328474 0.00699673 0.9335729  0.01614568]]\n",
      "Iteration 466, Accuracy 0.38592\n",
      "96.16537%change in label assignment\n",
      "0.08303016\n",
      "[[0.07574686 0.43109632 0.02586168 0.46729508]\n",
      " [0.07382983 0.04646695 0.01594103 0.86376226]\n",
      " [0.4127713  0.05278356 0.04785165 0.48659343]\n",
      " ...\n",
      " [0.01997081 0.9007294  0.00840293 0.07089686]\n",
      " [0.04313669 0.00698313 0.93377095 0.01610925]\n",
      " [0.04313657 0.00698311 0.93377113 0.0161092 ]]\n",
      "Iteration 467, Accuracy 0.38449\n",
      "94.11303%change in label assignment\n",
      "0.08579533\n",
      "[[0.02207734 0.8889054  0.00904    0.07997726]\n",
      " [0.07737263 0.19110468 0.02263244 0.70889026]\n",
      " [0.1072462  0.0470253  0.01984441 0.82588404]\n",
      " ...\n",
      " [0.03735443 0.85047257 0.018252   0.09392098]\n",
      " [0.04328294 0.00699479 0.93357724 0.01614502]\n",
      " [0.04328272 0.00699476 0.9335776  0.01614492]]\n",
      "Iteration 468, Accuracy 0.38867\n",
      "88.48628%change in label assignment\n",
      "0.09274009\n",
      "[[0.07701881 0.41030398 0.02615411 0.48652312]\n",
      " [0.07151888 0.04792211 0.01584453 0.86471444]\n",
      " [0.38931298 0.0536222  0.04714236 0.5099225 ]\n",
      " ...\n",
      " [0.01979294 0.9020312  0.00837    0.06980588]\n",
      " [0.04308428 0.00697905 0.9338353  0.01610142]\n",
      " [0.04308407 0.00697902 0.9338357  0.01610133]]\n",
      "Iteration 469, Accuracy 0.38857\n",
      "90.76938%change in label assignment\n",
      "0.089610875\n",
      "[[0.01848064 0.9107138  0.0077443  0.06306128]\n",
      " [0.08030158 0.25487912 0.02452724 0.64029205]\n",
      " [0.08656908 0.04622933 0.01708042 0.8501212 ]\n",
      " ...\n",
      " [0.04626193 0.81963694 0.0231361  0.11096504]\n",
      " [0.04326954 0.00699419 0.9335853  0.01615094]\n",
      " [0.04326926 0.00699414 0.9335858  0.01615081]]\n",
      "Iteration 470, Accuracy 0.38999\n",
      "88.52065%change in label assignment\n",
      "0.08479583\n",
      "[[0.05071163 0.6943684  0.01910819 0.23581174]\n",
      " [0.06294931 0.07350619 0.0160793  0.8474652 ]\n",
      " [0.2319282  0.05372165 0.03490332 0.6794468 ]\n",
      " ...\n",
      " [0.01776285 0.921619   0.00810761 0.05251059]\n",
      " [0.04306585 0.00697778 0.9338477  0.01610853]\n",
      " [0.04306569 0.00697775 0.9338481  0.01610846]]\n",
      "Iteration 471, Accuracy 0.39073\n",
      "91.97231%change in label assignment\n",
      "0.082546026\n",
      "[[0.04501056 0.7380141  0.01704195 0.1999334 ]\n",
      " [0.06392171 0.08339816 0.01645949 0.8362207 ]\n",
      " [0.20411994 0.05260915 0.03130187 0.711969  ]\n",
      " ...\n",
      " [0.01927207 0.91626436 0.00880304 0.05566054]\n",
      " [0.04308797 0.00697715 0.93382585 0.01610897]\n",
      " [0.04308777 0.00697712 0.9338262  0.01610888]]\n",
      "Iteration 472, Accuracy 0.38425\n",
      "98.88054%change in label assignment\n",
      "0.07869735\n",
      "[[0.0632403  0.58411556 0.02278482 0.32985935]\n",
      " [0.06174688 0.0548185  0.01460417 0.8688305 ]\n",
      " [0.27569145 0.05440969 0.03844838 0.6314505 ]\n",
      " ...\n",
      " [0.01557821 0.9269909  0.00678659 0.05064432]\n",
      " [0.04302258 0.00696884 0.93391883 0.01608974]\n",
      " [0.04302237 0.0069688  0.93391925 0.01608966]]\n",
      "Iteration 473, Accuracy 0.3842\n",
      "96.97059%change in label assignment\n",
      "0.08189604\n",
      "[[0.02485282 0.87167764 0.01008084 0.09338868]\n",
      " [0.07203329 0.13744637 0.0202     0.7703203 ]\n",
      " [0.11804812 0.04706514 0.02136713 0.8135196 ]\n",
      " ...\n",
      " [0.02777001 0.8850396  0.01322843 0.07396191]\n",
      " [0.04303761 0.00697218 0.9338777  0.01611244]\n",
      " [0.04303743 0.00697215 0.933878   0.01611237]]\n",
      "Iteration 474, Accuracy 0.38774\n",
      "94.72185%change in label assignment\n",
      "0.08342765\n",
      "[[0.06739424 0.5421918  0.0239056  0.3665084 ]\n",
      " [0.06272618 0.05242829 0.01460847 0.870237  ]\n",
      " [0.29066372 0.05442854 0.03970573 0.615202  ]\n",
      " ...\n",
      " [0.01616774 0.9231207  0.0069682  0.0537434 ]\n",
      " [0.042881   0.00696055 0.9340796  0.0160788 ]\n",
      " [0.04288078 0.00696051 0.93408    0.0160787 ]]\n",
      "Iteration 475, Accuracy 0.38503\n",
      "94.10321%change in label assignment\n",
      "0.083136745\n",
      "[[0.04019683 0.7727599  0.01543549 0.17160778]\n",
      " [0.06572743 0.09341136 0.01722872 0.82363254]\n",
      " [0.16415054 0.05048659 0.02689923 0.7584637 ]\n",
      " ...\n",
      " [0.02103792 0.9098049  0.0097027  0.05945452]\n",
      " [0.04292314 0.00696424 0.9340262  0.01608645]\n",
      " [0.04292291 0.0069642  0.93402654 0.01608635]]\n",
      "Iteration 476, Accuracy 0.38911\n",
      "95.14902%change in label assignment\n",
      "0.082925506\n",
      "[[0.07821655 0.37852845 0.02601724 0.5172378 ]\n",
      " [0.08666243 0.04529568 0.01757735 0.8504646 ]\n",
      " [0.43606436 0.05181113 0.04858553 0.46353897]\n",
      " ...\n",
      " [0.02605986 0.86402035 0.01061418 0.09930565]\n",
      " [0.04277184 0.00695207 0.9342367  0.01603936]\n",
      " [0.04277171 0.00695204 0.93423694 0.0160393 ]]\n",
      "Iteration 477, Accuracy 0.38479\n",
      "89.9347%change in label assignment\n",
      "0.088033706\n",
      "[[0.0221312  0.88831896 0.00906194 0.08048792]\n",
      " [0.07327446 0.1469304  0.02061907 0.7591761 ]\n",
      " [0.11312004 0.04712744 0.02061795 0.81913453]\n",
      " ...\n",
      " [0.03384384 0.8629727  0.01638603 0.08679736]\n",
      " [0.04291025 0.00696412 0.9340524  0.01607325]\n",
      " [0.04291008 0.00696409 0.93405265 0.01607318]]\n",
      "Iteration 478, Accuracy 0.39343\n",
      "88.8938%change in label assignment\n",
      "0.08754273\n",
      "[[0.07403956 0.4583433  0.02555949 0.44205764]\n",
      " [0.07358721 0.04633636 0.01591466 0.8641618 ]\n",
      " [0.36836863 0.05394279 0.04557674 0.53211176]\n",
      " ...\n",
      " [0.01972561 0.90211713 0.00830764 0.06984966]\n",
      " [0.04271388 0.00694879 0.9343001  0.01603721]\n",
      " [0.04271371 0.00694876 0.9343004  0.01603714]]\n",
      "Iteration 479, Accuracy 0.38577\n",
      "91.94285%change in label assignment\n",
      "[[0.03933338 0.77974087 0.01508919 0.16583656]\n",
      " [0.067881   0.10397649 0.0179731  0.8101694 ]\n",
      " [0.15186806 0.05011418 0.02534467 0.77267313]\n",
      " ...\n",
      " [0.02604518 0.8911284  0.01220968 0.07061673]\n",
      " [0.0428336  0.00695676 0.9341496  0.01606004]\n",
      " [0.04283304 0.00695666 0.9341506  0.0160598 ]]\n",
      "Iteration 480, Accuracy 0.39269\n",
      "90.46497%change in label assignment\n",
      "0.08477518\n",
      "[[0.04789864 0.71591985 0.01825712 0.21792443]\n",
      " [0.06341174 0.07760878 0.01642458 0.8425549 ]\n",
      " [0.18445437 0.05184766 0.03001622 0.7336818 ]\n",
      " ...\n",
      " [0.01976034 0.9141599  0.00915873 0.05692109]\n",
      " [0.04273902 0.00695075 0.93427354 0.0160367 ]\n",
      " [0.04273822 0.00695062 0.93427473 0.01603638]]\n",
      "Iteration 481, Accuracy 0.38302\n",
      "97.69235%change in label assignment\n",
      "0.08203723\n",
      "[[0.03161251 0.82989883 0.01245883 0.1260299 ]\n",
      " [0.06502572 0.08924721 0.01684661 0.8288804 ]\n",
      " [0.16388236 0.05068235 0.02679544 0.7586398 ]\n",
      " ...\n",
      " [0.02747912 0.88592875 0.01300814 0.07358395]\n",
      " [0.04278616 0.00695254 0.934217   0.01604442]\n",
      " [0.04278583 0.00695248 0.93421745 0.01604428]]\n",
      "Iteration 482, Accuracy 0.38474\n",
      "96.56307%change in label assignment\n",
      "0.082647026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03330633 0.81799257 0.01321663 0.13548447]\n",
      " [0.06282669 0.0771119  0.01618637 0.84387505]\n",
      " [0.15953821 0.05023526 0.02692736 0.7632991 ]\n",
      " ...\n",
      " [0.01981857 0.9141214  0.00917596 0.05688408]\n",
      " [0.04270585 0.00694615 0.9343225  0.01602552]\n",
      " [0.04270542 0.00694608 0.93432313 0.01602534]]\n",
      "Iteration 483, Accuracy 0.38567\n",
      "97.41739%change in label assignment\n",
      "0.083682805\n",
      "[[0.03380596 0.81523275 0.01324592 0.13771532]\n",
      " [0.06232161 0.07395904 0.01563876 0.8480806 ]\n",
      " [0.1754304  0.05127298 0.02817054 0.7451261 ]\n",
      " ...\n",
      " [0.02118092 0.9091323  0.00977668 0.0599101 ]\n",
      " [0.0427149  0.00694589 0.93431413 0.01602505]\n",
      " [0.0427146  0.00694584 0.93431467 0.01602492]]\n",
      "Iteration 484, Accuracy 0.38592\n",
      "98.54176%change in label assignment\n",
      "0.0856521\n",
      "[[0.01615617 0.9229581  0.00696072 0.05392507]\n",
      " [0.07695118 0.19390894 0.02303194 0.7061079 ]\n",
      " [0.07219825 0.04538489 0.01539945 0.86701745]\n",
      " ...\n",
      " [0.03125694 0.87215686 0.01518087 0.0814053 ]\n",
      " [0.0427179  0.00694651 0.9342963  0.01603921]\n",
      " [0.04271731 0.00694641 0.93429726 0.01603895]]\n",
      "Iteration 485, Accuracy 0.38621\n",
      "91.33402%change in label assignment\n",
      "0.085989535\n",
      "[[0.05276888 0.67709726 0.01960168 0.25053215]\n",
      " [0.06078975 0.05797511 0.01456137 0.8666738 ]\n",
      " [0.22226872 0.05317553 0.03335366 0.6912021 ]\n",
      " ...\n",
      " [0.01544856 0.92921746 0.00681433 0.04851959]\n",
      " [0.04261706 0.00693715 0.93443257 0.01601331]\n",
      " [0.04261672 0.0069371  0.934433   0.01601317]]\n",
      "Iteration 486, Accuracy 0.37728\n",
      "91.28983%change in label assignment\n",
      "0.08805671\n",
      "[[0.02063735 0.89643615 0.00853641 0.07439006]\n",
      " [0.07354225 0.15455486 0.02101443 0.75088847]\n",
      " [0.101097   0.04579301 0.01910845 0.83400154]\n",
      " ...\n",
      " [0.0322772  0.868537   0.01563004 0.08355578]\n",
      " [0.04268873 0.00694346 0.93434453 0.01602333]\n",
      " [0.04268852 0.00694342 0.9343449  0.01602323]]\n",
      "Iteration 487, Accuracy 0.38557\n",
      "95.29631%change in label assignment\n",
      "0.088352285\n",
      "[[0.05642139 0.64585894 0.02087019 0.27684945]\n",
      " [0.0616017  0.05463301 0.01461717 0.8691481 ]\n",
      " [0.25213286 0.05414229 0.03661339 0.6571114 ]\n",
      " ...\n",
      " [0.01549709 0.92850715 0.00685193 0.0491438 ]\n",
      " [0.04259415 0.00693347 0.9344744  0.01599792]\n",
      " [0.04259393 0.00693343 0.9344748  0.01599782]]\n",
      "Iteration 488, Accuracy 0.37983\n",
      "94.76604%change in label assignment\n",
      "0.08594861\n",
      "[[0.02061672 0.89671135 0.00851982 0.07415208]\n",
      " [0.07435322 0.16208802 0.02132882 0.74222994]\n",
      " [0.0957971  0.04568581 0.01835309 0.84016407]\n",
      " ...\n",
      " [0.03163386 0.87077904 0.01525085 0.08233628]\n",
      " [0.04271153 0.00694025 0.9343331  0.01601507]\n",
      " [0.04271121 0.0069402  0.9343336  0.01601492]]\n",
      "Iteration 489, Accuracy 0.38567\n",
      "93.40109%change in label assignment\n",
      "0.08640599\n",
      "[[0.05775192 0.63370895 0.0213309  0.28720823]\n",
      " [0.06140115 0.06536527 0.01535663 0.8578769 ]\n",
      " [0.19748321 0.05250391 0.03144143 0.7185715 ]\n",
      " ...\n",
      " [0.01557025 0.9276814  0.00687331 0.04987499]\n",
      " [0.04258284 0.00692989 0.9345033  0.01598392]\n",
      " [0.04258128 0.00692964 0.93450576 0.01598328]]\n",
      "Iteration 490, Accuracy 0.37968\n",
      "92.29145%change in label assignment\n",
      "0.090295054\n",
      "[[0.01757574 0.9149411  0.00741191 0.06007127]\n",
      " [0.07774131 0.2053615  0.02308238 0.6938149 ]\n",
      " [0.08214602 0.04542069 0.01648332 0.85595   ]\n",
      " ...\n",
      " [0.03643771 0.8536307  0.017809   0.09212254]\n",
      " [0.04266883 0.00693695 0.9343848  0.01600946]\n",
      " [0.04266843 0.00693688 0.9343854  0.01600929]]\n",
      "Iteration 491, Accuracy 0.38229\n",
      "92.0165%change in label assignment\n",
      "0.09166392\n",
      "[[0.07406719 0.45283428 0.02562601 0.4474725 ]\n",
      " [0.07381007 0.04643512 0.01603496 0.8637199 ]\n",
      " [0.3394968  0.05470949 0.04412175 0.561672  ]\n",
      " ...\n",
      " [0.02116776 0.89331186 0.00887527 0.07664516]\n",
      " [0.04250079 0.00692357 0.9346139  0.01596175]\n",
      " [0.04250022 0.00692348 0.9346147  0.01596152]]\n",
      "Iteration 492, Accuracy 0.37821\n",
      "89.57628%change in label assignment\n",
      "0.09289078\n",
      "[[0.01788808 0.9208409  0.00803637 0.05323466]\n",
      " [0.07999337 0.29176593 0.02507183 0.6031689 ]\n",
      " [0.07115608 0.04700857 0.01506451 0.86677086]\n",
      " ...\n",
      " [0.05719641 0.7826769  0.02956375 0.13056296]\n",
      " [0.04273068 0.00694252 0.93431014 0.01601656]\n",
      " [0.04273064 0.00694251 0.9343104  0.01601654]]\n",
      "Iteration 493, Accuracy 0.3896\n",
      "84.86277%change in label assignment\n",
      "0.0943818\n",
      "[[0.04444671 0.74035686 0.0171058  0.19809064]\n",
      " [0.0613771  0.06011003 0.0150582  0.8634547 ]\n",
      " [0.26610005 0.05463203 0.03829684 0.64097106]\n",
      " ...\n",
      " [0.01908245 0.91666985 0.00881781 0.05542986]\n",
      " [0.04250893 0.00692688 0.93459123 0.01597298]\n",
      " [0.04250889 0.00692687 0.93459135 0.01597297]]\n",
      "Iteration 494, Accuracy 0.39171\n",
      "90.37659%change in label assignment\n",
      "0.082144074\n",
      "[[0.02398086 0.8772452  0.00972977 0.08904421]\n",
      " [0.06633741 0.09872225 0.01751079 0.8174295 ]\n",
      " [0.15810722 0.0502358  0.02614828 0.7655087 ]\n",
      " ...\n",
      " [0.02714151 0.88707465 0.01281187 0.07297198]\n",
      " [0.04259652 0.00692944 0.9344741  0.01599985]\n",
      " [0.04259651 0.00692944 0.9344741  0.01599985]]\n",
      "Iteration 495, Accuracy 0.38631\n",
      "96.95586%change in label assignment\n",
      "0.081920445\n",
      "[[0.07974707 0.2519476  0.02517831 0.64312696]\n",
      " [0.14486863 0.04970596 0.02584238 0.7795831 ]\n",
      " [0.5743966  0.044223   0.05286121 0.32851923]\n",
      " ...\n",
      " [0.04127408 0.7647152  0.01617135 0.17783944]\n",
      " [0.04247411 0.00691176 0.93464273 0.01597142]\n",
      " [0.04247406 0.00691175 0.9346428  0.0159714 ]]\n",
      "Iteration 496, Accuracy 0.38449\n",
      "88.54029%change in label assignment\n",
      "0.0995252\n",
      "[[0.04273764 0.8316371  0.02114186 0.1044834 ]\n",
      " [0.06113828 0.6100347  0.02188348 0.30694348]\n",
      " [0.06580761 0.0825577  0.01652119 0.8351135 ]\n",
      " ...\n",
      " [0.09004655 0.6799232  0.05024425 0.17978603]\n",
      " [0.04281538 0.00694577 0.93417096 0.01606786]\n",
      " [0.04281541 0.00694577 0.93417096 0.01606786]]\n",
      "Iteration 497, Accuracy 0.39309\n",
      "79.48642%change in label assignment\n",
      "0.096400514\n",
      "[[0.01608155 0.92335886 0.00695374 0.05360586]\n",
      " [0.07255737 0.14351192 0.02085651 0.7630742 ]\n",
      " [0.14429502 0.04902045 0.02523788 0.78144664]\n",
      " ...\n",
      " [0.04413971 0.8267254  0.02234446 0.10679038]\n",
      " [0.04254783 0.006927   0.9345043  0.01602098]\n",
      " [0.04254786 0.006927   0.9345042  0.01602097]]\n",
      "Iteration 498, Accuracy 0.40674\n",
      "89.0411%change in label assignment\n",
      "0.08688338\n",
      "[[0.03212896 0.82593596 0.01271393 0.12922111]\n",
      " [0.06101887 0.06716219 0.01516416 0.85665476]\n",
      " [0.24420731 0.0535388  0.03535115 0.66690266]\n",
      " ...\n",
      " [0.02248401 0.9045499  0.0105097  0.06245644]\n",
      " [0.0425036  0.00691849 0.9345679  0.01601007]\n",
      " [0.04250355 0.00691848 0.93456787 0.01601005]]\n",
      "Iteration 499, Accuracy 0.39093\n",
      "96.19482%change in label assignment\n",
      "0.0807066\n",
      "[[0.06034809 0.6115945  0.02191965 0.3061378 ]\n",
      " [0.06719228 0.04604685 0.01469792 0.87206286]\n",
      " [0.37844533 0.05332832 0.0452284  0.52299803]\n",
      " ...\n",
      " [0.01518799 0.9296352  0.00665612 0.04852066]\n",
      " [0.04246567 0.00691106 0.9346293  0.0159939 ]\n",
      " [0.0424656  0.00691105 0.9346295  0.01599387]]\n",
      "Iteration 500, Accuracy 0.38494\n",
      "96.45505%change in label assignment\n",
      "0.07802187\n",
      "[[0.03911061 0.7793581  0.01514721 0.16638409]\n",
      " [0.06046452 0.05774728 0.01445553 0.8673327 ]\n",
      " [0.2653913  0.0539512  0.03708392 0.64357364]\n",
      " ...\n",
      " [0.01836969 0.9197829  0.00840045 0.05344695]\n",
      " [0.04247751 0.00691111 0.9346147  0.01599673]\n",
      " [0.04247746 0.00691111 0.9346148  0.01599671]]\n",
      "Iteration 501, Accuracy 0.3867\n",
      "97.1719%change in label assignment\n",
      "0.07936598\n",
      "[[0.05182334 0.68465334 0.01941651 0.2441068 ]\n",
      " [0.06465309 0.04738633 0.0144699  0.8734907 ]\n",
      " [0.3377667  0.05403142 0.04288156 0.56532025]\n",
      " ...\n",
      " [0.01527949 0.93012416 0.00678365 0.04781266]\n",
      " [0.04243194 0.00690482 0.93467987 0.01598334]\n",
      " [0.04243188 0.00690481 0.9346799  0.01598332]]\n",
      "Iteration 502, Accuracy 0.38366\n",
      "98.24716%change in label assignment\n",
      "0.0802215\n",
      "[[0.05442006 0.6632412  0.02012486 0.26221383]\n",
      " [0.06186144 0.05072322 0.0141846  0.8732307 ]\n",
      " [0.31904504 0.05428499 0.04122817 0.5854418 ]\n",
      " ...\n",
      " [0.01602638 0.9280067  0.00717938 0.04878748]\n",
      " [0.0424461  0.00690396 0.9346763  0.01597365]\n",
      " [0.04244596 0.00690393 0.9346765  0.01597359]]\n",
      "Iteration 503, Accuracy 0.3867\n",
      "98.96401%change in label assignment\n",
      "0.07943759\n",
      "[[0.05996167 0.6138201  0.02191652 0.30430174]\n",
      " [0.06119772 0.05190848 0.01430513 0.87258863]\n",
      " [0.3201168  0.05427796 0.04179516 0.58381015]\n",
      " ...\n",
      " [0.01585352 0.92856437 0.0071309  0.04845118]\n",
      " [0.04239318 0.00689844 0.93475056 0.01595783]\n",
      " [0.04239296 0.0068984  0.934751   0.01595774]]\n",
      "Iteration 504, Accuracy 0.3868\n",
      "99.14077%change in label assignment\n",
      "0.08385181\n",
      "[[0.07704752 0.39873397 0.02578825 0.49843028]\n",
      " [0.08469226 0.04433335 0.01700675 0.85396767]\n",
      " [0.50096285 0.04858827 0.04980775 0.40064114]\n",
      " ...\n",
      " [0.0174931  0.91460127 0.00741324 0.06049241]\n",
      " [0.04235613 0.0068938  0.93480724 0.01594274]\n",
      " [0.04235597 0.00689378 0.9348076  0.01594267]]\n",
      "Iteration 505, Accuracy 0.38675\n",
      "94.0983%change in label assignment\n",
      "0.08184191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02653143 0.860609   0.01073866 0.1021209 ]\n",
      " [0.06503527 0.09425075 0.01725858 0.8234554 ]\n",
      " [0.17959265 0.05118636 0.02881955 0.7404014 ]\n",
      " ...\n",
      " [0.0300239  0.87666345 0.01450568 0.07880698]\n",
      " [0.04242704 0.00690044 0.9347132  0.01595935]\n",
      " [0.0424269  0.00690042 0.93471336 0.01595929]]\n",
      "Iteration 506, Accuracy 0.38553\n",
      "91.56479%change in label assignment\n",
      "0.08450965\n",
      "[[0.07831827 0.36745045 0.02611307 0.5281182 ]\n",
      " [0.11141378 0.04639398 0.02105113 0.8211411 ]\n",
      " [0.5362971  0.04656277 0.05148082 0.36565924]\n",
      " ...\n",
      " [0.02362494 0.8783402  0.00976952 0.08826528]\n",
      " [0.04225993 0.00688411 0.9349382  0.01591777]\n",
      " [0.0422598  0.00688409 0.93493843 0.01591771]]\n",
      "Iteration 507, Accuracy 0.38597\n",
      "90.56317%change in label assignment\n",
      "0.08497277\n",
      "[[0.01857705 0.9089183  0.00779438 0.06471022]\n",
      " [0.07110269 0.13143566 0.01975382 0.77770793]\n",
      " [0.13263872 0.04834934 0.02314557 0.7958664 ]\n",
      " ...\n",
      " [0.0390454  0.8444867  0.01929974 0.09716813]\n",
      " [0.04241086 0.00689699 0.93473613 0.01595597]\n",
      " [0.04241071 0.00689696 0.93473643 0.0159559 ]]\n",
      "Iteration 508, Accuracy 0.38759\n",
      "88.63848%change in label assignment\n",
      "0.093276545\n",
      "[[0.077756   0.38728127 0.02633218 0.5086305 ]\n",
      " [0.09642316 0.04590163 0.01931653 0.8383587 ]\n",
      " [0.49638435 0.04900261 0.05170582 0.40290725]\n",
      " ...\n",
      " [0.02082086 0.8954587  0.00878905 0.07493135]\n",
      " [0.04220768 0.00688005 0.9350033  0.01590902]\n",
      " [0.04220749 0.00688002 0.93500346 0.01590893]]\n",
      "Iteration 509, Accuracy 0.38823\n",
      "90.15564%change in label assignment\n",
      "0.08855796\n",
      "[[0.01878801 0.9084259  0.00787525 0.0649108 ]\n",
      " [0.07495593 0.16263203 0.021447   0.740965  ]\n",
      " [0.12309633 0.047829   0.02189564 0.80717903]\n",
      " ...\n",
      " [0.04528279 0.8229009  0.02269515 0.10912113]\n",
      " [0.04240201 0.00689603 0.93474114 0.01596086]\n",
      " [0.0424018  0.006896   0.93474144 0.01596076]]\n",
      "Iteration 510, Accuracy 0.38891\n",
      "89.44371%change in label assignment\n",
      "0.08333987\n",
      "[[0.06024897 0.6113031  0.02214815 0.30629984]\n",
      " [0.06231331 0.05141497 0.01464885 0.87162286]\n",
      " [0.34448445 0.05416111 0.04423069 0.5571238 ]\n",
      " ...\n",
      " [0.01663259 0.9257458  0.00757606 0.05004553]\n",
      " [0.04218814 0.00687852 0.93501925 0.01591408]\n",
      " [0.0421879  0.00687848 0.9350196  0.01591396]]\n",
      "Iteration 511, Accuracy 0.3898\n",
      "93.17523%change in label assignment\n",
      "0.08184278\n",
      "[[0.04442815 0.74126816 0.01692855 0.19737513]\n",
      " [0.06127825 0.07105348 0.01541014 0.8522581 ]\n",
      " [0.21903658 0.05290971 0.03288351 0.69517016]\n",
      " ...\n",
      " [0.02278562 0.9032804  0.01066722 0.06326679]\n",
      " [0.04224207 0.00688034 0.93495613 0.0159214 ]\n",
      " [0.0422416  0.00688027 0.93495697 0.01592121]]\n",
      "Iteration 512, Accuracy 0.38553\n",
      "93.11141%change in label assignment\n",
      "0.08062298\n",
      "[[0.07758085 0.38599074 0.0261077  0.51032066]\n",
      " [0.07342374 0.04489584 0.01577411 0.86590624]\n",
      " [0.3838393  0.05334218 0.04641224 0.51640636]\n",
      " ...\n",
      " [0.01657563 0.9200609  0.00714681 0.0562167 ]\n",
      " [0.04213099 0.00686979 0.93510604 0.01589325]\n",
      " [0.04212971 0.00686959 0.93510795 0.01589273]]\n",
      "Iteration 513, Accuracy 0.38145\n",
      "93.63186%change in label assignment\n",
      "0.08615567\n",
      "[[0.02845548 0.8492671  0.01141019 0.11086717]\n",
      " [0.07095107 0.13196969 0.01983094 0.7772483 ]\n",
      " [0.12463577 0.04733774 0.02219838 0.8058281 ]\n",
      " ...\n",
      " [0.03624796 0.854465   0.01781076 0.0914763 ]\n",
      " [0.04222818 0.0068798  0.93495846 0.01593355]\n",
      " [0.0422268  0.00687957 0.93496066 0.01593297]]\n",
      "Iteration 514, Accuracy 0.38602\n",
      "91.15235%change in label assignment\n",
      "0.091276824\n",
      "[[0.0721613  0.4830803  0.0253759  0.41938245]\n",
      " [0.07270048 0.04594835 0.01592887 0.8654223 ]\n",
      " [0.37377024 0.05366101 0.04647997 0.52608883]\n",
      " ...\n",
      " [0.01672997 0.9196303  0.00725084 0.05638881]\n",
      " [0.04201515 0.00686444 0.935229   0.01589144]\n",
      " [0.04201476 0.00686438 0.9352295  0.01589128]]\n",
      "Iteration 515, Accuracy 0.37713\n",
      "88.41754%change in label assignment\n",
      "0.08284336\n",
      "[[0.04788181 0.71747667 0.01797947 0.21666205]\n",
      " [0.06291305 0.07731687 0.01593062 0.84383947]\n",
      " [0.20165743 0.0522956  0.03088361 0.71516335]\n",
      " ...\n",
      " [0.02315578 0.9018585  0.01078229 0.06420346]\n",
      " [0.04213604 0.0068724  0.93507457 0.01591692]\n",
      " [0.04213519 0.00687225 0.93507606 0.01591655]]\n",
      "Iteration 516, Accuracy 0.38695\n",
      "92.64987%change in label assignment\n",
      "0.0869129\n",
      "[[0.07905768 0.24217032 0.02479338 0.6539786 ]\n",
      " [0.11668839 0.04707634 0.0220338  0.8142014 ]\n",
      " [0.5475412  0.04588611 0.05237775 0.3541949 ]\n",
      " ...\n",
      " [0.02987364 0.83992845 0.01211175 0.11808618]\n",
      " [0.04196959 0.00685834 0.9353074  0.01586458]\n",
      " [0.04196917 0.00685828 0.9353081  0.0158644 ]]\n",
      "Iteration 517, Accuracy 0.37796\n",
      "88.05912%change in label assignment\n",
      "0.09004778\n",
      "[[0.0307268  0.8357976  0.01218183 0.12129369]\n",
      " [0.06966069 0.11895347 0.01898163 0.7924042 ]\n",
      " [0.14460185 0.04936059 0.02454386 0.7814937 ]\n",
      " ...\n",
      " [0.03660581 0.853056   0.0179008  0.09243736]\n",
      " [0.04217438 0.00687556 0.93503785 0.01591219]\n",
      " [0.04217343 0.0068754  0.93503934 0.01591178]]\n",
      "Iteration 518, Accuracy 0.39053\n",
      "85.72691%change in label assignment\n",
      "0.0926595\n",
      "[[0.07902227 0.23656917 0.02477216 0.65963644]\n",
      " [0.09891238 0.04613138 0.019745   0.8352113 ]\n",
      " [0.48122808 0.04980754 0.05147175 0.4174927 ]\n",
      " ...\n",
      " [0.02178731 0.8896776  0.00915439 0.07938074]\n",
      " [0.04195587 0.00686074 0.935313   0.01587035]\n",
      " [0.04195245 0.00686019 0.9353184  0.01586897]]\n",
      "Iteration 519, Accuracy 0.37796\n",
      "89.20312%change in label assignment\n",
      "[[0.07272997 0.47995478 0.02485516 0.42246005]\n",
      " [0.06425752 0.05027492 0.01436924 0.8710983 ]\n",
      " [0.3474501  0.05437915 0.04256699 0.5556037 ]\n",
      " ...\n",
      " [0.01728194 0.92255884 0.00767966 0.05247952]\n",
      " [0.04209637 0.00686867 0.93514603 0.01588888]\n",
      " [0.04209533 0.00686851 0.9351477  0.01588844]]\n",
      "Iteration 520, Accuracy 0.38307\n",
      "94.47145%change in label assignment\n",
      "0.08543362\n",
      "[[0.04295384 0.7503807  0.01665425 0.19001117]\n",
      " [0.06874879 0.11991832 0.01933433 0.7919986 ]\n",
      " [0.14371239 0.04888245 0.02517326 0.78223187]\n",
      " ...\n",
      " [0.02679797 0.88814867 0.01290691 0.07214641]\n",
      " [0.04209445 0.00686994 0.93515193 0.01588371]\n",
      " [0.04209217 0.00686957 0.93515545 0.01588276]]\n",
      "Iteration 521, Accuracy 0.38577\n",
      "92.27181%change in label assignment\n",
      "0.08344501\n",
      "[[0.05262286 0.6763072  0.01958552 0.25148442]\n",
      " [0.06647114 0.10578792 0.01805842 0.80968255]\n",
      " [0.14356923 0.04880574 0.02469569 0.78292936]\n",
      " ...\n",
      " [0.01805279 0.92072403 0.00825379 0.05296939]\n",
      " [0.04209175 0.00686863 0.9351573  0.01588228]\n",
      " [0.04208172 0.00686716 0.9351725  0.01587861]]\n",
      "Iteration 522, Accuracy 0.3735\n",
      "94.80041%change in label assignment\n",
      "0.08505659\n",
      "[[0.04397108 0.7426213  0.01693141 0.19647627]\n",
      " [0.07018637 0.1322753  0.0199579  0.77758044]\n",
      " [0.11196212 0.04621472 0.02092812 0.820895  ]\n",
      " ...\n",
      " [0.0182299  0.9200206  0.00840705 0.05334251]\n",
      " [0.042127   0.00687352 0.9351087  0.01589066]\n",
      " [0.04206753 0.00686693 0.93519163 0.01587386]]\n",
      "Iteration 523, Accuracy 0.36603\n",
      "96.63672%change in label assignment\n",
      "0.08504195\n",
      "[[0.04165086 0.75971824 0.01604372 0.18258722]\n",
      " [0.07356754 0.1625832  0.02140303 0.7424462 ]\n",
      " [0.09176286 0.04465812 0.01795116 0.84562784]\n",
      " ...\n",
      " [0.01800695 0.92084485 0.00823979 0.05290833]\n",
      " [0.08238722 0.01114054 0.8791981  0.02727418]\n",
      " [0.04213363 0.00687156 0.9351086  0.01588626]]\n",
      "Iteration 524, Accuracy 0.36019\n",
      "95.21775%change in label assignment\n",
      "0.090090044\n",
      "[[0.02087692 0.8944251  0.00876079 0.07593717]\n",
      " [0.07911274 0.26079425 0.02507893 0.63501406]\n",
      " [0.06683632 0.04611021 0.01487256 0.87218094]\n",
      " ...\n",
      " [0.02707863 0.8871091  0.01306431 0.07274789]\n",
      " [0.04207566 0.00686769 0.93516636 0.01589032]\n",
      " [0.04206186 0.00686573 0.93518704 0.01588536]]\n",
      "Iteration 525, Accuracy 0.35008\n",
      "92.80699%change in label assignment\n",
      "0.08821701\n",
      "[[0.0621361  0.59137964 0.02240488 0.3240793 ]\n",
      " [0.062487   0.08220672 0.01625042 0.83905584]\n",
      " [0.16465183 0.05026001 0.02720321 0.757885  ]\n",
      " ...\n",
      " [0.01537835 0.9277099  0.00668662 0.05022511]\n",
      " [0.0420601  0.00686356 0.93519837 0.01587797]\n",
      " [0.04203692 0.00686088 0.9352312  0.01587107]]\n",
      "Iteration 526, Accuracy 0.35489\n",
      "93.01321%change in label assignment\n",
      "0.0905641\n",
      "[[0.02083118 0.89435995 0.00867496 0.07613388]\n",
      " [0.07908247 0.29096922 0.02528481 0.6046635 ]\n",
      " [0.06989224 0.0447575  0.01498425 0.870366  ]\n",
      " ...\n",
      " [0.03593045 0.8551922  0.01777065 0.09110676]\n",
      " [0.04212311 0.00686912 0.9351195  0.01588832]\n",
      " [0.04212034 0.00686867 0.93512386 0.01588714]]\n",
      "Iteration 527, Accuracy 0.36407\n",
      "90.43551%change in label assignment\n",
      "0.09063557\n",
      "[[0.07002016 0.50211245 0.02469767 0.40316972]\n",
      " [0.05998365 0.06559085 0.01513076 0.8592947 ]\n",
      " [0.24061148 0.0537733  0.03574067 0.66987455]\n",
      " ...\n",
      " [0.01515453 0.9290265  0.00667627 0.04914275]\n",
      " [0.04202804 0.00685813 0.9352551  0.01585876]\n",
      " [0.04202647 0.00685789 0.9352576  0.0158581 ]]\n",
      "Iteration 528, Accuracy 0.36309\n",
      "89.97889%change in label assignment\n",
      "0.08601699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02246266 0.8849088  0.00923773 0.08339077]\n",
      " [0.07912552 0.28000045 0.02494661 0.61592746]\n",
      " [0.07448247 0.04478542 0.01549983 0.8652322 ]\n",
      " ...\n",
      " [0.0364973  0.8531795  0.01798365 0.09233953]\n",
      " [0.04217407 0.00686826 0.9350706  0.01588703]\n",
      " [0.04217286 0.00686807 0.93507254 0.01588649]]\n",
      "Iteration 529, Accuracy 0.37895\n",
      "92.03614%change in label assignment\n",
      "0.08833277\n",
      "[[0.06300126 0.58139753 0.02308153 0.33251968]\n",
      " [0.06385215 0.0881258  0.0172223  0.83079976]\n",
      " [0.19031098 0.0520904  0.03109937 0.72649926]\n",
      " ...\n",
      " [0.01598121 0.9274674  0.00724958 0.04930182]\n",
      " [0.04203489 0.00685415 0.9352565  0.01585448]\n",
      " [0.042034   0.00685402 0.93525785 0.0158541 ]]\n",
      "Iteration 530, Accuracy 0.36854\n",
      "92.88064%change in label assignment\n",
      "0.08698445\n",
      "[[0.01915518 0.90546864 0.00801116 0.06736502]\n",
      " [0.07964162 0.30894205 0.02534093 0.5860754 ]\n",
      " [0.06842081 0.0472404  0.01475426 0.8695845 ]\n",
      " ...\n",
      " [0.03730675 0.8503482  0.01832633 0.09401874]\n",
      " [0.04215115 0.00686264 0.93510056 0.0158857 ]\n",
      " [0.04214983 0.00686243 0.93510264 0.01588511]]\n",
      "Iteration 531, Accuracy 0.37497\n",
      "91.59916%change in label assignment\n",
      "0.090957776\n",
      "[[0.07445472 0.4426562  0.02605133 0.45683774]\n",
      " [0.06199576 0.0563188  0.01523212 0.86645323]\n",
      " [0.30029696 0.05507256 0.04220438 0.60242605]\n",
      " ...\n",
      " [0.01703586 0.91781276 0.00744612 0.05770529]\n",
      " [0.04196875 0.00684655 0.93535006 0.01583463]\n",
      " [0.0419682  0.00684648 0.93535095 0.01583439]]\n",
      "Iteration 532, Accuracy 0.36922\n",
      "86.6647%change in label assignment\n",
      "0.087874874\n",
      "[[0.01688756 0.91997266 0.00723933 0.05590045]\n",
      " [0.07979401 0.30889767 0.025333   0.5859753 ]\n",
      " [0.07715493 0.04608169 0.01587172 0.8608917 ]\n",
      " ...\n",
      " [0.04934893 0.8086257  0.02501999 0.11700535]\n",
      " [0.04219653 0.00686447 0.93505377 0.01588522]\n",
      " [0.04219638 0.00686445 0.93505406 0.01588512]]\n",
      "Iteration 533, Accuracy 0.38081\n",
      "89.63028%change in label assignment\n",
      "0.091359936\n",
      "[[0.04725278 0.717157   0.01816421 0.21742599]\n",
      " [0.06447956 0.09432027 0.01757574 0.82362443]\n",
      " [0.18691444 0.05186863 0.03068635 0.73053056]\n",
      " ...\n",
      " [0.01858189 0.9185086  0.00863814 0.05427139]\n",
      " [0.04200583 0.00684885 0.93530154 0.01584375]\n",
      " [0.04200534 0.00684879 0.9353024  0.01584352]]\n",
      "Iteration 534, Accuracy 0.38857\n",
      "93.68095%change in label assignment\n",
      "0.08179162\n",
      "[[0.01981097 0.9013107  0.00825585 0.07062255]\n",
      " [0.0779915  0.22367178 0.02369534 0.6746414 ]\n",
      " [0.08580643 0.04456218 0.01704334 0.852588  ]\n",
      " ...\n",
      " [0.03326538 0.86476177 0.01616937 0.08580351]\n",
      " [0.04208291 0.00685192 0.93519324 0.01587193]\n",
      " [0.0420824  0.00685184 0.93519396 0.01587168]]\n",
      "Iteration 535, Accuracy 0.3789\n",
      "95.28158%change in label assignment\n",
      "0.08398092\n",
      "[[0.07957713 0.29413083 0.02603613 0.6002559 ]\n",
      " [0.07611246 0.04552845 0.01659536 0.8617637 ]\n",
      " [0.4011266  0.0531241  0.04869228 0.49705705]\n",
      " ...\n",
      " [0.03369216 0.81498355 0.01362387 0.13770044]\n",
      " [0.04193812 0.00683203 0.9353902  0.01583965]\n",
      " [0.04193746 0.00683193 0.9353912  0.01583936]]\n",
      "Iteration 536, Accuracy 0.38273\n",
      "89.30623%change in label assignment\n",
      "0.09847474\n",
      "[[0.04685682 0.8173024  0.0235425  0.11229828]\n",
      " [0.02537551 0.87005097 0.01029439 0.09427916]\n",
      " [0.08050669 0.24613221 0.02447597 0.6488852 ]\n",
      " ...\n",
      " [0.09495398 0.6650172  0.05378745 0.18624127]\n",
      " [0.04227297 0.00686832 0.93491864 0.01594   ]\n",
      " [0.04227149 0.00686806 0.9349211  0.01593931]]\n",
      "Iteration 537, Accuracy 0.38719\n",
      "71.66004%change in label assignment\n",
      "0.10308258\n",
      "[[0.01654778 0.9201315  0.00719201 0.05612874]\n",
      " [0.07899079 0.2507469  0.02508058 0.6451817 ]\n",
      " [0.08003446 0.04470142 0.01693823 0.8583259 ]\n",
      " ...\n",
      " [0.03840293 0.8463023  0.01930982 0.09598494]\n",
      " [0.04196733 0.00684658 0.9353096  0.0158765 ]\n",
      " [0.04196652 0.00684646 0.9353109  0.01587616]]\n",
      "Iteration 538, Accuracy 0.3923\n",
      "83.13448%change in label assignment\n",
      "0.08869158\n",
      "[[0.03034118 0.8367077  0.01211307 0.12083805]\n",
      " [0.07433508 0.16868746 0.0217438  0.73523366]\n",
      " [0.10340958 0.04506491 0.01953413 0.83199143]\n",
      " ...\n",
      " [0.02289413 0.90290964 0.01075723 0.06343894]\n",
      " [0.04199664 0.00684322 0.93527615 0.01588402]\n",
      " [0.04199398 0.00684279 0.9352803  0.01588291]]\n",
      "Iteration 539, Accuracy 0.38346\n",
      "95.03609%change in label assignment\n",
      "0.08344476\n",
      "[[0.04789476 0.713513   0.01830717 0.22028504]\n",
      " [0.0661009  0.10387854 0.01821142 0.8118091 ]\n",
      " [0.15643147 0.04937229 0.02677832 0.76741797]\n",
      " ...\n",
      " [0.01572203 0.9292304  0.00712941 0.04791811]\n",
      " [0.04196278 0.00683629 0.9353272  0.0158737 ]\n",
      " [0.04196092 0.006836   0.9353302  0.01587292]]\n",
      "Iteration 540, Accuracy 0.37104\n",
      "96.56798%change in label assignment\n",
      "0.08306446\n",
      "[[0.03073588 0.83412933 0.0122711  0.12286367]\n",
      " [0.06919266 0.12413105 0.01934749 0.7873288 ]\n",
      " [0.11766449 0.04627694 0.021415   0.8146435 ]\n",
      " ...\n",
      " [0.01841278 0.91958475 0.00846164 0.05354083]\n",
      " [0.04200171 0.00683652 0.93528086 0.01588088]\n",
      " [0.04199994 0.00683624 0.93528366 0.01588013]]\n",
      "Iteration 541, Accuracy 0.37448\n",
      "97.97221%change in label assignment\n",
      "0.079701476\n",
      "[[0.03945498 0.7751111  0.01549123 0.1699427 ]\n",
      " [0.06908337 0.12420521 0.01961075 0.7871007 ]\n",
      " [0.11423767 0.04596566 0.02135323 0.8184434 ]\n",
      " ...\n",
      " [0.01647018 0.92660856 0.00754145 0.04937978]\n",
      " [0.04199239 0.00683567 0.9352956  0.01587636]\n",
      " [0.04198053 0.00683374 0.9353142  0.01587155]]\n",
      "Iteration 542, Accuracy 0.37684\n",
      "94.78568%change in label assignment\n",
      "0.08431059\n",
      "[[0.05622771 0.64542466 0.02077267 0.27757496]\n",
      " [0.06233432 0.08317816 0.01634857 0.83813894]\n",
      " [0.16573478 0.0500009  0.02739181 0.7568724 ]\n",
      " ...\n",
      " [0.01492838 0.9306782  0.00657364 0.04781979]\n",
      " [0.04198228 0.00683362 0.93532056 0.01586354]\n",
      " [0.04197279 0.00683208 0.93533534 0.01585969]]\n",
      "Iteration 543, Accuracy 0.3678\n",
      "96.65145%change in label assignment\n",
      "0.08557348\n",
      "[[0.02606958 0.8624682  0.01068437 0.1007779 ]\n",
      " [0.0775708  0.22470221 0.02405301 0.673674  ]\n",
      " [0.0747223  0.04357835 0.01575393 0.86594546]\n",
      " ...\n",
      " [0.02634975 0.8899087  0.01270524 0.07103634]\n",
      " [0.04208351 0.00684434 0.93518525 0.01588694]\n",
      " [0.0420245  0.00683827 0.935266   0.01587123]]\n",
      "Iteration 544, Accuracy 0.37212\n",
      "92.42893%change in label assignment\n",
      "0.09113029\n",
      "[[0.0531078  0.67115337 0.02000001 0.2557388 ]\n",
      " [0.06290421 0.08630729 0.01683493 0.8339535 ]\n",
      " [0.16545835 0.0499765  0.02783377 0.7567314 ]\n",
      " ...\n",
      " [0.01481441 0.931164   0.00657919 0.04744235]\n",
      " [0.04193587 0.00682726 0.9353822  0.0158547 ]\n",
      " [0.04193358 0.0068269  0.9353857  0.01585375]]\n",
      "Iteration 545, Accuracy 0.36176\n",
      "89.89542%change in label assignment\n",
      "0.084034406\n",
      "[[0.03456879 0.8087664  0.01364342 0.14302137]\n",
      " [0.07131942 0.14186539 0.02036911 0.766446  ]\n",
      " [0.12668662 0.0471254  0.02258446 0.8036035 ]\n",
      " ...\n",
      " [0.01990812 0.9139543  0.0092433  0.05689427]\n",
      " [0.0420114  0.00683298 0.9352835  0.01587212]\n",
      " [0.04201062 0.00683286 0.93528473 0.01587177]]\n",
      "Iteration 546, Accuracy 0.37512\n",
      "96.2341%change in label assignment\n",
      "0.08248395\n",
      "[[0.07315688 0.45982045 0.02563244 0.44139025]\n",
      " [0.05992737 0.05284516 0.01446046 0.872767  ]\n",
      " [0.32124883 0.05425185 0.04289514 0.5816041 ]\n",
      " ...\n",
      " [0.02024031 0.8978977  0.00856724 0.07329471]\n",
      " [0.04189368 0.00681897 0.9354528  0.01583448]\n",
      " [0.04189312 0.0068189  0.9354537  0.01583423]]\n",
      "Iteration 547, Accuracy 0.37747\n",
      "92.0165%change in label assignment\n",
      "0.08673656\n",
      "[[0.01800175 0.9118553  0.00761894 0.06252406]\n",
      " [0.07852568 0.23631234 0.02413745 0.6610245 ]\n",
      " [0.0838374  0.04449256 0.01679159 0.8548785 ]\n",
      " ...\n",
      " [0.03216605 0.86871666 0.01564771 0.08346959]\n",
      " [0.04205547 0.00683123 0.9352462  0.01586712]\n",
      " [0.04205505 0.00683117 0.9352469  0.01586691]]\n",
      "Iteration 548, Accuracy 0.38582\n",
      "91.30456%change in label assignment\n",
      "0.090957664\n",
      "[[0.06919162 0.5146005  0.02493362 0.39127424]\n",
      " [0.06156257 0.05280879 0.01492823 0.8707004 ]\n",
      " [0.29532015 0.05458167 0.04165162 0.60844654]\n",
      " ...\n",
      " [0.02118165 0.8927011  0.00899849 0.0771188 ]\n",
      " [0.04185804 0.00681275 0.93551177 0.01581739]\n",
      " [0.04185729 0.00681265 0.935513   0.01581708]]\n",
      "Iteration 549, Accuracy 0.384\n",
      "92.54677%change in label assignment\n",
      "0.08924857\n",
      "[[0.01663165 0.9247384  0.0074     0.05122992]\n",
      " [0.07732796 0.39741474 0.0257706  0.49948668]\n",
      " [0.06318697 0.05114704 0.01434088 0.8713251 ]\n",
      " ...\n",
      " [0.04871028 0.81093484 0.02477197 0.11558286]\n",
      " [0.04207281 0.00683174 0.9352183  0.0158772 ]\n",
      " [0.04207203 0.00683162 0.9352195  0.01587683]]\n",
      "Iteration 550, Accuracy 0.38621\n",
      "87.40119%change in label assignment\n",
      "0.0892455\n",
      "[[0.03661574 0.7945384  0.01463312 0.15421271]\n",
      " [0.07286186 0.15470627 0.02162612 0.75080574]\n",
      " [0.11040261 0.04608947 0.02128799 0.8222199 ]\n",
      " ...\n",
      " [0.02071678 0.91065633 0.00980891 0.05881797]\n",
      " [0.04185195 0.00681337 0.9355063  0.01582841]\n",
      " [0.04184842 0.00681282 0.9355117  0.01582696]]\n",
      "Iteration 551, Accuracy 0.38528\n",
      "92.91501%change in label assignment\n",
      "0.08425431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03912775 0.77761006 0.01525038 0.1680118 ]\n",
      " [0.06916457 0.1264429  0.01949107 0.78490156]\n",
      " [0.12487824 0.04682929 0.02247305 0.80581945]\n",
      " ...\n",
      " [0.01788251 0.9214619  0.00821307 0.05244258]\n",
      " [0.04185878 0.0068108  0.93550235 0.01582802]\n",
      " [0.04185678 0.00681049 0.9355056  0.01582718]]\n",
      "Iteration 552, Accuracy 0.36927\n",
      "97.05897%change in label assignment\n",
      "0.07828083\n",
      "[[0.05273803 0.6736665  0.01999562 0.25359994]\n",
      " [0.06088109 0.07461746 0.01599344 0.84850794]\n",
      " [0.19389614 0.05167165 0.03137414 0.72305804]\n",
      " ...\n",
      " [0.01507465 0.92874557 0.00665566 0.04952413]\n",
      " [0.04178178 0.00680187 0.935613   0.01580333]\n",
      " [0.04178086 0.00680173 0.93561447 0.01580294]]\n",
      "Iteration 553, Accuracy 0.37359\n",
      "94.57456%change in label assignment\n",
      "0.08344218\n",
      "[[0.01809944 0.9107607  0.00766448 0.06347538]\n",
      " [0.07908111 0.27076802 0.0250174  0.6251335 ]\n",
      " [0.07108624 0.04400841 0.01509552 0.8698098 ]\n",
      " ...\n",
      " [0.03200475 0.869434   0.01564087 0.08292038]\n",
      " [0.04186372 0.00680817 0.935493   0.01583507]\n",
      " [0.0418624  0.00680796 0.9354951  0.01583449]]\n",
      "Iteration 554, Accuracy 0.38312\n",
      "91.87902%change in label assignment\n",
      "0.08765\n",
      "[[0.06130645 0.5980505  0.02266054 0.3179826 ]\n",
      " [0.06038263 0.06969476 0.01568986 0.8542328 ]\n",
      " [0.21695407 0.05267954 0.03402741 0.6963391 ]\n",
      " ...\n",
      " [0.0166971  0.9187367  0.00724084 0.0573253 ]\n",
      " [0.04167616 0.0067915  0.9357436  0.01578879]\n",
      " [0.04167525 0.00679137 0.93574494 0.0157884 ]]\n",
      "Iteration 555, Accuracy 0.3785\n",
      "90.86267%change in label assignment\n",
      "0.08348047\n",
      "[[0.02261749 0.8842722  0.00934036 0.0837699 ]\n",
      " [0.07548684 0.18418796 0.02247771 0.71784747]\n",
      " [0.09921895 0.0447736  0.01897575 0.8370317 ]\n",
      " ...\n",
      " [0.02252715 0.9041649  0.0106002  0.06270768]\n",
      " [0.04177514 0.00679839 0.9356148  0.01581165]\n",
      " [0.04177478 0.00679834 0.9356155  0.01581146]]\n",
      "Iteration 556, Accuracy 0.3841\n",
      "94.01974%change in label assignment\n",
      "0.08244047\n",
      "[[0.07259238 0.46568185 0.02551456 0.4362112 ]\n",
      " [0.05954126 0.05218691 0.01438995 0.8738819 ]\n",
      " [0.30232358 0.05424566 0.04160815 0.6018226 ]\n",
      " ...\n",
      " [0.02308544 0.8806084  0.00964253 0.08666368]\n",
      " [0.04160755 0.00678184 0.9358559  0.0157547 ]\n",
      " [0.04160684 0.00678175 0.9358571  0.0157544 ]]\n",
      "Iteration 557, Accuracy 0.38297\n",
      "92.70879%change in label assignment\n",
      "0.086944565\n",
      "[[0.0195062  0.9026164  0.00819062 0.06968682]\n",
      " [0.07856641 0.24821056 0.02449183 0.6487312 ]\n",
      " [0.07956024 0.04395475 0.01625983 0.86022514]\n",
      " ...\n",
      " [0.03057048 0.8744642  0.01484954 0.08011577]\n",
      " [0.04174365 0.00679323 0.93567795 0.01578517]\n",
      " [0.04174293 0.00679313 0.9356792  0.01578483]]\n",
      "Iteration 558, Accuracy 0.3868\n",
      "90.85776%change in label assignment\n",
      "0.085183196\n",
      "[[0.06938643 0.50991005 0.02494348 0.39576003]\n",
      " [0.06020817 0.05325108 0.01470575 0.871835  ]\n",
      " [0.29208782 0.05438948 0.04128397 0.6122388 ]\n",
      " ...\n",
      " [0.02346901 0.87854254 0.00984726 0.08814117]\n",
      " [0.04153107 0.0067749  0.9359555  0.01573843]\n",
      " [0.0415305  0.00677483 0.93595654 0.01573817]]\n",
      "Iteration 559, Accuracy 0.3814\n",
      "89.6401%change in label assignment\n",
      "[[0.0208182  0.8954573  0.00867602 0.07504843]\n",
      " [0.07703011 0.20469178 0.02322502 0.6950531 ]\n",
      " [0.09400708 0.04491528 0.01824787 0.84282976]\n",
      " ...\n",
      " [0.02637419 0.88971287 0.01256916 0.07134379]\n",
      " [0.04166114 0.00678495 0.93578714 0.01576677]\n",
      " [0.04166075 0.0067849  0.9357878  0.01576657]]\n",
      "Iteration 560, Accuracy 0.387\n",
      "93.72514%change in label assignment\n",
      "0.08293298\n",
      "[[0.04157033 0.75954443 0.0164389  0.18244627]\n",
      " [0.06468584 0.0958322  0.01792999 0.8215519 ]\n",
      " [0.17397863 0.05081233 0.02952521 0.7456838 ]\n",
      " ...\n",
      " [0.01591457 0.9278498  0.00729327 0.04894235]\n",
      " [0.04153603 0.006774   0.93595546 0.01573455]\n",
      " [0.04153559 0.00677395 0.9359561  0.01573435]]\n",
      "Iteration 561, Accuracy 0.38341\n",
      "96.89203%change in label assignment\n",
      "0.07942776\n",
      "[[0.01666485 0.92520595 0.00750872 0.05062045]\n",
      " [0.07605245 0.415482   0.02570223 0.48276326]\n",
      " [0.05999242 0.05866165 0.01446242 0.86688346]\n",
      " ...\n",
      " [0.03437982 0.8606982  0.01687351 0.08804845]\n",
      " [0.04164094 0.00678057 0.9358205  0.01575795]\n",
      " [0.04163997 0.00678043 0.935822   0.01575753]]\n",
      "Iteration 562, Accuracy 0.38533\n",
      "88.3979%change in label assignment\n",
      "0.08752728\n",
      "[[0.04376147 0.7437556  0.01721643 0.19526646]\n",
      " [0.06202577 0.07928625 0.01666001 0.842028  ]\n",
      " [0.16538943 0.05040091 0.02867745 0.75553226]\n",
      " ...\n",
      " [0.01648967 0.9203618  0.00724555 0.055903  ]\n",
      " [0.04144595 0.00676365 0.9360804  0.01571007]\n",
      " [0.041445   0.00676352 0.9360818  0.01570968]]\n",
      "Iteration 563, Accuracy 0.38174\n",
      "91.10817%change in label assignment\n",
      "0.088417314\n",
      "[[0.01855519 0.9183919  0.0084755  0.0545775 ]\n",
      " [0.0770447  0.39784867 0.02579971 0.49930692]\n",
      " [0.06080676 0.05522721 0.01433338 0.8696326 ]\n",
      " ...\n",
      " [0.04116653 0.8367325  0.0205832  0.10151765]\n",
      " [0.04160209 0.00677771 0.9358696  0.01575065]\n",
      " [0.04160141 0.00677761 0.93587065 0.01575034]]\n",
      "Iteration 564, Accuracy 0.38651\n",
      "92.44857%change in label assignment\n",
      "0.093036175\n",
      "[[0.02157314 0.8902686  0.0091952  0.07896298]\n",
      " [0.07191911 0.14357804 0.02135849 0.76314443]\n",
      " [0.10675737 0.04614318 0.02115979 0.8259396 ]\n",
      " ...\n",
      " [0.01894786 0.91691315 0.00893121 0.05520772]\n",
      " [0.04143513 0.00676279 0.936081   0.01572108]\n",
      " [0.04143473 0.00676275 0.93608165 0.0157209 ]]\n",
      "Iteration 565, Accuracy 0.38621\n",
      "93.1605%change in label assignment\n",
      "0.08178971\n",
      "[[0.02537097 0.867957   0.01036105 0.09631098]\n",
      " [0.06898145 0.12345071 0.01930893 0.78825897]\n",
      " [0.126865   0.04715395 0.02265209 0.80332893]\n",
      " ...\n",
      " [0.01853301 0.9187974  0.00851008 0.05415948]\n",
      " [0.04147772 0.00676164 0.9360337  0.01572693]\n",
      " [0.04147718 0.00676157 0.93603456 0.01572668]]\n",
      "Iteration 566, Accuracy 0.38248\n",
      "98.90018%change in label assignment\n",
      "0.08252832\n",
      "[[0.03049004 0.83420527 0.01242053 0.12288412]\n",
      " [0.07008576 0.13573077 0.02043533 0.7737481 ]\n",
      " [0.12208013 0.04658376 0.0227084  0.80862767]\n",
      " ...\n",
      " [0.01747384 0.9228488  0.00814086 0.0515365 ]\n",
      " [0.04141964 0.00675598 0.9361206  0.01570368]\n",
      " [0.04141897 0.0067559  0.93612176 0.01570338]]\n",
      "Iteration 567, Accuracy 0.38238\n",
      "98.1637%change in label assignment\n",
      "0.08187938\n",
      "[[0.02977161 0.8392906  0.01201929 0.11891846]\n",
      " [0.07120976 0.14513847 0.02064732 0.7630045 ]\n",
      " [0.11710215 0.04606909 0.02154994 0.8152788 ]\n",
      " ...\n",
      " [0.01824799 0.9200859  0.00845467 0.0532114 ]\n",
      " [0.04143303 0.00675384 0.936108   0.01570522]\n",
      " [0.04143236 0.00675375 0.93610895 0.01570492]]\n",
      "Iteration 568, Accuracy 0.38125\n",
      "99.34698%change in label assignment\n",
      "0.08047844\n",
      "[[0.03195915 0.82441676 0.01293467 0.13068955]\n",
      " [0.07361265 0.17058273 0.022173   0.73363155]\n",
      " [0.09389845 0.04398808 0.01868806 0.84342545]\n",
      " ...\n",
      " [0.01682346 0.9253089  0.00779904 0.05006867]\n",
      " [0.04138684 0.0067467  0.93618286 0.01568367]\n",
      " [0.04138592 0.00674658 0.9361842  0.01568328]]\n",
      "Iteration 569, Accuracy 0.38017\n",
      "96.73982%change in label assignment\n",
      "0.08126912\n",
      "[[0.03178906 0.8258078  0.01278656 0.12961654]\n",
      " [0.07575676 0.19837762 0.02312403 0.70274156]\n",
      " [0.08239733 0.04321716 0.01681797 0.8575675 ]\n",
      " ...\n",
      " [0.01680191 0.9254292  0.00773837 0.05003044]\n",
      " [0.04137461 0.00674381 0.93620265 0.01567889]\n",
      " [0.04137334 0.00674362 0.9362046  0.01567836]]\n",
      "Iteration 570, Accuracy 0.37467\n",
      "97.34865%change in label assignment\n",
      "0.08532921\n",
      "[[0.03142751 0.8277274  0.01274503 0.12810001]\n",
      " [0.07760295 0.23908846 0.02457155 0.658737  ]\n",
      " [0.06982268 0.04338572 0.01522526 0.8715664 ]\n",
      " ...\n",
      " [0.01626994 0.927271   0.007511   0.04894808]\n",
      " [0.0413359  0.00674103 0.93625844 0.01566466]\n",
      " [0.04133336 0.00674064 0.9362625  0.01566362]]\n",
      "Iteration 571, Accuracy 0.37065\n",
      "94.48127%change in label assignment\n",
      "0.08593031\n",
      "[[0.05284624 0.6707647  0.01995763 0.25643143]\n",
      " [0.06517302 0.10516528 0.01812149 0.8115402 ]\n",
      " [0.13013211 0.04722134 0.02347878 0.79916775]\n",
      " ...\n",
      " [0.015809   0.9234267  0.00686547 0.05389883]\n",
      " [0.0412978  0.00673648 0.93632    0.01564572]\n",
      " [0.04129631 0.00673626 0.93632245 0.0156451 ]]\n",
      "Iteration 572, Accuracy 0.36034\n",
      "93.12614%change in label assignment\n",
      "0.083635986\n",
      "[[0.0172224  0.91482013 0.00740133 0.06055613]\n",
      " [0.07819678 0.2740309  0.02519863 0.6225737 ]\n",
      " [0.06916335 0.04346594 0.01502558 0.8723451 ]\n",
      " ...\n",
      " [0.02568909 0.89213955 0.01243386 0.06973749]\n",
      " [0.04134534 0.00674228 0.93625593 0.01565649]\n",
      " [0.04134472 0.0067422  0.93625677 0.0156562 ]]\n",
      "Iteration 573, Accuracy 0.37369\n",
      "92.80699%change in label assignment\n",
      "0.085644566\n",
      "[[0.05433247 0.65728235 0.02054071 0.26784444]\n",
      " [0.06007277 0.07586464 0.01593758 0.848125  ]\n",
      " [0.18132384 0.05099328 0.03002155 0.7376613 ]\n",
      " ...\n",
      " [0.01669115 0.9179783  0.00722592 0.0581047 ]\n",
      " [0.0412329  0.00673129 0.936409   0.01562678]\n",
      " [0.0412319  0.00673116 0.9364106  0.01562636]]\n",
      "Iteration 574, Accuracy 0.37811\n",
      "94.3487%change in label assignment\n",
      "0.082795076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02319424 0.9014644  0.01102309 0.06431824]\n",
      " [0.05269731 0.67321014 0.01982301 0.2542695 ]\n",
      " [0.06717328 0.11459772 0.01869141 0.7995376 ]\n",
      " ...\n",
      " [0.05228414 0.79846746 0.02719648 0.12205193]\n",
      " [0.04138162 0.0067458  0.93619543 0.01567712]\n",
      " [0.04138056 0.00674564 0.93619716 0.01567665]]\n",
      "Iteration 575, Accuracy 0.37904\n",
      "83.33088%change in label assignment\n",
      "0.099920854\n",
      "[[0.06819198 0.5221383  0.02480694 0.38486284]\n",
      " [0.05997841 0.06930026 0.01579365 0.85492766]\n",
      " [0.19631277 0.05194727 0.03236588 0.7193741 ]\n",
      " ...\n",
      " [0.02645604 0.85990363 0.01103579 0.10260461]\n",
      " [0.04120174 0.00672499 0.9364317  0.01564163]\n",
      " [0.04120032 0.00672479 0.93643385 0.01564105]]\n",
      "Iteration 576, Accuracy 0.37796\n",
      "85.18682%change in label assignment\n",
      "0.08645314\n",
      "[[0.02843347 0.881956   0.01360718 0.07600337]\n",
      " [0.04549617 0.734885   0.01737344 0.20224544]\n",
      " [0.07183719 0.13531011 0.02015976 0.7726929 ]\n",
      " ...\n",
      " [0.06172828 0.76753926 0.03254788 0.13818453]\n",
      " [0.04141611 0.00674833 0.936132   0.01570355]\n",
      " [0.04141508 0.00674817 0.9361337  0.01570309]]\n",
      "Iteration 577, Accuracy 0.37507\n",
      "85.0297%change in label assignment\n",
      "0.09997105\n",
      "[[0.03648603 0.79472107 0.01470434 0.15408853]\n",
      " [0.07155594 0.14625323 0.02129614 0.76089466]\n",
      " [0.11635553 0.04659751 0.02245838 0.81458855]\n",
      " ...\n",
      " [0.01566838 0.9288151  0.00719794 0.04831853]\n",
      " [0.04114719 0.0067276  0.9364798  0.01564548]\n",
      " [0.04114614 0.00672746 0.9364814  0.01564505]]\n",
      "Iteration 578, Accuracy 0.37836\n",
      "86.50759%change in label assignment\n",
      "0.08244779\n",
      "[[0.03866065 0.7810936  0.01512665 0.16511917]\n",
      " [0.07249696 0.1543436  0.02109142 0.75206804]\n",
      " [0.10678318 0.04522403 0.02010009 0.8278927 ]\n",
      " ...\n",
      " [0.01561308 0.92894405 0.00699748 0.04844543]\n",
      " [0.04124745 0.00673029 0.9363608  0.01566154]\n",
      " [0.04124606 0.00673009 0.93636286 0.01566095]]\n",
      "Iteration 579, Accuracy 0.37438\n",
      "97.82491%change in label assignment\n",
      "0.08167338\n",
      "[[0.04618213 0.7243422  0.01803152 0.21144411]\n",
      " [0.06665929 0.11217155 0.01901282 0.8021564 ]\n",
      " [0.13619049 0.04781448 0.02483163 0.7911634 ]\n",
      " ...\n",
      " [0.01503215 0.92868125 0.00669338 0.04959325]\n",
      " [0.04119536 0.00672396 0.93643576 0.01564495]\n",
      " [0.04119441 0.00672384 0.93643725 0.01564455]]\n",
      "Iteration 580, Accuracy 0.3706\n",
      "96.80365%change in label assignment\n",
      "0.08082523\n",
      "[[0.03213229 0.8243654  0.0128658  0.13063648]\n",
      " [0.07409613 0.17298543 0.0220189  0.7308995 ]\n",
      " [0.09383749 0.04410999 0.01834245 0.84371   ]\n",
      " ...\n",
      " [0.01613786 0.9275758  0.00733323 0.04895306]\n",
      " [0.04124292 0.00672494 0.9363773  0.01565495]\n",
      " [0.04124167 0.00672475 0.9363791  0.01565442]]\n",
      "Iteration 581, Accuracy 0.37733\n",
      "96.99514%change in label assignment\n",
      "0.078705415\n",
      "[[0.0454474  0.73021734 0.01786566 0.20646963]\n",
      " [0.06322865 0.08975837 0.01748867 0.82952434]\n",
      " [0.165511   0.0500925  0.02872781 0.7556687 ]\n",
      " ...\n",
      " [0.01575815 0.92431456 0.00698651 0.05294075]\n",
      " [0.041152   0.00671661 0.93649876 0.01563263]\n",
      " [0.04115161 0.00671658 0.93649936 0.01563245]]\n",
      "Iteration 582, Accuracy 0.3734\n",
      "93.24397%change in label assignment\n",
      "0.081329644\n",
      "[[0.02730435 0.85568017 0.01110808 0.10590737]\n",
      " [0.07490677 0.17986234 0.02230811 0.72292274]\n",
      " [0.09215381 0.04448166 0.01807945 0.84528506]\n",
      " ...\n",
      " [0.01820585 0.9198482  0.00837211 0.05357391]\n",
      " [0.04124721 0.00672165 0.93638664 0.01564449]\n",
      " [0.0412462  0.00672151 0.93638825 0.01564405]]\n",
      "Iteration 583, Accuracy 0.38047\n",
      "94.66785%change in label assignment\n",
      "0.082114816\n",
      "[[0.04849156 0.70667255 0.01897003 0.2258659 ]\n",
      " [0.06358916 0.09125217 0.01771704 0.8274417 ]\n",
      " [0.16032618 0.05000294 0.02834338 0.7613275 ]\n",
      " ...\n",
      " [0.01629945 0.92118484 0.00722454 0.05529115]\n",
      " [0.04111673 0.0067091  0.93656725 0.01560693]\n",
      " [0.04111607 0.00670903 0.9365683  0.01560665]]\n",
      "Iteration 584, Accuracy 0.37531\n",
      "96.15064%change in label assignment\n",
      "0.08458658\n",
      "[[0.0363476  0.796957   0.01434576 0.15234965]\n",
      " [0.06941199 0.12900056 0.01966634 0.7819211 ]\n",
      " [0.11897597 0.04667566 0.02174232 0.81260604]\n",
      " ...\n",
      " [0.01570154 0.92805094 0.00701358 0.04923392]\n",
      " [0.04119968 0.00671357 0.9364639  0.01562279]\n",
      " [0.04119886 0.00671347 0.93646526 0.01562243]]\n",
      "Iteration 585, Accuracy 0.38027\n",
      "98.46811%change in label assignment\n",
      "0.08067863\n",
      "[[0.0584356  0.6230839  0.02227477 0.29620576]\n",
      " [0.0623371  0.08177751 0.01711718 0.8387682 ]\n",
      " [0.17866723 0.05134694 0.0307961  0.7391898 ]\n",
      " ...\n",
      " [0.0188086  0.90647745 0.0082241  0.06648979]\n",
      " [0.04110671 0.00670523 0.9365895  0.01559855]\n",
      " [0.04110594 0.00670514 0.9365908  0.01559822]]\n",
      "Iteration 586, Accuracy 0.37836\n",
      "97.70708%change in label assignment\n",
      "0.08069021\n",
      "[[0.02647533 0.8614347  0.0108106  0.10127937]\n",
      " [0.07446129 0.17113978 0.02194838 0.7324505 ]\n",
      " [0.09548194 0.04519404 0.01855571 0.8407683 ]\n",
      " ...\n",
      " [0.01877489 0.91742224 0.00862593 0.05517694]\n",
      " [0.0411862  0.00671078 0.93648493 0.01561807]\n",
      " [0.04118548 0.00671068 0.9364861  0.01561775]]\n",
      "Iteration 587, Accuracy 0.37944\n",
      "96.63672%change in label assignment\n",
      "0.08343233\n",
      "[[0.07408474 0.43647474 0.0263043  0.46313617]\n",
      " [0.06141788 0.04993564 0.01494628 0.87370014]\n",
      " [0.30668074 0.05470518 0.04350698 0.59510714]\n",
      " ...\n",
      " [0.03229026 0.8223278  0.0133082  0.13207382]\n",
      " [0.04101057 0.00669512 0.9367309  0.01556335]\n",
      " [0.04101004 0.00669508 0.93673176 0.01556313]]\n",
      "Iteration 588, Accuracy 0.37733\n",
      "92.28163%change in label assignment\n",
      "0.08796311\n",
      "[[0.01815837 0.9196023  0.00831301 0.05392637]\n",
      " [0.07336096 0.45809948 0.02537457 0.44316497]\n",
      " [0.06012913 0.0616622  0.01474872 0.86345994]\n",
      " ...\n",
      " [0.04527334 0.82218736 0.02301512 0.10952412]\n",
      " [0.04122353 0.00671468 0.9364366  0.01562526]\n",
      " [0.0412228  0.00671457 0.93643767 0.01562493]]\n",
      "Iteration 589, Accuracy 0.38484\n",
      "86.1197%change in label assignment\n",
      "0.09633696\n",
      "[[0.04825757 0.7082474  0.01895044 0.22454458]\n",
      " [0.06232501 0.08318146 0.01716141 0.8373321 ]\n",
      " [0.15308166 0.04996737 0.02784339 0.7691077 ]\n",
      " ...\n",
      " [0.01690714 0.9176046  0.0074813  0.05800699]\n",
      " [0.04098853 0.00669615 0.93674535 0.01556992]\n",
      " [0.04098703 0.00669594 0.9367478  0.01556931]]\n",
      "Iteration 590, Accuracy 0.38729\n",
      "90.88722%change in label assignment\n",
      "0.08009258\n",
      "[[0.02840791 0.84823954 0.01153112 0.11182143]\n",
      " [0.07357261 0.16907874 0.02179323 0.73555547]\n",
      " [0.08450352 0.04372881 0.01708493 0.8546827 ]\n",
      " ...\n",
      " [0.01790468 0.9208442  0.00823615 0.053015  ]\n",
      " [0.0411005  0.0067017  0.9366108  0.01558706]\n",
      " [0.04109824 0.00670135 0.93661433 0.01558614]]\n",
      "Iteration 591, Accuracy 0.37875\n",
      "94.15722%change in label assignment\n",
      "0.08247107\n",
      "[[0.04651557 0.72020555 0.01821605 0.21506286]\n",
      " [0.0649648  0.10440232 0.01845281 0.8121801 ]\n",
      " [0.12113181 0.04680929 0.02308682 0.80897206]\n",
      " ...\n",
      " [0.0148687  0.92951363 0.00666437 0.04895324]\n",
      " [0.04097637 0.00669295 0.9367714  0.01555928]\n",
      " [0.0409738  0.00669256 0.9367754  0.01555825]]\n",
      "Iteration 592, Accuracy 0.3708\n",
      "97.31919%change in label assignment\n",
      "0.07850621\n",
      "[[0.04042469 0.7668075  0.01581678 0.17695108]\n",
      " [0.0669013  0.11694944 0.018852   0.7972973 ]\n",
      " [0.10744672 0.04527372 0.02038234 0.8268972 ]\n",
      " ...\n",
      " [0.01498122 0.9303685  0.00667306 0.04797725]\n",
      " [0.04102713 0.00669408 0.9367135  0.01556535]\n",
      " [0.04102479 0.00669373 0.9367171  0.01556439]]\n",
      "Iteration 593, Accuracy 0.37026\n",
      "99.13095%change in label assignment\n",
      "0.08154875\n",
      "[[0.04228768 0.7518997  0.01678323 0.18902946]\n",
      " [0.06566425 0.10891084 0.0188006  0.8066243 ]\n",
      " [0.11729584 0.04642172 0.02254476 0.8137377 ]\n",
      " ...\n",
      " [0.01485059 0.9296416  0.00666569 0.04884215]\n",
      " [0.04094625 0.0066871  0.93681806 0.01554866]\n",
      " [0.04094473 0.00668688 0.9368204  0.01554804]]\n",
      "Iteration 594, Accuracy 0.37153\n",
      "97.41739%change in label assignment\n",
      "0.07835996\n",
      "[[0.03935478 0.7734689  0.01550456 0.17167172]\n",
      " [0.0645072  0.10378452 0.01796861 0.81373966]\n",
      " [0.13367796 0.04747896 0.02400512 0.794838  ]\n",
      " ...\n",
      " [0.01486569 0.92926174 0.00655094 0.04932165]\n",
      " [0.04094687 0.00668499 0.93682355 0.01554453]\n",
      " [0.0409462  0.00668491 0.9368247  0.01554423]]\n",
      "Iteration 595, Accuracy 0.37571\n",
      "94.8937%change in label assignment\n",
      "0.07769407\n",
      "[[0.03434319 0.8072351  0.01387159 0.14455004]\n",
      " [0.07081957 0.15034898 0.02114022 0.75769126]\n",
      " [0.10079858 0.0445012  0.01985501 0.8348452 ]\n",
      " ...\n",
      " [0.01491321 0.93190306 0.00683258 0.04635112]\n",
      " [0.04092357 0.00668264 0.9368552  0.01553852]\n",
      " [0.04092248 0.0066825  0.9368569  0.01553806]]\n",
      "Iteration 596, Accuracy 0.38174\n",
      "95.88059%change in label assignment\n",
      "0.08001943\n",
      "[[0.07119823 0.47366658 0.0251962  0.42993903]\n",
      " [0.05700083 0.06093052 0.01452762 0.867541  ]\n",
      " [0.23158337 0.05319243 0.03533814 0.6798861 ]\n",
      " ...\n",
      " [0.02321676 0.8784827  0.00970206 0.08859852]\n",
      " [0.04084529 0.00667402 0.93697643 0.01550428]\n",
      " [0.0408437  0.0066738  0.9369789  0.01550362]]\n",
      "Iteration 597, Accuracy 0.37615\n",
      "94.94771%change in label assignment\n",
      "0.0848558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02208999 0.8852162  0.00927947 0.08341432]\n",
      " [0.07739776 0.2635703  0.02495135 0.6340806 ]\n",
      " [0.06894826 0.04302232 0.01503886 0.8729906 ]\n",
      " ...\n",
      " [0.02485041 0.89502335 0.01204437 0.06808192]\n",
      " [0.04092354 0.00668258 0.9368709  0.01552309]\n",
      " [0.04092174 0.00668232 0.93687356 0.01552234]]\n",
      "Iteration 598, Accuracy 0.38214\n",
      "90.88722%change in label assignment\n",
      "0.084701814\n",
      "[[0.0732246  0.44131368 0.02590733 0.45955434]\n",
      " [0.0575239  0.05688819 0.01461334 0.87097454]\n",
      " [0.24805437 0.05396596 0.0379253  0.6600544 ]\n",
      " ...\n",
      " [0.02751407 0.85176045 0.0114368  0.10928871]\n",
      " [0.04074039 0.00666785 0.93710715 0.01548464]\n",
      " [0.04073866 0.00666762 0.9371098  0.01548395]]\n",
      "Iteration 599, Accuracy 0.36967\n",
      "91.00506%change in label assignment\n",
      "[[0.02207856 0.8864335  0.00921592 0.08227205]\n",
      " [0.07819546 0.31167933 0.02553211 0.5845931 ]\n",
      " [0.06641554 0.04473107 0.01460249 0.8742509 ]\n",
      " ...\n",
      " [0.02959195 0.8774831  0.01443304 0.0784919 ]\n",
      " [0.0408989  0.00668102 0.9369002  0.01551996]\n",
      " [0.04089709 0.00668075 0.936903   0.0155192 ]]\n",
      "Iteration 600, Accuracy 0.38086\n",
      "89.70884%change in label assignment\n",
      "0.088390514\n",
      "[[0.0354902  0.79964244 0.014484   0.15038335]\n",
      " [0.07675156 0.22977199 0.02473447 0.66874206]\n",
      " [0.07413484 0.04388809 0.01641684 0.86556023]\n",
      " ...\n",
      " [0.01671637 0.924793   0.00783473 0.05065591]\n",
      " [0.04080521 0.00667202 0.93702334 0.01549947]\n",
      " [0.04080167 0.0066715  0.9370288  0.01549805]]\n",
      "Iteration 601, Accuracy 0.36441\n",
      "94.61384%change in label assignment\n",
      "0.08142897\n",
      "[[0.03852329 0.78025836 0.01515188 0.16606645]\n",
      " [0.0743405  0.18353936 0.02237356 0.71974653]\n",
      " [0.09955227 0.0449384  0.01923224 0.83627707]\n",
      " ...\n",
      " [0.0179114  0.92071384 0.00825988 0.05311488]\n",
      " [0.04086066 0.00667395 0.9369553  0.01551011]\n",
      " [0.04085891 0.0066737  0.936958   0.01550939]]\n",
      "Iteration 602, Accuracy 0.35405\n",
      "94.71203%change in label assignment\n",
      "0.08189195\n",
      "[[0.04013798 0.7667273  0.01611936 0.17701532]\n",
      " [0.07226442 0.16381425 0.02214048 0.74178094]\n",
      " [0.11326596 0.04614209 0.02207833 0.8185136 ]\n",
      " ...\n",
      " [0.01695562 0.92416    0.00796839 0.05091598]\n",
      " [0.04078687 0.00666788 0.937052   0.01549323]\n",
      " [0.04078573 0.00666774 0.9370538  0.01549275]]\n",
      "Iteration 603, Accuracy 0.36495\n",
      "98.23734%change in label assignment\n",
      "0.0818698\n",
      "[[0.04216857 0.7539696  0.01641212 0.18744971]\n",
      " [0.07492191 0.1922346  0.0227179  0.71012557]\n",
      " [0.10132577 0.04519555 0.01947826 0.8340004 ]\n",
      " ...\n",
      " [0.01772291 0.9213901  0.00816912 0.05271778]\n",
      " [0.04084442 0.00666982 0.9369853  0.01550044]\n",
      " [0.04084242 0.00666953 0.93698835 0.01549961]]\n",
      "Iteration 604, Accuracy 0.36623\n",
      "97.14244%change in label assignment\n",
      "0.08617585\n",
      "[[0.03003585 0.8360697  0.01251474 0.12137971]\n",
      " [0.07805431 0.2987124  0.02624274 0.5969906 ]\n",
      " [0.07220086 0.04400282 0.01617108 0.8676252 ]\n",
      " ...\n",
      " [0.02288291 0.9018104  0.01113857 0.06416804]\n",
      " [0.04079048 0.006667   0.93704414 0.0154983 ]\n",
      " [0.04078885 0.00666679 0.9370467  0.01549763]]\n",
      "Iteration 605, Accuracy 0.36147\n",
      "97.23572%change in label assignment\n",
      "0.085537545\n",
      "[[0.06850186 0.51237136 0.02443094 0.39469588]\n",
      " [0.06131899 0.08612895 0.01650722 0.83604485]\n",
      " [0.19643997 0.05192678 0.03128054 0.7203527 ]\n",
      " ...\n",
      " [0.01563158 0.92467344 0.00682165 0.05287329]\n",
      " [0.04076716 0.00666053 0.93708986 0.01548253]\n",
      " [0.0407653  0.00666028 0.93709266 0.01548176]]\n",
      "Iteration 606, Accuracy 0.3568\n",
      "92.29636%change in label assignment\n",
      "0.08741667\n",
      "[[0.01801303 0.9092999  0.00778105 0.06490602]\n",
      " [0.0759799  0.37648493 0.02614118 0.521394  ]\n",
      " [0.06309827 0.04381069 0.01436442 0.8787266 ]\n",
      " ...\n",
      " [0.03699519 0.85051113 0.01877808 0.09371559]\n",
      " [0.04083383 0.00666893 0.9369978  0.01549945]\n",
      " [0.04083318 0.00666887 0.93699884 0.01549916]]\n",
      "Iteration 607, Accuracy 0.36962\n",
      "92.22271%change in label assignment\n",
      "0.08760648\n",
      "[[0.06356373 0.56440103 0.02342281 0.34861246]\n",
      " [0.06171644 0.09142439 0.01711287 0.82974637]\n",
      " [0.17408173 0.05058116 0.02942427 0.74591285]\n",
      " ...\n",
      " [0.01484805 0.9284109  0.00656945 0.05017154]\n",
      " [0.04072317 0.0066567  0.93715113 0.01546888]\n",
      " [0.04072114 0.00665642 0.9371544  0.01546806]]\n",
      "Iteration 608, Accuracy 0.37453\n",
      "91.92812%change in label assignment\n",
      "0.08381215\n",
      "[[0.01818683 0.9083777  0.00779574 0.06563972]\n",
      " [0.07680197 0.3495074  0.02588665 0.54780406]\n",
      " [0.06075788 0.04513126 0.0139455  0.8801653 ]\n",
      " ...\n",
      " [0.0314128  0.8707232  0.01558755 0.08227646]\n",
      " [0.0408176  0.0066642  0.9370297  0.01548854]\n",
      " [0.04081633 0.00666403 0.9370317  0.01548799]]\n",
      "Iteration 609, Accuracy 0.36918\n",
      "93.2096%change in label assignment\n",
      "0.08677838\n",
      "[[0.05326501 0.6626879  0.02051747 0.2635296 ]\n",
      " [0.06713369 0.12396263 0.01972045 0.78918326]\n",
      " [0.1200219  0.04659048 0.02295052 0.8104371 ]\n",
      " ...\n",
      " [0.01449913 0.9315579  0.00655865 0.0473843 ]\n",
      " [0.04068874 0.00665038 0.93720484 0.01545599]\n",
      " [0.04068672 0.00665011 0.93720794 0.01545517]]\n",
      "Iteration 610, Accuracy 0.36888\n",
      "95.54672%change in label assignment\n",
      "0.08650419\n",
      "[[0.01606292 0.9221527  0.0069892  0.05479523]\n",
      " [0.07390016 0.43278354 0.02556703 0.46774927]\n",
      " [0.05777082 0.05727899 0.01420863 0.87074155]\n",
      " ...\n",
      " [0.03288029 0.8654303  0.01626128 0.08542815]\n",
      " [0.04080083 0.00665975 0.9370543  0.01548518]\n",
      " [0.04079752 0.00665925 0.93705946 0.01548381]]\n",
      "Iteration 611, Accuracy 0.36387\n",
      "91.18181%change in label assignment\n",
      "0.090740055\n",
      "[[0.06914142 0.501503   0.02540963 0.40394598]\n",
      " [0.05957393 0.07110363 0.01612623 0.8531962 ]\n",
      " [0.2005481  0.05290198 0.03386481 0.7126851 ]\n",
      " ...\n",
      " [0.02031666 0.89645773 0.00883504 0.07439061]\n",
      " [0.04061138 0.00664317 0.93731457 0.01543083]\n",
      " [0.0406097  0.00664296 0.9373172  0.01543016]]\n",
      "Iteration 612, Accuracy 0.35916\n",
      "87.13114%change in label assignment\n",
      "0.088564016\n",
      "[[0.01678288 0.92427754 0.00764627 0.05129329]\n",
      " [0.06868876 0.51220155 0.02443798 0.39467165]\n",
      " [0.05878409 0.05738008 0.01433511 0.86950076]\n",
      " ...\n",
      " [0.05208551 0.7984123  0.02709146 0.12241068]\n",
      " [0.04085482 0.006664   0.93698937 0.01549179]\n",
      " [0.04085475 0.00666403 0.9369895  0.01549171]]\n",
      "Iteration 613, Accuracy 0.37178\n",
      "87.54849%change in label assignment\n",
      "0.0918999\n",
      "[[0.04633827 0.71923804 0.01829536 0.21612826]\n",
      " [0.0670052  0.12239411 0.01974143 0.79085934]\n",
      " [0.16629495 0.0506102  0.02936766 0.75372726]\n",
      " ...\n",
      " [0.01709907 0.92364293 0.00806647 0.05119155]\n",
      " [0.04063318 0.00664614 0.93727905 0.01544162]\n",
      " [0.04063355 0.00664629 0.9372784  0.01544174]]\n",
      "Iteration 614, Accuracy 0.38715\n",
      "90.55826%change in label assignment\n",
      "0.080438174\n",
      "[[0.02264469 0.8831812  0.00945119 0.08472287]\n",
      " [0.0781358  0.2820348  0.02517816 0.6146512 ]\n",
      " [0.07736797 0.04340756 0.01614587 0.86307865]\n",
      " ...\n",
      " [0.02785121 0.88372004 0.01349764 0.07493105]\n",
      " [0.04073841 0.00664936 0.93713963 0.01547262]\n",
      " [0.04073803 0.00664934 0.93714017 0.01547242]]\n",
      "Iteration 615, Accuracy 0.37512\n",
      "94.0983%change in label assignment\n",
      "0.08009059\n",
      "[[0.07825487 0.2819444  0.02632523 0.61347544]\n",
      " [0.06080968 0.05131321 0.01521025 0.87266684]\n",
      " [0.3273739  0.05482607 0.04609815 0.57170194]\n",
      " ...\n",
      " [0.03482685 0.80492467 0.01441417 0.14583437]\n",
      " [0.04059472 0.0066295  0.93733597 0.01543981]\n",
      " [0.04059393 0.00662944 0.93733716 0.0154395 ]]\n",
      "Iteration 616, Accuracy 0.37993\n",
      "91.13271%change in label assignment\n",
      "0.09368117\n",
      "[[0.04537855 0.8214421  0.02312714 0.11005216]\n",
      " [0.01748497 0.9157705  0.00757626 0.05916827]\n",
      " [0.07929703 0.30298722 0.02564278 0.59207296]\n",
      " ...\n",
      " [0.09066967 0.67634654 0.0516448  0.18133901]\n",
      " [0.04091932 0.00666453 0.9368781  0.01553809]\n",
      " [0.04091792 0.00666433 0.93688023 0.01553746]]\n",
      "Iteration 617, Accuracy 0.37541\n",
      "72.56837%change in label assignment\n",
      "0.10144435\n",
      "[[0.0163858  0.9192283  0.00720821 0.05717772]\n",
      " [0.07436382 0.4193069  0.02623661 0.48009264]\n",
      " [0.05868249 0.04845276 0.01430691 0.87855786]\n",
      " ...\n",
      " [0.03410629 0.8609037  0.01724654 0.08774349]\n",
      " [0.04063369 0.00664482 0.93724275 0.01547865]\n",
      " [0.04063124 0.00664446 0.9372466  0.01547766]]\n",
      "Iteration 618, Accuracy 0.38621\n",
      "88.992%change in label assignment\n",
      "0.0893914\n",
      "[[0.03246221 0.8200161  0.01312425 0.13439745]\n",
      " [0.07513896 0.20292589 0.02328221 0.69865304]\n",
      " [0.08117203 0.04247385 0.0168236  0.85953045]\n",
      " ...\n",
      " [0.01648757 0.92648894 0.00764448 0.04937898]\n",
      " [0.04064608 0.00664048 0.9372307  0.01548271]\n",
      " [0.0406429  0.00664    0.93723565 0.01548143]]\n",
      "Iteration 619, Accuracy 0.36662\n",
      "95.5369%change in label assignment\n",
      "0.081276245\n",
      "[[0.0283923  0.8458839  0.01174071 0.11398307]\n",
      " [0.07672708 0.23962517 0.02466241 0.6589853 ]\n",
      " [0.06708401 0.04256974 0.01498666 0.87535954]\n",
      " ...\n",
      " [0.01667246 0.925849   0.00782264 0.04965589]\n",
      " [0.04066041 0.00663904 0.93721294 0.01548755]\n",
      " [0.04065416 0.00663807 0.9372228  0.01548505]]\n",
      "Iteration 620, Accuracy 0.35774\n",
      "95.8364%change in label assignment\n",
      "0.08309305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03824608 0.7802808  0.01524471 0.16622843]\n",
      " [0.07264827 0.17115991 0.02206998 0.73412186]\n",
      " [0.07855335 0.04221037 0.01648086 0.86275536]\n",
      " ...\n",
      " [0.01421916 0.9339263  0.00642687 0.04542761]\n",
      " [0.04067608 0.00663719 0.93719405 0.01549266]\n",
      " [0.04066891 0.00663608 0.9372052  0.0154898 ]]\n",
      "Iteration 621, Accuracy 0.35071\n",
      "97.95748%change in label assignment\n",
      "0.0811694\n",
      "[[0.01589041 0.9219735  0.00697965 0.05515641]\n",
      " [0.07763749 0.31040752 0.02600013 0.58595484]\n",
      " [0.05855962 0.0459791  0.01391616 0.88154507]\n",
      " ...\n",
      " [0.02309067 0.9016696  0.01122317 0.06401657]\n",
      " [0.04069484 0.0066384  0.9371683  0.01549847]\n",
      " [0.04069297 0.00663813 0.93717116 0.01549771]]\n",
      "Iteration 622, Accuracy 0.35248\n",
      "93.46983%change in label assignment\n",
      "0.08187499\n",
      "[[0.06800867 0.5151429  0.02465691 0.39219153]\n",
      " [0.05677009 0.04896585 0.01383038 0.8804337 ]\n",
      " [0.27522337 0.05382647 0.03974821 0.63120204]\n",
      " ...\n",
      " [0.03029539 0.8336569  0.01242532 0.12362242]\n",
      " [0.04057633 0.00662449 0.9373409  0.01545825]\n",
      " [0.04057569 0.00662443 0.93734187 0.01545799]]\n",
      "Iteration 623, Accuracy 0.37055\n",
      "83.72367%change in label assignment\n",
      "0.087661706\n",
      "[[0.01970479 0.9144489  0.00934832 0.05649799]\n",
      " [0.06591359 0.5417493  0.02401204 0.36832502]\n",
      " [0.0587008  0.07239873 0.01543578 0.8534647 ]\n",
      " ...\n",
      " [0.04779111 0.8132505  0.024878   0.11408043]\n",
      " [0.04073288 0.00664053 0.9371193  0.01550732]\n",
      " [0.0407321  0.00664043 0.9371205  0.01550697]]\n",
      "Iteration 624, Accuracy 0.38248\n",
      "84.632%change in label assignment\n",
      "0.09204354\n",
      "[[0.02417607 0.8726586  0.01026378 0.09290154]\n",
      " [0.07265753 0.16797866 0.0224664  0.7368974 ]\n",
      " [0.09103995 0.04362009 0.01890226 0.8464377 ]\n",
      " ...\n",
      " [0.01645962 0.9262923  0.00775348 0.04949457]\n",
      " [0.04056171 0.00662491 0.9373415  0.0154718 ]\n",
      " [0.04056071 0.00662479 0.93734306 0.01547139]]\n",
      "Iteration 625, Accuracy 0.38754\n",
      "92.37001%change in label assignment\n",
      "0.08015022\n",
      "[[0.05012637 0.69334656 0.01916328 0.23736383]\n",
      " [0.06111431 0.0845558  0.01647988 0.83785003]\n",
      " [0.17361781 0.05022323 0.02880735 0.74735165]\n",
      " ...\n",
      " [0.0165187  0.9194624  0.00716547 0.0568534 ]\n",
      " [0.04059111 0.00662254 0.93731374 0.01547261]\n",
      " [0.04059033 0.00662245 0.93731487 0.01547228]]\n",
      "Iteration 626, Accuracy 0.37634\n",
      "95.52708%change in label assignment\n",
      "0.078280814\n",
      "[[0.02107773 0.8913007  0.00905683 0.07856476]\n",
      " [0.07534663 0.20655575 0.02397699 0.69412065]\n",
      " [0.08137071 0.04258335 0.01728619 0.85875976]\n",
      " ...\n",
      " [0.01915908 0.9163042  0.00919188 0.05534489]\n",
      " [0.04056909 0.00662119 0.9373431  0.01546658]\n",
      " [0.04056837 0.00662111 0.9373442  0.01546628]]\n",
      "Iteration 627, Accuracy 0.37806\n",
      "95.36505%change in label assignment\n",
      "0.081124485\n",
      "[[0.07224352 0.45604396 0.02558997 0.44612256]\n",
      " [0.0558916  0.05275947 0.01390879 0.87744015]\n",
      " [0.2829238  0.05388561 0.04020565 0.62298495]\n",
      " ...\n",
      " [0.02746799 0.85179114 0.01136448 0.1093764 ]\n",
      " [0.04050021 0.00661041 0.9374566  0.01543272]\n",
      " [0.04049952 0.00661034 0.9374576  0.01543244]]\n",
      "Iteration 628, Accuracy 0.37777\n",
      "92.15888%change in label assignment\n",
      "0.08534649\n",
      "[[0.01410518 0.93401426 0.00637114 0.04550946]\n",
      " [0.07171834 0.46400216 0.02551612 0.43876335]\n",
      " [0.0567238  0.04954868 0.01373474 0.8799928 ]\n",
      " ...\n",
      " [0.04104149 0.83654016 0.02107002 0.1013483 ]\n",
      " [0.04060521 0.00662361 0.93730056 0.01547059]\n",
      " [0.04060444 0.00662353 0.9373019  0.01547025]]\n",
      "Iteration 629, Accuracy 0.38116\n",
      "87.58776%change in label assignment\n",
      "0.0923603\n",
      "[[0.05877994 0.6141675  0.02242375 0.3046288 ]\n",
      " [0.06184284 0.0897384  0.01744409 0.8309747 ]\n",
      " [0.17439091 0.05054842 0.03036593 0.7446947 ]\n",
      " ...\n",
      " [0.01615771 0.9206643  0.00716693 0.05601103]\n",
      " [0.04042928 0.00660745 0.9375358  0.01542742]\n",
      " [0.04042764 0.00660724 0.9375384  0.01542676]]\n",
      "Iteration 630, Accuracy 0.38194\n",
      "92.38474%change in label assignment\n",
      "0.07867422\n",
      "[[0.03527636 0.8013202  0.01415559 0.14924785]\n",
      " [0.07633749 0.22465311 0.0240189  0.6749905 ]\n",
      " [0.08010566 0.04262078 0.01661707 0.86065644]\n",
      " ...\n",
      " [0.019031   0.9168966  0.00897226 0.05510015]\n",
      " [0.04053028 0.0066135  0.937412   0.01544421]\n",
      " [0.04052623 0.00661288 0.93741834 0.01544259]]\n",
      "Iteration 631, Accuracy 0.37394\n",
      "90.10654%change in label assignment\n",
      "0.08339226\n",
      "[[0.05381125 0.65878206 0.020895   0.26651177]\n",
      " [0.06402914 0.10258814 0.0184895  0.8148932 ]\n",
      " [0.15289204 0.04921388 0.0277965  0.77009755]\n",
      " ...\n",
      " [0.01541592 0.92522424 0.00690337 0.05245652]\n",
      " [0.0403754  0.0066016  0.9376151  0.01540794]\n",
      " [0.04037366 0.00660137 0.9376177  0.01540724]]\n",
      "Iteration 632, Accuracy 0.35803\n",
      "91.50587%change in label assignment\n",
      "0.07745823\n",
      "[[0.02740148 0.8531787  0.01128098 0.10813892]\n",
      " [0.07423998 0.18843184 0.02274591 0.71458226]\n",
      " [0.08764383 0.04317795 0.01772901 0.8514492 ]\n",
      " ...\n",
      " [0.0172396  0.9234293  0.0080128  0.05131825]\n",
      " [0.04045389 0.00660491 0.9375201  0.01542114]\n",
      " [0.04045231 0.0066047  0.9375225  0.01542049]]\n",
      "Iteration 633, Accuracy 0.37237\n",
      "96.60726%change in label assignment\n",
      "0.0806115\n",
      "[[0.05280153 0.6669887  0.02053942 0.2596704 ]\n",
      " [0.06252726 0.09483691 0.01782565 0.8248101 ]\n",
      " [0.14505781 0.04849475 0.02669035 0.7797571 ]\n",
      " ...\n",
      " [0.01620859 0.9201817  0.00718643 0.0564233 ]\n",
      " [0.04034181 0.00659447 0.9376699  0.01539388]\n",
      " [0.04034004 0.00659423 0.9376726  0.01539317]]\n",
      "Iteration 634, Accuracy 0.3705\n",
      "96.19973%change in label assignment\n",
      "0.07778602\n",
      "[[0.03135037 0.826923   0.01277934 0.12894724]\n",
      " [0.06967699 0.14482585 0.02069721 0.7647999 ]\n",
      " [0.10794507 0.04477322 0.02074997 0.82653177]\n",
      " ...\n",
      " [0.01451454 0.93305975 0.00660472 0.04582098]\n",
      " [0.04037971 0.00659623 0.93762356 0.01540049]\n",
      " [0.04037888 0.00659614 0.93762475 0.01540014]]\n",
      "Iteration 635, Accuracy 0.3703\n",
      "95.57618%change in label assignment\n",
      "0.07756224\n",
      "[[0.03406116 0.80810297 0.01397299 0.14386283]\n",
      " [0.07078795 0.15504687 0.02166861 0.7524966 ]\n",
      " [0.0995425  0.04409738 0.02011228 0.8362478 ]\n",
      " ...\n",
      " [0.01433218 0.9336377  0.0066028  0.04542734]\n",
      " [0.04031787 0.00659011 0.9377028  0.01538919]\n",
      " [0.04031671 0.00658997 0.9377046  0.01538872]]\n",
      "Iteration 636, Accuracy 0.37467\n",
      "97.22099%change in label assignment\n",
      "0.078817576\n",
      "[[0.06892794 0.50081813 0.02487575 0.40537816]\n",
      " [0.05556565 0.05194026 0.01383253 0.8786615 ]\n",
      " [0.26649064 0.05378361 0.03897329 0.64075243]\n",
      " ...\n",
      " [0.03140182 0.8259424  0.01284018 0.12981567]\n",
      " [0.04023952 0.00658106 0.93782634 0.01535309]\n",
      " [0.04023888 0.006581   0.9378272  0.01535283]]\n",
      "Iteration 637, Accuracy 0.37139\n",
      "90.94123%change in label assignment\n",
      "0.08503054\n",
      "[[0.0141609  0.9326202  0.00635607 0.04686283]\n",
      " [0.07700852 0.32878914 0.0260248  0.5681776 ]\n",
      " [0.06153269 0.04338709 0.01412322 0.88095707]\n",
      " ...\n",
      " [0.0283137  0.882087   0.01403139 0.07556788]\n",
      " [0.04032192 0.00659071 0.9377095  0.01537784]\n",
      " [0.04032184 0.00659075 0.9377096  0.01537778]]\n",
      "Iteration 638, Accuracy 0.38204\n",
      "92.01159%change in label assignment\n",
      "0.082907915\n",
      "[[0.04789629 0.706471   0.01888186 0.22675087]\n",
      " [0.05876466 0.07569193 0.01614906 0.8493944 ]\n",
      " [0.18033767 0.05084528 0.03103669 0.73778033]\n",
      " ...\n",
      " [0.01956347 0.8998926  0.00847718 0.07206668]\n",
      " [0.04014713 0.00657362 0.9379399  0.01533935]\n",
      " [0.04014672 0.00657361 0.9379405  0.01533918]]\n",
      "Iteration 639, Accuracy 0.38636\n",
      "93.37654%change in label assignment\n",
      "[[0.04105967 0.7601388  0.016223   0.18257855]\n",
      " [0.06413607 0.10580025 0.01812951 0.81193423]\n",
      " [0.1441059  0.04827496 0.02551178 0.7821073 ]\n",
      " ...\n",
      " [0.0148768  0.9288387  0.00659732 0.04968726]\n",
      " [0.04018223 0.00657462 0.93790525 0.01533785]\n",
      " [0.04018156 0.00657455 0.9379064  0.01533758]]\n",
      "Iteration 640, Accuracy 0.38008\n",
      "98.09005%change in label assignment\n",
      "0.08076845\n",
      "[[0.01420533 0.933733   0.00653133 0.04553032]\n",
      " [0.07655425 0.34969336 0.02648292 0.5472695 ]\n",
      " [0.0584899  0.04548653 0.01406516 0.8819584 ]\n",
      " ...\n",
      " [0.02732912 0.8854682  0.01361077 0.07359185]\n",
      " [0.04017721 0.00657681 0.937904   0.01534203]\n",
      " [0.04017693 0.00657681 0.93790436 0.01534191]]\n",
      "Iteration 641, Accuracy 0.37988\n",
      "94.43708%change in label assignment\n",
      "0.08148603\n",
      "[[0.0552063  0.64453644 0.02110562 0.2791517 ]\n",
      " [0.05733468 0.07069394 0.01540744 0.8565639 ]\n",
      " [0.19713621 0.0515511  0.03232048 0.71899223]\n",
      " ...\n",
      " [0.01893774 0.90350074 0.00816416 0.06939739]\n",
      " [0.04007921 0.00656477 0.9380427  0.0153133 ]\n",
      " [0.04007826 0.00656466 0.9380442  0.01531292]]\n",
      "Iteration 642, Accuracy 0.38567\n",
      "92.98866%change in label assignment\n",
      "0.082719915\n",
      "[[0.0181226  0.920287   0.00857853 0.0530118 ]\n",
      " [0.06477749 0.55004764 0.02384018 0.3613347 ]\n",
      " [0.05782428 0.0715484  0.01534361 0.85528374]\n",
      " ...\n",
      " [0.03784305 0.8475983  0.01927872 0.0952799 ]\n",
      " [0.04018382 0.00657607 0.93789494 0.01534511]\n",
      " [0.04018284 0.00657595 0.9378965  0.0153447 ]]\n",
      "Iteration 643, Accuracy 0.38209\n",
      "88.12785%change in label assignment\n",
      "0.08929956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03585214 0.79554194 0.01470776 0.15389818]\n",
      " [0.06690382 0.12435453 0.01997676 0.78876495]\n",
      " [0.11595905 0.04595316 0.02276146 0.81532633]\n",
      " ...\n",
      " [0.01514079 0.9265495  0.00680528 0.05150442]\n",
      " [0.04001464 0.00656047 0.938121   0.01530381]\n",
      " [0.04001302 0.00656026 0.9381235  0.01530317]]\n",
      "Iteration 644, Accuracy 0.38469\n",
      "92.66951%change in label assignment\n",
      "0.08684922\n",
      "[[0.02640887 0.8891908  0.01288523 0.07151507]\n",
      " [0.05026182 0.6897971  0.01933871 0.24060233]\n",
      " [0.06761011 0.12602949 0.01954687 0.78681356]\n",
      " ...\n",
      " [0.04166762 0.8342929  0.02133421 0.10270529]\n",
      " [0.04017514 0.00657359 0.9378924  0.0153588 ]\n",
      " [0.04017372 0.00657339 0.9378947  0.01535822]]\n",
      "Iteration 645, Accuracy 0.37811\n",
      "88.24569%change in label assignment\n",
      "0.089422226\n",
      "[[0.03139817 0.82586783 0.01307619 0.1296578 ]\n",
      " [0.06818265 0.13268258 0.02058676 0.778548  ]\n",
      " [0.10188022 0.04474328 0.02083104 0.83254546]\n",
      " ...\n",
      " [0.01549351 0.9245144  0.00695061 0.05304144]\n",
      " [0.03997016 0.00655561 0.9381725  0.01530175]\n",
      " [0.03996874 0.00655543 0.93817466 0.01530119]]\n",
      "Iteration 646, Accuracy 0.38459\n",
      "89.3406%change in label assignment\n",
      "0.086793475\n",
      "[[0.01914654 0.91616476 0.00900693 0.05568182]\n",
      " [0.0670967  0.5291771  0.0242898  0.37943634]\n",
      " [0.05928764 0.07467001 0.01566364 0.85037863]\n",
      " ...\n",
      " [0.03379717 0.8621394  0.01686696 0.08719644]\n",
      " [0.04013724 0.00656879 0.9379523  0.0153417 ]\n",
      " [0.04013599 0.00656862 0.9379542  0.01534119]]\n",
      "Iteration 647, Accuracy 0.37713\n",
      "93.25379%change in label assignment\n",
      "0.086855754\n",
      "[[0.04935662 0.695195   0.01958866 0.23585965]\n",
      " [0.05999383 0.07979326 0.01682331 0.84338963]\n",
      " [0.1633048  0.05024623 0.02961597 0.756833  ]\n",
      " ...\n",
      " [0.02117428 0.8906345  0.00921373 0.0789775 ]\n",
      " [0.03995823 0.00655156 0.93820095 0.01528924]\n",
      " [0.03995716 0.00655144 0.93820256 0.01528883]]\n",
      "Iteration 648, Accuracy 0.38351\n",
      "89.60082%change in label assignment\n",
      "0.0855012\n",
      "[[0.01811488 0.91992164 0.00847529 0.05348824]\n",
      " [0.06684181 0.5310107  0.02424008 0.37790743]\n",
      " [0.05925703 0.07518935 0.0157004  0.8498532 ]\n",
      " ...\n",
      " [0.03410709 0.8609101  0.01705522 0.08792756]\n",
      " [0.04015964 0.00656722 0.9379392  0.01533391]\n",
      " [0.04015836 0.00656704 0.93794125 0.01533338]]\n",
      "Iteration 649, Accuracy 0.3816\n",
      "88.44209%change in label assignment\n",
      "0.089092344\n",
      "[[0.03601593 0.7945116  0.01486914 0.15460338]\n",
      " [0.06973092 0.14566186 0.02145986 0.7631474 ]\n",
      " [0.08901583 0.04397098 0.01907573 0.84793746]\n",
      " ...\n",
      " [0.01689907 0.91616666 0.00754069 0.05939353]\n",
      " [0.03997559 0.00655027 0.9381879  0.01528624]\n",
      " [0.03997386 0.00655002 0.9381906  0.01528556]]\n",
      "Iteration 650, Accuracy 0.38057\n",
      "92.15397%change in label assignment\n",
      "0.08676532\n",
      "[[0.01542611 0.9277187  0.00690393 0.04995124]\n",
      " [0.07746541 0.3449977  0.02600214 0.5515348 ]\n",
      " [0.05860987 0.04926554 0.01392928 0.8781953 ]\n",
      " ...\n",
      " [0.02438422 0.8964333  0.01174854 0.06743395]\n",
      " [0.04010389 0.00656013 0.9380164  0.01531956]\n",
      " [0.04010287 0.00656    0.93801796 0.01531914]]\n",
      "Iteration 651, Accuracy 0.37251\n",
      "93.49438%change in label assignment\n",
      "0.08603911\n",
      "[[0.05378471 0.6571719  0.02112707 0.26791632]\n",
      " [0.0594032  0.0771009  0.01662022 0.84687567]\n",
      " [0.16703568 0.05066967 0.03025092 0.75204366]\n",
      " ...\n",
      " [0.02212783 0.88472885 0.00961942 0.08352392]\n",
      " [0.03994871 0.00654555 0.9382355  0.01527018]\n",
      " [0.0399477  0.00654544 0.9382371  0.01526978]]\n",
      "Iteration 652, Accuracy 0.38008\n",
      "92.96902%change in label assignment\n",
      "0.08442144\n",
      "[[0.02731988 0.8854176  0.01330989 0.07395265]\n",
      " [0.0563732  0.6386713  0.02123568 0.28371987]\n",
      " [0.06497277 0.1063586  0.01821334 0.81045526]\n",
      " ...\n",
      " [0.04644549 0.8173958  0.02400179 0.11215691]\n",
      " [0.04016847 0.00656468 0.9379397  0.01532706]\n",
      " [0.04016759 0.00656456 0.9379412  0.01532669]]\n",
      "Iteration 653, Accuracy 0.38047\n",
      "88.44209%change in label assignment\n",
      "0.09352109\n",
      "[[0.02417019 0.8717027  0.0103683  0.09375881]\n",
      " [0.07285044 0.18024193 0.02311036 0.7237972 ]\n",
      " [0.08554129 0.04375201 0.01855687 0.8521498 ]\n",
      " ...\n",
      " [0.01476997 0.9314488  0.0068768  0.0469044 ]\n",
      " [0.03993316 0.00654697 0.93824655 0.01527337]\n",
      " [0.03993202 0.00654682 0.93824834 0.01527292]]\n",
      "Iteration 654, Accuracy 0.38695\n",
      "92.5615%change in label assignment\n",
      "0.08146606\n",
      "[[0.01948548 0.91437966 0.00910736 0.0570275 ]\n",
      " [0.0658399  0.54752684 0.02393057 0.36270267]\n",
      " [0.06079255 0.08155543 0.01629991 0.8413521 ]\n",
      " ...\n",
      " [0.03111873 0.8715854  0.0153197  0.08197616]\n",
      " [0.04007347 0.0065557  0.93805254 0.01531829]\n",
      " [0.04007191 0.00655547 0.9380549  0.01531766]]\n",
      "Iteration 655, Accuracy 0.37448\n",
      "90.04272%change in label assignment\n",
      "0.086358376\n",
      "[[0.06395941 0.5650245  0.0246091  0.3464069 ]\n",
      " [0.05960323 0.0653441  0.0162878  0.8587648 ]\n",
      " [0.18193452 0.05224782 0.03304447 0.7327731 ]\n",
      " ...\n",
      " [0.04217874 0.75345236 0.01739159 0.18697733]\n",
      " [0.03989666 0.00653372 0.93829036 0.0152793 ]\n",
      " [0.03989553 0.00653358 0.938292   0.01527886]]\n",
      "Iteration 656, Accuracy 0.37438\n",
      "86.60578%change in label assignment\n",
      "0.09781883\n",
      "[[0.07679196 0.71858865 0.04250641 0.16211297]\n",
      " [0.01861724 0.9171084  0.00858044 0.05569392]\n",
      " [0.06279442 0.585226   0.02308212 0.32889745]\n",
      " ...\n",
      " [0.09272309 0.6700071  0.05329981 0.18397003]\n",
      " [0.04024339 0.00657208 0.93779683 0.01538767]\n",
      " [0.04024195 0.00657186 0.93779916 0.01538707]]\n",
      "Iteration 657, Accuracy 0.379\n",
      "73.55526%change in label assignment\n",
      "0.10296594\n",
      "[[0.02156013 0.90710473 0.01054377 0.06079145]\n",
      " [0.06900361 0.49920204 0.02550065 0.40629363]\n",
      " [0.05757527 0.06729809 0.01564705 0.8594796 ]\n",
      " ...\n",
      " [0.03422516 0.8602534  0.01750148 0.08802   ]\n",
      " [0.03993448 0.00654967 0.9381972  0.01531868]\n",
      " [0.0399338  0.00654959 0.9381982  0.01531841]]\n",
      "Iteration 658, Accuracy 0.39107\n",
      "80.54205%change in label assignment\n",
      "0.08662474\n",
      "[[0.01780165 0.9105717  0.00770532 0.06392137]\n",
      " [0.06950159 0.14588356 0.02084927 0.7637656 ]\n",
      " [0.09333624 0.04290118 0.01891555 0.844847  ]\n",
      " ...\n",
      " [0.01408339 0.93446916 0.00640774 0.04503974]\n",
      " [0.03993464 0.00654235 0.93820894 0.01531409]\n",
      " [0.03993426 0.00654232 0.9382095  0.01531393]]\n",
      "Iteration 659, Accuracy 0.38749\n",
      "91.71208%change in label assignment\n",
      "0.077211335\n",
      "[[0.0213456  0.8889891  0.0091195  0.08054582]\n",
      " [0.06869289 0.14080985 0.02066182 0.7698354 ]\n",
      " [0.0936444  0.04296353 0.01908935 0.8443027 ]\n",
      " ...\n",
      " [0.01384529 0.93449533 0.00627295 0.0453865 ]\n",
      " [0.03995017 0.0065389  0.93820214 0.01530882]\n",
      " [0.03994963 0.00653884 0.9382029  0.01530859]]\n",
      "Iteration 660, Accuracy 0.38003\n",
      "98.47302%change in label assignment\n",
      "0.07715692\n",
      "[[0.02528327 0.86479115 0.01064397 0.09928159]\n",
      " [0.06052083 0.08878514 0.01702369 0.8336703 ]\n",
      " [0.13407686 0.04686911 0.02482031 0.7942337 ]\n",
      " ...\n",
      " [0.01512902 0.926001   0.006717   0.05215297]\n",
      " [0.0399283  0.00653383 0.938237   0.01530082]\n",
      " [0.03992841 0.00653389 0.9382368  0.01530086]]\n",
      "Iteration 661, Accuracy 0.38012\n",
      "94.59911%change in label assignment\n",
      "0.07537711\n",
      "[[0.01472769 0.9329166  0.00682869 0.04552706]\n",
      " [0.07724144 0.29868242 0.02581417 0.598262  ]\n",
      " [0.05584457 0.04760056 0.01362261 0.8829322 ]\n",
      " ...\n",
      " [0.02136993 0.9081339  0.0103525  0.0601436 ]\n",
      " [0.03998066 0.0065367  0.93817097 0.01531164]\n",
      " [0.03998018 0.00653665 0.9381717  0.01531144]]\n",
      "Iteration 662, Accuracy 0.38204\n",
      "91.20145%change in label assignment\n",
      "0.07972327\n",
      "[[0.05353506 0.6586786  0.02084233 0.26694402]\n",
      " [0.05609131 0.06390029 0.01504627 0.86496216]\n",
      " [0.17469083 0.05017534 0.03045354 0.7446802 ]\n",
      " ...\n",
      " [0.02729147 0.85197216 0.01151222 0.10922413]\n",
      " [0.03984176 0.00652237 0.938366   0.0152698 ]\n",
      " [0.03984069 0.00652223 0.9383677  0.01526938]]\n",
      "Iteration 663, Accuracy 0.38749\n",
      "92.22762%change in label assignment\n",
      "0.08458791\n",
      "[[0.02088184 0.9099013  0.01004986 0.05916698]\n",
      " [0.06956607 0.492829   0.02514039 0.4124646 ]\n",
      " [0.05917378 0.07967    0.01611388 0.84504235]\n",
      " ...\n",
      " [0.03541881 0.8563286  0.01796131 0.09029136]\n",
      " [0.03996433 0.0065349  0.9381915  0.01530931]\n",
      " [0.0399634  0.00653476 0.9381929  0.01530893]]\n",
      "Iteration 664, Accuracy 0.38302\n",
      "86.92493%change in label assignment\n",
      "0.09047208\n",
      "[[0.0227649  0.88062793 0.00984596 0.08676123]\n",
      " [0.0665416  0.12120453 0.0200201  0.79223377]\n",
      " [0.09328974 0.04383647 0.01975652 0.8431173 ]\n",
      " ...\n",
      " [0.01512986 0.9267838  0.00686801 0.05121828]\n",
      " [0.0397801  0.00651862 0.938433   0.01526823]\n",
      " [0.03977952 0.00651855 0.93843395 0.015268  ]]\n",
      "Iteration 665, Accuracy 0.38337\n",
      "89.88069%change in label assignment\n",
      "0.07917237\n",
      "[[0.01605307 0.9229253  0.00708037 0.05394123]\n",
      " [0.07543897 0.20266    0.02343948 0.69846153]\n",
      " [0.0650673  0.04332383 0.01455355 0.8770553 ]\n",
      " ...\n",
      " [0.01662668 0.9253008  0.00768558 0.05038698]\n",
      " [0.03988807 0.00652543 0.93829507 0.01529146]\n",
      " [0.03988732 0.00652533 0.9382962  0.01529116]]\n",
      "Iteration 666, Accuracy 0.37865\n",
      "96.02298%change in label assignment\n",
      "0.07824716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03863907 0.77625364 0.01590636 0.1692009 ]\n",
      " [0.06080659 0.08615948 0.01740505 0.83562887]\n",
      " [0.11427027 0.04587777 0.02299942 0.8168525 ]\n",
      " ...\n",
      " [0.02144211 0.8888948  0.00936902 0.08029405]\n",
      " [0.03975948 0.00651233 0.9384743  0.0152538 ]\n",
      " [0.03975836 0.00651218 0.938476   0.01525336]]\n",
      "Iteration 667, Accuracy 0.37718\n",
      "94.86424%change in label assignment\n",
      "0.08485504\n",
      "[[0.01542801 0.9269243  0.00688649 0.05076123]\n",
      " [0.07796405 0.3091936  0.02585901 0.58698326]\n",
      " [0.05686089 0.05354168 0.01404507 0.8755524 ]\n",
      " ...\n",
      " [0.02205551 0.90518206 0.01057322 0.06218925]\n",
      " [0.03992703 0.00652672 0.93825835 0.0152879 ]\n",
      " [0.03992357 0.00652619 0.9382637  0.01528652]]\n",
      "Iteration 668, Accuracy 0.37575\n",
      "89.40443%change in label assignment\n",
      "0.08894014\n",
      "[[0.05258926 0.6688646  0.02094235 0.25760385]\n",
      " [0.06089729 0.08356172 0.01745504 0.8380859 ]\n",
      " [0.11930139 0.04714478 0.02422394 0.80932987]\n",
      " ...\n",
      " [0.02423123 0.8723254  0.01055062 0.09289271]\n",
      " [0.03974627 0.00651182 0.938498   0.01524398]\n",
      " [0.03974149 0.0065111  0.93850535 0.01524211]]\n",
      "Iteration 669, Accuracy 0.3624\n",
      "93.63677%change in label assignment\n",
      "0.09112381\n",
      "[[0.02965206 0.87700975 0.01459874 0.07873948]\n",
      " [0.05057683 0.6897904  0.01947451 0.24015819]\n",
      " [0.07190971 0.15860362 0.0215509  0.7479358 ]\n",
      " ...\n",
      " [0.05282038 0.79594105 0.02780055 0.123438  ]\n",
      " [0.0399623  0.00653188 0.9381928  0.0153131 ]\n",
      " [0.03995914 0.00653139 0.9381976  0.01531182]]\n",
      "Iteration 670, Accuracy 0.36662\n",
      "85.95768%change in label assignment\n",
      "0.091737725\n",
      "[[0.02231183 0.88311654 0.00966009 0.08491159]\n",
      " [0.07312281 0.1836847  0.02333987 0.7198526 ]\n",
      " [0.06777071 0.04305254 0.01581197 0.87336475]\n",
      " ...\n",
      " [0.01461849 0.9325447  0.00684473 0.04599206]\n",
      " [0.03970068 0.00651223 0.9385345  0.01525256]\n",
      " [0.0396978  0.0065118  0.9385389  0.01525143]]\n",
      "Iteration 671, Accuracy 0.37551\n",
      "90.00344%change in label assignment\n",
      "0.08181176\n",
      "[[0.02114737 0.8907374  0.00900808 0.0791071 ]\n",
      " [0.0703609  0.15426682 0.02126227 0.75411   ]\n",
      " [0.08011398 0.04178093 0.0168709  0.8612342 ]\n",
      " ...\n",
      " [0.0142605  0.9329743  0.00644922 0.046316  ]\n",
      " [0.03975848 0.00651189 0.9384752  0.01525446]\n",
      " [0.03975755 0.00651177 0.9384765  0.01525408]]\n",
      "Iteration 672, Accuracy 0.36549\n",
      "92.19816%change in label assignment\n",
      "0.074537836\n",
      "[[0.02004356 0.89640296 0.00871757 0.0748359 ]\n",
      " [0.06967612 0.15172228 0.02154199 0.75705963]\n",
      " [0.079245   0.04202306 0.01727479 0.8614571 ]\n",
      " ...\n",
      " [0.01392845 0.9337673  0.00637693 0.04592726]\n",
      " [0.03972694 0.00650617 0.93852645 0.01524035]\n",
      " [0.03972652 0.00650613 0.9385272  0.01524018]]\n",
      "Iteration 673, Accuracy 0.37713\n",
      "98.50248%change in label assignment\n",
      "0.077799\n",
      "[[0.01722746 0.9136595  0.00751648 0.06159652]\n",
      " [0.07448439 0.20645744 0.02361809 0.69544005]\n",
      " [0.06707831 0.04139521 0.01494472 0.87658167]\n",
      " ...\n",
      " [0.01475046 0.9327419  0.00684816 0.0456595 ]\n",
      " [0.03974568 0.00650545 0.9385     0.01524894]\n",
      " [0.03974497 0.00650537 0.93850106 0.01524865]]\n",
      "Iteration 674, Accuracy 0.3785\n",
      "98.36009%change in label assignment\n",
      "0.07480028\n",
      "[[0.03376102 0.80879444 0.01391546 0.1435291 ]\n",
      " [0.06659014 0.1282812  0.0199789  0.78514975]\n",
      " [0.08957881 0.04253516 0.01865268 0.8492333 ]\n",
      " ...\n",
      " [0.01433474 0.9305292  0.0064556  0.04868044]\n",
      " [0.03965977 0.00649853 0.9386167  0.01522495]\n",
      " [0.03965783 0.00649825 0.93861973 0.01522418]]\n",
      "Iteration 675, Accuracy 0.37634\n",
      "94.86424%change in label assignment\n",
      "0.07957845\n",
      "[[0.01527204 0.9248166  0.00679212 0.05311925]\n",
      " [0.07626032 0.34656304 0.02633579 0.55084085]\n",
      " [0.05431441 0.05170513 0.0138364  0.8801441 ]\n",
      " ...\n",
      " [0.02266617 0.90308076 0.01111741 0.06313565]\n",
      " [0.03971601 0.00650491 0.9385321  0.01524692]\n",
      " [0.03970999 0.00650398 0.9385415  0.01524453]]\n",
      "Iteration 676, Accuracy 0.37261\n",
      "88.8938%change in label assignment\n",
      "0.08554648\n",
      "[[0.06253103 0.57038987 0.02367894 0.3434002 ]\n",
      " [0.05595224 0.06610087 0.0151923  0.8627546 ]\n",
      " [0.14855318 0.04844357 0.02741134 0.77559197]\n",
      " ...\n",
      " [0.02698667 0.8533641  0.01141636 0.1082329 ]\n",
      " [0.03956066 0.00649296 0.9387422  0.01520417]\n",
      " [0.03955531 0.00649214 0.9387505  0.01520206]]\n",
      "Iteration 677, Accuracy 0.35361\n",
      "91.10817%change in label assignment\n",
      "0.09115875\n",
      "[[0.01474777 0.9323951  0.00681276 0.04604442]\n",
      " [0.07201491 0.45204407 0.02570025 0.4502407 ]\n",
      " [0.0561198  0.06558835 0.01483875 0.8634531 ]\n",
      " ...\n",
      " [0.03305595 0.8645917  0.01667037 0.08568193]\n",
      " [0.03972339 0.00650753 0.938527   0.01524212]\n",
      " [0.03972046 0.00650709 0.9385316  0.01524095]]\n",
      "Iteration 678, Accuracy 0.36441\n",
      "89.76285%change in label assignment\n",
      "0.08824707\n",
      "[[0.05946327 0.6022337  0.02294812 0.31535485]\n",
      " [0.05651147 0.06624864 0.01547387 0.86176604]\n",
      " [0.16639239 0.05025971 0.0302898  0.75305814]\n",
      " ...\n",
      " [0.02797009 0.8473032  0.01188795 0.11283869]\n",
      " [0.03950165 0.00648992 0.9388164  0.01519197]\n",
      " [0.03949964 0.00648963 0.93881947 0.01519119]]\n",
      "Iteration 679, Accuracy 0.36348\n",
      "87.82344%change in label assignment\n",
      "[[0.01516722 0.9279872  0.00678652 0.05005905]\n",
      " [0.07669361 0.35882035 0.02612939 0.53835666]\n",
      " [0.05672373 0.04970379 0.01379217 0.8797803 ]\n",
      " ...\n",
      " [0.02782987 0.8836013  0.01369489 0.07487398]\n",
      " [0.03968431 0.0065053  0.9385777  0.01523272]\n",
      " [0.03968246 0.00650503 0.93858063 0.01523197]]\n",
      "Iteration 680, Accuracy 0.37639\n",
      "93.26852%change in label assignment\n",
      "0.08557754\n",
      "[[0.045428   0.7244125  0.01843145 0.211728  ]\n",
      " [0.06078286 0.08873597 0.0176235  0.83285767]\n",
      " [0.14480413 0.04886148 0.02760907 0.7787254 ]\n",
      " ...\n",
      " [0.01516092 0.92661464 0.00694267 0.05128177]\n",
      " [0.03956272 0.00649208 0.93874854 0.01519664]\n",
      " [0.0395615  0.00649193 0.93875045 0.01519617]]\n",
      "Iteration 681, Accuracy 0.37011\n",
      "92.20798%change in label assignment\n",
      "0.082335785\n",
      "[[0.0190252  0.9163433  0.00899838 0.05563316]\n",
      " [0.06665347 0.5310322  0.02433367 0.3779807 ]\n",
      " [0.0585653  0.07391278 0.01561956 0.85190237]\n",
      " ...\n",
      " [0.04207812 0.8323902  0.02163175 0.10389999]\n",
      " [0.03976572 0.00650863 0.93848026 0.01524547]\n",
      " [0.03976446 0.00650845 0.93848217 0.01524496]]\n",
      "Iteration 682, Accuracy 0.37821\n",
      "89.62537%change in label assignment\n",
      "0.08963952\n",
      "[[0.04164482 0.75233376 0.01699524 0.1890262 ]\n",
      " [0.06210592 0.09912083 0.01823659 0.8205367 ]\n",
      " [0.12328508 0.0466971  0.02433874 0.805679  ]\n",
      " ...\n",
      " [0.01502189 0.9267397  0.00682475 0.05141371]\n",
      " [0.03956264 0.00649119 0.9387515  0.01519467]\n",
      " [0.0395609  0.00649095 0.93875414 0.015194  ]]\n",
      "Iteration 683, Accuracy 0.37566\n",
      "91.11798%change in label assignment\n",
      "0.08425585\n",
      "[[0.01589936 0.92786485 0.00736497 0.04887086]\n",
      " [0.07030632 0.4816253  0.02524341 0.422825  ]\n",
      " [0.05680377 0.06650537 0.01492682 0.861764  ]\n",
      " ...\n",
      " [0.03483392 0.8580368  0.01757439 0.0895548 ]\n",
      " [0.03972994 0.00650543 0.9385278  0.01523673]\n",
      " [0.03972762 0.00650509 0.9385315  0.01523579]]\n",
      "Iteration 684, Accuracy 0.3735\n",
      "90.7792%change in label assignment\n",
      "0.0924202\n",
      "[[0.02806064 0.84664404 0.01199744 0.11329794]\n",
      " [0.07162036 0.17058876 0.02286142 0.7349294 ]\n",
      " [0.07443118 0.04319236 0.0170915  0.8652849 ]\n",
      " ...\n",
      " [0.01461704 0.93039376 0.00676875 0.04822046]\n",
      " [0.03957195 0.00649205 0.9387366  0.01519938]\n",
      " [0.03956962 0.00649171 0.9387403  0.01519846]]\n",
      "Iteration 685, Accuracy 0.36201\n",
      "94.07866%change in label assignment\n",
      "0.08667137\n",
      "[[0.01710436 0.9154103  0.00747205 0.06001327]\n",
      " [0.0763145  0.23898451 0.02448973 0.66021127]\n",
      " [0.06312648 0.0429087  0.01433217 0.87963265]\n",
      " ...\n",
      " [0.02020164 0.9119852  0.00963463 0.05817858]\n",
      " [0.03969543 0.00649876 0.9385799  0.0152259 ]\n",
      " [0.03969418 0.00649858 0.9385819  0.01522539]]\n",
      "Iteration 686, Accuracy 0.35813\n",
      "94.45181%change in label assignment\n",
      "0.08443706\n",
      "[[0.02197842 0.8844343  0.00958324 0.08400405]\n",
      " [0.07549203 0.24023925 0.02521971 0.65904903]\n",
      " [0.06149836 0.04372556 0.0148897  0.8798864 ]\n",
      " ...\n",
      " [0.01520901 0.93040687 0.00722246 0.04716165]\n",
      " [0.03962709 0.00649298 0.93867356 0.01520633]\n",
      " [0.03962469 0.00649263 0.9386773  0.01520539]]\n",
      "Iteration 687, Accuracy 0.36947\n",
      "94.41253%change in label assignment\n",
      "0.084523305\n",
      "[[0.02505508 0.8665286  0.01052514 0.0978912 ]\n",
      " [0.07608048 0.24274743 0.02457522 0.6565969 ]\n",
      " [0.06296638 0.0424013  0.01433279 0.8802995 ]\n",
      " ...\n",
      " [0.01555478 0.9291802  0.00720998 0.04805504]\n",
      " [0.03971111 0.00649729 0.93856907 0.01522259]\n",
      " [0.03970817 0.00649685 0.9385736  0.01522142]]\n",
      "Iteration 688, Accuracy 0.35523\n",
      "99.13095%change in label assignment\n",
      "0.08343561\n",
      "[[0.03351495 0.8094219  0.01403366 0.14302948]\n",
      " [0.07057136 0.16584693 0.02238315 0.7411986 ]\n",
      " [0.08241018 0.04280466 0.01813183 0.85665333]\n",
      " ...\n",
      " [0.01410837 0.93338555 0.00656336 0.04594274]\n",
      " [0.03962692 0.00649068 0.9386869  0.01519553]\n",
      " [0.03962496 0.00649041 0.93868995 0.01519476]]\n",
      "Iteration 689, Accuracy 0.3572\n",
      "95.90514%change in label assignment\n",
      "0.08470145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01602505 0.9206983  0.00707083 0.05620581]\n",
      " [0.07648104 0.2859768  0.02548322 0.61205894]\n",
      " [0.06123469 0.04233264 0.01413322 0.88229936]\n",
      " ...\n",
      " [0.02389673 0.89809316 0.01169229 0.06631783]\n",
      " [0.03971641 0.00649568 0.9385695  0.01521839]\n",
      " [0.03971594 0.00649564 0.93857014 0.01521819]]\n",
      "Iteration 690, Accuracy 0.36456\n",
      "93.18996%change in label assignment\n",
      "0.084631056\n",
      "[[0.05354938 0.6535417  0.02107172 0.27183717]\n",
      " [0.06018381 0.09228365 0.0175083  0.83002424]\n",
      " [0.14819016 0.04881833 0.02775664 0.7752349 ]\n",
      " ...\n",
      " [0.01549153 0.9231005  0.00697216 0.05443585]\n",
      " [0.03960819 0.0064834  0.93873066 0.0151777 ]\n",
      " [0.03960727 0.00648331 0.938732   0.01517734]]\n",
      "Iteration 691, Accuracy 0.37605\n",
      "94.55492%change in label assignment\n",
      "0.08683005\n",
      "[[0.01499151 0.9273019  0.00669972 0.05100686]\n",
      " [0.07624913 0.33243868 0.02596823 0.565344  ]\n",
      " [0.05644554 0.04608597 0.0136101  0.88385844]\n",
      " ...\n",
      " [0.02755904 0.8842824  0.01366275 0.07449577]\n",
      " [0.03975901 0.00649559 0.9385334  0.015212  ]\n",
      " [0.03975816 0.00649549 0.9385347  0.01521166]]\n",
      "Iteration 692, Accuracy 0.37325\n",
      "94.11794%change in label assignment\n",
      "0.084932745\n",
      "[[0.07096013 0.4545028  0.02614262 0.44839442]\n",
      " [0.05548997 0.04837944 0.01420853 0.8819221 ]\n",
      " [0.2557904  0.05433284 0.04061068 0.64926606]\n",
      " ...\n",
      " [0.029547   0.83571255 0.01254294 0.1221976 ]\n",
      " [0.03954878 0.00647676 0.9388204  0.01515398]\n",
      " [0.03954769 0.00647663 0.93882215 0.01515355]]\n",
      "Iteration 693, Accuracy 0.37286\n",
      "90.22929%change in label assignment\n",
      "0.08973802\n",
      "[[0.01672704 0.9249122  0.00787912 0.05048165]\n",
      " [0.0667465  0.5205222  0.02450266 0.38822868]\n",
      " [0.05672913 0.07024898 0.01519772 0.85782415]\n",
      " ...\n",
      " [0.04370445 0.8262302  0.02274281 0.10732251]\n",
      " [0.03979499 0.00649977 0.9384825  0.01522269]\n",
      " [0.03979392 0.00649963 0.9384842  0.01522225]]\n",
      "Iteration 694, Accuracy 0.38111\n",
      "86.52723%change in label assignment\n",
      "0.09474265\n",
      "[[0.07524896 0.36538032 0.02698211 0.5323886 ]\n",
      " [0.05913652 0.04604824 0.01485173 0.8799635 ]\n",
      " [0.31749147 0.05508479 0.04694453 0.5804792 ]\n",
      " ...\n",
      " [0.03062459 0.82914585 0.01305572 0.12717386]\n",
      " [0.03953885 0.00647556 0.93882865 0.01515692]\n",
      " [0.03953786 0.00647546 0.9388302  0.01515654]]\n",
      "Iteration 695, Accuracy 0.37227\n",
      "83.85133%change in label assignment\n",
      "0.08817132\n",
      "[[0.04876601 0.8088398  0.02544077 0.11695349]\n",
      " [0.0268204  0.85745394 0.01120104 0.10452466]\n",
      " [0.07520369 0.20443584 0.0235093  0.6968512 ]\n",
      " ...\n",
      " [0.08884024 0.6805106  0.05101314 0.17963605]\n",
      " [0.03981596 0.00650654 0.9384276  0.01524989]\n",
      " [0.03981562 0.00650652 0.9384281  0.01524973]]\n",
      "Iteration 696, Accuracy 0.38194\n",
      "81.9021%change in label assignment\n",
      "0.10288887\n",
      "[[0.01388043 0.9328692  0.00631808 0.04693221]\n",
      " [0.07576147 0.33420104 0.02636038 0.5636771 ]\n",
      " [0.05905375 0.04283484 0.01424724 0.88386416]\n",
      " ...\n",
      " [0.03406304 0.86047626 0.01749421 0.08796648]\n",
      " [0.03957875 0.00648682 0.93872523 0.01520924]\n",
      " [0.03957784 0.00648672 0.93872654 0.01520887]]\n",
      "Iteration 697, Accuracy 0.39672\n",
      "91.58934%change in label assignment\n",
      "0.085898116\n",
      "[[0.03249378 0.8164263  0.01342963 0.1376503 ]\n",
      " [0.0716674  0.18057057 0.02257909 0.725183  ]\n",
      " [0.08263491 0.04171155 0.01760922 0.8580443 ]\n",
      " ...\n",
      " [0.0157676  0.9290544  0.00746543 0.04771266]\n",
      " [0.03958524 0.00648002 0.9387303  0.01520453]\n",
      " [0.0395833  0.00647976 0.9387332  0.01520376]]\n",
      "Iteration 698, Accuracy 0.37826\n",
      "94.85442%change in label assignment\n",
      "0.082335934\n",
      "[[0.03412434 0.8048541  0.01411769 0.14690389]\n",
      " [0.06972798 0.1609451  0.02180301 0.7475239 ]\n",
      " [0.09259696 0.04278175 0.01933576 0.84528553]\n",
      " ...\n",
      " [0.01466479 0.933134   0.00692279 0.04527846]\n",
      " [0.03959984 0.00647725 0.9387164  0.01520646]\n",
      " [0.03959884 0.00647715 0.93871796 0.01520606]]\n",
      "Iteration 699, Accuracy 0.36515\n",
      "96.40104%change in label assignment\n",
      "0.0779618\n",
      "[[0.04725296 0.70784247 0.01872096 0.22618356]\n",
      " [0.06817923 0.14609364 0.02087772 0.7648494 ]\n",
      " [0.10167585 0.04363132 0.02046713 0.8342258 ]\n",
      " ...\n",
      " [0.01365677 0.9361588  0.00630833 0.04387623]\n",
      " [0.03960669 0.00647456 0.9387153  0.01520341]\n",
      " [0.03960463 0.00647429 0.9387185  0.01520259]]\n",
      "Iteration 700, Accuracy 0.36873\n",
      "96.60235%change in label assignment\n",
      "0.0791264\n",
      "[[0.0373344  0.7821608  0.01534218 0.16516258]\n",
      " [0.07497994 0.24077374 0.02485837 0.65938795]\n",
      " [0.0643576  0.04099029 0.01486979 0.87978226]\n",
      " ...\n",
      " [0.01566189 0.92943996 0.00747855 0.04741957]\n",
      " [0.03962201 0.00647593 0.9386909  0.01521109]\n",
      " [0.03961447 0.0064748  0.93870264 0.01520811]]\n",
      "Iteration 701, Accuracy 0.36485\n",
      "91.18181%change in label assignment\n",
      "0.08040602\n",
      "[[0.04138137 0.75270873 0.01677008 0.18913978]\n",
      " [0.07554784 0.25912625 0.02524095 0.640085  ]\n",
      " [0.06150685 0.04127232 0.01441047 0.8828103 ]\n",
      " ...\n",
      " [0.01505008 0.93175465 0.00713539 0.04605985]\n",
      " [0.03962751 0.00647768 0.9386759  0.01521899]\n",
      " [0.03960837 0.0064747  0.93870544 0.01521142]]\n",
      "Iteration 702, Accuracy 0.3463\n",
      "96.00334%change in label assignment\n",
      "0.08096312\n",
      "[[0.05932176 0.5986507  0.02276449 0.3192631 ]\n",
      " [0.06964362 0.161502   0.02178958 0.7470648 ]\n",
      " [0.08176467 0.04163192 0.01761447 0.85898894]\n",
      " ...\n",
      " [0.0136887  0.9339924  0.00623814 0.04608076]\n",
      " [0.0396281  0.00647967 0.9386755  0.01521675]\n",
      " [0.03958404 0.00647316 0.9387424  0.01520037]]\n",
      "Iteration 703, Accuracy 0.33711\n",
      "96.64654%change in label assignment\n",
      "0.0833796\n",
      "[[0.02733192 0.84976596 0.01155607 0.111346  ]\n",
      " [0.07579519 0.28887314 0.02573251 0.6095991 ]\n",
      " [0.06005635 0.04156009 0.01417133 0.88421226]\n",
      " ...\n",
      " [0.01872396 0.91774935 0.00909144 0.05443522]\n",
      " [0.03960301 0.00647407 0.9387246  0.01519829]\n",
      " [0.03959983 0.00647362 0.9387295  0.01519704]]\n",
      "Iteration 704, Accuracy 0.33854\n",
      "91.52551%change in label assignment\n",
      "0.087190464\n",
      "[[0.05229765 0.66370666 0.02060818 0.2633875 ]\n",
      " [0.0694292  0.1606175  0.02182601 0.7481273 ]\n",
      " [0.08683813 0.0422384  0.01854614 0.85237736]\n",
      " ...\n",
      " [0.0135905  0.93462795 0.00622709 0.04555456]\n",
      " [0.03957036 0.00646865 0.93877006 0.01519097]\n",
      " [0.03956617 0.00646804 0.9387765  0.01518932]]\n",
      "Iteration 705, Accuracy 0.35494\n",
      "97.26027%change in label assignment\n",
      "0.081583254\n",
      "[[0.02846484 0.8426327  0.01195673 0.11694565]\n",
      " [0.0735536  0.21196723 0.02381886 0.69066036]\n",
      " [0.08226129 0.04161805 0.01753394 0.85858667]\n",
      " ...\n",
      " [0.01569847 0.9292775  0.0074624  0.04756166]\n",
      " [0.0396066  0.00647135 0.93872124 0.01520078]\n",
      " [0.03960675 0.00647144 0.93872094 0.01520083]]\n",
      "Iteration 706, Accuracy 0.35032\n",
      "88.75632%change in label assignment\n",
      "0.0773617\n",
      "[[0.06254717 0.5632995  0.02381569 0.35033768]\n",
      " [0.05769867 0.08237687 0.01648924 0.84343517]\n",
      " [0.1820449  0.05075837 0.03181937 0.73537743]\n",
      " ...\n",
      " [0.01841357 0.9051022  0.00810426 0.06838004]\n",
      " [0.03951966 0.00645901 0.938852   0.0151693 ]\n",
      " [0.03951991 0.00645913 0.93885154 0.01516941]]\n",
      "Iteration 707, Accuracy 0.37251\n",
      "94.88879%change in label assignment\n",
      "0.080635026\n",
      "[[0.01537642 0.92363524 0.00685519 0.05413314]\n",
      " [0.07526708 0.35471806 0.02623713 0.5437777 ]\n",
      " [0.0578317  0.0426528  0.01374969 0.8857658 ]\n",
      " ...\n",
      " [0.02640984 0.88860285 0.01319117 0.07179609]\n",
      " [0.03963177 0.0064685  0.9387038  0.01519593]\n",
      " [0.03963215 0.00646862 0.9387031  0.01519607]]\n",
      "Iteration 708, Accuracy 0.37865\n",
      "93.52875%change in label assignment\n",
      "0.084860735\n",
      "[[0.06462012 0.5391229  0.02463969 0.3716173 ]\n",
      " [0.05567924 0.07043634 0.01568214 0.8582023 ]\n",
      " [0.19913091 0.05203571 0.03453762 0.7142957 ]\n",
      " ...\n",
      " [0.02272759 0.87882453 0.00990465 0.08854319]\n",
      " [0.03946056 0.00645025 0.93894213 0.01514707]\n",
      " [0.03946082 0.00645037 0.93894166 0.01514718]]\n",
      "Iteration 709, Accuracy 0.37934\n",
      "92.2669%change in label assignment\n",
      "0.086947694\n",
      "[[0.01557283 0.92937696 0.00732889 0.04772132]\n",
      " [0.06504045 0.5395934  0.0242245  0.37114164]\n",
      " [0.05425761 0.05757052 0.01413445 0.87403744]\n",
      " ...\n",
      " [0.04303972 0.8285792  0.02248616 0.10589489]\n",
      " [0.03963931 0.006468   0.938689   0.01520377]\n",
      " [0.03963999 0.00646817 0.9386878  0.01520402]]\n",
      "Iteration 710, Accuracy 0.37826\n",
      "91.35366%change in label assignment\n",
      "0.08121715\n",
      "[[0.03016679 0.83087385 0.01277505 0.12618427]\n",
      " [0.07311543 0.20762272 0.02403647 0.69522536]\n",
      " [0.08391267 0.04205126 0.01826728 0.8557688 ]\n",
      " ...\n",
      " [0.01590466 0.9283204  0.00766193 0.04811301]\n",
      " [0.0394251  0.00644734 0.9389785  0.01514908]\n",
      " [0.03942462 0.00644733 0.93897915 0.0151489 ]]\n",
      "Iteration 711, Accuracy 0.38891\n",
      "94.83478%change in label assignment\n",
      "0.07727907\n",
      "[[0.03646581 0.7880959  0.01497009 0.16046818]\n",
      " [0.07015336 0.16815868 0.02204979 0.73963815]\n",
      " [0.09431043 0.04285112 0.01943119 0.8434073 ]\n",
      " ...\n",
      " [0.01404986 0.9351267  0.00658097 0.04424249]\n",
      " [0.03942142 0.00644351 0.93899435 0.01514064]\n",
      " [0.03942056 0.00644344 0.9389957  0.01514029]]\n",
      "Iteration 712, Accuracy 0.37335\n",
      "98.67924%change in label assignment\n",
      "0.072900414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04281968 0.74036264 0.01744421 0.19937353]\n",
      " [0.06873157 0.1561315  0.0216899  0.753447  ]\n",
      " [0.09309239 0.04282867 0.01961442 0.8444645 ]\n",
      " ...\n",
      " [0.01342042 0.936735   0.00626896 0.04357557]\n",
      " [0.03936618 0.0064359  0.9390756  0.01512236]\n",
      " [0.03936448 0.00643569 0.93907815 0.01512169]]\n",
      "Iteration 713, Accuracy 0.3732\n",
      "96.8773%change in label assignment\n",
      "0.07784325\n",
      "[[0.02928892 0.836809   0.01234459 0.12155756]\n",
      " [0.07322666 0.21000324 0.0238814  0.69288874]\n",
      " [0.07247756 0.04073639 0.01611035 0.8706757 ]\n",
      " ...\n",
      " [0.01497174 0.93197936 0.00712189 0.04592697]\n",
      " [0.03935515 0.006434   0.93908423 0.01512666]\n",
      " [0.03935391 0.00643387 0.93908596 0.01512618]]\n",
      "Iteration 714, Accuracy 0.36692\n",
      "97.73653%change in label assignment\n",
      "0.07519439\n",
      "[[0.05093553 0.67465335 0.02025268 0.25415844]\n",
      " [0.06207807 0.10731841 0.01858751 0.812016  ]\n",
      " [0.12852232 0.0465246  0.02494023 0.8000129 ]\n",
      " ...\n",
      " [0.01574686 0.9209143  0.00706988 0.056269  ]\n",
      " [0.03924329 0.0064234  0.9392372  0.01509606]\n",
      " [0.03924288 0.00642341 0.9392377  0.01509592]]\n",
      "Iteration 715, Accuracy 0.37006\n",
      "94.56474%change in label assignment\n",
      "0.07670258\n",
      "[[0.01442815 0.92916584 0.0065134  0.0498926 ]\n",
      " [0.07385496 0.3944416  0.02635626 0.50534713]\n",
      " [0.05305032 0.04841991 0.01350558 0.8850242 ]\n",
      " ...\n",
      " [0.02687309 0.8868679  0.01350746 0.07275157]\n",
      " [0.03931122 0.00642939 0.93914086 0.01511856]\n",
      " [0.03931011 0.00642928 0.9391425  0.01511812]]\n",
      "Iteration 716, Accuracy 0.37526\n",
      "91.06398%change in label assignment\n",
      "0.081758246\n",
      "[[0.07183193 0.43298355 0.02633403 0.46885046]\n",
      " [0.05304549 0.05629119 0.01431431 0.876349  ]\n",
      " [0.2245773  0.0529804  0.03730661 0.68513566]\n",
      " ...\n",
      " [0.03114223 0.824111   0.01316795 0.1315789 ]\n",
      " [0.03910588 0.00641052 0.9394278  0.01505583]\n",
      " [0.03910482 0.00641041 0.9394294  0.01505543]]\n",
      "Iteration 717, Accuracy 0.37782\n",
      "88.79069%change in label assignment\n",
      "0.086626165\n",
      "[[0.01404633 0.93462634 0.00653012 0.04479713]\n",
      " [0.06702919 0.5124112  0.02486309 0.39569655]\n",
      " [0.05368433 0.05668361 0.01406282 0.8755692 ]\n",
      " ...\n",
      " [0.03926102 0.84176695 0.02039625 0.09857581]\n",
      " [0.03927333 0.00642744 0.93919206 0.01510721]\n",
      " [0.03927254 0.00642738 0.9391932  0.01510689]]\n",
      "Iteration 718, Accuracy 0.37983\n",
      "90.19983%change in label assignment\n",
      "0.08592574\n",
      "[[0.05027757 0.6802699  0.02019561 0.24925697]\n",
      " [0.06050022 0.09720628 0.01802875 0.82426476]\n",
      " [0.13126068 0.04713372 0.02580862 0.79579705]\n",
      " ...\n",
      " [0.01533653 0.92357576 0.00696833 0.05411935]\n",
      " [0.03906074 0.00640882 0.93947154 0.01505888]\n",
      " [0.03905891 0.00640859 0.9394744  0.01505817]]\n",
      "Iteration 719, Accuracy 0.38425\n",
      "92.09015%change in label assignment\n",
      "[[0.05504756 0.6414373  0.02128971 0.2822254 ]\n",
      " [0.06150218 0.10105065 0.0178447  0.81960243]\n",
      " [0.13878597 0.04761488 0.02567569 0.7879234 ]\n",
      " ...\n",
      " [0.0155385  0.92313296 0.00692927 0.05439928]\n",
      " [0.03910121 0.00640921 0.9394339  0.01505581]\n",
      " [0.03909952 0.006409   0.93943626 0.01505515]]\n",
      "Iteration 720, Accuracy 0.37448\n",
      "98.365%change in label assignment\n",
      "0.083775185\n",
      "[[0.01440526 0.93375075 0.00687568 0.04496835]\n",
      " [0.0597292  0.5909788  0.02323329 0.32605863]\n",
      " [0.05556526 0.07304692 0.0157757  0.8556121 ]\n",
      " ...\n",
      " [0.03702062 0.8493184  0.01938858 0.09427243]\n",
      " [0.03915527 0.00641666 0.93935084 0.01507716]\n",
      " [0.03915396 0.00641653 0.93935287 0.01507665]]\n",
      "Iteration 721, Accuracy 0.3761\n",
      "90.14582%change in label assignment\n",
      "0.08761654\n",
      "[[0.0562508  0.6258395  0.02205049 0.29585916]\n",
      " [0.06463829 0.12488485 0.01983374 0.7906431 ]\n",
      " [0.1141474  0.0451389  0.02297358 0.81774014]\n",
      " ...\n",
      " [0.01594092 0.9196675  0.00716053 0.05723109]\n",
      " [0.03903507 0.00640328 0.9395166  0.01504498]\n",
      " [0.03903273 0.00640299 0.9395201  0.01504408]]\n",
      "Iteration 722, Accuracy 0.37566\n",
      "91.92812%change in label assignment\n",
      "0.082713574\n",
      "[[0.01506575 0.925344   0.00675438 0.05283589]\n",
      " [0.07460437 0.37246028 0.02632933 0.5266061 ]\n",
      " [0.05547218 0.04376906 0.01352359 0.8872352 ]\n",
      " ...\n",
      " [0.02647565 0.8882927  0.01327177 0.07195991]\n",
      " [0.03912646 0.00641183 0.9393897  0.01507193]\n",
      " [0.03912564 0.00641177 0.939391   0.01507162]]\n",
      "Iteration 723, Accuracy 0.36706\n",
      "93.42073%change in label assignment\n",
      "0.08301061\n",
      "[[0.05208075 0.66403216 0.02084624 0.26304084]\n",
      " [0.05688726 0.07864009 0.01646645 0.8480062 ]\n",
      " [0.17072862 0.05030888 0.03124464 0.74771786]\n",
      " ...\n",
      " [0.01521195 0.9242356  0.00692891 0.05362359]\n",
      " [0.03899754 0.00639755 0.9395706  0.01503433]\n",
      " [0.03899713 0.00639756 0.9395711  0.01503419]]\n",
      "Iteration 724, Accuracy 0.37585\n",
      "92.23253%change in label assignment\n",
      "0.08527532\n",
      "[[0.0178418  0.92097086 0.00858597 0.05260136]\n",
      " [0.05982688 0.594775   0.0228602  0.322538  ]\n",
      " [0.05644328 0.07364304 0.01558009 0.8543336 ]\n",
      " ...\n",
      " [0.04588072 0.81861377 0.02425582 0.11124967]\n",
      " [0.03917407 0.00641308 0.9393187  0.01509413]\n",
      " [0.03917395 0.00641312 0.93931895 0.01509409]]\n",
      "Iteration 725, Accuracy 0.37792\n",
      "90.19001%change in label assignment\n",
      "0.0875147\n",
      "[[0.06144714 0.57314396 0.02381317 0.3415958 ]\n",
      " [0.05884015 0.08964738 0.01734124 0.83417124]\n",
      " [0.162684   0.04962627 0.03008763 0.75760216]\n",
      " ...\n",
      " [0.01561277 0.92171067 0.00707338 0.05560321]\n",
      " [0.03895906 0.00639244 0.93962103 0.01502747]\n",
      " [0.03895791 0.00639233 0.9396227  0.01502703]]\n",
      "Iteration 726, Accuracy 0.38921\n",
      "89.5861%change in label assignment\n",
      "0.083619095\n",
      "[[0.01458003 0.9287878  0.00657843 0.0500537 ]\n",
      " [0.06895446 0.48599395 0.02535514 0.41969648]\n",
      " [0.05342285 0.05042569 0.01362007 0.8825314 ]\n",
      " ...\n",
      " [0.03476833 0.85780925 0.01784715 0.08957521]\n",
      " [0.03910828 0.00640531 0.93942034 0.01506608]\n",
      " [0.03910716 0.0064052  0.939422   0.01506563]]\n",
      "Iteration 727, Accuracy 0.37507\n",
      "92.84627%change in label assignment\n",
      "0.08660102\n",
      "[[0.07164721 0.43478155 0.02661188 0.46695936]\n",
      " [0.05483889 0.06745049 0.01553143 0.86217916]\n",
      " [0.20553796 0.05248281 0.03587915 0.70610005]\n",
      " ...\n",
      " [0.02118488 0.88811696 0.00937597 0.08132227]\n",
      " [0.03892517 0.00638759 0.9396763  0.01501092]\n",
      " [0.03892355 0.00638742 0.93967867 0.01501031]]\n",
      "Iteration 728, Accuracy 0.37826\n",
      "91.89866%change in label assignment\n",
      "0.08493244\n",
      "[[0.01412587 0.93212694 0.00643791 0.04730928]\n",
      " [0.0653206  0.5336227  0.02441741 0.37663934]\n",
      " [0.05381892 0.05803444 0.01418306 0.87396353]\n",
      " ...\n",
      " [0.03844155 0.84455085 0.01992902 0.09707861]\n",
      " [0.03911519 0.00640403 0.93942046 0.01506038]\n",
      " [0.03911365 0.00640385 0.9394228  0.01505977]]\n",
      "Iteration 729, Accuracy 0.37757\n",
      "90.10654%change in label assignment\n",
      "0.088804945\n",
      "[[0.05505873 0.63716674 0.02204574 0.28572875]\n",
      " [0.06561488 0.12986502 0.02064291 0.78387713]\n",
      " [0.11697742 0.04622469 0.02423017 0.81256765]\n",
      " ...\n",
      " [0.01428031 0.93068194 0.00666064 0.04837717]\n",
      " [0.03892035 0.00638536 0.9396853  0.01500898]\n",
      " [0.03891889 0.00638521 0.93968743 0.01500843]]\n",
      "Iteration 730, Accuracy 0.37615\n",
      "93.88226%change in label assignment\n",
      "0.08409892\n",
      "[[0.01562006 0.9234053  0.00699297 0.05398168]\n",
      " [0.07250264 0.43478522 0.02603239 0.4666797 ]\n",
      " [0.05417494 0.05033785 0.01370752 0.88177973]\n",
      " ...\n",
      " [0.02996005 0.87525076 0.01507589 0.07971324]\n",
      " [0.0390566  0.00639579 0.9394985  0.01504907]\n",
      " [0.03905527 0.00639564 0.9395005  0.01504856]]\n",
      "Iteration 731, Accuracy 0.36942\n",
      "94.64821%change in label assignment\n",
      "0.086662605\n",
      "[[0.05621764 0.6257842  0.0224301  0.29556805]\n",
      " [0.06249679 0.10957666 0.01920919 0.80871737]\n",
      " [0.11987369 0.04654979 0.02471479 0.80886173]\n",
      " ...\n",
      " [0.01712842 0.9128994  0.00778088 0.0621913 ]\n",
      " [0.03889723 0.00638074 0.93972325 0.01499889]\n",
      " [0.03889517 0.00638048 0.9397263  0.01499811]]\n",
      "Iteration 732, Accuracy 0.37585\n",
      "92.96411%change in label assignment\n",
      "0.08311868\n",
      "[[0.01520712 0.9296762  0.00706781 0.04804887]\n",
      " [0.06153369 0.58015656 0.02327052 0.33503923]\n",
      " [0.05716449 0.07445963 0.01569529 0.85268056]\n",
      " ...\n",
      " [0.03670591 0.85066557 0.01883603 0.09379242]\n",
      " [0.03907865 0.00639719 0.9394783  0.01504591]\n",
      " [0.03907612 0.00639684 0.9394822  0.01504491]]\n",
      "Iteration 733, Accuracy 0.36731\n",
      "91.10326%change in label assignment\n",
      "0.091037266\n",
      "[[0.03842536 0.77274007 0.01617112 0.17266339]\n",
      " [0.07146917 0.18715397 0.02367871 0.7176981 ]\n",
      " [0.08051859 0.04277427 0.01845335 0.8582538 ]\n",
      " ...\n",
      " [0.01411264 0.9322388  0.00664027 0.04700829]\n",
      " [0.03887418 0.00638132 0.93974787 0.01499658]\n",
      " [0.03887151 0.00638097 0.939752   0.01499555]]\n",
      "Iteration 734, Accuracy 0.36937\n",
      "94.37325%change in label assignment\n",
      "0.08132529\n",
      "[[0.01615512 0.9257356  0.00749515 0.05061415]\n",
      " [0.06598236 0.53533554 0.02446571 0.37421638]\n",
      " [0.05614143 0.06543338 0.01496868 0.8634565 ]\n",
      " ...\n",
      " [0.03263041 0.8653389  0.01644772 0.08558295]\n",
      " [0.03901955 0.0063924  0.9395457  0.01504234]\n",
      " [0.03901747 0.00639212 0.93954885 0.01504152]]\n",
      "Iteration 735, Accuracy 0.36265\n",
      "93.951%change in label assignment\n",
      "0.08730869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.073915   0.39751312 0.02752445 0.50104743]\n",
      " [0.05595879 0.05035943 0.01503625 0.8786455 ]\n",
      " [0.24042153 0.05457801 0.04149884 0.6635016 ]\n",
      " ...\n",
      " [0.04639972 0.7141693  0.01930186 0.22012906]\n",
      " [0.03883247 0.00636896 0.9398036  0.01499503]\n",
      " [0.03883099 0.0063688  0.9398056  0.01499447]]\n",
      "Iteration 736, Accuracy 0.3708\n",
      "86.34556%change in label assignment\n",
      "0.09998995\n",
      "[[0.06620878 0.75069916 0.03649893 0.14659315]\n",
      " [0.01554173 0.9273127  0.00713678 0.05000889]\n",
      " [0.07408096 0.41651195 0.02630526 0.48310184]\n",
      " ...\n",
      " [0.09262719 0.66860294 0.0542495  0.18452036]\n",
      " [0.03919424 0.00640916 0.9392862  0.01511039]\n",
      " [0.03919339 0.00640907 0.93928754 0.01511004]]\n",
      "Iteration 737, Accuracy 0.37723\n",
      "79.83994%change in label assignment\n",
      "0.09754699\n",
      "[[0.01513287 0.9311906  0.00730693 0.0463695 ]\n",
      " [0.07362534 0.390594   0.02679452 0.5089861 ]\n",
      " [0.05426893 0.04508581 0.0139294  0.8867158 ]\n",
      " ...\n",
      " [0.03392104 0.8605286  0.01767598 0.08787446]\n",
      " [0.03889358 0.00638649 0.93968    0.01503994]\n",
      " [0.03889317 0.00638648 0.9396806  0.01503979]]\n",
      "Iteration 738, Accuracy 0.39991\n",
      "84.16065%change in label assignment\n",
      "0.08452003\n",
      "[[0.0155755  0.9218721  0.00698736 0.05556506]\n",
      " [0.0726768  0.20486686 0.02376135 0.69869494]\n",
      " [0.07038675 0.04026072 0.01594722 0.87340534]\n",
      " ...\n",
      " [0.0160695  0.92789555 0.00774789 0.04828709]\n",
      " [0.03890674 0.00638025 0.9396761  0.01503694]\n",
      " [0.03890582 0.00638015 0.9396775  0.01503659]]\n",
      "Iteration 739, Accuracy 0.38803\n",
      "96.89203%change in label assignment\n",
      "0.07776376\n",
      "[[0.03564842 0.7929039  0.01482748 0.15662016]\n",
      " [0.0645057  0.12470915 0.01974096 0.7910441 ]\n",
      " [0.09551403 0.0426843  0.02003926 0.8417624 ]\n",
      " ...\n",
      " [0.01396427 0.9315342  0.0063698  0.04813178]\n",
      " [0.03889562 0.00637461 0.9397016  0.01502806]\n",
      " [0.03889311 0.00637427 0.93970555 0.01502709]]\n",
      "Iteration 740, Accuracy 0.37664\n",
      "94.55001%change in label assignment\n",
      "0.07806806\n",
      "[[0.02132157 0.8869936  0.00931949 0.08236533]\n",
      " [0.0713032  0.18703192 0.02313516 0.7185297 ]\n",
      " [0.06524593 0.04010421 0.01515738 0.8794925 ]\n",
      " ...\n",
      " [0.01357779 0.9369494  0.00642632 0.04304656]\n",
      " [0.03891324 0.00637492 0.9396786  0.01503327]\n",
      " [0.03891093 0.00637461 0.9396821  0.01503237]]\n",
      "Iteration 741, Accuracy 0.37134\n",
      "97.65798%change in label assignment\n",
      "0.075761184\n",
      "[[0.0352999  0.7949884  0.01476814 0.15494357]\n",
      " [0.06446792 0.12523873 0.01987403 0.7904193 ]\n",
      " [0.09247063 0.04240984 0.01970601 0.8454135 ]\n",
      " ...\n",
      " [0.01409233 0.9306025  0.00644429 0.04886098]\n",
      " [0.03886873 0.00636938 0.9397416  0.01502029]\n",
      " [0.03886643 0.00636908 0.93974507 0.01501939]]\n",
      "Iteration 742, Accuracy 0.36829\n",
      "97.89856%change in label assignment\n",
      "0.07819873\n",
      "[[0.01690719 0.9138131  0.00752777 0.06175194]\n",
      " [0.07532078 0.28315145 0.02577202 0.61575574]\n",
      " [0.05281759 0.04517341 0.01337862 0.88863045]\n",
      " ...\n",
      " [0.01603657 0.927962   0.00774077 0.04826063]\n",
      " [0.03893481 0.00637574 0.93965614 0.01503334]\n",
      " [0.0389294  0.00637493 0.9396644  0.01503121]]\n",
      "Iteration 743, Accuracy 0.36972\n",
      "89.79722%change in label assignment\n",
      "0.08256229\n",
      "[[0.04529119 0.71951264 0.01855908 0.21663712]\n",
      " [0.05895545 0.09258767 0.01758899 0.8308679 ]\n",
      " [0.10132778 0.04385845 0.02147028 0.8333435 ]\n",
      " ...\n",
      " [0.01968886 0.8967997  0.00876385 0.0747476 ]\n",
      " [0.0388145  0.00636518 0.9398254  0.01499485]\n",
      " [0.03881063 0.00636461 0.93983144 0.01499334]]\n",
      "Iteration 744, Accuracy 0.35494\n",
      "93.92154%change in label assignment\n",
      "0.08969052\n",
      "[[0.01481487 0.93228406 0.00703431 0.04586679]\n",
      " [0.06959393 0.47209072 0.02567491 0.4326405 ]\n",
      " [0.0554265  0.07304689 0.01553691 0.8559897 ]\n",
      " ...\n",
      " [0.02864278 0.8800999  0.01452791 0.07672939]\n",
      " [0.03895961 0.00637847 0.93962026 0.01504173]\n",
      " [0.03895624 0.00637799 0.9396254  0.0150404 ]]\n",
      "Iteration 745, Accuracy 0.36358\n",
      "90.78902%change in label assignment\n",
      "0.08896529\n",
      "[[0.05573181 0.6303153  0.02232276 0.2916301 ]\n",
      " [0.0538058  0.06132235 0.01513403 0.8697378 ]\n",
      " [0.1568275  0.04973616 0.03034433 0.763092  ]\n",
      " ...\n",
      " [0.03147889 0.821716   0.01354729 0.13325772]\n",
      " [0.03876321 0.00635989 0.9398843  0.01499265]\n",
      " [0.0387615  0.00635968 0.9398869  0.014992  ]]\n",
      "Iteration 746, Accuracy 0.36191\n",
      "85.70727%change in label assignment\n",
      "0.086076386\n",
      "[[0.02529955 0.8925018  0.01257854 0.0696201 ]\n",
      " [0.06600081 0.5279034  0.02466489 0.38143092]\n",
      " [0.05665438 0.07494026 0.01575319 0.8526522 ]\n",
      " ...\n",
      " [0.03844969 0.844519   0.01994144 0.09708988]\n",
      " [0.03895062 0.00637779 0.9396233  0.01504827]\n",
      " [0.03895047 0.00637781 0.9396235  0.0150482 ]]\n",
      "Iteration 747, Accuracy 0.37679\n",
      "89.16875%change in label assignment\n",
      "0.088068254\n",
      "[[0.02488149 0.8643281  0.01085775 0.0999327 ]\n",
      " [0.06179127 0.10875992 0.01894167 0.8105072 ]\n",
      " [0.09651231 0.04342277 0.02084457 0.8392204 ]\n",
      " ...\n",
      " [0.01512718 0.9243039  0.00692457 0.05364437]\n",
      " [0.03876836 0.00635895 0.93988097 0.01499176]\n",
      " [0.03876803 0.00635894 0.9398815  0.01499164]]\n",
      "Iteration 748, Accuracy 0.3896\n",
      "93.73496%change in label assignment\n",
      "0.07834608\n",
      "[[0.01778857 0.9092191  0.00786173 0.06513056]\n",
      " [0.06672018 0.13910545 0.02054128 0.77363306]\n",
      " [0.08015279 0.04116058 0.0173699  0.86131674]\n",
      " ...\n",
      " [0.01370568 0.9348441  0.00633131 0.04511893]\n",
      " [0.03884896 0.00636247 0.9397805  0.01500814]\n",
      " [0.0388489  0.00636249 0.9397805  0.01500813]]\n",
      "Iteration 749, Accuracy 0.37865\n",
      "97.6678%change in label assignment\n",
      "0.07783571\n",
      "[[0.01567734 0.9208592  0.00714734 0.05631612]\n",
      " [0.06833952 0.15792395 0.02208566 0.7516508 ]\n",
      " [0.06905637 0.04068035 0.01620411 0.87405914]\n",
      " ...\n",
      " [0.01336339 0.9367845  0.00634086 0.04351125]\n",
      " [0.03882324 0.00635853 0.9398165  0.01500176]\n",
      " [0.03882316 0.00635855 0.93981653 0.01500173]]\n",
      "Iteration 750, Accuracy 0.38008\n",
      "98.8609%change in label assignment\n",
      "0.07186061\n",
      "[[0.0199672  0.8952651  0.00876661 0.07600114]\n",
      " [0.06555643 0.13395107 0.02028693 0.78020555]\n",
      " [0.07379489 0.04038939 0.01648751 0.86932814]\n",
      " ...\n",
      " [0.01331968 0.93585795 0.00615404 0.04466827]\n",
      " [0.03877786 0.00635156 0.93988603 0.01498445]\n",
      " [0.03877712 0.00635147 0.93988717 0.01498416]]\n",
      "Iteration 751, Accuracy 0.37934\n",
      "97.53523%change in label assignment\n",
      "0.0770102\n",
      "[[0.02242114 0.8795629  0.00983436 0.08818156]\n",
      " [0.06471084 0.1299875  0.02023477 0.78506684]\n",
      " [0.08119255 0.041256   0.01803253 0.8595189 ]\n",
      " ...\n",
      " [0.01327675 0.9356042  0.0061754  0.04494364]\n",
      " [0.03870346 0.00634581 0.93998724 0.01496343]\n",
      " [0.03870293 0.00634577 0.9399881  0.01496323]]\n",
      "Iteration 752, Accuracy 0.37757\n",
      "98.13424%change in label assignment\n",
      "0.07161405\n",
      "[[0.02244308 0.8795993  0.00979272 0.08816499]\n",
      " [0.06542232 0.13496351 0.0204163  0.7791978 ]\n",
      " [0.07852083 0.04082769 0.01741116 0.86324036]\n",
      " ...\n",
      " [0.01330844 0.9354258  0.00615404 0.04511165]\n",
      " [0.03867795 0.00634105 0.94002765 0.01495336]\n",
      " [0.03867749 0.00634102 0.94002825 0.01495319]]\n",
      "Iteration 753, Accuracy 0.37919\n",
      "99.24387%change in label assignment\n",
      "0.07572414\n",
      "[[0.01501746 0.9246155  0.00683667 0.05353029]\n",
      " [0.06835333 0.1599363  0.02199749 0.7497129 ]\n",
      " [0.0664538  0.04007929 0.01556674 0.87790024]\n",
      " ...\n",
      " [0.0130373  0.93812555 0.00614579 0.04269134]\n",
      " [0.03862808 0.00633664 0.9400867  0.01494846]\n",
      " [0.03862822 0.00633671 0.9400866  0.01494853]]\n",
      "Iteration 754, Accuracy 0.3786\n",
      "97.32901%change in label assignment\n",
      "0.071109466\n",
      "[[0.01786347 0.9077428  0.00794822 0.0664455 ]\n",
      " [0.06984863 0.17397091 0.0225248  0.73365575]\n",
      " [0.0610024  0.04011815 0.01448284 0.8843967 ]\n",
      " ...\n",
      " [0.01318468 0.93792826 0.00620631 0.0426807 ]\n",
      " [0.03857904 0.00633048 0.94015783 0.01493265]\n",
      " [0.03857792 0.00633035 0.9401595  0.01493223]]\n",
      "Iteration 755, Accuracy 0.38111\n",
      "94.39289%change in label assignment\n",
      "0.07604609\n",
      "[[0.01629925 0.91693413 0.00739712 0.05936943]\n",
      " [0.0722216  0.20744023 0.02423888 0.6960993 ]\n",
      " [0.05590809 0.04196142 0.01407207 0.8880584 ]\n",
      " ...\n",
      " [0.01376953 0.9360456  0.00661412 0.04357079]\n",
      " [0.03850607 0.00632492 0.94024885 0.01492022]\n",
      " [0.03850494 0.00632479 0.94025046 0.0149198 ]]\n",
      "Iteration 756, Accuracy 0.37448\n",
      "97.54014%change in label assignment\n",
      "0.0767258\n",
      "[[0.04808412 0.69649255 0.01936861 0.23605469]\n",
      " [0.05367803 0.06789175 0.01511053 0.8633197 ]\n",
      " [0.12022763 0.04549551 0.02396785 0.810309  ]\n",
      " ...\n",
      " [0.02545945 0.860512   0.0109867  0.10304185]\n",
      " [0.03843322 0.00631638 0.94036484 0.01488555]\n",
      " [0.03843194 0.00631621 0.9403668  0.01488507]]\n",
      "Iteration 757, Accuracy 0.37276\n",
      "94.36343%change in label assignment\n",
      "0.08347555\n",
      "[[0.01342796 0.937288   0.00638354 0.0429005 ]\n",
      " [0.0747671  0.31078938 0.0262857  0.5881579 ]\n",
      " [0.05114292 0.05052918 0.01363759 0.88469034]\n",
      " ...\n",
      " [0.02094155 0.9089034  0.01046481 0.05969027]\n",
      " [0.03847849 0.00632354 0.9402971  0.01490083]\n",
      " [0.03847774 0.00632347 0.94029826 0.01490056]]\n",
      "Iteration 758, Accuracy 0.37561\n",
      "93.93136%change in label assignment\n",
      "0.0774073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03239194 0.81394196 0.01380732 0.13985877]\n",
      " [0.06141323 0.10927191 0.01887702 0.81043786]\n",
      " [0.08121262 0.04139891 0.01822122 0.8591673 ]\n",
      " ...\n",
      " [0.01523391 0.9232416  0.00696004 0.05456447]\n",
      " [0.03835612 0.00631166 0.94045746 0.01487474]\n",
      " [0.03835442 0.00631144 0.94046    0.01487409]]\n",
      "Iteration 759, Accuracy 0.38081\n",
      "95.26194%change in label assignment\n",
      "[[0.02598603 0.85721886 0.01117195 0.10562313]\n",
      " [0.06059678 0.10384066 0.01817815 0.81738436]\n",
      " [0.08495264 0.04156217 0.01840941 0.8550758 ]\n",
      " ...\n",
      " [0.01570324 0.9206462  0.00707637 0.05657423]\n",
      " [0.03834761 0.00630904 0.9404816  0.01486178]\n",
      " [0.03834696 0.00630898 0.9404826  0.01486154]]\n",
      "Iteration 760, Accuracy 0.37099\n",
      "95.49271%change in label assignment\n",
      "0.08293345\n",
      "[[0.02327083 0.89995    0.0117632  0.06501593]\n",
      " [0.06440286 0.53279984 0.02477196 0.37802538]\n",
      " [0.06014754 0.10278764 0.0182499  0.81881493]\n",
      " ...\n",
      " [0.02951265 0.8764608  0.01524533 0.07878122]\n",
      " [0.03840324 0.00631639 0.9403964  0.01488399]\n",
      " [0.03840256 0.00631633 0.94039744 0.01488374]]\n",
      "Iteration 761, Accuracy 0.37963\n",
      "90.94123%change in label assignment\n",
      "0.08431503\n",
      "[[0.03596335 0.7887266  0.01515465 0.16015545]\n",
      " [0.05543057 0.07760739 0.01619768 0.85076433]\n",
      " [0.11457996 0.04512556 0.02357954 0.81671494]\n",
      " ...\n",
      " [0.01945787 0.8976777  0.008677   0.07418744]\n",
      " [0.03826593 0.00630292 0.9405814  0.01484981]\n",
      " [0.03826531 0.00630287 0.9405822  0.01484958]]\n",
      "Iteration 762, Accuracy 0.38543\n",
      "89.14911%change in label assignment\n",
      "0.081595756\n",
      "[[0.02380074 0.8980556  0.01193621 0.06620742]\n",
      " [0.07028371 0.4546089  0.02596954 0.44913793]\n",
      " [0.05482112 0.0723709  0.0154827  0.85732526]\n",
      " ...\n",
      " [0.03378842 0.86096966 0.01751501 0.08772695]\n",
      " [0.03838364 0.00631469 0.94041586 0.01488589]\n",
      " [0.03838356 0.00631471 0.9404159  0.01488587]]\n",
      "Iteration 763, Accuracy 0.38179\n",
      "92.13433%change in label assignment\n",
      "0.084964834\n",
      "[[0.02461692 0.8653968  0.01080441 0.09918188]\n",
      " [0.06225309 0.11489473 0.01937243 0.8034798 ]\n",
      " [0.08350664 0.04176633 0.01872755 0.85599947]\n",
      " ...\n",
      " [0.01539271 0.92233175 0.00706104 0.05521448]\n",
      " [0.03824548 0.00629963 0.9406109  0.01484403]\n",
      " [0.03824471 0.00629955 0.94061196 0.01484374]]\n",
      "Iteration 764, Accuracy 0.39102\n",
      "92.80208%change in label assignment\n",
      "0.08595748\n",
      "[[0.0343117  0.8591264  0.0177965  0.08876543]\n",
      " [0.03915665 0.76721966 0.01615744 0.17746624]\n",
      " [0.07237113 0.20495297 0.02376851 0.6989074 ]\n",
      " ...\n",
      " [0.03982292 0.83933455 0.02098709 0.09985548]\n",
      " [0.03840987 0.0063137  0.9403729  0.01490354]\n",
      " [0.03840872 0.00631356 0.94037473 0.01490309]]\n",
      "Iteration 765, Accuracy 0.37855\n",
      "84.632%change in label assignment\n",
      "0.090957224\n",
      "[[0.01413326 0.930195   0.00657907 0.04909268]\n",
      " [0.07471114 0.28863272 0.02646932 0.6101868 ]\n",
      " [0.05182105 0.05217588 0.0142304  0.8817727 ]\n",
      " ...\n",
      " [0.01373613 0.9330116  0.00644994 0.04680239]\n",
      " [0.03824585 0.00629892 0.94060034 0.01485493]\n",
      " [0.03824461 0.00629875 0.9406022  0.01485446]]\n",
      "Iteration 766, Accuracy 0.38204\n",
      "91.79064%change in label assignment\n",
      "0.085409984\n",
      "[[0.01664    0.92521286 0.0080056  0.05014152]\n",
      " [0.07114138 0.44714805 0.02606237 0.45564815]\n",
      " [0.05738033 0.08301793 0.01646394 0.8431378 ]\n",
      " ...\n",
      " [0.01758024 0.92171115 0.00852375 0.05218485]\n",
      " [0.03834456 0.00630521 0.9404688  0.01488146]\n",
      " [0.03834365 0.00630509 0.9404701  0.01488111]]\n",
      "Iteration 767, Accuracy 0.35872\n",
      "96.05735%change in label assignment\n",
      "0.08542564\n",
      "[[0.03513909 0.79515135 0.01511973 0.15458992]\n",
      " [0.05520157 0.0722052  0.01627539 0.8563178 ]\n",
      " [0.11686967 0.04635124 0.02487245 0.81190664]\n",
      " ...\n",
      " [0.02749974 0.84743255 0.01213104 0.11293666]\n",
      " [0.03820618 0.00629105 0.9406708  0.01483195]\n",
      " [0.03820591 0.00629105 0.9406712  0.01483186]]\n",
      "Iteration 768, Accuracy 0.36893\n",
      "85.97732%change in label assignment\n",
      "0.08785862\n",
      "[[0.03467268 0.85768914 0.01791324 0.08972497]\n",
      " [0.0461697  0.71478385 0.01859263 0.22045384]\n",
      " [0.06995861 0.16892506 0.02220489 0.7389114 ]\n",
      " ...\n",
      " [0.03962056 0.8399549  0.0207725  0.09965201]\n",
      " [0.03843775 0.00631098 0.9403564  0.01489501]\n",
      " [0.03843683 0.00631088 0.9403576  0.01489466]]\n",
      "Iteration 769, Accuracy 0.37674\n",
      "85.06407%change in label assignment\n",
      "0.08831629\n",
      "[[0.02024697 0.8931154  0.0091525  0.07748516]\n",
      " [0.06623515 0.1414118  0.0215141  0.7708389 ]\n",
      " [0.07787951 0.04204337 0.01826571 0.86181146]\n",
      " ...\n",
      " [0.01481536 0.92646915 0.006935   0.05178046]\n",
      " [0.03823528 0.00629222 0.9406316  0.01484085]\n",
      " [0.03823515 0.00629229 0.9406317  0.01484084]]\n",
      "Iteration 770, Accuracy 0.38464\n",
      "86.85128%change in label assignment\n",
      "0.08251792\n",
      "[[0.01592335 0.9274508  0.00757175 0.04905406]\n",
      " [0.07511117 0.3598063  0.02649735 0.5385851 ]\n",
      " [0.05305368 0.05086559 0.01374496 0.8823358 ]\n",
      " ...\n",
      " [0.02141368 0.9068824  0.01053834 0.06116556]\n",
      " [0.0383586  0.00630061 0.9404671  0.01487366]\n",
      " [0.03835835 0.00630067 0.9404674  0.01487359]]\n",
      "Iteration 771, Accuracy 0.37644\n",
      "96.78401%change in label assignment\n",
      "0.08298903\n",
      "[[0.03320366 0.80809957 0.01435575 0.14434104]\n",
      " [0.05583523 0.07806602 0.016676   0.8494227 ]\n",
      " [0.1269828  0.0471851  0.02637382 0.79945827]\n",
      " ...\n",
      " [0.02089443 0.88903666 0.0094296  0.0806393 ]\n",
      " [0.03821848 0.00628678 0.94066834 0.01482644]\n",
      " [0.03821857 0.00628689 0.940668   0.01482652]]\n",
      "Iteration 772, Accuracy 0.38278\n",
      "92.74316%change in label assignment\n",
      "0.08165638\n",
      "[[0.03036954 0.8732996  0.01549313 0.0808377 ]\n",
      " [0.06196265 0.5686512  0.02373629 0.3456498 ]\n",
      " [0.05886071 0.09027532 0.01710288 0.83376104]\n",
      " ...\n",
      " [0.03909994 0.8417069  0.02047604 0.09871715]\n",
      " [0.03839305 0.00630264 0.9404291  0.01487517]\n",
      " [0.03839315 0.00630273 0.9404289  0.01487522]]\n",
      "Iteration 773, Accuracy 0.38017\n",
      "91.09344%change in label assignment\n",
      "0.08803732\n",
      "[[0.01429177 0.9288437  0.00664713 0.05021744]\n",
      " [0.06975122 0.1801735  0.02326452 0.7268108 ]\n",
      " [0.06456096 0.04064867 0.01570978 0.8790806 ]\n",
      " ...\n",
      " [0.01331752 0.9367276  0.00637541 0.04357949]\n",
      " [0.03820478 0.00628646 0.9406807  0.01482805]\n",
      " [0.03820437 0.00628647 0.94068134 0.01482792]]\n",
      "Iteration 774, Accuracy 0.3926\n",
      "94.46163%change in label assignment\n",
      "0.075792134\n",
      "[[0.01595519 0.9275598  0.00763329 0.0488517 ]\n",
      " [0.07430781 0.37646306 0.02651693 0.52271223]\n",
      " [0.05233496 0.05499297 0.01396151 0.87871057]\n",
      " ...\n",
      " [0.01998769 0.9123743  0.00981321 0.0578248 ]\n",
      " [0.03827962 0.00629058 0.9405754  0.01485438]\n",
      " [0.03827859 0.00629048 0.9405769  0.01485399]]\n",
      "Iteration 775, Accuracy 0.37865\n",
      "95.01645%change in label assignment\n",
      "0.0765726\n",
      "[[0.05247409 0.6590108  0.02171732 0.26679775]\n",
      " [0.05352008 0.0544167  0.01509115 0.8769721 ]\n",
      " [0.16854838 0.05116354 0.03321597 0.7470721 ]\n",
      " ...\n",
      " [0.04102084 0.7531825  0.01756662 0.18823011]\n",
      " [0.03813356 0.0062721  0.9407757  0.01481857]\n",
      " [0.03813328 0.00627213 0.9407761  0.0148185 ]]\n",
      "Iteration 776, Accuracy 0.38057\n",
      "89.35042%change in label assignment\n",
      "0.09595924\n",
      "[[0.07918725 0.7089584  0.0453592  0.1664951 ]\n",
      " [0.01545085 0.9267185  0.00712108 0.05070965]\n",
      " [0.06864244 0.49863186 0.0254806  0.4072451 ]\n",
      " ...\n",
      " [0.08666681 0.68591046 0.05051199 0.17691076]\n",
      " [0.03844151 0.00630729 0.9403309  0.0149203 ]\n",
      " [0.03844086 0.00630724 0.94033176 0.01492005]]\n",
      "Iteration 777, Accuracy 0.38096\n",
      "78.57318%change in label assignment\n",
      "0.09663709\n",
      "[[0.02712679 0.88524115 0.01401374 0.07361828]\n",
      " [0.06035995 0.57880056 0.02385122 0.33698824]\n",
      " [0.05859433 0.09443011 0.01783392 0.8291417 ]\n",
      " ...\n",
      " [0.03402656 0.85969216 0.01795429 0.08832699]\n",
      " [0.03818654 0.00628827 0.9406658  0.01485941]\n",
      " [0.03818469 0.00628803 0.9406686  0.01485871]]\n",
      "Iteration 778, Accuracy 0.40806\n",
      "89.81195%change in label assignment\n",
      "0.08529261\n",
      "[[0.01623232 0.9171346  0.00733365 0.05929944]\n",
      " [0.06609504 0.14282927 0.02097738 0.7700983 ]\n",
      " [0.07607493 0.0403143  0.01719307 0.8664177 ]\n",
      " ...\n",
      " [0.01334156 0.93477374 0.00618806 0.04569669]\n",
      " [0.03812999 0.00627818 0.94075423 0.01483767]\n",
      " [0.0381287  0.00627803 0.9407561  0.01483719]]\n",
      "Iteration 779, Accuracy 0.3814\n",
      "91.15235%change in label assignment\n",
      "0.0762856\n",
      "[[0.01295319 0.93842185 0.00609822 0.04252673]\n",
      " [0.07421371 0.25903517 0.02545777 0.6412934 ]\n",
      " [0.05241502 0.04295409 0.01335826 0.89127266]\n",
      " ...\n",
      " [0.01453389 0.9335587  0.00702607 0.04488136]\n",
      " [0.0381944  0.00628104 0.9406757  0.0148489 ]\n",
      " [0.03819245 0.00628079 0.9406786  0.01484816]]\n",
      "Iteration 780, Accuracy 0.37792\n",
      "95.57618%change in label assignment\n",
      "0.07736704\n",
      "[[0.03087075 0.823884   0.01326459 0.13198067]\n",
      " [0.05585537 0.08134989 0.01653196 0.8462628 ]\n",
      " [0.10736289 0.04404923 0.02253743 0.8260504 ]\n",
      " ...\n",
      " [0.0212254  0.88651836 0.0094347  0.0828216 ]\n",
      " [0.03811302 0.00627069 0.94079417 0.01482211]\n",
      " [0.03811104 0.00627045 0.9407972  0.01482136]]\n",
      "Iteration 781, Accuracy 0.37413\n",
      "94.39289%change in label assignment\n",
      "0.07937919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03323083 0.86292064 0.01727643 0.08657206]\n",
      " [0.05074693 0.67386776 0.0203445  0.25504076]\n",
      " [0.06573451 0.13766323 0.02058068 0.77602154]\n",
      " ...\n",
      " [0.04208484 0.8311599  0.02242453 0.1043307 ]\n",
      " [0.03825812 0.00628678 0.94058645 0.01486858]\n",
      " [0.03825554 0.00628643 0.94059044 0.01486759]]\n",
      "Iteration 782, Accuracy 0.37944\n",
      "86.66961%change in label assignment\n",
      "0.088597745\n",
      "[[0.0201548  0.8933373  0.00907857 0.07742928]\n",
      " [0.06436482 0.12974183 0.02058166 0.7853117 ]\n",
      " [0.07680538 0.04114045 0.0178597  0.8641945 ]\n",
      " ...\n",
      " [0.01526819 0.9231373  0.00707524 0.05451925]\n",
      " [0.03805098 0.00627038 0.94085854 0.01482003]\n",
      " [0.03804802 0.00626997 0.9408631  0.0148189 ]]\n",
      "Iteration 783, Accuracy 0.38032\n",
      "90.98051%change in label assignment\n",
      "0.07833878\n",
      "[[0.01497748 0.9315491  0.00718455 0.04628888]\n",
      " [0.07502469 0.327688   0.02640282 0.57088447]\n",
      " [0.05130562 0.0494604  0.01350999 0.88572395]\n",
      " ...\n",
      " [0.01881435 0.91703236 0.00926246 0.05489086]\n",
      " [0.03814583 0.00627646 0.94073653 0.01484122]\n",
      " [0.03814361 0.00627617 0.94073987 0.01484038]]\n",
      "Iteration 784, Accuracy 0.37801\n",
      "95.36014%change in label assignment\n",
      "0.08492885\n",
      "[[0.02340886 0.8729088  0.01042107 0.09326134]\n",
      " [0.06035524 0.10429253 0.01878331 0.8165689 ]\n",
      " [0.08489569 0.04205278 0.01933746 0.8537141 ]\n",
      " ...\n",
      " [0.01787577 0.90725327 0.00815843 0.06671259]\n",
      " [0.03803886 0.00626448 0.9408872  0.01480943]\n",
      " [0.03803716 0.00626429 0.9408898  0.0148088 ]]\n",
      "Iteration 785, Accuracy 0.37625\n",
      "94.88879%change in label assignment\n",
      "0.08086124\n",
      "[[0.02468947 0.8944963  0.01237055 0.06844364]\n",
      " [0.06444553 0.5420179  0.0244996  0.36903694]\n",
      " [0.05968573 0.09435856 0.01750384 0.8284519 ]\n",
      " ...\n",
      " [0.03286673 0.86422384 0.01693523 0.08597427]\n",
      " [0.03821969 0.00628087 0.94063973 0.01485965]\n",
      " [0.03821724 0.00628055 0.94064355 0.0148587 ]]\n",
      "Iteration 786, Accuracy 0.37752\n",
      "90.5779%change in label assignment\n",
      "0.08632855\n",
      "[[0.03109194 0.82243663 0.0135441  0.13292736]\n",
      " [0.057019   0.08570873 0.0173237  0.8399486 ]\n",
      " [0.09899145 0.04391019 0.02190921 0.83518916]\n",
      " ...\n",
      " [0.02304601 0.8753714  0.01032485 0.09125777]\n",
      " [0.03801901 0.00626089 0.94091845 0.01480164]\n",
      " [0.03801686 0.00626063 0.94092166 0.01480083]]\n",
      "Iteration 787, Accuracy 0.37747\n",
      "91.24073%change in label assignment\n",
      "0.084400624\n",
      "[[0.02671869 0.88684165 0.013509   0.07293066]\n",
      " [0.05617857 0.62771803 0.02202938 0.29407403]\n",
      " [0.06473856 0.12592061 0.01983095 0.7895099 ]\n",
      " ...\n",
      " [0.03834709 0.844379   0.02009924 0.09717467]\n",
      " [0.03824141 0.00628104 0.94061726 0.01486027]\n",
      " [0.03823747 0.00628049 0.9406232  0.01485874]]\n",
      "Iteration 788, Accuracy 0.37487\n",
      "85.38322%change in label assignment\n",
      "0.0912186\n",
      "[[0.03252746 0.8126238  0.01416219 0.14068659]\n",
      " [0.05869836 0.09415009 0.01814896 0.82900256]\n",
      " [0.09694614 0.04398318 0.02174408 0.8373266 ]\n",
      " ...\n",
      " [0.02177395 0.8834782  0.00984094 0.08490689]\n",
      " [0.03801112 0.00626016 0.9409318  0.01479684]\n",
      " [0.03800828 0.0062598  0.94093615 0.01479577]]\n",
      "Iteration 789, Accuracy 0.36844\n",
      "87.3079%change in label assignment\n",
      "0.08741262\n",
      "[[0.03484772 0.8569472  0.01806851 0.09013665]\n",
      " [0.04987121 0.6843083  0.01995644 0.24586406]\n",
      " [0.06774855 0.14909178 0.02126486 0.7618948 ]\n",
      " ...\n",
      " [0.04446556 0.82279265 0.023678   0.10906377]\n",
      " [0.038236   0.00628026 0.94061804 0.01486575]\n",
      " [0.03823398 0.00628001 0.940621   0.01486497]]\n",
      "Iteration 790, Accuracy 0.37232\n",
      "89.81195%change in label assignment\n",
      "0.085407205\n",
      "[[0.01322044 0.9355291  0.0062431  0.04500737]\n",
      " [0.07382216 0.2622673  0.02598389 0.6379267 ]\n",
      " [0.05222101 0.04423237 0.01383801 0.8897086 ]\n",
      " ...\n",
      " [0.01431804 0.93395185 0.00700462 0.0447255 ]\n",
      " [0.03802617 0.0062626  0.94089687 0.0148144 ]\n",
      " [0.0380233  0.00626223 0.9409012  0.01481331]]\n",
      "Iteration 791, Accuracy 0.3814\n",
      "93.64168%change in label assignment\n",
      "0.07891321\n",
      "[[0.01545747 0.9221123  0.00700959 0.05542067]\n",
      " [0.07368074 0.24022801 0.02492328 0.661168  ]\n",
      " [0.05215656 0.04340795 0.01329591 0.89113957]\n",
      " ...\n",
      " [0.01384126 0.93574786 0.00660676 0.04380406]\n",
      " [0.03810616 0.00626752 0.94079727 0.01482898]\n",
      " [0.03809881 0.00626643 0.9408086  0.01482611]]\n",
      "Iteration 792, Accuracy 0.36584\n",
      "94.97717%change in label assignment\n",
      "0.07463914\n",
      "[[0.02003142 0.89349985 0.00902678 0.07744189]\n",
      " [0.07084531 0.19932465 0.02402505 0.70580494]\n",
      " [0.05522647 0.04119768 0.01412352 0.8894524 ]\n",
      " ...\n",
      " [0.01301283 0.9373355  0.00620409 0.0434476 ]\n",
      " [0.03805031 0.00626271 0.9408743  0.01481266]\n",
      " [0.03804082 0.00626131 0.9408889  0.01480897]]\n",
      "Iteration 793, Accuracy 0.35538\n",
      "98.1637%change in label assignment\n",
      "0.07930499\n",
      "[[0.01308122 0.9369451  0.0061158  0.0438579 ]\n",
      " [0.07276594 0.39535215 0.02658375 0.5052982 ]\n",
      " [0.05251134 0.06519121 0.01484059 0.86745685]\n",
      " ...\n",
      " [0.01976482 0.91336405 0.00985107 0.05702013]\n",
      " [0.03814194 0.00627278 0.9407349  0.01485035]\n",
      " [0.03812206 0.00626973 0.9407656  0.01484253]]\n",
      "Iteration 794, Accuracy 0.35258\n",
      "92.55659%change in label assignment\n",
      "0.082794026\n",
      "[[0.03691557 0.7804188  0.01576726 0.16689841]\n",
      " [0.06219288 0.11848573 0.01973644 0.7995849 ]\n",
      " [0.07504608 0.04086402 0.01762732 0.8664625 ]\n",
      " ...\n",
      " [0.0188966  0.90058154 0.00857841 0.07194351]\n",
      " [0.03795713 0.0062568  0.94098675 0.01479934]\n",
      " [0.03794821 0.00625549 0.94100046 0.01479588]]\n",
      "Iteration 795, Accuracy 0.33947\n",
      "91.01979%change in label assignment\n",
      "0.08496814\n",
      "[[0.02352716 0.8988704  0.01183425 0.0657682 ]\n",
      " [0.0494522  0.6849898  0.01989705 0.24566095]\n",
      " [0.0686088  0.16297932 0.02197108 0.7464408 ]\n",
      " ...\n",
      " [0.04226366 0.8303765  0.02249816 0.10486173]\n",
      " [0.03821803 0.00628316 0.94062006 0.01487867]\n",
      " [0.03818934 0.00627872 0.9406646  0.01486735]]\n",
      "Iteration 796, Accuracy 0.35381\n",
      "85.41268%change in label assignment\n",
      "0.08955327\n",
      "[[0.01763    0.90857166 0.0081053  0.06569298]\n",
      " [0.07341439 0.247979   0.02590511 0.65270144]\n",
      " [0.0537177  0.04415671 0.01430435 0.8878212 ]\n",
      " ...\n",
      " [0.01368059 0.935251   0.00663343 0.04443498]\n",
      " [0.03793992 0.00625863 0.94099605 0.0148054 ]\n",
      " [0.03793081 0.00625732 0.94101006 0.01480187]]\n",
      "Iteration 797, Accuracy 0.35096\n",
      "87.84308%change in label assignment\n",
      "0.084426194\n",
      "[[0.02156555 0.8862754  0.00944101 0.08271805]\n",
      " [0.072658   0.20586075 0.0238058  0.6976754 ]\n",
      " [0.05837629 0.04113676 0.01406256 0.8864244 ]\n",
      " ...\n",
      " [0.01435819 0.9306807  0.0066258  0.04833535]\n",
      " [0.03806216 0.00626325 0.9408567  0.01481787]\n",
      " [0.03805618 0.00626241 0.9408658  0.01481556]]\n",
      "Iteration 798, Accuracy 0.35317\n",
      "96.21937%change in label assignment\n",
      "0.07599454\n",
      "[[0.01394738 0.9306723  0.00655825 0.04882214]\n",
      " [0.07390832 0.31231886 0.02682295 0.5869499 ]\n",
      " [0.05056335 0.04902408 0.01394062 0.886472  ]\n",
      " ...\n",
      " [0.01462239 0.93251276 0.00719805 0.04566678]\n",
      " [0.03803878 0.00626119 0.9408873  0.01481271]\n",
      " [0.03803472 0.00626067 0.9408935  0.01481117]]\n",
      "Iteration 799, Accuracy 0.35945\n",
      "97.9182%change in label assignment\n",
      "[[0.03883516 0.76721066 0.01616671 0.1777875 ]\n",
      " [0.05984057 0.10328203 0.0181199  0.8187575 ]\n",
      " [0.08387707 0.04124268 0.01842335 0.8564568 ]\n",
      " ...\n",
      " [0.02315765 0.8743573  0.01013799 0.09234709]\n",
      " [0.03801377 0.00625527 0.94093454 0.01479643]\n",
      " [0.03800831 0.00625453 0.94094276 0.01479433]]\n",
      "Iteration 800, Accuracy 0.35945\n",
      "94.82987%change in label assignment\n",
      "0.09170474\n",
      "[[0.05171575 0.79701996 0.02852307 0.12274126]\n",
      " [0.01525618 0.92228144 0.00699508 0.05546725]\n",
      " [0.06984185 0.44384736 0.02635359 0.4599572 ]\n",
      " ...\n",
      " [0.05439978 0.78786606 0.03021047 0.12752368]\n",
      " [0.03818319 0.00627508 0.9406835  0.01485822]\n",
      " [0.03817694 0.0062742  0.940693   0.0148558 ]]\n",
      "Iteration 801, Accuracy 0.36495\n",
      "80.08052%change in label assignment\n",
      "0.099605605\n",
      "[[0.01337473 0.9366035  0.00647774 0.04354404]\n",
      " [0.06874383 0.46453208 0.02656987 0.44015414]\n",
      " [0.05482477 0.07725231 0.01654852 0.8513743 ]\n",
      " ...\n",
      " [0.01490074 0.93138665 0.00735642 0.04635623]\n",
      " [0.03799916 0.00625872 0.9409268  0.01481534]\n",
      " [0.03799344 0.00625795 0.9409355  0.01481315]]\n",
      "Iteration 802, Accuracy 0.36579\n",
      "90.61717%change in label assignment\n",
      "0.08640935\n",
      "[[0.01662207 0.9156309  0.00748399 0.06026307]\n",
      " [0.07455336 0.26903    0.02558463 0.630832  ]\n",
      " [0.05168435 0.04475776 0.01329437 0.89026344]\n",
      " ...\n",
      " [0.01373289 0.9339518  0.00638997 0.04592541]\n",
      " [0.03809406 0.00625998 0.94081116 0.01483479]\n",
      " [0.03809005 0.00625948 0.9408172  0.01483325]]\n",
      "Iteration 803, Accuracy 0.34718\n",
      "93.43546%change in label assignment\n",
      "0.08202496\n",
      "[[0.01493278 0.93146515 0.0073803  0.04622169]\n",
      " [0.06094241 0.56722814 0.02434084 0.34748864]\n",
      " [0.05953598 0.10374391 0.01869463 0.81802547]\n",
      " ...\n",
      " [0.01807125 0.9193546  0.0091084  0.05346576]\n",
      " [0.03810898 0.00626197 0.9407879  0.01484104]\n",
      " [0.03810219 0.00626106 0.9407983  0.01483843]]\n",
      "Iteration 804, Accuracy 0.35641\n",
      "92.20798%change in label assignment\n",
      "0.087085724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01921909 0.8988232  0.00856457 0.07339317]\n",
      " [0.07426836 0.2944228  0.02605872 0.6052502 ]\n",
      " [0.05000451 0.04741715 0.01329398 0.8892843 ]\n",
      " ...\n",
      " [0.01512097 0.92358553 0.00689975 0.05439368]\n",
      " [0.03811576 0.00625934 0.9407851  0.01483978]\n",
      " [0.03810534 0.00625785 0.9408011  0.01483573]]\n",
      "Iteration 805, Accuracy 0.34993\n",
      "95.44852%change in label assignment\n",
      "0.08611661\n",
      "[[0.01820941 0.91907406 0.0091549  0.05356164]\n",
      " [0.04381649 0.7263613  0.01833054 0.21149164]\n",
      " [0.06986821 0.19187473 0.02361824 0.7146389 ]\n",
      " ...\n",
      " [0.01940728 0.91439146 0.00981838 0.05638289]\n",
      " [0.03818329 0.00626763 0.94068545 0.01486365]\n",
      " [0.03816639 0.00626511 0.94071144 0.01485705]]\n",
      "Iteration 806, Accuracy 0.34202\n",
      "87.93637%change in label assignment\n",
      "0.08806549\n",
      "[[0.02979134 0.82979393 0.01289327 0.12752146]\n",
      " [0.06885457 0.1790943  0.02289193 0.72915924]\n",
      " [0.05777435 0.03945876 0.01430306 0.88846374]\n",
      " ...\n",
      " [0.024105   0.8673662  0.01063835 0.09789047]\n",
      " [0.03806908 0.00625488 0.9408532  0.01482282]\n",
      " [0.03805919 0.00625348 0.9408683  0.014819  ]]\n",
      "Iteration 807, Accuracy 0.35867\n",
      "86.02642%change in label assignment\n",
      "0.09184232\n",
      "[[0.0349444  0.85621345 0.01838409 0.09045807]\n",
      " [0.03120225 0.8209788  0.0133501  0.1344689 ]\n",
      " [0.07239926 0.2274425  0.02457092 0.67558724]\n",
      " ...\n",
      " [0.03921748 0.84080243 0.02088678 0.0990933 ]\n",
      " [0.03825178 0.0062732  0.9405964  0.01487861]\n",
      " [0.03824807 0.00627275 0.940602   0.0148772 ]]\n",
      "Iteration 808, Accuracy 0.34522\n",
      "88.01002%change in label assignment\n",
      "0.09025459\n",
      "[[0.01539036 0.9214749  0.00714797 0.05598676]\n",
      " [0.0736203  0.31892645 0.02686347 0.5805897 ]\n",
      " [0.05039541 0.04628667 0.01374207 0.88957584]\n",
      " ...\n",
      " [0.01341158 0.9340177  0.00636431 0.04620646]\n",
      " [0.03805959 0.00625678 0.940858   0.01482564]\n",
      " [0.03805446 0.00625612 0.9408657  0.01482369]]\n",
      "Iteration 809, Accuracy 0.36834\n",
      "91.00506%change in label assignment\n",
      "0.082293995\n",
      "[[0.01374783 0.93424493 0.00643488 0.04557236]\n",
      " [0.06916112 0.4687457  0.02584325 0.4362499 ]\n",
      " [0.05346469 0.06792317 0.01508249 0.8635297 ]\n",
      " ...\n",
      " [0.01454067 0.9327117  0.00695909 0.04578853]\n",
      " [0.03819066 0.00626527 0.94068927 0.01485474]\n",
      " [0.03818279 0.00626417 0.94070137 0.01485169]]\n",
      "Iteration 810, Accuracy 0.35047\n",
      "96.47469%change in label assignment\n",
      "0.08586888\n",
      "[[0.02626705 0.85361475 0.01178208 0.10833614]\n",
      " [0.07029999 0.19657905 0.0243342  0.7087867 ]\n",
      " [0.05982826 0.04138096 0.01537903 0.88341177]\n",
      " ...\n",
      " [0.02021183 0.8925131  0.00928971 0.07798536]\n",
      " [0.03808149 0.00625389 0.94085395 0.01481063]\n",
      " [0.03807467 0.00625299 0.94086426 0.01480805]]\n",
      "Iteration 811, Accuracy 0.34477\n",
      "95.21284%change in label assignment\n",
      "0.086943656\n",
      "[[0.01876412 0.9163211  0.00914992 0.0557649 ]\n",
      " [0.05778728 0.61157894 0.02256728 0.30806652]\n",
      " [0.06319594 0.11535655 0.01911486 0.80233264]\n",
      " ...\n",
      " [0.02325372 0.8993512  0.01160192 0.06579316]\n",
      " [0.03829731 0.00627394 0.9405599  0.01486887]\n",
      " [0.03828798 0.0062726  0.9405742  0.01486524]]\n",
      "Iteration 812, Accuracy 0.35184\n",
      "92.57623%change in label assignment\n",
      "0.087807514\n",
      "[[0.03513649 0.7930237  0.01543753 0.15640232]\n",
      " [0.06592613 0.146406   0.02211537 0.76555246]\n",
      " [0.06971782 0.04205854 0.01742634 0.8707973 ]\n",
      " ...\n",
      " [0.02677759 0.85088485 0.01209049 0.11024708]\n",
      " [0.03806853 0.00625354 0.94087    0.01480789]\n",
      " [0.03806083 0.00625251 0.9408817  0.01480496]]\n",
      "Iteration 813, Accuracy 0.346\n",
      "92.03123%change in label assignment\n",
      "0.093088165\n",
      "[[0.02215814 0.90353006 0.01102266 0.0632891 ]\n",
      " [0.06206895 0.5644376  0.02391804 0.34957537]\n",
      " [0.06079157 0.10352301 0.01820125 0.81748414]\n",
      " ...\n",
      " [0.02719784 0.8845662  0.01380816 0.07442782]\n",
      " [0.0383278  0.00627514 0.9405155  0.01488159]\n",
      " [0.03832491 0.00627483 0.94051975 0.0148805 ]]\n",
      "Iteration 814, Accuracy 0.35067\n",
      "90.87249%change in label assignment\n",
      "0.08761987\n",
      "[[0.04964032 0.67795974 0.02091441 0.25148553]\n",
      " [0.05158522 0.05628972 0.01497832 0.8771467 ]\n",
      " [0.14156505 0.04935413 0.02983621 0.77924466]\n",
      " ...\n",
      " [0.03981419 0.7584307  0.01727489 0.18448023]\n",
      " [0.03810645 0.00625123 0.9408285  0.01481383]\n",
      " [0.03810458 0.00625113 0.9408311  0.0148132 ]]\n",
      "Iteration 815, Accuracy 0.36967\n",
      "86.8611%change in label assignment\n",
      "0.09481715\n",
      "[[0.08429463 0.69217193 0.04910911 0.17442437]\n",
      " [0.01699925 0.9222677  0.00812379 0.05260925]\n",
      " [0.05842166 0.608534   0.02278747 0.31025684]\n",
      " ...\n",
      " [0.09231805 0.6678033  0.05478261 0.18509601]\n",
      " [0.03843326 0.00629038 0.9403444  0.014932  ]\n",
      " [0.03843196 0.00629033 0.9403462  0.01493151]]\n",
      "Iteration 816, Accuracy 0.37487\n",
      "78.36697%change in label assignment\n",
      "0.09907493\n",
      "[[0.03110457 0.8699661  0.01638053 0.08254878]\n",
      " [0.0549967  0.62654614 0.02227576 0.2961814 ]\n",
      " [0.06082361 0.11532002 0.01929869 0.8045577 ]\n",
      " ...\n",
      " [0.03855468 0.84277266 0.02072997 0.09794273]\n",
      " [0.03819552 0.00626919 0.9406567  0.0148786 ]\n",
      " [0.0381935  0.00626904 0.94065964 0.01487787]]\n",
      "Iteration 817, Accuracy 0.40315\n",
      "88.37826%change in label assignment\n",
      "0.0871966\n",
      "[[0.01264742 0.93851316 0.00596892 0.04287054]\n",
      " [0.0729458  0.25647897 0.02545033 0.6451249 ]\n",
      " [0.04982835 0.04355072 0.01317592 0.8934451 ]\n",
      " ...\n",
      " [0.01313062 0.9384964  0.00634714 0.04202583]\n",
      " [0.03815541 0.00625883 0.9407253  0.0148604 ]\n",
      " [0.03815057 0.00625821 0.9407327  0.01485855]]\n",
      "Iteration 818, Accuracy 0.38337\n",
      "92.67442%change in label assignment\n",
      "0.07926184\n",
      "[[0.01261768 0.9385906  0.00596193 0.04282981]\n",
      " [0.07315308 0.26630926 0.02570895 0.6348287 ]\n",
      " [0.04935304 0.04475565 0.01320139 0.8926899 ]\n",
      " ...\n",
      " [0.01289742 0.93929267 0.00622647 0.04158349]\n",
      " [0.03817003 0.00625653 0.9407111  0.01486238]\n",
      " [0.03816591 0.00625602 0.9407172  0.01486082]]\n",
      "Iteration 819, Accuracy 0.36952\n",
      "99.36171%change in label assignment\n",
      "0.07597132\n",
      "[[0.01302155 0.93563884 0.00611526 0.04522431]\n",
      " [0.07248185 0.24536617 0.02530315 0.65684885]\n",
      " [0.04990153 0.04317754 0.01326005 0.8936609 ]\n",
      " ...\n",
      " [0.01249305 0.9400852  0.00597934 0.04144249]\n",
      " [0.03814116 0.00625063 0.9407552  0.01485306]\n",
      " [0.03813868 0.00625039 0.9407588  0.01485215]]\n",
      "Iteration 820, Accuracy 0.36785\n",
      "97.5205%change in label assignment\n",
      "0.07708768\n",
      "[[0.01300811 0.9359567  0.00609341 0.04494174]\n",
      " [0.07321471 0.26392058 0.02564536 0.63721937]\n",
      " [0.04905871 0.05081424 0.01352096 0.8866061 ]\n",
      " ...\n",
      " [0.01252874 0.9398164  0.00596263 0.04169222]\n",
      " [0.03815257 0.00624914 0.94074184 0.01485651]\n",
      " [0.03814676 0.00624834 0.9407506  0.01485426]]\n",
      "Iteration 821, Accuracy 0.36706\n",
      "93.80861%change in label assignment\n",
      "0.07706678\n",
      "[[0.01296491 0.93597156 0.00612184 0.04494166]\n",
      " [0.07329594 0.28511596 0.02625591 0.61533225]\n",
      " [0.04903016 0.05220314 0.01379087 0.8849758 ]\n",
      " ...\n",
      " [0.01256703 0.93989086 0.00605389 0.04148817]\n",
      " [0.0381164  0.0062454  0.9407907  0.01484746]\n",
      " [0.03811058 0.00624462 0.9407996  0.01484522]]\n",
      "Iteration 822, Accuracy 0.35798\n",
      "99.27824%change in label assignment\n",
      "0.07726309\n",
      "[[0.0188506  0.9001854  0.00848522 0.07247878]\n",
      " [0.06815287 0.17301376 0.02256928 0.73626405]\n",
      " [0.05535398 0.03939911 0.01385611 0.89139074]\n",
      " ...\n",
      " [0.01445373 0.92683995 0.00667501 0.05203132]\n",
      " [0.03807123 0.00623941 0.94086206 0.01482726]\n",
      " [0.03806728 0.00623895 0.940868   0.01482578]]\n",
      "Iteration 823, Accuracy 0.35621\n",
      "93.75951%change in label assignment\n",
      "0.07843729\n",
      "[[0.01247331 0.9400008  0.00597424 0.04155174]\n",
      " [0.07335204 0.30402166 0.02644863 0.59617764]\n",
      " [0.04911161 0.04487447 0.01327316 0.8927407 ]\n",
      " ...\n",
      " [0.01410338 0.9349358  0.00694002 0.04402076]\n",
      " [0.038049   0.00623789 0.94089115 0.01482196]\n",
      " [0.03804896 0.00623811 0.9408909  0.01482205]]\n",
      "Iteration 824, Accuracy 0.36648\n",
      "93.9019%change in label assignment\n",
      "0.08228289\n",
      "[[0.0171402  0.9104198  0.00781037 0.0646296 ]\n",
      " [0.06987617 0.19627362 0.02368223 0.71016794]\n",
      " [0.05408964 0.03964446 0.01375177 0.8925141 ]\n",
      " ...\n",
      " [0.01314604 0.93475986 0.00617077 0.04592333]\n",
      " [0.03801407 0.00623042 0.9409448  0.01481075]\n",
      " [0.03801218 0.00623033 0.9409474  0.01481011]]\n",
      "Iteration 825, Accuracy 0.37723\n",
      "97.85437%change in label assignment\n",
      "0.07611868\n",
      "[[0.01299747 0.9389296  0.00628799 0.0417849 ]\n",
      " [0.07087731 0.41786128 0.02661013 0.4846512 ]\n",
      " [0.05195859 0.06866439 0.01519301 0.864184  ]\n",
      " ...\n",
      " [0.01701529 0.92377037 0.00850104 0.05071328]\n",
      " [0.03805633 0.00623572 0.94087934 0.01482856]\n",
      " [0.03805094 0.00623504 0.94088745 0.01482651]]\n",
      "Iteration 826, Accuracy 0.36996\n",
      "92.36019%change in label assignment\n",
      "0.07888846\n",
      "[[0.02818456 0.8397513  0.01243826 0.11962589]\n",
      " [0.06289179 0.13012125 0.02046528 0.7865217 ]\n",
      " [0.0646356  0.03935952 0.01584787 0.88015705]\n",
      " ...\n",
      " [0.01993541 0.89314306 0.00906717 0.07785437]\n",
      " [0.03790938 0.00622006 0.9410881  0.01478247]\n",
      " [0.03790459 0.00621949 0.9410953  0.01478067]]\n",
      "Iteration 827, Accuracy 0.36569\n",
      "95.06555%change in label assignment\n",
      "0.083836116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02260978 0.9020424  0.01148868 0.06385911]\n",
      " [0.05133812 0.66351527 0.02081306 0.26433352]\n",
      " [0.06601176 0.14996941 0.02133412 0.7626847 ]\n",
      " ...\n",
      " [0.03231873 0.8655985  0.01696018 0.08512253]\n",
      " [0.03806694 0.00623623 0.94086856 0.0148283 ]\n",
      " [0.03806031 0.00623536 0.9408786  0.01482577]]\n",
      "Iteration 828, Accuracy 0.36643\n",
      "87.63195%change in label assignment\n",
      "0.08989138\n",
      "[[0.03430287 0.79736656 0.01499797 0.15333259]\n",
      " [0.05728935 0.09458362 0.01805764 0.83006936]\n",
      " [0.08327411 0.04189423 0.01958285 0.8552488 ]\n",
      " ...\n",
      " [0.0269873  0.84806    0.01207945 0.11287326]\n",
      " [0.03782999 0.00621252 0.9412005  0.01475695]\n",
      " [0.03782683 0.00621221 0.9412051  0.01475581]]\n",
      "Iteration 829, Accuracy 0.36373\n",
      "85.30957%change in label assignment\n",
      "0.08940645\n",
      "[[0.04483742 0.82076746 0.02420798 0.11018713]\n",
      " [0.03057337 0.8259272  0.01313983 0.13035965]\n",
      " [0.07395438 0.27129325 0.02575938 0.628993  ]\n",
      " ...\n",
      " [0.05171252 0.7970199  0.02840463 0.12286288]\n",
      " [0.03807752 0.00623732 0.94084615 0.01483904]\n",
      " [0.03807531 0.00623713 0.9408493  0.01483823]]\n",
      "Iteration 830, Accuracy 0.36868\n",
      "87.04277%change in label assignment\n",
      "0.08606192\n",
      "[[0.01254893 0.93941844 0.0060498  0.04198286]\n",
      " [0.07310504 0.28990716 0.02646834 0.6105194 ]\n",
      " [0.048812   0.04811647 0.01362312 0.8894484 ]\n",
      " ...\n",
      " [0.01334882 0.937436   0.00655935 0.04265579]\n",
      " [0.03783399 0.00621481 0.9411792  0.01477194]\n",
      " [0.03783049 0.00621441 0.94118446 0.01477064]]\n",
      "Iteration 831, Accuracy 0.38891\n",
      "90.46006%change in label assignment\n",
      "0.07752508\n",
      "[[0.01467746 0.9256264  0.00676673 0.05292939]\n",
      " [0.06958649 0.19020544 0.02330621 0.7169019 ]\n",
      " [0.05299987 0.04023113 0.0134772  0.89329183]\n",
      " ...\n",
      " [0.0131044  0.9353418  0.00614322 0.04541053]\n",
      " [0.03784892 0.00621194 0.9411756  0.01476356]\n",
      " [0.03784569 0.00621157 0.9411804  0.01476236]]\n",
      "Iteration 832, Accuracy 0.3679\n",
      "97.08843%change in label assignment\n",
      "0.07212946\n",
      "[[0.01381016 0.93029207 0.00647302 0.0494248 ]\n",
      " [0.07241024 0.25813118 0.02567842 0.6437802 ]\n",
      " [0.04843932 0.0466318  0.01333442 0.89159447]\n",
      " ...\n",
      " [0.01257798 0.9400597  0.00610278 0.04125955]\n",
      " [0.03783847 0.00620945 0.94119596 0.0147561 ]\n",
      " [0.03783198 0.00620859 0.9412058  0.01475363]]\n",
      "Iteration 833, Accuracy 0.37094\n",
      "93.86262%change in label assignment\n",
      "0.07615179\n",
      "[[0.01593176 0.9173935  0.00735077 0.05932393]\n",
      " [0.068243   0.18073006 0.02310329 0.7279237 ]\n",
      " [0.05345419 0.03970616 0.01378594 0.89305365]\n",
      " ...\n",
      " [0.01371294 0.9309087  0.00643529 0.048943  ]\n",
      " [0.03775385 0.0062004  0.9413101  0.01473565]\n",
      " [0.0377519  0.00620027 0.9413129  0.01473498]]\n",
      "Iteration 834, Accuracy 0.35975\n",
      "93.60731%change in label assignment\n",
      "0.071383536\n",
      "[[0.01270712 0.93770134 0.00600779 0.04358371]\n",
      " [0.07280044 0.26354644 0.02566041 0.63799274]\n",
      " [0.04862114 0.04632161 0.01321682 0.89184046]\n",
      " ...\n",
      " [0.0126269  0.9399917  0.00608969 0.0412917 ]\n",
      " [0.0377588  0.00619963 0.9413062  0.01473544]\n",
      " [0.03775617 0.00619938 0.9413099  0.01473449]]\n",
      "Iteration 835, Accuracy 0.36716\n",
      "97.3241%change in label assignment\n",
      "0.07494688\n",
      "[[0.01307169 0.93493795 0.00619928 0.04579112]\n",
      " [0.07091751 0.22147201 0.0248185  0.682792  ]\n",
      " [0.05002435 0.04225297 0.01344875 0.894274  ]\n",
      " ...\n",
      " [0.01280301 0.9367926  0.00610803 0.0442964 ]\n",
      " [0.03767317 0.00619152 0.9414193  0.01471601]\n",
      " [0.03767316 0.00619168 0.94141906 0.01471609]]\n",
      "Iteration 836, Accuracy 0.36608\n",
      "95.57618%change in label assignment\n",
      "0.07281524\n",
      "[[0.01906589 0.8985032  0.0086094  0.07382147]\n",
      " [0.0636704  0.13630028 0.0205643  0.779465  ]\n",
      " [0.06501079 0.03898333 0.01559405 0.8804118 ]\n",
      " ...\n",
      " [0.01790775 0.9055262  0.00813869 0.06842737]\n",
      " [0.03763518 0.00618537 0.9414876  0.01469186]\n",
      " [0.03763546 0.00618558 0.9414869  0.01469206]]\n",
      "Iteration 837, Accuracy 0.37296\n",
      "95.98861%change in label assignment\n",
      "0.076016136\n",
      "[[0.01273083 0.9397051  0.00620243 0.04136167]\n",
      " [0.07262614 0.3412995  0.02681567 0.5592587 ]\n",
      " [0.04840212 0.0524542  0.01373559 0.88540804]\n",
      " ...\n",
      " [0.0139505  0.93537945 0.00690004 0.04377006]\n",
      " [0.0376354  0.00618547 0.9414873  0.01469182]\n",
      " [0.03763509 0.0061856  0.9414875  0.0146918 ]]\n",
      "Iteration 838, Accuracy 0.37585\n",
      "95.37978%change in label assignment\n",
      "0.071347974\n",
      "[[0.01816281 0.9035608  0.00832419 0.06995216]\n",
      " [0.06694252 0.16890112 0.02263243 0.741524  ]\n",
      " [0.05850658 0.03898564 0.01477386 0.887734  ]\n",
      " ...\n",
      " [0.01693032 0.91113514 0.00781714 0.06411745]\n",
      " [0.03752478 0.00617393 0.9416361  0.01466508]\n",
      " [0.03752575 0.00617429 0.94163436 0.01466559]]\n",
      "Iteration 839, Accuracy 0.37521\n",
      "95.2914%change in label assignment\n",
      "[[0.01317368 0.93478465 0.00618847 0.0458531 ]\n",
      " [0.07213815 0.24096112 0.02508596 0.66181475]\n",
      " [0.05064998 0.04168779 0.01324395 0.89441824]\n",
      " ...\n",
      " [0.01272229 0.93773496 0.00602566 0.04351706]\n",
      " [0.03752872 0.00617354 0.94163543 0.01466228]\n",
      " [0.0375296  0.00617387 0.94163376 0.01466273]]\n",
      "Iteration 840, Accuracy 0.3733\n",
      "98.4632%change in label assignment\n",
      "0.07866435\n",
      "[[0.01542457 0.9202452  0.00719633 0.05713394]\n",
      " [0.07031658 0.21521385 0.02460094 0.6898686 ]\n",
      " [0.05057681 0.0412393  0.01350894 0.89467496]\n",
      " ...\n",
      " [0.01623997 0.9153238  0.00755528 0.06088096]\n",
      " [0.03748702 0.00616706 0.9417007  0.01464527]\n",
      " [0.0374865  0.00616716 0.9417011  0.01464518]]\n",
      "Iteration 841, Accuracy 0.37738\n",
      "96.58271%change in label assignment\n",
      "0.07660354\n",
      "[[0.01653346 0.9138511  0.00757535 0.0620401 ]\n",
      " [0.06796833 0.17709519 0.02277676 0.7321598 ]\n",
      " [0.05507885 0.0392193  0.01389768 0.89180416]\n",
      " ...\n",
      " [0.01543625 0.92043084 0.00712613 0.0570068 ]\n",
      " [0.03749017 0.00616701 0.9416982  0.01464462]\n",
      " [0.03749026 0.00616722 0.9416978  0.01464476]]\n",
      "Iteration 842, Accuracy 0.37497\n",
      "97.92311%change in label assignment\n",
      "0.07658777\n",
      "[[0.0135688  0.9315322  0.00640918 0.0484898 ]\n",
      " [0.07085442 0.22678807 0.02491353 0.677444  ]\n",
      " [0.04915439 0.04268954 0.01330654 0.89484954]\n",
      " ...\n",
      " [0.01373894 0.9305218  0.006492   0.04924723]\n",
      " [0.03744651 0.00616397 0.9417554  0.01463411]\n",
      " [0.03744606 0.00616409 0.9417558  0.01463405]]\n",
      "Iteration 843, Accuracy 0.37836\n",
      "97.79054%change in label assignment\n",
      "0.07513212\n",
      "[[0.01489355 0.9235876  0.00690779 0.05461103]\n",
      " [0.06766099 0.17638406 0.0227645  0.7331904 ]\n",
      " [0.05147356 0.04031359 0.01338709 0.89482576]\n",
      " ...\n",
      " [0.01661032 0.9130663  0.0076367  0.06268669]\n",
      " [0.03742237 0.0061609  0.94179153 0.01462519]\n",
      " [0.03742158 0.00616095 0.9417925  0.01462498]]\n",
      "Iteration 844, Accuracy 0.37684\n",
      "99.1064%change in label assignment\n",
      "0.08124699\n",
      "[[0.01576266 0.9283106  0.00792836 0.04799839]\n",
      " [0.06958639 0.4286238  0.02677669 0.4750131 ]\n",
      " [0.05587034 0.09202355 0.01750901 0.8345972 ]\n",
      " ...\n",
      " [0.01295091 0.9388398  0.00637051 0.04183882]\n",
      " [0.03742745 0.00616245 0.94177204 0.01463806]\n",
      " [0.03742636 0.00616244 0.9417734  0.01463773]]\n",
      "Iteration 845, Accuracy 0.37792\n",
      "93.74478%change in label assignment\n",
      "0.08269575\n",
      "[[0.01792229 0.9048724  0.00820494 0.06900039]\n",
      " [0.06833915 0.18693997 0.02331387 0.72140694]\n",
      " [0.04796267 0.04581893 0.01321699 0.89300144]\n",
      " ...\n",
      " [0.02805069 0.8394799  0.01240374 0.12006558]\n",
      " [0.03735906 0.00615375 0.9418766  0.01461059]\n",
      " [0.0373552  0.00615327 0.9418824  0.01460916]]\n",
      "Iteration 846, Accuracy 0.37197\n",
      "92.4142%change in label assignment\n",
      "0.08642897\n",
      "[[0.02817093 0.88054764 0.01481056 0.07647087]\n",
      " [0.04818333 0.68572414 0.02005714 0.24603534]\n",
      " [0.07015882 0.21343173 0.0243466  0.6920628 ]\n",
      " ...\n",
      " [0.01978979 0.91262317 0.01011891 0.05746812]\n",
      " [0.03743136 0.00616382 0.94176257 0.0146422 ]\n",
      " [0.0374293  0.00616364 0.94176567 0.01464148]]\n",
      "Iteration 847, Accuracy 0.36436\n",
      "90.79393%change in label assignment\n",
      "0.08899216\n",
      "[[0.02491957 0.8601115  0.01118864 0.10378029]\n",
      " [0.0640949  0.14630322 0.02141618 0.7681857 ]\n",
      " [0.05219507 0.04011384 0.01377905 0.8939121 ]\n",
      " ...\n",
      " [0.0362232  0.7811514  0.0157627  0.16686273]\n",
      " [0.03731234 0.00615149 0.9419373  0.0145989 ]\n",
      " [0.03730868 0.00615104 0.9419428  0.01459754]]\n",
      "Iteration 848, Accuracy 0.37404\n",
      "90.40605%change in label assignment\n",
      "0.08860799\n",
      "[[0.03153707 0.8680313  0.0165677  0.08386386]\n",
      " [0.03596601 0.7857681  0.01530338 0.16296251]\n",
      " [0.07342822 0.2988396  0.02615924 0.60157293]\n",
      " ...\n",
      " [0.02084124 0.9085013  0.01058105 0.06007639]\n",
      " [0.03753111 0.00617429 0.9416332  0.0146614 ]\n",
      " [0.03752358 0.00617321 0.94164467 0.01465849]]\n",
      "Iteration 849, Accuracy 0.36554\n",
      "85.23101%change in label assignment\n",
      "0.0915564\n",
      "[[0.02263605 0.8758132  0.01045103 0.09109972]\n",
      " [0.06671016 0.16642119 0.02317578 0.7436929 ]\n",
      " [0.05277058 0.0432435  0.01456412 0.8894218 ]\n",
      " ...\n",
      " [0.03437709 0.7959505  0.01533741 0.15433502]\n",
      " [0.03728272 0.00615242 0.94197637 0.01458852]\n",
      " [0.03727882 0.00615191 0.9419822  0.01458706]]\n",
      "Iteration 850, Accuracy 0.34806\n",
      "86.87583%change in label assignment\n",
      "0.09292121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03930187 0.839769   0.02093188 0.0999972 ]\n",
      " [0.03575154 0.7894499  0.01516377 0.15963481]\n",
      " [0.07383545 0.2747679  0.02580696 0.62558967]\n",
      " ...\n",
      " [0.02950918 0.87547517 0.01530699 0.07970867]\n",
      " [0.03753176 0.00617416 0.9416304  0.01466369]\n",
      " [0.03752877 0.00617382 0.9416349  0.01466259]]\n",
      "Iteration 851, Accuracy 0.36495\n",
      "90.18019%change in label assignment\n",
      "0.09202722\n",
      "[[0.01588874 0.9174463  0.00749012 0.05917475]\n",
      " [0.06794389 0.18543544 0.02370281 0.72291785]\n",
      " [0.05165172 0.04179144 0.01403415 0.89252263]\n",
      " ...\n",
      " [0.02326608 0.871144   0.01064287 0.09494703]\n",
      " [0.03733313 0.00615569 0.94190913 0.01460208]\n",
      " [0.03733064 0.00615544 0.9419127  0.0146012 ]]\n",
      "Iteration 852, Accuracy 0.37517\n",
      "90.17037%change in label assignment\n",
      "0.08210749\n",
      "[[0.02275757 0.9008354  0.01152435 0.06488268]\n",
      " [0.06610598 0.50341517 0.02537131 0.40510756]\n",
      " [0.05806106 0.09558639 0.01758747 0.8287651 ]\n",
      " ...\n",
      " [0.01712377 0.9225384  0.00846026 0.05187758]\n",
      " [0.03752333 0.0061711  0.94165987 0.01464569]\n",
      " [0.0375215  0.00617096 0.9416625  0.01464505]]\n",
      "Iteration 853, Accuracy 0.37644\n",
      "96.42068%change in label assignment\n",
      "0.0841308\n",
      "[[0.01283119 0.9364779  0.00619842 0.04449249]\n",
      " [0.07171761 0.26767766 0.02615742 0.6344473 ]\n",
      " [0.04799718 0.04816433 0.01369799 0.8901405 ]\n",
      " ...\n",
      " [0.01617801 0.91543144 0.00760247 0.0607881 ]\n",
      " [0.0373876  0.00615847 0.9418443  0.01460963]\n",
      " [0.03738516 0.00615826 0.9418478  0.01460878]]\n",
      "Iteration 854, Accuracy 0.37865\n",
      "96.69073%change in label assignment\n",
      "0.07653933\n",
      "[[0.02707585 0.884532   0.01398068 0.07441145]\n",
      " [0.05211305 0.654486   0.02114798 0.272253  ]\n",
      " [0.06643778 0.1575219  0.02171234 0.75432795]\n",
      " ...\n",
      " [0.0192578  0.91443837 0.00966779 0.05663606]\n",
      " [0.03750094 0.00616817 0.9416824  0.01464852]\n",
      " [0.03749845 0.00616794 0.941686   0.01464763]]\n",
      "Iteration 855, Accuracy 0.37433\n",
      "94.5451%change in label assignment\n",
      "0.08208068\n",
      "[[0.04204493 0.73634547 0.01824264 0.20336698]\n",
      " [0.05293101 0.07630401 0.01653351 0.85423154]\n",
      " [0.0856827  0.04245635 0.02047024 0.8513907 ]\n",
      " ...\n",
      " [0.05451399 0.62629724 0.02288342 0.29630536]\n",
      " [0.03732925 0.00614594 0.94192326 0.01460152]\n",
      " [0.03732717 0.00614578 0.9419263  0.01460081]]\n",
      "Iteration 856, Accuracy 0.37188\n",
      "89.4388%change in label assignment\n",
      "0.10072404\n",
      "[[0.08726908 0.6819165  0.05191602 0.17889851]\n",
      " [0.01761789 0.9205467  0.0086918  0.05314357]\n",
      " [0.04360939 0.73076713 0.01809097 0.20753253]\n",
      " ...\n",
      " [0.07633989 0.715753   0.04439709 0.16350992]\n",
      " [0.03764745 0.00618483 0.9414524  0.01471529]\n",
      " [0.03764577 0.00618471 0.9414548  0.0147147 ]]\n",
      "Iteration 857, Accuracy 0.37757\n",
      "80.00687%change in label assignment\n",
      "0.09789457\n",
      "[[0.02993688 0.8734358  0.01598451 0.08064289]\n",
      " [0.05763536 0.59224904 0.02366086 0.32645476]\n",
      " [0.05890273 0.11043672 0.01917642 0.8114841 ]\n",
      " ...\n",
      " [0.02214213 0.90303105 0.01154196 0.06328489]\n",
      " [0.03735834 0.00616158 0.94184023 0.01463979]\n",
      " [0.03735792 0.00616171 0.94184065 0.01463975]]\n",
      "Iteration 858, Accuracy 0.40217\n",
      "83.40453%change in label assignment\n",
      "0.081790015\n",
      "[[0.01248546 0.9401107  0.00602831 0.04137553]\n",
      " [0.07220123 0.266028   0.02567789 0.63609296]\n",
      " [0.04817021 0.04393073 0.01305501 0.89484406]\n",
      " ...\n",
      " [0.01324044 0.93356824 0.00623994 0.04695139]\n",
      " [0.03737663 0.00615557 0.94183624 0.01463149]\n",
      " [0.03737474 0.00615548 0.9418389  0.01463087]]\n",
      "Iteration 859, Accuracy 0.38361\n",
      "95.12938%change in label assignment\n",
      "0.07454157\n",
      "[[0.01225485 0.9410177  0.00594768 0.04077986]\n",
      " [0.07088861 0.23460963 0.02501182 0.6694899 ]\n",
      " [0.04799091 0.04345005 0.01310719 0.8954519 ]\n",
      " ...\n",
      " [0.01348202 0.9317153  0.00636753 0.04843512]\n",
      " [0.03737975 0.00615292 0.94183975 0.01462755]\n",
      " [0.03737924 0.00615304 0.94184023 0.01462747]]\n",
      "Iteration 860, Accuracy 0.38052\n",
      "98.17352%change in label assignment\n",
      "0.073300876\n",
      "[[0.01227657 0.9409795  0.00595987 0.04078402]\n",
      " [0.07124044 0.24368303 0.02526778 0.65980875]\n",
      " [0.0474037  0.04599549 0.01317368 0.8934272 ]\n",
      " ...\n",
      " [0.01339686 0.9322548  0.00632995 0.0480184 ]\n",
      " [0.037369   0.00614923 0.94185805 0.0146237 ]\n",
      " [0.03736865 0.00614937 0.94185835 0.01462369]]\n",
      "Iteration 861, Accuracy 0.38268\n",
      "98.92964%change in label assignment\n",
      "0.071546584\n",
      "[[0.01218998 0.940681   0.00588206 0.0412469 ]\n",
      " [0.06914164 0.20306139 0.02397393 0.70382303]\n",
      " [0.0486873  0.04179152 0.01313764 0.8963836 ]\n",
      " ...\n",
      " [0.01403418 0.92827433 0.00660031 0.0510912 ]\n",
      " [0.03733964 0.00614545 0.94190097 0.0146139 ]\n",
      " [0.03734031 0.00614577 0.9418996  0.01461429]]\n",
      "Iteration 862, Accuracy 0.38017\n",
      "96.88712%change in label assignment\n",
      "0.0732161\n",
      "[[0.01221578 0.9403363  0.00588254 0.04156547]\n",
      " [0.06672618 0.17323326 0.0226667  0.7373738 ]\n",
      " [0.05192212 0.03938333 0.01355977 0.8951348 ]\n",
      " ...\n",
      " [0.0139825  0.9285422  0.00657704 0.0508982 ]\n",
      " [0.0373198  0.00614322 0.9419346  0.01460236]\n",
      " [0.03732195 0.0061438  0.9419309  0.01460336]]\n",
      "Iteration 863, Accuracy 0.38165\n",
      "96.05735%change in label assignment\n",
      "0.073112115\n",
      "[[0.01297045 0.93890536 0.00638312 0.04174103]\n",
      " [0.07174703 0.26304695 0.02570425 0.6395017 ]\n",
      " [0.04737521 0.0500509  0.01342456 0.88914937]\n",
      " ...\n",
      " [0.01218561 0.9405282  0.00588114 0.04140508]\n",
      " [0.03728886 0.00613739 0.9419844  0.01458929]\n",
      " [0.03728885 0.00613761 0.9419841  0.01458943]]\n",
      "Iteration 864, Accuracy 0.38229\n",
      "95.56145%change in label assignment\n",
      "0.07826466\n",
      "[[0.01272875 0.93967324 0.00628175 0.04131623]\n",
      " [0.07169095 0.27020276 0.02602637 0.63207996]\n",
      " [0.04852346 0.05913499 0.01437041 0.8779711 ]\n",
      " ...\n",
      " [0.01248286 0.9380953  0.00601005 0.04341181]\n",
      " [0.03723213 0.0061318  0.9420575  0.01457857]\n",
      " [0.03722917 0.0061315  0.9420618  0.01457752]]\n",
      "Iteration 865, Accuracy 0.38189\n",
      "93.4551%change in label assignment\n",
      "0.07503241\n",
      "[[0.01234627 0.94062144 0.00598213 0.04105017]\n",
      " [0.07210524 0.27337834 0.02587234 0.62864405]\n",
      " [0.0498982  0.06452972 0.01474979 0.8708223 ]\n",
      " ...\n",
      " [0.01289477 0.9354781  0.00611759 0.04550954]\n",
      " [0.03724376 0.00613496 0.9420331  0.01458817]\n",
      " [0.03723617 0.00613391 0.9420447  0.01458527]]\n",
      "Iteration 866, Accuracy 0.37453\n",
      "93.84789%change in label assignment\n",
      "0.07512889\n",
      "[[0.01225663 0.94064057 0.00598653 0.04111629]\n",
      " [0.07180383 0.2809251  0.02625896 0.6210121 ]\n",
      " [0.04969707 0.06519166 0.0149944  0.8701169 ]\n",
      " ...\n",
      " [0.01284947 0.9356698  0.00616917 0.04531156]\n",
      " [0.0371768  0.00613117 0.9421201  0.01457196]\n",
      " [0.03716625 0.00612967 0.9421361  0.01456793]]\n",
      "Iteration 867, Accuracy 0.36775\n",
      "97.54505%change in label assignment\n",
      "0.077756725\n",
      "[[0.01522619 0.9209648  0.00708014 0.05672887]\n",
      " [0.06635736 0.1698585  0.02240321 0.74138093]\n",
      " [0.04850964 0.04205997 0.0130588  0.8963716 ]\n",
      " ...\n",
      " [0.01857323 0.9002513  0.00850239 0.07267303]\n",
      " [0.03712733 0.00612678 0.9421956  0.01455033]\n",
      " [0.03711786 0.00612548 0.9422099  0.01454674]]\n",
      "Iteration 868, Accuracy 0.36215\n",
      "94.74149%change in label assignment\n",
      "0.0787653\n",
      "[[0.02120892 0.90677536 0.01097639 0.06103931]\n",
      " [0.05912063 0.57306886 0.02395333 0.3438572 ]\n",
      " [0.06370571 0.14790528 0.02135285 0.76703614]\n",
      " ...\n",
      " [0.0191566  0.91476524 0.00984394 0.05623425]\n",
      " [0.03718643 0.00613538 0.94210273 0.01457551]\n",
      " [0.0371722  0.00613335 0.94212437 0.01457006]]\n",
      "Iteration 869, Accuracy 0.37158\n",
      "90.35204%change in label assignment\n",
      "0.08340989\n",
      "[[0.02130667 0.8826375  0.0097287  0.0863271 ]\n",
      " [0.06265638 0.14034398 0.02097776 0.77602196]\n",
      " [0.05138924 0.03969536 0.01367206 0.8952434 ]\n",
      " ...\n",
      " [0.02469384 0.8605989  0.01116462 0.10354268]\n",
      " [0.03706682 0.00612442 0.94226986 0.01453895]\n",
      " [0.03705102 0.00612214 0.94229394 0.0145329 ]]\n",
      "Iteration 870, Accuracy 0.35616\n",
      "93.50911%change in label assignment\n",
      "0.08063809\n",
      "[[0.02511266 0.8917894  0.01308702 0.07001097]\n",
      " [0.05270812 0.6418895  0.0216455  0.28375688]\n",
      " [0.06631593 0.16986622 0.02238194 0.7414359 ]\n",
      " ...\n",
      " [0.02297718 0.8999763  0.01190044 0.06514613]\n",
      " [0.0371635  0.00613353 0.9421346  0.01456828]\n",
      " [0.03715244 0.00613199 0.9421515  0.01456406]]\n",
      "Iteration 871, Accuracy 0.35759\n",
      "91.97722%change in label assignment\n",
      "0.081436865\n",
      "[[0.01662878 0.9119684  0.00777051 0.06363226]\n",
      " [0.06644032 0.1765715  0.02298646 0.7340017 ]\n",
      " [0.04958    0.04101384 0.01349036 0.89591575]\n",
      " ...\n",
      " [0.01906485 0.89683795 0.00882634 0.07527088]\n",
      " [0.03698264 0.00611848 0.94238305 0.01451584]\n",
      " [0.03697141 0.00611694 0.9424001  0.01451159]]\n",
      "Iteration 872, Accuracy 0.35724\n",
      "92.47803%change in label assignment\n",
      "0.076106764\n",
      "[[0.01686038 0.92366666 0.00845282 0.05102009]\n",
      " [0.06601329 0.48910448 0.02562292 0.41925937]\n",
      " [0.05700726 0.09893918 0.01778264 0.82627094]\n",
      " ...\n",
      " [0.01680049 0.9239285  0.0084324  0.05083865]\n",
      " [0.03710517 0.0061284  0.94222283 0.01454357]\n",
      " [0.0370899  0.00612625 0.94224614 0.01453773]]\n",
      "Iteration 873, Accuracy 0.36299\n",
      "95.06064%change in label assignment\n",
      "0.07948812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01565336 0.9180157  0.00738997 0.05894098]\n",
      " [0.06898325 0.21279441 0.02458751 0.6936348 ]\n",
      " [0.04817114 0.04338874 0.01349111 0.894949  ]\n",
      " ...\n",
      " [0.01623311 0.91451883 0.00765371 0.06159432]\n",
      " [0.03694974 0.00611392 0.94243515 0.01450119]\n",
      " [0.03693837 0.0061124  0.9424523  0.01449693]]\n",
      "Iteration 874, Accuracy 0.35837\n",
      "95.9346%change in label assignment\n",
      "0.074932694\n",
      "[[0.01470834 0.93209934 0.00731323 0.04587907]\n",
      " [0.06949905 0.42037317 0.02649127 0.48363653]\n",
      " [0.05095232 0.07085721 0.0152916  0.8628988 ]\n",
      " ...\n",
      " [0.01338884 0.93711686 0.00659493 0.04289936]\n",
      " [0.03701245 0.006117   0.9423573  0.01451315]\n",
      " [0.03700836 0.00611666 0.9423632  0.01451175]]\n",
      "Iteration 875, Accuracy 0.35872\n",
      "93.406%change in label assignment\n",
      "0.07447044\n",
      "[[0.0123699  0.9405909  0.00608965 0.04094954]\n",
      " [0.07037182 0.38300687 0.02693418 0.5196871 ]\n",
      " [0.04901287 0.06404422 0.01482476 0.8721181 ]\n",
      " ...\n",
      " [0.01233314 0.9405888  0.00606851 0.04100959]\n",
      " [0.03694917 0.0061106  0.9424422  0.01449805]\n",
      " [0.03694221 0.00610983 0.9424524  0.01449555]]\n",
      "Iteration 876, Accuracy 0.37276\n",
      "96.131%change in label assignment\n",
      "0.07447019\n",
      "[[0.01493731 0.92229146 0.00698103 0.05579021]\n",
      " [0.06875817 0.20760229 0.02397825 0.69966125]\n",
      " [0.04977586 0.04016704 0.01325242 0.8968047 ]\n",
      " ...\n",
      " [0.01708903 0.90896076 0.00790333 0.06604688]\n",
      " [0.03688971 0.00610314 0.9425371  0.01447009]\n",
      " [0.03688661 0.00610299 0.94254136 0.01446911]]\n",
      "Iteration 877, Accuracy 0.36328\n",
      "93.13105%change in label assignment\n",
      "0.07710012\n",
      "[[0.01668484 0.92433196 0.00847381 0.05050937]\n",
      " [0.06591616 0.47845122 0.02593025 0.4297024 ]\n",
      " [0.05268321 0.08190964 0.01639715 0.84901   ]\n",
      " ...\n",
      " [0.01411293 0.93440384 0.00706639 0.04441685]\n",
      " [0.03692366 0.00610632 0.9424891  0.01448095]\n",
      " [0.03692364 0.00610667 0.94248843 0.0144812 ]]\n",
      "Iteration 878, Accuracy 0.37404\n",
      "95.49762%change in label assignment\n",
      "0.070689216\n",
      "[[0.01255356 0.9370314  0.00602019 0.0443949 ]\n",
      " [0.07118348 0.2774425  0.02599573 0.62537825]\n",
      " [0.04648109 0.04824099 0.01329948 0.89197844]\n",
      " ...\n",
      " [0.01334795 0.9319086  0.00635265 0.04839082]\n",
      " [0.03684028 0.00609613 0.9426064  0.0144573 ]\n",
      " [0.03683638 0.00609585 0.9426118  0.01445601]]\n",
      "Iteration 879, Accuracy 0.37379\n",
      "96.01316%change in label assignment\n",
      "[[0.01246571 0.9403735  0.00612133 0.04103945]\n",
      " [0.07146158 0.33060223 0.02659121 0.571345  ]\n",
      " [0.04752716 0.05660328 0.01395733 0.88191223]\n",
      " ...\n",
      " [0.01220293 0.93958795 0.00588439 0.04232468]\n",
      " [0.03681462 0.00609393 0.9426436  0.01444784]\n",
      " [0.03681412 0.00609417 0.9426439  0.01444787]]\n",
      "Iteration 880, Accuracy 0.36942\n",
      "96.78892%change in label assignment\n",
      "0.08207617\n",
      "[[0.0160037  0.9153952  0.00748842 0.06111273]\n",
      " [0.06819496 0.20524307 0.02404764 0.7025144 ]\n",
      " [0.04700553 0.04316563 0.01310792 0.8967209 ]\n",
      " ...\n",
      " [0.02180695 0.8787421  0.0099738  0.08947716]\n",
      " [0.03682192 0.00609011 0.94264597 0.01444194]\n",
      " [0.03681968 0.00609008 0.942649   0.0144413 ]]\n",
      "Iteration 881, Accuracy 0.37261\n",
      "96.18992%change in label assignment\n",
      "0.0819914\n",
      "[[0.01311938 0.9380743  0.00647482 0.0423315 ]\n",
      " [0.06881622 0.42808497 0.02646207 0.47663674]\n",
      " [0.0535356  0.08509111 0.01661684 0.8447565 ]\n",
      " ...\n",
      " [0.01231474 0.9389787  0.00591843 0.04278821]\n",
      " [0.03689981 0.00609749 0.9425343  0.01446834]\n",
      " [0.03689614 0.00609724 0.9425396  0.01446714]]\n",
      "Iteration 882, Accuracy 0.36947\n",
      "96.40104%change in label assignment\n",
      "0.08331232\n",
      "[[0.04093088 0.7408795  0.01786753 0.20032212]\n",
      " [0.04767904 0.05696429 0.01437835 0.8809783 ]\n",
      " [0.10467951 0.04488413 0.02408016 0.82635623]\n",
      " ...\n",
      " [0.04477967 0.70914197 0.01938678 0.22669157]\n",
      " [0.03676731 0.00608579 0.94271904 0.01442787]\n",
      " [0.03676901 0.00608644 0.9427157  0.01442883]]\n",
      "Iteration 883, Accuracy 0.36746\n",
      "86.86601%change in label assignment\n",
      "0.09210982\n",
      "[[0.04680983 0.81238294 0.02581928 0.11498791]\n",
      " [0.03095719 0.81886286 0.01353549 0.13664444]\n",
      " [0.07216515 0.2972085  0.02616182 0.6044646 ]\n",
      " ...\n",
      " [0.03915207 0.8394985  0.02122213 0.10012721]\n",
      " [0.03700886 0.00611137 0.9423671  0.01451263]\n",
      " [0.03701103 0.00611209 0.9423632  0.01451374]]\n",
      "Iteration 884, Accuracy 0.37885\n",
      "86.19335%change in label assignment\n",
      "0.09969331\n",
      "[[0.02058866 0.88663036 0.00956508 0.08321593]\n",
      " [0.0548639  0.09387796 0.01782864 0.8334295 ]\n",
      " [0.07257038 0.04061805 0.01810399 0.8687076 ]\n",
      " ...\n",
      " [0.02345922 0.86806893 0.01080986 0.09766199]\n",
      " [0.03678991 0.00609061 0.942682   0.01443753]\n",
      " [0.03679126 0.00609119 0.9426792  0.01443834]]\n",
      "Iteration 885, Accuracy 0.38994\n",
      "87.2097%change in label assignment\n",
      "0.08136752\n",
      "[[0.0169951  0.9229805  0.00857286 0.05145154]\n",
      " [0.07131542 0.36259946 0.02666885 0.5394163 ]\n",
      " [0.04966001 0.06531189 0.01476279 0.8702653 ]\n",
      " ...\n",
      " [0.01522903 0.9299114  0.00761709 0.04724252]\n",
      " [0.03693709 0.00610186 0.94247574 0.01448536]\n",
      " [0.03693952 0.00610261 0.9424713  0.01448656]]\n",
      "Iteration 886, Accuracy 0.37939\n",
      "95.6351%change in label assignment\n",
      "0.081074305\n",
      "[[0.01942317 0.89388317 0.00902626 0.07766736]\n",
      " [0.05320848 0.08609828 0.01702622 0.8436671 ]\n",
      " [0.07432808 0.04046062 0.01828514 0.86692613]\n",
      " ...\n",
      " [0.02193785 0.8777866  0.01011803 0.09015755]\n",
      " [0.03682075 0.00608983 0.9426412  0.01444816]\n",
      " [0.03682435 0.00609078 0.94263494 0.01444986]]\n",
      "Iteration 887, Accuracy 0.37963\n",
      "93.86753%change in label assignment\n",
      "0.07878895\n",
      "[[0.01570863 0.9279941  0.00786915 0.04842817]\n",
      " [0.07197086 0.30796236 0.0263051  0.5937617 ]\n",
      " [0.04856946 0.0594257  0.01420346 0.87780136]\n",
      " ...\n",
      " [0.01401349 0.9345653  0.00694944 0.04447183]\n",
      " [0.0369204  0.00609678 0.94250476 0.01447804]\n",
      " [0.0369225  0.00609747 0.94250095 0.01447911]]\n",
      "Iteration 888, Accuracy 0.37841\n",
      "94.55001%change in label assignment\n",
      "0.078459725\n",
      "[[0.03086594 0.81663126 0.01380377 0.13869908]\n",
      " [0.0491593  0.06702043 0.01515974 0.86866057]\n",
      " [0.08652285 0.04205799 0.02052022 0.850899  ]\n",
      " ...\n",
      " [0.0317336  0.8104624  0.01418816 0.14361578]\n",
      " [0.03676365 0.00608128 0.94272894 0.01442605]\n",
      " [0.03676396 0.00608169 0.942728   0.01442644]]\n",
      "Iteration 889, Accuracy 0.37939\n",
      "94.18667%change in label assignment\n",
      "0.08282498\n",
      "[[0.02576504 0.8889072  0.01351841 0.07180932]\n",
      " [0.06122413 0.5453603  0.02453935 0.36887628]\n",
      " [0.06007809 0.12327027 0.01963463 0.797017  ]\n",
      " ...\n",
      " [0.02451815 0.8936748  0.0128296  0.06897746]\n",
      " [0.03689699 0.0060956  0.9425333  0.01447409]\n",
      " [0.03689807 0.00609615 0.942531   0.01447478]]\n",
      "Iteration 890, Accuracy 0.38327\n",
      "91.52551%change in label assignment\n",
      "0.09042295\n",
      "[[0.03024731 0.82092935 0.01363817 0.13518517]\n",
      " [0.04708937 0.05520591 0.01419055 0.88351417]\n",
      " [0.10750745 0.04533069 0.02471471 0.82244724]\n",
      " ...\n",
      " [0.03215409 0.80735856 0.01444977 0.1460375 ]\n",
      " [0.03671743 0.0060772  0.9427963  0.01440914]\n",
      " [0.03671997 0.00607796 0.9427917  0.01441041]]\n",
      "Iteration 891, Accuracy 0.37958\n",
      "89.36515%change in label assignment\n",
      "0.08561744\n",
      "[[0.02790399 0.8807077  0.01465687 0.07673153]\n",
      " [0.0665612  0.4749582  0.02588903 0.43259165]\n",
      " [0.05808521 0.1074838  0.01847614 0.8159548 ]\n",
      " ...\n",
      " [0.02494581 0.8919557  0.01299874 0.0700997 ]\n",
      " [0.03691734 0.00609447 0.9425193  0.01446881]\n",
      " [0.03691839 0.00609496 0.9425172  0.01446946]]\n",
      "Iteration 892, Accuracy 0.37934\n",
      "90.01817%change in label assignment\n",
      "0.0859813\n",
      "[[0.03252691 0.8043619  0.01454542 0.14856581]\n",
      " [0.04775777 0.06012983 0.01457295 0.87753946]\n",
      " [0.09754536 0.04382728 0.02277322 0.8358541 ]\n",
      " ...\n",
      " [0.03029574 0.82059026 0.01366763 0.13544644]\n",
      " [0.03671154 0.006075   0.9428024  0.01441112]\n",
      " [0.03671112 0.00607528 0.94280237 0.01441122]]\n",
      "Iteration 893, Accuracy 0.38101\n",
      "91.84956%change in label assignment\n",
      "0.083432145\n",
      "[[0.01487669 0.9311632  0.00743371 0.04652646]\n",
      " [0.07034092 0.23823048 0.02492967 0.6664989 ]\n",
      " [0.04690509 0.04708387 0.01314547 0.89286554]\n",
      " ...\n",
      " [0.01572925 0.92789006 0.00791925 0.04846149]\n",
      " [0.03686502 0.00609028 0.9425826  0.01446218]\n",
      " [0.03686798 0.0060911  0.94257724 0.0144636 ]]\n",
      "Iteration 894, Accuracy 0.38194\n",
      "94.61384%change in label assignment\n",
      "0.078240484\n",
      "[[0.02075333 0.8852107  0.00965507 0.08438094]\n",
      " [0.05080862 0.0750166  0.01609907 0.8580757 ]\n",
      " [0.07916677 0.04123444 0.01940661 0.8601922 ]\n",
      " ...\n",
      " [0.02047633 0.88710177 0.00955774 0.08286413]\n",
      " [0.03674087 0.00607582 0.9427546  0.01442864]\n",
      " [0.03674344 0.0060766  0.94275004 0.01442993]]\n",
      "Iteration 895, Accuracy 0.38474\n",
      "96.98041%change in label assignment\n",
      "0.06926152\n",
      "[[0.01731873 0.9213388  0.00867742 0.05266496]\n",
      " [0.07220053 0.26821736 0.02573764 0.63384444]\n",
      " [0.04862431 0.05316132 0.01377032 0.88444406]\n",
      " ...\n",
      " [0.01776444 0.91968966 0.0089429  0.05360302]\n",
      " [0.03682034 0.00608336 0.9426376  0.0144587 ]\n",
      " [0.0368223  0.00608401 0.9426339  0.01445972]]\n",
      "Iteration 896, Accuracy 0.38494\n",
      "96.34703%change in label assignment\n",
      "0.07783229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05952686 0.5611831  0.0246952  0.35459483]\n",
      " [0.05732242 0.03906629 0.01518237 0.888429  ]\n",
      " [0.22204982 0.05459831 0.04264983 0.68070203]\n",
      " ...\n",
      " [0.05259528 0.6367721  0.02239328 0.28823933]\n",
      " [0.03663055 0.00606375 0.9428972  0.01440857]\n",
      " [0.0366336  0.00606464 0.94289166 0.01441009]]\n",
      "Iteration 897, Accuracy 0.38577\n",
      "88.70722%change in label assignment\n",
      "0.08789212\n",
      "[[0.02491264 0.89217836 0.0130277  0.06988128]\n",
      " [0.07131478 0.36016783 0.02675434 0.541763  ]\n",
      " [0.04867264 0.06058771 0.01435303 0.8763866 ]\n",
      " ...\n",
      " [0.03030854 0.8717601  0.01613782 0.08179364]\n",
      " [0.03686123 0.00608801 0.94257104 0.01447972]\n",
      " [0.03686633 0.00608919 0.9425624  0.014482  ]]\n",
      "Iteration 898, Accuracy 0.3899\n",
      "91.19163%change in label assignment\n",
      "0.08322619\n",
      "[[0.01342625 0.93109435 0.00645974 0.04901972]\n",
      " [0.05448295 0.09401955 0.01772428 0.8337732 ]\n",
      " [0.07177605 0.03988142 0.01783998 0.8705026 ]\n",
      " ...\n",
      " [0.0124683  0.93742996 0.00608045 0.04402135]\n",
      " [0.03665229 0.00606805 0.94285655 0.01442317]\n",
      " [0.03665501 0.00606882 0.9428516  0.01442451]]\n",
      "Iteration 899, Accuracy 0.39063\n",
      "96.44032%change in label assignment\n",
      "0.07024522\n",
      "[[0.02053544 0.8869719  0.00940556 0.08308703]\n",
      " [0.04953967 0.06834011 0.01510168 0.8670185 ]\n",
      " [0.09064467 0.04216693 0.02084782 0.8463406 ]\n",
      " ...\n",
      " [0.01683271 0.91019046 0.00784369 0.06513312]\n",
      " [0.03663661 0.00606217 0.94289047 0.01441077]\n",
      " [0.03663829 0.00606279 0.9428873  0.0144117 ]]\n",
      "Iteration 900, Accuracy 0.38489\n",
      "98.27662%change in label assignment\n",
      "0.0688562\n",
      "[[0.01244368 0.9372903  0.00601314 0.04425288]\n",
      " [0.06096835 0.13401791 0.02056364 0.7844501 ]\n",
      " [0.05296461 0.03838055 0.01401474 0.89464   ]\n",
      " ...\n",
      " [0.01196346 0.9408756  0.0058528  0.04130819]\n",
      " [0.03660961 0.00605854 0.9429252  0.01440658]\n",
      " [0.03660926 0.00605883 0.9429252  0.01440672]]\n",
      "Iteration 901, Accuracy 0.38774\n",
      "95.37978%change in label assignment\n",
      "0.06940979\n",
      "[[0.02068234 0.8856184  0.00952863 0.08417064]\n",
      " [0.0473219  0.05907908 0.01427861 0.8793204 ]\n",
      " [0.09650641 0.04306568 0.02221048 0.83821744]\n",
      " ...\n",
      " [0.0201624  0.8889171  0.00932391 0.08159655]\n",
      " [0.03652004 0.00605095 0.9430434  0.01438552]\n",
      " [0.03652113 0.00605143 0.94304127 0.0143862 ]]\n",
      "Iteration 902, Accuracy 0.3868\n",
      "94.91825%change in label assignment\n",
      "0.07244633\n",
      "[[0.01318518 0.9378682  0.00658659 0.04236007]\n",
      " [0.06670712 0.1865272  0.02326514 0.72350055]\n",
      " [0.04659412 0.04267842 0.01298084 0.8977467 ]\n",
      " ...\n",
      " [0.01337129 0.9371878  0.00670306 0.04273782]\n",
      " [0.03656338 0.00605431 0.9429881  0.01439421]\n",
      " [0.03656309 0.00605457 0.9429881  0.01439434]]\n",
      "Iteration 903, Accuracy 0.38847\n",
      "94.13758%change in label assignment\n",
      "0.07408823\n",
      "[[0.03375759 0.79506177 0.01501793 0.15616268]\n",
      " [0.04586822 0.04491756 0.01320127 0.896013  ]\n",
      " [0.13150132 0.0476584  0.02859417 0.79224616]\n",
      " ...\n",
      " [0.03094512 0.81560177 0.01391069 0.13954239]\n",
      " [0.03642276 0.00604049 0.94318664 0.01435007]\n",
      " [0.03642393 0.00604099 0.94318426 0.01435079]]\n",
      "Iteration 904, Accuracy 0.38626\n",
      "92.83645%change in label assignment\n",
      "0.077943206\n",
      "[[0.01374292 0.9356162  0.00685746 0.04378346]\n",
      " [0.06801534 0.19957864 0.02374197 0.70866406]\n",
      " [0.04669851 0.04474929 0.01304171 0.8955105 ]\n",
      " ...\n",
      " [0.01457561 0.9324645  0.00733574 0.04562411]\n",
      " [0.03651949 0.00605005 0.9430493  0.01438125]\n",
      " [0.03651917 0.00605029 0.94304913 0.01438136]]\n",
      "Iteration 905, Accuracy 0.38695\n",
      "94.2014%change in label assignment\n",
      "0.077154174\n",
      "[[0.04382835 0.71512973 0.01908782 0.22195403]\n",
      " [0.04702659 0.04229718 0.01338417 0.8972921 ]\n",
      " [0.14313142 0.04911559 0.03089748 0.7768555 ]\n",
      " ...\n",
      " [0.0377864  0.76447767 0.01677665 0.18095927]\n",
      " [0.0363531  0.00603432 0.9432767  0.01433594]\n",
      " [0.03635274 0.00603457 0.94327664 0.01433606]]\n",
      "Iteration 906, Accuracy 0.38739\n",
      "94.28487%change in label assignment\n",
      "0.077378556\n",
      "[[0.01742862 0.9212022  0.00889681 0.0524724 ]\n",
      " [0.07146342 0.3063564  0.02641566 0.5957645 ]\n",
      " [0.04833224 0.06154334 0.0144451  0.8756793 ]\n",
      " ...\n",
      " [0.02166653 0.9046064  0.01127505 0.06245194]\n",
      " [0.03647514 0.00604709 0.9430996  0.01437814]\n",
      " [0.036475   0.00604734 0.94309944 0.01437831]]\n",
      "Iteration 907, Accuracy 0.38999\n",
      "92.29636%change in label assignment\n",
      "0.08205131\n",
      "[[0.0374799  0.7664361  0.01658058 0.1795034 ]\n",
      " [0.04638566 0.05444206 0.01401223 0.8851601 ]\n",
      " [0.09976214 0.04394262 0.02328191 0.8330133 ]\n",
      " ...\n",
      " [0.02508586 0.85624385 0.01153273 0.1071375 ]\n",
      " [0.03629598 0.00602983 0.9433515  0.01432263]\n",
      " [0.03629321 0.00602969 0.9433552  0.01432178]]\n",
      "Iteration 908, Accuracy 0.38435\n",
      "94.23577%change in label assignment\n",
      "0.075698346\n",
      "[[0.01350773 0.9364143  0.00672253 0.04335538]\n",
      " [0.07122121 0.28070998 0.02601292 0.62205595]\n",
      " [0.04759076 0.05784552 0.0140758  0.8804879 ]\n",
      " ...\n",
      " [0.01902686 0.91490453 0.00979768 0.05627088]\n",
      " [0.0364145  0.006041   0.9431826  0.01436195]\n",
      " [0.03641027 0.00604064 0.9431886  0.01436053]]\n",
      "Iteration 909, Accuracy 0.38719\n",
      "92.91501%change in label assignment\n",
      "0.081521325\n",
      "[[0.03816211 0.7610939  0.01690265 0.18384133]\n",
      " [0.04628439 0.05315338 0.0139704  0.88659185]\n",
      " [0.10322991 0.04454907 0.02405713 0.8281639 ]\n",
      " ...\n",
      " [0.02575624 0.8517651  0.01184707 0.11063161]\n",
      " [0.03624742 0.00602665 0.9434104  0.01431559]\n",
      " [0.03624363 0.00602635 0.9434156  0.01431435]]\n",
      "Iteration 910, Accuracy 0.38238\n",
      "93.76933%change in label assignment\n",
      "0.073868856\n",
      "[[0.01236143 0.93815917 0.00597032 0.04350908]\n",
      " [0.06443708 0.16298135 0.02211231 0.7504692 ]\n",
      " [0.04757443 0.04081157 0.01304843 0.89856553]\n",
      " ...\n",
      " [0.01250195 0.94021374 0.00620522 0.04107912]\n",
      " [0.03631479 0.0060315  0.9433206  0.01433311]\n",
      " [0.03631235 0.00603142 0.9433238  0.0143324 ]]\n",
      "Iteration 911, Accuracy 0.38685\n",
      "96.17028%change in label assignment\n",
      "0.07288102\n",
      "[[0.0176327  0.9045203  0.00826579 0.06958128]\n",
      " [0.05998836 0.13003154 0.02031841 0.7896617 ]\n",
      " [0.05079379 0.03870773 0.01373831 0.89676017]\n",
      " ...\n",
      " [0.01207022 0.9396349  0.00590239 0.04239248]\n",
      " [0.03623837 0.00602497 0.94342524 0.01431137]\n",
      " [0.03623268 0.00602438 0.9434336  0.01430939]]\n",
      "Iteration 912, Accuracy 0.3844\n",
      "95.87077%change in label assignment\n",
      "0.06979099\n",
      "[[0.03918914 0.7529803  0.01709857 0.19073202]\n",
      " [0.04789693 0.06360399 0.01467861 0.8738205 ]\n",
      " [0.08409503 0.04139363 0.02000592 0.8545054 ]\n",
      " ...\n",
      " [0.02238221 0.8740436  0.01028355 0.09329061]\n",
      " [0.0361767  0.00601686 0.9435176  0.01428877]\n",
      " [0.03617395 0.00601677 0.9435213  0.01428798]]\n",
      "Iteration 913, Accuracy 0.37914\n",
      "94.87897%change in label assignment\n",
      "0.07586081\n",
      "[[0.01403909 0.9345418  0.00709857 0.04432056]\n",
      " [0.06797391 0.42919233 0.02669101 0.47614267]\n",
      " [0.05520572 0.09911072 0.01795979 0.82772374]\n",
      " ...\n",
      " [0.02575704 0.8886586  0.01370045 0.07188397]\n",
      " [0.03626096 0.00602639 0.94338316 0.01432958]\n",
      " [0.0362548  0.00602578 0.943392   0.01432744]]\n",
      "Iteration 914, Accuracy 0.38292\n",
      "87.58776%change in label assignment\n",
      "0.079727985\n",
      "[[0.02723431 0.84124005 0.01242085 0.11910479]\n",
      " [0.05610846 0.10530965 0.01868889 0.81989306]\n",
      " [0.05547791 0.03851596 0.01481542 0.8911907 ]\n",
      " ...\n",
      " [0.01422536 0.9258478  0.00684738 0.05307945]\n",
      " [0.03608235 0.00601243 0.9436218  0.01428337]\n",
      " [0.0360749  0.0060116  0.94363284 0.01428074]]\n",
      "Iteration 915, Accuracy 0.37261\n",
      "94.60893%change in label assignment\n",
      "0.07480462\n",
      "[[0.0135657  0.9305145  0.00645853 0.04946123]\n",
      " [0.06970177 0.23595457 0.02499596 0.6693477 ]\n",
      " [0.04645436 0.05289135 0.01360446 0.88704985]\n",
      " ...\n",
      " [0.01438941 0.93311286 0.00725073 0.04524697]\n",
      " [0.03616508 0.00601855 0.94351053 0.01430584]\n",
      " [0.03615511 0.00601734 0.9435254  0.01430221]]\n",
      "Iteration 916, Accuracy 0.37453\n",
      "95.42888%change in label assignment\n",
      "0.075088546\n",
      "[[0.04995036 0.6586968  0.02129473 0.2700581 ]\n",
      " [0.0471735  0.06061198 0.01448801 0.8777265 ]\n",
      " [0.08667282 0.04198222 0.02074984 0.8505951 ]\n",
      " ...\n",
      " [0.02349728 0.866423   0.0108345  0.09924525]\n",
      " [0.03602665 0.00600609 0.9437088  0.01425842]\n",
      " [0.03601722 0.006005   0.9437227  0.01425506]]\n",
      "Iteration 917, Accuracy 0.37016\n",
      "93.44528%change in label assignment\n",
      "0.08081356\n",
      "[[0.01253485 0.9400023  0.00623042 0.0412324 ]\n",
      " [0.07078537 0.32301152 0.02664183 0.57956123]\n",
      " [0.04824571 0.06479111 0.01476586 0.87219733]\n",
      " ...\n",
      " [0.02042434 0.90927356 0.01064917 0.05965294]\n",
      " [0.03611404 0.00601437 0.9435832  0.01428842]\n",
      " [0.03611175 0.00601444 0.9435859  0.01428788]]\n",
      "Iteration 918, Accuracy 0.37782\n",
      "92.34055%change in label assignment\n",
      "0.07394948\n",
      "[[0.03939986 0.7504434  0.01731451 0.19284227]\n",
      " [0.05279158 0.08815254 0.01711723 0.8419386 ]\n",
      " [0.06832831 0.03928872 0.0172185  0.87516445]\n",
      " ...\n",
      " [0.01651285 0.91139877 0.00781904 0.06426936]\n",
      " [0.03598639 0.00600152 0.9437584  0.01425365]\n",
      " [0.03598164 0.00600124 0.94376487 0.01425218]]\n",
      "Iteration 919, Accuracy 0.37831\n",
      "94.47636%change in label assignment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01251917 0.936967   0.00603929 0.04447456]\n",
      " [0.07074054 0.28111658 0.02604864 0.6220943 ]\n",
      " [0.04629302 0.05347267 0.01365345 0.8865808 ]\n",
      " ...\n",
      " [0.01809841 0.9184498  0.00933648 0.05411529]\n",
      " [0.03604149 0.00600731 0.9436787  0.01427264]\n",
      " [0.03603673 0.00600702 0.94368505 0.01427116]]\n",
      "Iteration 920, Accuracy 0.38042\n",
      "96.61217%change in label assignment\n",
      "0.08518045\n",
      "[[0.06543328 0.46948004 0.02663976 0.43844685]\n",
      " [0.04972954 0.03981413 0.0139185  0.8965378 ]\n",
      " [0.18898979 0.05324373 0.03877937 0.71898705]\n",
      " ...\n",
      " [0.03077096 0.81596905 0.01401846 0.13924153]\n",
      " [0.03592363 0.00599413 0.9438551  0.01422718]\n",
      " [0.03592507 0.00599491 0.9438518  0.01422822]]\n",
      "Iteration 921, Accuracy 0.37988\n",
      "88.88889%change in label assignment\n",
      "0.08550147\n",
      "[[0.02496154 0.8916413  0.0131058  0.0702913 ]\n",
      " [0.05614053 0.6030661  0.02303625 0.31775713]\n",
      " [0.06337579 0.14870477 0.02132055 0.76659894]\n",
      " ...\n",
      " [0.0455807  0.8159876  0.02532066 0.11311101]\n",
      " [0.03616296 0.00601617 0.94351935 0.01430145]\n",
      " [0.03616036 0.00601625 0.94352263 0.01430083]]\n",
      "Iteration 922, Accuracy 0.38111\n",
      "84.43561%change in label assignment\n",
      "0.09274669\n",
      "[[0.03375285 0.7938324  0.01522038 0.15719436]\n",
      " [0.05667357 0.11011468 0.01915242 0.8140594 ]\n",
      " [0.06190144 0.03902895 0.01625926 0.88281035]\n",
      " ...\n",
      " [0.01306781 0.93321174 0.00640224 0.04731819]\n",
      " [0.03597626 0.00599998 0.9437746  0.01424918]\n",
      " [0.03596832 0.00599926 0.94378585 0.01424651]]\n",
      "Iteration 923, Accuracy 0.38705\n",
      "91.04434%change in label assignment\n",
      "0.07779019\n",
      "[[0.02990778 0.82371277 0.01330988 0.13306962]\n",
      " [0.06009815 0.1270687  0.01994797 0.7928851 ]\n",
      " [0.05993691 0.03845715 0.01518557 0.8864203 ]\n",
      " ...\n",
      " [0.01226758 0.93969625 0.00598583 0.04205035]\n",
      " [0.03603847 0.00600262 0.9436944  0.01426456]\n",
      " [0.0360337  0.00600247 0.9437007  0.01426318]]\n",
      "Iteration 924, Accuracy 0.37575\n",
      "97.69726%change in label assignment\n",
      "0.081522964\n",
      "[[0.01598417 0.9147036  0.00765391 0.06165831]\n",
      " [0.06836127 0.22723411 0.02525299 0.6791516 ]\n",
      " [0.04542591 0.04576823 0.01344147 0.8953644 ]\n",
      " ...\n",
      " [0.0134942  0.9361243  0.00689274 0.04348874]\n",
      " [0.03603091 0.00600195 0.94369483 0.01427228]\n",
      " [0.03602504 0.00600165 0.9437028  0.0142705 ]]\n",
      "Iteration 925, Accuracy 0.37664\n",
      "95.99843%change in label assignment\n",
      "0.07871124\n",
      "[[0.05344593 0.6251907  0.02237246 0.2989909 ]\n",
      " [0.04929588 0.07054485 0.01531722 0.86484206]\n",
      " [0.08982964 0.04215449 0.02101368 0.8470022 ]\n",
      " ...\n",
      " [0.01864331 0.89814925 0.00867945 0.07452798]\n",
      " [0.03599276 0.00599599 0.94375604 0.01425518]\n",
      " [0.03598759 0.00599589 0.94376284 0.01425375]]\n",
      "Iteration 926, Accuracy 0.37134\n",
      "94.92316%change in label assignment\n",
      "0.08374429\n",
      "[[0.01195939 0.94203156 0.00595936 0.04004966]\n",
      " [0.06733195 0.43453437 0.02671209 0.4714216 ]\n",
      " [0.05273421 0.08856412 0.01708936 0.8416124 ]\n",
      " ...\n",
      " [0.02399971 0.89519626 0.01277242 0.06803159]\n",
      " [0.03608343 0.0060062  0.94361997 0.0142904 ]\n",
      " [0.03607556 0.00600566 0.9436309  0.01428788]]\n",
      "Iteration 927, Accuracy 0.37502\n",
      "91.3242%change in label assignment\n",
      "0.08925697\n",
      "[[0.06537786 0.4677911  0.02659347 0.44023755]\n",
      " [0.04549877 0.04378997 0.0132919  0.89741933]\n",
      " [0.14417593 0.04950479 0.0315601  0.77475923]\n",
      " ...\n",
      " [0.03475553 0.7859397  0.01565019 0.16365455]\n",
      " [0.03591899 0.00599013 0.9438585  0.01423239]\n",
      " [0.03591476 0.0059902  0.9438637  0.01423137]]\n",
      "Iteration 928, Accuracy 0.37139\n",
      "89.51736%change in label assignment\n",
      "0.087090716\n",
      "[[0.02185944 0.9035811  0.01142689 0.06313255]\n",
      " [0.05287321 0.63304603 0.02207629 0.29200444]\n",
      " [0.06418131 0.16217682 0.02207546 0.7515664 ]\n",
      " ...\n",
      " [0.04196094 0.8286283  0.02323206 0.10617874]\n",
      " [0.03615667 0.00601306 0.9435187  0.01431152]\n",
      " [0.03615306 0.0060132  0.94352305 0.01431067]]\n",
      "Iteration 929, Accuracy 0.37536\n",
      "87.27353%change in label assignment\n",
      "0.08978784\n",
      "[[0.04398438 0.71119106 0.01937582 0.2254487 ]\n",
      " [0.04792031 0.06528116 0.01520917 0.87158936]\n",
      " [0.09586395 0.04384943 0.02314683 0.8371398 ]\n",
      " ...\n",
      " [0.01639085 0.91215086 0.00787633 0.06358195]\n",
      " [0.03594712 0.00599367 0.9438139  0.01424531]\n",
      " [0.03594843 0.00599464 0.9438104  0.01424648]]\n",
      "Iteration 930, Accuracy 0.37875\n",
      "86.37502%change in label assignment\n",
      "0.08096373\n",
      "[[0.01267226 0.9369422  0.0061342  0.04425135]\n",
      " [0.07014045 0.2478751  0.02533306 0.6566513 ]\n",
      " [0.04613089 0.0460908  0.01311355 0.89466476]\n",
      " ...\n",
      " [0.02158312 0.9046441  0.01124382 0.06252903]\n",
      " [0.03609328 0.00600623 0.9436051  0.01429535]\n",
      " [0.03609749 0.00600758 0.94359726 0.01429757]]\n",
      "Iteration 931, Accuracy 0.37875\n",
      "95.85113%change in label assignment\n",
      "0.08284432\n",
      "[[0.06560659 0.46248278 0.02673881 0.44517183]\n",
      " [0.05510037 0.03859787 0.01495603 0.8913458 ]\n",
      " [0.21959804 0.05491819 0.04340046 0.6820833 ]\n",
      " ...\n",
      " [0.03356722 0.7946916  0.01521187 0.15652932]\n",
      " [0.03594895 0.00599218 0.94381493 0.01424393]\n",
      " [0.03595724 0.00599423 0.9438006  0.01424783]]\n",
      "Iteration 932, Accuracy 0.379\n",
      "88.85943%change in label assignment\n",
      "0.084646255\n",
      "[[0.01989813 0.9111731  0.01031086 0.0586179 ]\n",
      " [0.06788243 0.43516243 0.02655287 0.4704023 ]\n",
      " [0.05210083 0.08169623 0.01638562 0.84981734]\n",
      " ...\n",
      " [0.04248436 0.82671565 0.02353324 0.10726666]\n",
      " [0.03616522 0.00601243 0.94350946 0.01431288]\n",
      " [0.03617156 0.00601408 0.94349843 0.01431591]]\n",
      "Iteration 933, Accuracy 0.37772\n",
      "89.18348%change in label assignment\n",
      "0.08773933\n",
      "[[0.04000277 0.7441123  0.01776054 0.19812436]\n",
      " [0.04588694 0.05614407 0.01420266 0.88376635]\n",
      " [0.12102071 0.04697468 0.02769184 0.8043127 ]\n",
      " ...\n",
      " [0.0124063  0.9371232  0.00611913 0.04435141]\n",
      " [0.03595493 0.00599509 0.9437938  0.01425624]\n",
      " [0.0359633  0.00599718 0.94377935 0.0142602 ]]\n",
      "Iteration 934, Accuracy 0.38852\n",
      "90.5779%change in label assignment\n",
      "0.07511495\n",
      "[[0.01327985 0.9321655  0.0063697  0.04818494]\n",
      " [0.06715494 0.19795306 0.02375449 0.71113753]\n",
      " [0.04703955 0.04072243 0.01302264 0.89921534]\n",
      " ...\n",
      " [0.01885322 0.91535485 0.00976125 0.05603074]\n",
      " [0.03604268 0.00599839 0.9436735  0.01428542]\n",
      " [0.03604658 0.00599964 0.9436662  0.0142875 ]]\n",
      "Iteration 935, Accuracy 0.37738\n",
      "93.10159%change in label assignment\n",
      "0.07733669\n",
      "[[0.07020431 0.30729878 0.0271504  0.59534645]\n",
      " [0.05523138 0.03849517 0.0150435  0.89123   ]\n",
      " [0.2161898  0.05460877 0.04308828 0.6861132 ]\n",
      " ...\n",
      " [0.0464036  0.69048977 0.02036999 0.24273665]\n",
      " [0.03590643 0.00597906 0.94386804 0.01424653]\n",
      " [0.03590916 0.0059802  0.94386244 0.01424823]]\n",
      "Iteration 936, Accuracy 0.3816\n",
      "92.70879%change in label assignment\n",
      "0.09522992\n",
      "[[0.04780896 0.80826247 0.02668491 0.11724366]\n",
      " [0.02761983 0.8416164  0.01235998 0.11840378]\n",
      " [0.07186207 0.2998227  0.02647883 0.60183644]\n",
      " ...\n",
      " [0.07970171 0.70304495 0.04768736 0.16956602]\n",
      " [0.03616715 0.00601215 0.9434738  0.01434682]\n",
      " [0.03617012 0.00601325 0.9434681  0.01434852]]\n",
      "Iteration 937, Accuracy 0.3761\n",
      "83.09029%change in label assignment\n",
      "0.09189226\n",
      "[[0.01361688 0.9359383  0.00697557 0.04346926]\n",
      " [0.06907406 0.38567325 0.02730264 0.51795006]\n",
      " [0.04742001 0.06369232 0.0149517  0.87393606]\n",
      " ...\n",
      " [0.03624751 0.8488284  0.0201239  0.09480023]\n",
      " [0.03592973 0.00599308 0.94378984 0.01428736]\n",
      " [0.03592775 0.00599337 0.94379187 0.0142871 ]]\n",
      "Iteration 938, Accuracy 0.40693\n",
      "91.79064%change in label assignment\n",
      "0.084582746\n",
      "[[0.01940279 0.8929094  0.00907976 0.07860806]\n",
      " [0.06265742 0.1541028  0.02184001 0.7613998 ]\n",
      " [0.05140098 0.03788318 0.01391657 0.8967993 ]\n",
      " ...\n",
      " [0.01371398 0.93573743 0.00700114 0.04354744]\n",
      " [0.03589388 0.00598434 0.94385415 0.01426767]\n",
      " [0.0358882  0.00598412 0.94386166 0.01426601]]\n",
      "Iteration 939, Accuracy 0.3844\n",
      "96.09172%change in label assignment\n",
      "0.07488001\n",
      "[[0.0327206  0.8016104  0.01466658 0.15100245]\n",
      " [0.05326452 0.09157811 0.01739555 0.83776176]\n",
      " [0.06888341 0.03894154 0.01731605 0.874859  ]\n",
      " ...\n",
      " [0.01211484 0.9388112  0.00592222 0.04315165]\n",
      " [0.03588114 0.00597904 0.9438788  0.01426103]\n",
      " [0.03587815 0.00597924 0.9438822  0.01426043]]\n",
      "Iteration 940, Accuracy 0.37531\n",
      "98.02622%change in label assignment\n",
      "0.07221613\n",
      "[[0.01785394 0.9029806  0.00838247 0.07078297]\n",
      " [0.06533021 0.17849854 0.02306389 0.7331074 ]\n",
      " [0.04542511 0.04185835 0.01291787 0.8997987 ]\n",
      " ...\n",
      " [0.01290353 0.93894684 0.00652687 0.04162275]\n",
      " [0.0359222  0.00598139 0.9438178  0.01427862]\n",
      " [0.03590889 0.00597992 0.9438372  0.01427391]]\n",
      "Iteration 941, Accuracy 0.37526\n",
      "93.59749%change in label assignment\n",
      "0.072755925\n",
      "[[0.03003061 0.8208814  0.01365014 0.13543779]\n",
      " [0.05533886 0.1034545  0.01852165 0.82268494]\n",
      " [0.05373328 0.03768151 0.01446274 0.8941224 ]\n",
      " ...\n",
      " [0.01290972 0.93360996 0.00629373 0.04718655]\n",
      " [0.03586604 0.0059761  0.9438927  0.01426511]\n",
      " [0.03584855 0.00597397 0.94391865 0.01425875]]\n",
      "Iteration 942, Accuracy 0.37212\n",
      "97.76599%change in label assignment\n",
      "0.07424392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0175082  0.9053348  0.00821219 0.06894486]\n",
      " [0.06548326 0.17788956 0.02297448 0.73365265]\n",
      " [0.04561846 0.04246121 0.01291725 0.899003  ]\n",
      " ...\n",
      " [0.01322941 0.93766296 0.00668839 0.04241924]\n",
      " [0.03589282 0.00598031 0.94385284 0.01427403]\n",
      " [0.0358739  0.00597794 0.9438811  0.01426709]]\n",
      "Iteration 943, Accuracy 0.36991\n",
      "97.78563%change in label assignment\n",
      "0.07346939\n",
      "[[0.05022646 0.65392315 0.02167428 0.27417615]\n",
      " [0.04612643 0.05830411 0.01434146 0.88122797]\n",
      " [0.08730144 0.04193616 0.0212207  0.84954166]\n",
      " ...\n",
      " [0.01942001 0.8926115  0.00918543 0.07878302]\n",
      " [0.03576896 0.00596852 0.94403124 0.01423124]\n",
      " [0.03575338 0.00596673 0.94405425 0.01422568]]\n",
      "Iteration 944, Accuracy 0.37271\n",
      "94.57947%change in label assignment\n",
      "0.08046742\n",
      "[[0.01228654 0.9381195  0.00597778 0.04361629]\n",
      " [0.07007655 0.2646185  0.02589416 0.6394108 ]\n",
      " [0.04583297 0.0542318  0.01375623 0.886179  ]\n",
      " ...\n",
      " [0.01988711 0.9113521  0.01041222 0.05834861]\n",
      " [0.03584675 0.00597712 0.9439106  0.0142655 ]\n",
      " [0.03583849 0.00597648 0.9439222  0.01426281]]\n",
      "Iteration 945, Accuracy 0.38066\n",
      "94.2996%change in label assignment\n",
      "0.08023195\n",
      "[[0.06350384 0.49792045 0.02620413 0.41237158]\n",
      " [0.04490418 0.04367415 0.01322349 0.8981982 ]\n",
      " [0.12343349 0.0469278  0.02806189 0.8015768 ]\n",
      " ...\n",
      " [0.03060273 0.8165474  0.01400556 0.1388443 ]\n",
      " [0.03571131 0.00596399 0.94409984 0.01422493]\n",
      " [0.03569913 0.00596273 0.94411755 0.0142207 ]]\n",
      "Iteration 946, Accuracy 0.3788\n",
      "92.90028%change in label assignment\n",
      "0.081050776\n",
      "[[0.01301945 0.9381198  0.0065163  0.04234438]\n",
      " [0.0671003  0.4500527  0.02651932 0.45632765]\n",
      " [0.05768607 0.11246408 0.01903049 0.8108193 ]\n",
      " ...\n",
      " [0.03307457 0.860966   0.01797501 0.08798444]\n",
      " [0.03590128 0.00598468 0.94382423 0.01428988]\n",
      " [0.03587877 0.00598175 0.94385797 0.0142815 ]]\n",
      "Iteration 947, Accuracy 0.38376\n",
      "86.12461%change in label assignment\n",
      "0.08969576\n",
      "[[0.06399219 0.4896646  0.0265561  0.41978708]\n",
      " [0.04578619 0.0430945  0.01353008 0.89758927]\n",
      " [0.12945622 0.04813392 0.02965853 0.7927514 ]\n",
      " ...\n",
      " [0.02986562 0.82199764 0.01380938 0.13432737]\n",
      " [0.03563556 0.00596142 0.9441949  0.01420807]\n",
      " [0.03562174 0.00595989 0.9442152  0.0142032 ]]\n",
      "Iteration 948, Accuracy 0.37364\n",
      "87.36191%change in label assignment\n",
      "0.08449809\n",
      "[[0.01336602 0.9363527  0.00664485 0.04363643]\n",
      " [0.07074203 0.36689264 0.02694718 0.5354181 ]\n",
      " [0.05221932 0.08014486 0.01632576 0.8513101 ]\n",
      " ...\n",
      " [0.03232794 0.86373085 0.0174462  0.08649497]\n",
      " [0.03586676 0.00598275 0.94387084 0.01427959]\n",
      " [0.03585156 0.00598099 0.94389325 0.01427413]]\n",
      "Iteration 949, Accuracy 0.38253\n",
      "92.45348%change in label assignment\n",
      "0.08686337\n",
      "[[0.05204799 0.635201   0.02253989 0.29021114]\n",
      " [0.04595093 0.05701709 0.01440673 0.8826252 ]\n",
      " [0.09343355 0.04330364 0.02282781 0.84043497]\n",
      " ...\n",
      " [0.01705491 0.90778637 0.00821846 0.06694026]\n",
      " [0.03568133 0.00596629 0.94412786 0.01422455]\n",
      " [0.03566487 0.00596436 0.9441522  0.01421865]]\n",
      "Iteration 950, Accuracy 0.38106\n",
      "93.8528%change in label assignment\n",
      "0.073336884\n",
      "[[0.02770792 0.83789796 0.01253142 0.12186267]\n",
      " [0.05952899 0.12733158 0.02004621 0.7930932 ]\n",
      " [0.05135188 0.03815559 0.01373081 0.8967617 ]\n",
      " ...\n",
      " [0.01256929 0.93989587 0.00630415 0.04123075]\n",
      " [0.0357777  0.00597205 0.9440051  0.01424511]\n",
      " [0.03575973 0.00596991 0.9440317  0.01423861]]\n",
      "Iteration 951, Accuracy 0.38332\n",
      "97.20627%change in label assignment\n",
      "0.073074\n",
      "[[0.03177733 0.8071101  0.0144545  0.14665814]\n",
      " [0.05093827 0.08280358 0.01674948 0.84950864]\n",
      " [0.06626388 0.0389586  0.01719286 0.87758464]\n",
      " ...\n",
      " [0.01211513 0.9386081  0.00599522 0.04328152]\n",
      " [0.03564605 0.00596096 0.94418645 0.01420649]\n",
      " [0.03564214 0.0059611  0.9441911  0.01420562]]\n",
      "Iteration 952, Accuracy 0.38184\n",
      "95.06064%change in label assignment\n",
      "0.068994924\n",
      "[[0.03613193 0.7755502  0.01600583 0.17231202]\n",
      " [0.0493065  0.07293593 0.01557643 0.8621812 ]\n",
      " [0.07454113 0.0398194  0.01833698 0.8673025 ]\n",
      " ...\n",
      " [0.01314534 0.93211865 0.00634642 0.04838965]\n",
      " [0.03565863 0.00595902 0.94417775 0.01420454]\n",
      " [0.0356599  0.00596    0.9441743  0.01420573]]\n",
      "Iteration 953, Accuracy 0.38435\n",
      "96.84293%change in label assignment\n",
      "0.07047387\n",
      "[[0.01716619 0.90664375 0.00814988 0.0680402 ]\n",
      " [0.06540156 0.18824723 0.02360175 0.7227495 ]\n",
      " [0.04459364 0.04206895 0.01296714 0.90037024]\n",
      " ...\n",
      " [0.01466354 0.9318751  0.00757759 0.04588383]\n",
      " [0.03565562 0.00595854 0.9441729  0.01421287]\n",
      " [0.03565524 0.00595929 0.9441721  0.01421344]]\n",
      "Iteration 954, Accuracy 0.3871\n",
      "94.57947%change in label assignment\n",
      "0.06829676\n",
      "[[0.03494136 0.7836062  0.0156716  0.16578084]\n",
      " [0.0556889  0.10784008 0.01878348 0.8176876 ]\n",
      " [0.05376242 0.03752885 0.01447702 0.89423174]\n",
      " ...\n",
      " [0.01167638 0.941546   0.00578409 0.04099352]\n",
      " [0.03557819 0.00595022 0.94427985 0.0141917 ]\n",
      " [0.03556762 0.00594931 0.94429487 0.0141882 ]]\n",
      "Iteration 955, Accuracy 0.37983\n",
      "96.29302%change in label assignment\n",
      "0.0694335\n",
      "[[0.0279921  0.8345361  0.01279427 0.12467751]\n",
      " [0.05938917 0.131973   0.02050883 0.78812903]\n",
      " [0.04870075 0.03813773 0.0135022  0.8996593 ]\n",
      " ...\n",
      " [0.01167313 0.94294786 0.00585531 0.03952371]\n",
      " [0.03554109 0.00594769 0.94432837 0.01418285]\n",
      " [0.03553213 0.00594706 0.9443408  0.01418001]]\n",
      "Iteration 956, Accuracy 0.37939\n",
      "98.1637%change in label assignment\n",
      "0.068730704\n",
      "[[0.02849187 0.8307892  0.01302491 0.12769401]\n",
      " [0.05379838 0.0977791  0.01796344 0.8304591 ]\n",
      " [0.05714219 0.03770745 0.015198   0.8899524 ]\n",
      " ...\n",
      " [0.01193557 0.9395292  0.00588494 0.04265028]\n",
      " [0.03547379 0.00594188 0.9444238  0.01416048]\n",
      " [0.0354732  0.00594256 0.9444233  0.01416095]]\n",
      "Iteration 957, Accuracy 0.37566\n",
      "95.91005%change in label assignment\n",
      "0.07283761\n",
      "[[0.02277422 0.8703078  0.01056021 0.09635776]\n",
      " [0.05886409 0.12813704 0.02020391 0.792795  ]\n",
      " [0.05013293 0.03778623 0.01372808 0.8983527 ]\n",
      " ...\n",
      " [0.0115856  0.9429279  0.00577732 0.0397091 ]\n",
      " [0.03548109 0.00594215 0.944413   0.01416377]\n",
      " [0.03548361 0.00594331 0.94440764 0.01416545]]\n",
      "Iteration 958, Accuracy 0.37811\n",
      "98.25207%change in label assignment\n",
      "0.06545828\n",
      "[[0.02474188 0.85683334 0.01143572 0.10698909]\n",
      " [0.05780935 0.1217452  0.01981043 0.800635  ]\n",
      " [0.05102453 0.03766981 0.0139659  0.8973398 ]\n",
      " ...\n",
      " [0.01156278 0.94248116 0.00575625 0.04019989]\n",
      " [0.03542788 0.00593633 0.94448686 0.01414896]\n",
      " [0.03542832 0.00593715 0.9444847  0.01414982]]\n",
      "Iteration 959, Accuracy 0.37703\n",
      "99.35189%change in label assignment\n",
      "[[0.03826128 0.7577686  0.01693823 0.18703194]\n",
      " [0.05273724 0.09119674 0.0173112  0.83875483]\n",
      " [0.06257141 0.03817653 0.01613262 0.8831194 ]\n",
      " ...\n",
      " [0.01277703 0.93404883 0.00621702 0.04695711]\n",
      " [0.03538255 0.00593132 0.9445528  0.01413331]\n",
      " [0.0353797  0.00593159 0.9445558  0.01413284]]\n",
      "Iteration 960, Accuracy 0.37639\n",
      "97.00496%change in label assignment\n",
      "0.07688397\n",
      "[[0.02186595 0.87590456 0.01024752 0.09198197]\n",
      " [0.06184942 0.15421994 0.02195659 0.76197404]\n",
      " [0.04636406 0.03920327 0.01323152 0.9012011 ]\n",
      " ...\n",
      " [0.01184818 0.9423255  0.00599788 0.0398285 ]\n",
      " [0.03539586 0.00593269 0.94453263 0.01413876]\n",
      " [0.03539475 0.00593326 0.94453305 0.01413901]]\n",
      "Iteration 961, Accuracy 0.38189\n",
      "97.6187%change in label assignment\n",
      "0.07497181\n",
      "[[0.04964779 0.65781486 0.02134157 0.27119586]\n",
      " [0.04739537 0.06638537 0.01496504 0.87125427]\n",
      " [0.08044391 0.04075393 0.01973601 0.8590661 ]\n",
      " ...\n",
      " [0.01716526 0.90647864 0.00815167 0.06820438]\n",
      " [0.03535622 0.00592748 0.94459236 0.01412393]\n",
      " [0.03535502 0.00592807 0.9445927  0.01412417]]\n",
      "Iteration 962, Accuracy 0.37787\n",
      "97.09825%change in label assignment\n",
      "0.075723216\n",
      "[[0.01305032 0.93819124 0.00666209 0.04209642]\n",
      " [0.06550871 0.46000525 0.02651551 0.44797054]\n",
      " [0.05709384 0.11728032 0.01945397 0.8061719 ]\n",
      " ...\n",
      " [0.0228969  0.89919734 0.01224717 0.06565861]\n",
      " [0.03543216 0.00593625 0.94447404 0.01415759]\n",
      " [0.03542268 0.00593554 0.94448715 0.01415456]]\n",
      "Iteration 963, Accuracy 0.38017\n",
      "87.3079%change in label assignment\n",
      "0.08825164\n",
      "[[0.05597954 0.5897364  0.02398441 0.3302996 ]\n",
      " [0.04471755 0.05333186 0.01394479 0.8880058 ]\n",
      " [0.08963311 0.0427158  0.02216866 0.8454824 ]\n",
      " ...\n",
      " [0.03156635 0.808113   0.01452854 0.14579219]\n",
      " [0.03528281 0.00592138 0.9446902  0.01410562]\n",
      " [0.03527045 0.00592023 0.9447079  0.01410148]]\n",
      "Iteration 964, Accuracy 0.37251\n",
      "93.32253%change in label assignment\n",
      "0.09353989\n",
      "[[0.03358589 0.858738   0.01836622 0.08930986]\n",
      " [0.03120752 0.8128339  0.01402991 0.14192864]\n",
      " [0.07011256 0.33944312 0.02695483 0.56348956]\n",
      " ...\n",
      " [0.04832505 0.80571824 0.0274015  0.11855518]\n",
      " [0.03549498 0.00594404 0.9443674  0.01419362]\n",
      " [0.03548357 0.00594312 0.94438344 0.01418989]]\n",
      "Iteration 965, Accuracy 0.37384\n",
      "85.51579%change in label assignment\n",
      "0.09469123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02281017 0.8697866  0.01080695 0.09659618]\n",
      " [0.06190915 0.15480597 0.02236803 0.7609168 ]\n",
      " [0.046337   0.04107757 0.01367135 0.8989141 ]\n",
      " ...\n",
      " [0.01259888 0.9359557  0.00631038 0.04513505]\n",
      " [0.03529228 0.00592714 0.94465095 0.01412957]\n",
      " [0.0352787  0.00592585 0.94467044 0.014125  ]]\n",
      "Iteration 966, Accuracy 0.38557\n",
      "89.53209%change in label assignment\n",
      "0.079922594\n",
      "[[0.02077266 0.88499826 0.00960813 0.08462095]\n",
      " [0.06726716 0.1988912  0.02383347 0.7100082 ]\n",
      " [0.04568496 0.04880149 0.01331949 0.89219403]\n",
      " ...\n",
      " [0.01251281 0.9375347  0.0061139  0.04383857]\n",
      " [0.03543218 0.00593622 0.9444705  0.01416105]\n",
      " [0.03540788 0.0059332  0.94450676 0.01415218]]\n",
      "Iteration 967, Accuracy 0.37075\n",
      "95.28158%change in label assignment\n",
      "0.080425054\n",
      "[[0.04524003 0.69742686 0.02026929 0.23706387]\n",
      " [0.04856645 0.07141646 0.01608129 0.8639359 ]\n",
      " [0.06995724 0.04048257 0.01864772 0.8709125 ]\n",
      " ...\n",
      " [0.02347339 0.8655976  0.01119665 0.09973233]\n",
      " [0.03530542 0.0059248  0.9446532  0.01411649]\n",
      " [0.03529429 0.00592398 0.94466877 0.01411297]]\n",
      "Iteration 968, Accuracy 0.36309\n",
      "92.48785%change in label assignment\n",
      "0.081381015\n",
      "[[0.01361519 0.9300134  0.00655507 0.04981627]\n",
      " [0.07038935 0.28384808 0.0261766  0.61958593]\n",
      " [0.04695443 0.05917963 0.01424198 0.87962395]\n",
      " ...\n",
      " [0.01696453 0.9224586  0.00874468 0.05183218]\n",
      " [0.03546575 0.00593841 0.9444335  0.0141623 ]\n",
      " [0.03545906 0.00593838 0.944442   0.01416057]]\n",
      "Iteration 969, Accuracy 0.37301\n",
      "96.0819%change in label assignment\n",
      "0.08098718\n",
      "[[0.05553522 0.5933033  0.02405083 0.3271106 ]\n",
      " [0.04485709 0.05380129 0.01418535 0.8871563 ]\n",
      " [0.1018218  0.04498698 0.02499891 0.8281923 ]\n",
      " ...\n",
      " [0.02428225 0.8595978  0.01150689 0.10461306]\n",
      " [0.03529821 0.00592313 0.94467044 0.0141082 ]\n",
      " [0.03530003 0.00592446 0.9446656  0.01410988]]\n",
      "Iteration 970, Accuracy 0.37467\n",
      "92.12452%change in label assignment\n",
      "0.084157944\n",
      "[[0.0137456  0.93507904 0.00695557 0.0442198 ]\n",
      " [0.06795501 0.41943988 0.02680323 0.48580188]\n",
      " [0.05199583 0.08528295 0.01675418 0.845967  ]\n",
      " ...\n",
      " [0.02964445 0.873306   0.01602376 0.08102585]\n",
      " [0.03547869 0.00594119 0.9444033  0.01417682]\n",
      " [0.03548374 0.00594301 0.9443936  0.01417972]]\n",
      "Iteration 971, Accuracy 0.37855\n",
      "94.17194%change in label assignment\n",
      "0.08706585\n",
      "[[0.06029601 0.5335908  0.02553008 0.3805832 ]\n",
      " [0.04394359 0.04926283 0.01360292 0.8931906 ]\n",
      " [0.1190917  0.04701235 0.02804015 0.80585575]\n",
      " ...\n",
      " [0.02405042 0.86083364 0.01134939 0.10376656]\n",
      " [0.03529762 0.00592269 0.94466645 0.01411327]\n",
      " [0.03530187 0.00592444 0.94465774 0.01411595]]\n",
      "Iteration 972, Accuracy 0.38101\n",
      "93.52384%change in label assignment\n",
      "0.07984303\n",
      "[[0.01226476 0.9386053  0.00601824 0.04311172]\n",
      " [0.06995416 0.34614545 0.0268671  0.5570333 ]\n",
      " [0.04601775 0.05715193 0.01400717 0.88282317]\n",
      " ...\n",
      " [0.02689664 0.8836717  0.01444157 0.07499014]\n",
      " [0.03547113 0.00593978 0.9444205  0.01416866]\n",
      " [0.03547922 0.00594219 0.9444058  0.01417289]]\n",
      "Iteration 973, Accuracy 0.37816\n",
      "94.78077%change in label assignment\n",
      "0.08253521\n",
      "[[0.06485856 0.46115282 0.02678837 0.4472002 ]\n",
      " [0.04354351 0.0467598  0.01331298 0.8963837 ]\n",
      " [0.13995835 0.0492989  0.03168489 0.77905786]\n",
      " ...\n",
      " [0.02006157 0.8872755  0.00956898 0.08309398]\n",
      " [0.03530044 0.00592427 0.9446576  0.01411765]\n",
      " [0.03531287 0.00592755 0.9446357  0.01412378]]\n",
      "Iteration 974, Accuracy 0.38047\n",
      "90.99524%change in label assignment\n",
      "0.080443315\n",
      "[[0.01217407 0.93990105 0.0060208  0.04190409]\n",
      " [0.0674157  0.42978778 0.02670792 0.4760886 ]\n",
      " [0.05077259 0.0799186  0.01624611 0.85306275]\n",
      " ...\n",
      " [0.03245654 0.862677   0.01770747 0.08715902]\n",
      " [0.03544761 0.00593586 0.944451   0.01416554]\n",
      " [0.03544937 0.00593719 0.94444627 0.01416719]]\n",
      "Iteration 975, Accuracy 0.37698\n",
      "88.95272%change in label assignment\n",
      "0.09085771\n",
      "[[0.06809567 0.24882662 0.02609609 0.6569816 ]\n",
      " [0.05026275 0.03822324 0.01425764 0.8972564 ]\n",
      " [0.18715471 0.05337735 0.03979779 0.7196701 ]\n",
      " ...\n",
      " [0.0448016  0.700307   0.02003012 0.23486122]\n",
      " [0.03526828 0.00591364 0.9447055  0.0141125 ]\n",
      " [0.03526918 0.00591485 0.94470215 0.01411385]]\n",
      "Iteration 976, Accuracy 0.37998\n",
      "90.00835%change in label assignment\n",
      "0.09432651\n",
      "[[0.03301811 0.8607068  0.01792005 0.08835498]\n",
      " [0.02384158 0.86575407 0.01091023 0.09949415]\n",
      " [0.07107043 0.31920987 0.02676502 0.58295465]\n",
      " ...\n",
      " [0.07385102 0.72057706 0.04399386 0.16157801]\n",
      " [0.03553343 0.00594584 0.9443106  0.01421011]\n",
      " [0.03552955 0.00594625 0.9443148  0.01420949]]\n",
      "Iteration 977, Accuracy 0.38179\n",
      "81.29327%change in label assignment\n",
      "0.09597195\n",
      "[[0.01238907 0.936387   0.00616831 0.04505555]\n",
      " [0.06816842 0.38287124 0.02745924 0.52150106]\n",
      " [0.04578864 0.06092648 0.01473817 0.8785467 ]\n",
      " ...\n",
      " [0.02777226 0.8797572  0.01527483 0.07719569]\n",
      " [0.03528918 0.00592699 0.94463515 0.01414875]\n",
      " [0.03528112 0.00592669 0.9446457  0.01414647]]\n",
      "Iteration 978, Accuracy 0.39652\n",
      "92.68915%change in label assignment\n",
      "0.08315277\n",
      "[[0.03297087 0.7972011  0.01492042 0.15490763]\n",
      " [0.05853586 0.12941797 0.02027805 0.7917681 ]\n",
      " [0.05535535 0.03733781 0.01485558 0.8924513 ]\n",
      " ...\n",
      " [0.01152109 0.9433186  0.00579653 0.03936369]\n",
      " [0.03528266 0.00592059 0.9446619  0.01413491]\n",
      " [0.03527685 0.00592062 0.9446691  0.01413348]]\n",
      "Iteration 979, Accuracy 0.37649\n",
      "97.06388%change in label assignment\n",
      "0.07319242\n",
      "[[0.04337457 0.713507   0.01906801 0.22405045]\n",
      " [0.05506243 0.10674413 0.01861593 0.8195776 ]\n",
      " [0.05813724 0.03751255 0.01537996 0.8889702 ]\n",
      " ...\n",
      " [0.01180452 0.94002926 0.00583406 0.04233222]\n",
      " [0.03530101 0.00591871 0.9446425  0.01413782]\n",
      " [0.03529353 0.00591857 0.944652   0.01413581]]\n",
      "Iteration 980, Accuracy 0.37497\n",
      "98.27662%change in label assignment\n",
      "0.07073749\n",
      "[[0.03875499 0.75163317 0.01735208 0.1922598 ]\n",
      " [0.0538874  0.1013318  0.01827149 0.82650924]\n",
      " [0.05604326 0.03735732 0.01510576 0.8914936 ]\n",
      " ...\n",
      " [0.01203579 0.93834245 0.00595408 0.04366768]\n",
      " [0.03528209 0.00591608 0.94466645 0.01413534]\n",
      " [0.03527965 0.00591668 0.9446684  0.01413528]]\n",
      "Iteration 981, Accuracy 0.37659\n",
      "98.35027%change in label assignment\n",
      "0.068688646\n",
      "[[0.02648853 0.8441724  0.01222835 0.11711076]\n",
      " [0.06192872 0.15676852 0.02201216 0.75929064]\n",
      " [0.04522755 0.03913139 0.01296378 0.9026773 ]\n",
      " ...\n",
      " [0.01152916 0.9435152  0.00581722 0.03913843]\n",
      " [0.03528826 0.00591562 0.9446546  0.01414161]\n",
      " [0.03528372 0.00591587 0.94465977 0.0141407 ]]\n",
      "Iteration 982, Accuracy 0.37639\n",
      "97.26027%change in label assignment\n",
      "0.07142739\n",
      "[[0.0377409  0.75970733 0.01697121 0.18558055]\n",
      " [0.05689516 0.11954805 0.01971301 0.8038438 ]\n",
      " [0.05065823 0.03728033 0.01404254 0.89801896]\n",
      " ...\n",
      " [0.01206538 0.9381296  0.00597447 0.04383057]\n",
      " [0.03523608 0.00591047 0.9447262  0.01412714]\n",
      " [0.03522873 0.00591033 0.94473565 0.01412519]]\n",
      "Iteration 983, Accuracy 0.37261\n",
      "98.53194%change in label assignment\n",
      "0.07149366\n",
      "[[0.03411973 0.7883505  0.01542296 0.1621068 ]\n",
      " [0.05888574 0.13236223 0.02051486 0.7882372 ]\n",
      " [0.04675116 0.03811667 0.01320057 0.9019316 ]\n",
      " ...\n",
      " [0.01159794 0.9413848  0.00576219 0.04125505]\n",
      " [0.03521911 0.00590816 0.9447522  0.01412051]\n",
      " [0.03521157 0.005908   0.9447619  0.01411848]]\n",
      "Iteration 984, Accuracy 0.37242\n",
      "99.28315%change in label assignment\n",
      "0.07465321\n",
      "[[0.03752611 0.761171   0.01692094 0.18438198]\n",
      " [0.05756829 0.12469689 0.02012109 0.7976138 ]\n",
      " [0.04722272 0.0379203  0.013438   0.901419  ]\n",
      " ...\n",
      " [0.01208356 0.9379702  0.00599494 0.04395133]\n",
      " [0.03517784 0.00590393 0.9448059  0.01411236]\n",
      " [0.03517111 0.00590395 0.9448143  0.01411071]]\n",
      "Iteration 985, Accuracy 0.37193\n",
      "98.89036%change in label assignment\n",
      "0.07084574\n",
      "[[0.03331676 0.79503477 0.0150298  0.1566187 ]\n",
      " [0.06239889 0.15897949 0.02206007 0.7565616 ]\n",
      " [0.04394116 0.04211211 0.01280403 0.90114266]\n",
      " ...\n",
      " [0.01148176 0.9428039  0.00572466 0.03998967]\n",
      " [0.03522567 0.00590926 0.9447357  0.0141294 ]\n",
      " [0.03519939 0.00590621 0.9447744  0.01412   ]]\n",
      "Iteration 986, Accuracy 0.37099\n",
      "95.76766%change in label assignment\n",
      "0.0716959\n",
      "[[0.05014984 0.64854497 0.02196302 0.27934214]\n",
      " [0.05265306 0.09590858 0.01796548 0.8334729 ]\n",
      " [0.05162606 0.03748472 0.01443363 0.8964555 ]\n",
      " ...\n",
      " [0.01557841 0.9158883  0.0075888  0.06094444]\n",
      " [0.03513452 0.00590178 0.9448611  0.01410253]\n",
      " [0.03510866 0.00589883 0.94489914 0.01409335]]\n",
      "Iteration 987, Accuracy 0.3703\n",
      "98.46811%change in label assignment\n",
      "0.07395002\n",
      "[[0.02140917 0.8788972  0.01000497 0.0896887 ]\n",
      " [0.06496405 0.1867714  0.0233953  0.7248692 ]\n",
      " [0.0435787  0.04561922 0.0129549  0.8978472 ]\n",
      " ...\n",
      " [0.0124687  0.9402611  0.00634573 0.04092447]\n",
      " [0.03515527 0.00590448 0.9448308  0.01410943]\n",
      " [0.03514582 0.00590409 0.9448434  0.0141067 ]]\n",
      "Iteration 988, Accuracy 0.36657\n",
      "94.28978%change in label assignment\n",
      "0.07345412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04161133 0.72666174 0.01869974 0.21302718]\n",
      " [0.04783706 0.07231608 0.01581107 0.8640358 ]\n",
      " [0.06507256 0.03880898 0.01735264 0.8787658 ]\n",
      " ...\n",
      " [0.01738709 0.9043641  0.00841398 0.06983487]\n",
      " [0.0350423  0.00589478 0.9449861  0.01407681]\n",
      " [0.03504378 0.00589607 0.9449817  0.0140784 ]]\n",
      "Iteration 989, Accuracy 0.37512\n",
      "94.3978%change in label assignment\n",
      "0.07688679\n",
      "[[0.01189016 0.9412544  0.00591624 0.04093915]\n",
      " [0.06971988 0.29224604 0.02641926 0.6116149 ]\n",
      " [0.04739761 0.06642235 0.01494881 0.87123126]\n",
      " ...\n",
      " [0.01983791 0.9110704  0.01046847 0.05862321]\n",
      " [0.0351577  0.00590586 0.9448205  0.01411588]\n",
      " [0.0351594  0.00590715 0.944816   0.01411751]]\n",
      "Iteration 990, Accuracy 0.37551\n",
      "96.8282%change in label assignment\n",
      "0.07131599\n",
      "[[0.02312223 0.8664997  0.01090238 0.09947569]\n",
      " [0.05195139 0.09266332 0.01763845 0.83774686]\n",
      " [0.0533211  0.03738621 0.01475951 0.8945332 ]\n",
      " ...\n",
      " [0.01255029 0.93486464 0.00622465 0.04636046]\n",
      " [0.03500326 0.00589133 0.94503593 0.01406948]\n",
      " [0.03500565 0.00589269 0.9450303  0.01407136]]\n",
      "Iteration 991, Accuracy 0.37796\n",
      "95.23248%change in label assignment\n",
      "0.06971177\n",
      "[[0.01395647 0.92618775 0.00676661 0.0530892 ]\n",
      " [0.06115206 0.1510881  0.02163368 0.7661262 ]\n",
      " [0.04388178 0.0409445  0.01279638 0.9023773 ]\n",
      " ...\n",
      " [0.01180538 0.9426462  0.00599096 0.03955744]\n",
      " [0.03503416 0.00589304 0.944997   0.01407586]\n",
      " [0.03503827 0.00589469 0.9449886  0.01407843]]\n",
      "Iteration 992, Accuracy 0.37924\n",
      "98.46811%change in label assignment\n",
      "0.06564581\n",
      "[[0.04256459 0.7183064  0.01897833 0.22015065]\n",
      " [0.04316474 0.04933158 0.01336434 0.8941393 ]\n",
      " [0.08883189 0.04225922 0.02201334 0.8468955 ]\n",
      " ...\n",
      " [0.02554262 0.84979993 0.01196228 0.11269519]\n",
      " [0.03493705 0.00588343 0.94513315 0.01404641]\n",
      " [0.03494648 0.00588603 0.94511634 0.01405121]]\n",
      "Iteration 993, Accuracy 0.37973\n",
      "92.861%change in label assignment\n",
      "0.073224306\n",
      "[[0.01409885 0.93391985 0.0072806  0.04470073]\n",
      " [0.06939149 0.3197221  0.02684562 0.5840408 ]\n",
      " [0.04945885 0.07819035 0.01613394 0.8562168 ]\n",
      " ...\n",
      " [0.0216559  0.9038453  0.01158051 0.06291837]\n",
      " [0.03501181 0.00589019 0.94502085 0.01407711]\n",
      " [0.03501525 0.00589167 0.9450137  0.01407936]]\n",
      "Iteration 994, Accuracy 0.37777\n",
      "91.75136%change in label assignment\n",
      "0.07004371\n",
      "[[0.02145038 0.8776314  0.01018201 0.09073623]\n",
      " [0.05206789 0.09366804 0.0177393  0.8365248 ]\n",
      " [0.051147   0.03731289 0.01432917 0.89721096]\n",
      " ...\n",
      " [0.01291816 0.9324386  0.00639349 0.04824973]\n",
      " [0.03483943 0.00587425 0.9452575  0.01402874]\n",
      " [0.03483952 0.00587516 0.9452557  0.01402964]]\n",
      "Iteration 995, Accuracy 0.38356\n",
      "96.45996%change in label assignment\n",
      "0.066212885\n",
      "[[0.02101288 0.88104737 0.009878   0.08806176]\n",
      " [0.0520574  0.09147933 0.01735066 0.8391126 ]\n",
      " [0.05243479 0.03718873 0.01428268 0.8960938 ]\n",
      " ...\n",
      " [0.01302502 0.93189263 0.00637125 0.04871108]\n",
      " [0.03483056 0.00587298 0.9452732  0.01402326]\n",
      " [0.03483251 0.0058742  0.9452684  0.01402493]]\n",
      "Iteration 996, Accuracy 0.38012\n",
      "97.40757%change in label assignment\n",
      "0.06463366\n",
      "[[0.01398797 0.9256438  0.00682246 0.05354579]\n",
      " [0.05721516 0.12390304 0.02000204 0.7988798 ]\n",
      " [0.04442705 0.03936093 0.01294648 0.9032656 ]\n",
      " ...\n",
      " [0.0113735  0.9427988  0.0057142  0.04011346]\n",
      " [0.03478555 0.00586853 0.94533503 0.01401084]\n",
      " [0.0347854  0.00586934 0.9453336  0.01401159]]\n",
      "Iteration 997, Accuracy 0.38317\n",
      "97.98203%change in label assignment\n",
      "0.06929277\n",
      "[[0.0240802  0.8597799  0.01129721 0.10484267]\n",
      " [0.05120812 0.08948423 0.01727368 0.8420339 ]\n",
      " [0.05282166 0.03716015 0.01458572 0.8954324 ]\n",
      " ...\n",
      " [0.01438789 0.92303854 0.00702663 0.05554699]\n",
      " [0.03472623 0.005863   0.94541794 0.01399283]\n",
      " [0.03472744 0.00586412 0.9454142  0.01399421]]\n",
      "Iteration 998, Accuracy 0.38263\n",
      "98.19807%change in label assignment\n",
      "0.063599885\n",
      "[[0.0120601  0.9380028  0.00595894 0.0439781 ]\n",
      " [0.06443603 0.18751857 0.02351436 0.72453105]\n",
      " [0.04302499 0.04762659 0.01312754 0.89622086]\n",
      " ...\n",
      " [0.01244912 0.940325   0.00639564 0.04083016]\n",
      " [0.03472619 0.00586253 0.9454185  0.01399275]\n",
      " [0.03472279 0.00586282 0.9454222  0.01399222]]\n",
      "Iteration 999, Accuracy 0.37782\n",
      "95.62528%change in label assignment\n",
      "[[0.04265698 0.7168111  0.01901297 0.22151893]\n",
      " [0.0434153  0.05202541 0.01358791 0.89097136]\n",
      " [0.0798841  0.04089241 0.02023409 0.85898936]\n",
      " ...\n",
      " [0.02576227 0.847868   0.01207134 0.11429837]\n",
      " [0.03461519 0.00585194 0.94557744 0.01395542]\n",
      " [0.03461131 0.00585217 0.9455818  0.01395471]]\n",
      "Iteration 1000, Accuracy 0.38022\n",
      "94.11794%change in label assignment\n",
      "0.078116484\n",
      "[[0.01413945 0.9336568  0.00733108 0.04487267]\n",
      " [0.06896928 0.32302526 0.02690575 0.58109975]\n",
      " [0.0500628  0.08307036 0.01660744 0.8502593 ]\n",
      " ...\n",
      " [0.01778736 0.91903454 0.00940823 0.05376989]\n",
      " [0.03471339 0.00586262 0.94543177 0.01399219]\n",
      " [0.03471059 0.005863   0.9454345  0.01399189]]\n",
      "Iteration 1001, Accuracy 0.38312\n",
      "94.12285%change in label assignment\n",
      "0.08317843\n",
      "[[0.06101959 0.51710373 0.02583031 0.3960464 ]\n",
      " [0.04363105 0.04066281 0.01302982 0.9026763 ]\n",
      " [0.11009429 0.04567393 0.02648019 0.8177516 ]\n",
      " ...\n",
      " [0.04513716 0.69407177 0.02020724 0.24058385]\n",
      " [0.03457957 0.00584825 0.94562656 0.01394557]\n",
      " [0.03457158 0.00584783 0.94563735 0.01394324]]\n",
      "Iteration 1002, Accuracy 0.38219\n",
      "91.73172%change in label assignment\n",
      "0.08556216\n",
      "[[0.03191004 0.86436105 0.01754514 0.08618379]\n",
      " [0.03955003 0.7457545  0.01756764 0.19712788]\n",
      " [0.06923875 0.299084   0.02657694 0.60510033]\n",
      " ...\n",
      " [0.03511376 0.85241127 0.01949352 0.09298143]\n",
      " [0.03478184 0.0058702  0.94532657 0.01402141]\n",
      " [0.03477078 0.0058693  0.94534206 0.01401786]]\n",
      "Iteration 1003, Accuracy 0.38631\n",
      "83.35543%change in label assignment\n",
      "0.09729793\n",
      "[[0.04699443 0.67702204 0.02110613 0.2548774 ]\n",
      " [0.0440809  0.05414621 0.01422015 0.88755274]\n",
      " [0.08604039 0.04272866 0.0222677  0.84896326]\n",
      " ...\n",
      " [0.03106483 0.8099461  0.01459152 0.14439753]\n",
      " [0.03455776 0.00585018 0.9456416  0.01395054]\n",
      " [0.03455148 0.00585009 0.9456495  0.01394895]]\n",
      "Iteration 1004, Accuracy 0.3816\n",
      "85.85948%change in label assignment\n",
      "0.08609704\n",
      "[[0.02364079 0.89596856 0.01263897 0.06775166]\n",
      " [0.05775664 0.57121116 0.02422525 0.346807  ]\n",
      " [0.06413988 0.17668079 0.02298761 0.7361917 ]\n",
      " ...\n",
      " [0.02697675 0.8830994  0.01459515 0.07532869]\n",
      " [0.03476705 0.00586906 0.94533587 0.01402793]\n",
      " [0.03476198 0.00586916 0.945342   0.01402679]]\n",
      "Iteration 1005, Accuracy 0.38052\n",
      "90.02799%change in label assignment\n",
      "0.09176467\n",
      "[[0.03028016 0.8151688  0.01414587 0.14040522]\n",
      " [0.05042781 0.08669323 0.01725124 0.8456277 ]\n",
      " [0.05493942 0.03786878 0.01541307 0.8917787 ]\n",
      " ...\n",
      " [0.02749012 0.83557546 0.01297282 0.12396158]\n",
      " [0.03460567 0.00585264 0.9455745  0.0139672 ]\n",
      " [0.03459383 0.0058517  0.9455911  0.01396342]]\n",
      "Iteration 1006, Accuracy 0.37836\n",
      "92.67933%change in label assignment\n",
      "0.08254533\n",
      "[[0.01675096 0.9230781  0.00871829 0.05145261]\n",
      " [0.06363246 0.4918138  0.02598803 0.41856572]\n",
      " [0.06107794 0.1467035  0.02135268 0.7708659 ]\n",
      " ...\n",
      " [0.01457554 0.9317608  0.0075128  0.0461509 ]\n",
      " [0.0347968  0.0058703  0.9453     0.01403291]\n",
      " [0.03477364 0.00586755 0.9453342  0.01402459]]\n",
      "Iteration 1007, Accuracy 0.36633\n",
      "90.51407%change in label assignment\n",
      "0.09075981\n",
      "[[0.05895773 0.54518855 0.02553556 0.37031814]\n",
      " [0.04406302 0.04237077 0.01349289 0.90007335]\n",
      " [0.11329267 0.04680211 0.02780942 0.81209576]\n",
      " ...\n",
      " [0.05707771 0.5699163  0.02499527 0.34801075]\n",
      " [0.03460153 0.00585253 0.9455829  0.01396291]\n",
      " [0.03458058 0.00585011 0.94561386 0.01395547]]\n",
      "Iteration 1008, Accuracy 0.36058\n",
      "89.05583%change in label assignment\n",
      "0.09639705\n",
      "[[0.03722918 0.84457505 0.02072322 0.09747263]\n",
      " [0.03342789 0.7941202  0.01508718 0.15736473]\n",
      " [0.0692471  0.2975067  0.02654007 0.60670614]\n",
      " ...\n",
      " [0.03500813 0.8527206  0.01940834 0.09286292]\n",
      " [0.03490682 0.0058826  0.9451442  0.01406629]\n",
      " [0.03488527 0.0058801  0.945176   0.0140586 ]]\n",
      "Iteration 1009, Accuracy 0.37531\n",
      "87.96583%change in label assignment\n",
      "0.09654041\n",
      "[[0.02204037 0.8732941  0.01061674 0.09404877]\n",
      " [0.06123318 0.16034533 0.02271816 0.7557033 ]\n",
      " [0.04589482 0.03950423 0.01372635 0.9008746 ]\n",
      " ...\n",
      " [0.02395744 0.86040646 0.01150126 0.10413487]\n",
      " [0.03476193 0.00587352 0.9453401  0.01402453]\n",
      " [0.03472302 0.00586815 0.945399   0.01400987]]\n",
      "Iteration 1010, Accuracy 0.37345\n",
      "89.94943%change in label assignment\n",
      "0.081626296\n",
      "[[0.01349589 0.9356514  0.00685616 0.04399654]\n",
      " [0.06717673 0.42523164 0.02679765 0.480794  ]\n",
      " [0.05198837 0.08812764 0.01701995 0.8428641 ]\n",
      " ...\n",
      " [0.01262587 0.93891364 0.00637416 0.04208631]\n",
      " [0.03488107 0.00588146 0.945178   0.01405952]\n",
      " [0.03484883 0.00587721 0.9452265  0.01404752]]\n",
      "Iteration 1011, Accuracy 0.36289\n",
      "96.07208%change in label assignment\n",
      "0.08271665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05434061 0.59933114 0.02388404 0.32244423]\n",
      " [0.04312127 0.0504518  0.01375889 0.892668  ]\n",
      " [0.09496479 0.04406696 0.02404315 0.83692515]\n",
      " ...\n",
      " [0.0513448  0.63247967 0.02286155 0.29331398]\n",
      " [0.03472007 0.00586565 0.9454143  0.01399995]\n",
      " [0.03469191 0.00586207 0.9454563  0.01398965]]\n",
      "Iteration 1012, Accuracy 0.36387\n",
      "91.32911%change in label assignment\n",
      "0.093051426\n",
      "[[0.03573246 0.84996986 0.01977608 0.09452155]\n",
      " [0.03710531 0.76587886 0.01653728 0.18047853]\n",
      " [0.06785117 0.23661926 0.02514529 0.6703843 ]\n",
      " ...\n",
      " [0.0364368  0.8473572  0.02023229 0.09597371]\n",
      " [0.03494864 0.00588872 0.9450859  0.01407674]\n",
      " [0.03493555 0.0058876  0.94510436 0.01407245]]\n",
      "Iteration 1013, Accuracy 0.37256\n",
      "88.83979%change in label assignment\n",
      "0.09627967\n",
      "[[0.01494828 0.91939646 0.00741691 0.05823835]\n",
      " [0.0646448  0.20320646 0.0246672  0.70748156]\n",
      " [0.04761482 0.03874586 0.01407493 0.8995644 ]\n",
      " ...\n",
      " [0.01387385 0.9264404  0.0069498  0.05273587]\n",
      " [0.03471923 0.00586941 0.9454044  0.01400689]\n",
      " [0.034709   0.00586881 0.94541836 0.01400384]]\n",
      "Iteration 1014, Accuracy 0.37752\n",
      "93.56803%change in label assignment\n",
      "0.07713515\n",
      "[[0.01257291 0.9369871  0.00621025 0.04422975]\n",
      " [0.06913706 0.265952   0.02587746 0.6390335 ]\n",
      " [0.04501908 0.04228895 0.01296896 0.899723  ]\n",
      " ...\n",
      " [0.01232111 0.93873334 0.00612334 0.04282223]\n",
      " [0.03483604 0.0058767  0.94524497 0.01404229]\n",
      " [0.03483247 0.00587712 0.94524854 0.01404182]]\n",
      "Iteration 1015, Accuracy 0.37409\n",
      "96.17028%change in label assignment\n",
      "0.07171717\n",
      "[[0.0467446  0.67818445 0.02110446 0.2539665 ]\n",
      " [0.04657847 0.06854042 0.01567405 0.8692071 ]\n",
      " [0.08258888 0.04214176 0.02164417 0.8536252 ]\n",
      " ...\n",
      " [0.0419096  0.72227603 0.01921575 0.21659864]\n",
      " [0.03475497 0.0058632  0.9453624  0.01401943]\n",
      " [0.03474341 0.00586249 0.94537824 0.01401592]]\n",
      "Iteration 1016, Accuracy 0.37978\n",
      "94.49109%change in label assignment\n",
      "0.0855537\n",
      "[[0.04858473 0.8040917  0.0275263  0.11979728]\n",
      " [0.02187212 0.87797123 0.01017157 0.08998513]\n",
      " [0.07036833 0.33039993 0.02692644 0.5723053 ]\n",
      " ...\n",
      " [0.04937726 0.80130625 0.02806354 0.12125296]\n",
      " [0.03498651 0.00589117 0.9450204  0.0141019 ]\n",
      " [0.03497531 0.00589044 0.9450358  0.01409844]]\n",
      "Iteration 1017, Accuracy 0.37487\n",
      "88.19659%change in label assignment\n",
      "0.09661254\n",
      "[[0.01163899 0.941575   0.00596151 0.04082443]\n",
      " [0.06811111 0.32741362 0.02742457 0.5770507 ]\n",
      " [0.04259563 0.04705805 0.01346354 0.8968828 ]\n",
      " ...\n",
      " [0.01177625 0.94112086 0.00605428 0.04104857]\n",
      " [0.03473864 0.0058702  0.94535714 0.01403399]\n",
      " [0.03472678 0.00586937 0.94537354 0.01403031]]\n",
      "Iteration 1018, Accuracy 0.37757\n",
      "94.1474%change in label assignment\n",
      "0.07725989\n",
      "[[0.01655573 0.9095284  0.00793858 0.06597728]\n",
      " [0.06094677 0.15374167 0.02173528 0.76357627]\n",
      " [0.05451958 0.03708709 0.01477347 0.89361984]\n",
      " ...\n",
      " [0.01439997 0.9230281  0.00699013 0.05558176]\n",
      " [0.034788   0.00587167 0.94529426 0.01404605]\n",
      " [0.03479261 0.00587352 0.9452849  0.01404895]]\n",
      "Iteration 1019, Accuracy 0.36942\n",
      "90.36186%change in label assignment\n",
      "0.07167578\n",
      "[[0.01285854 0.9323163  0.00633781 0.04848739]\n",
      " [0.06665233 0.23433943 0.02526767 0.67374057]\n",
      " [0.04250068 0.041017   0.01271966 0.9037626 ]\n",
      " ...\n",
      " [0.01139269 0.94192535 0.00571465 0.04096727]\n",
      " [0.0348177  0.00587085 0.94525766 0.01405375]\n",
      " [0.0348182  0.00587201 0.94525486 0.014055  ]]\n",
      "Iteration 1020, Accuracy 0.37904\n",
      "96.36667%change in label assignment\n",
      "0.070111014\n",
      "[[0.02124532 0.8781898  0.01011557 0.09044934]\n",
      " [0.05318374 0.10335798 0.01846555 0.82499266]\n",
      " [0.05487661 0.03702851 0.01515512 0.89293975]\n",
      " ...\n",
      " [0.01969213 0.8884686  0.00944457 0.08239462]\n",
      " [0.03476217 0.00586525 0.945332   0.01404064]\n",
      " [0.03476738 0.00586718 0.9453216  0.01404378]]\n",
      "Iteration 1021, Accuracy 0.37772\n",
      "96.06226%change in label assignment\n",
      "0.06897722\n",
      "[[0.01293611 0.9384052  0.00668337 0.0419753 ]\n",
      " [0.06820507 0.36178327 0.02713858 0.5428731 ]\n",
      " [0.04722241 0.07161457 0.01547083 0.8656922 ]\n",
      " ...\n",
      " [0.01267976 0.9394222  0.0065499  0.04134812]\n",
      " [0.03481892 0.00586887 0.94525087 0.01406138]\n",
      " [0.03481731 0.00586962 0.9452514  0.01406171]]\n",
      "Iteration 1022, Accuracy 0.37777\n",
      "92.71861%change in label assignment\n",
      "0.07533609\n",
      "[[0.02767719 0.83341706 0.01301661 0.12588914]\n",
      " [0.04997202 0.08667204 0.01714526 0.8462106 ]\n",
      " [0.06119056 0.03802104 0.01670922 0.88407916]\n",
      " ...\n",
      " [0.02693163 0.8388023  0.01272069 0.12154539]\n",
      " [0.03468101 0.00585575 0.9454445  0.01401876]\n",
      " [0.03468199 0.00585696 0.9454409  0.01402018]]\n",
      "Iteration 1023, Accuracy 0.37423\n",
      "93.87735%change in label assignment\n",
      "0.076790616\n",
      "[[0.01384591 0.93475354 0.0071861  0.04421451]\n",
      " [0.06817568 0.36897314 0.02713505 0.5357162 ]\n",
      " [0.04848097 0.0768799  0.01598974 0.85864943]\n",
      " ...\n",
      " [0.01392342 0.93446743 0.00724369 0.04436546]\n",
      " [0.03476994 0.0058656  0.9453112  0.01405328]\n",
      " [0.03477426 0.00586736 0.9453023  0.01405604]]\n",
      "Iteration 1024, Accuracy 0.37752\n",
      "95.75293%change in label assignment\n",
      "0.07916306\n",
      "[[0.02612993 0.84437823 0.01236504 0.11712676]\n",
      " [0.04859693 0.0801043  0.0165551  0.8547438 ]\n",
      " [0.05878252 0.03774071 0.01624481 0.88723195]\n",
      " ...\n",
      " [0.02699487 0.83828276 0.01277294 0.12194936]\n",
      " [0.034631   0.0058503  0.94551164 0.01400703]\n",
      " [0.03463461 0.00585196 0.94550395 0.01400954]]\n",
      "Iteration 1025, Accuracy 0.37625\n",
      "96.0328%change in label assignment\n",
      "0.07487481\n",
      "[[0.02180917 0.90297014 0.01167643 0.0635443 ]\n",
      " [0.0643172  0.47200245 0.02636722 0.4373132 ]\n",
      " [0.05519953 0.11022207 0.01889508 0.8156833 ]\n",
      " ...\n",
      " [0.01995762 0.91028565 0.01063281 0.05912391]\n",
      " [0.03476365 0.00586497 0.94531685 0.01405447]\n",
      " [0.03476877 0.00586685 0.94530684 0.01405755]]\n",
      "Iteration 1026, Accuracy 0.37458\n",
      "95.66456%change in label assignment\n",
      "0.08058601\n",
      "[[0.02303571 0.8658454  0.01100914 0.10010978]\n",
      " [0.05022104 0.08823995 0.01731471 0.84422433]\n",
      " [0.0524383  0.03710459 0.014862   0.89559513]\n",
      " ...\n",
      " [0.02379832 0.86066127 0.01137499 0.10416542]\n",
      " [0.03459232 0.00584523 0.9455637  0.01399879]\n",
      " [0.03459185 0.00584615 0.94556236 0.01399959]]\n",
      "Iteration 1027, Accuracy 0.3759\n",
      "94.87897%change in label assignment\n",
      "0.073804505\n",
      "[[0.01685656 0.9226668  0.00889419 0.05158246]\n",
      " [0.06715769 0.40497628 0.02708302 0.50078297]\n",
      " [0.05186146 0.0934699  0.01753541 0.8371332 ]\n",
      " ...\n",
      " [0.01592737 0.926408   0.00838235 0.04928223]\n",
      " [0.03469086 0.00585586 0.9454159  0.01403736]\n",
      " [0.03469237 0.0058571  0.9454116  0.01403894]]\n",
      "Iteration 1028, Accuracy 0.37472\n",
      "96.25374%change in label assignment\n",
      "0.07891\n",
      "[[0.02749509 0.8345512  0.01298086 0.12497281]\n",
      " [0.04755766 0.07540929 0.01610821 0.86092484]\n",
      " [0.05732623 0.03751876 0.01594972 0.8892053 ]\n",
      " ...\n",
      " [0.02823349 0.8292805  0.01333638 0.12914963]\n",
      " [0.03453801 0.00583924 0.94563437 0.01398835]\n",
      " [0.03453542 0.00583983 0.94563645 0.0139883 ]]\n",
      "Iteration 1029, Accuracy 0.37566\n",
      "95.34541%change in label assignment\n",
      "0.0803815\n",
      "[[0.03198766 0.86384547 0.01770588 0.08646101]\n",
      " [0.05060608 0.643828   0.0219526  0.28361326]\n",
      " [0.0663008  0.21792796 0.02472463 0.69104666]\n",
      " ...\n",
      " [0.02930878 0.87394166 0.01613037 0.08061922]\n",
      " [0.03469877 0.00585621 0.9453961  0.01404882]\n",
      " [0.03469056 0.00585585 0.94540715 0.01404647]]\n",
      "Iteration 1030, Accuracy 0.37625\n",
      "88.81033%change in label assignment\n",
      "0.080599114\n",
      "[[0.01180534 0.939093   0.00596995 0.04313172]\n",
      " [0.06555507 0.2198931  0.02515895 0.6893928 ]\n",
      " [0.04224899 0.04936332 0.0135129  0.8948748 ]\n",
      " ...\n",
      " [0.01180634 0.93924683 0.00598956 0.04295722]\n",
      " [0.03452227 0.00583927 0.9456431  0.01399536]\n",
      " [0.03450646 0.00583774 0.9456658  0.01399003]]\n",
      "Iteration 1031, Accuracy 0.36957\n",
      "95.32577%change in label assignment\n",
      "0.07073563\n",
      "[[0.01145413 0.9437699  0.00584688 0.03892911]\n",
      " [0.06849924 0.31129324 0.02682593 0.5933815 ]\n",
      " [0.04590518 0.06660647 0.01498053 0.8725078 ]\n",
      " ...\n",
      " [0.01153595 0.9435859  0.00591187 0.03896636]\n",
      " [0.03454732 0.0058407  0.9456099  0.01400199]\n",
      " [0.0345311  0.00583916 0.94563323 0.01399654]]\n",
      "Iteration 1032, Accuracy 0.36314\n",
      "98.68906%change in label assignment\n",
      "0.06872906\n",
      "[[0.02659429 0.8408739  0.0125254  0.12000638]\n",
      " [0.05266085 0.10135198 0.01833826 0.827649  ]\n",
      " [0.04940702 0.03676429 0.01407765 0.899751  ]\n",
      " ...\n",
      " [0.02358487 0.8619172  0.01123605 0.10326187]\n",
      " [0.03444178 0.00582909 0.9457626  0.01396648]\n",
      " [0.03442898 0.00582817 0.9457804  0.01396247]]\n",
      "Iteration 1033, Accuracy 0.36058\n",
      "93.54348%change in label assignment\n",
      "0.07787821\n",
      "[[0.02442665 0.89266014 0.01330702 0.06960619]\n",
      " [0.05152432 0.63214266 0.02241844 0.2939146 ]\n",
      " [0.06363777 0.18499053 0.02348166 0.7278901 ]\n",
      " ...\n",
      " [0.02598372 0.886596   0.01423928 0.07318101]\n",
      " [0.03453205 0.00584088 0.945615   0.01401217]\n",
      " [0.0345248  0.00584082 0.94562405 0.01401035]]\n",
      "Iteration 1034, Accuracy 0.36991\n",
      "94.09339%change in label assignment\n",
      "0.07678771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01921871 0.89130104 0.00932048 0.08015974]\n",
      " [0.05300215 0.10379716 0.01869898 0.8245017 ]\n",
      " [0.05636368 0.03738676 0.0157947  0.8904548 ]\n",
      " ...\n",
      " [0.02148311 0.87627095 0.01036287 0.09188303]\n",
      " [0.03433363 0.00582701 0.9458832  0.01395613]\n",
      " [0.03434407 0.00583    0.94586426 0.01396164]]\n",
      "Iteration 1035, Accuracy 0.37001\n",
      "84.41597%change in label assignment\n",
      "0.07246182\n",
      "[[0.01336617 0.93657047 0.00692582 0.04313756]\n",
      " [0.06839631 0.35799378 0.02715183 0.54645807]\n",
      " [0.04704056 0.07032726 0.01536666 0.8672656 ]\n",
      " ...\n",
      " [0.01363552 0.935546   0.00709424 0.04372432]\n",
      " [0.03443659 0.00583196 0.94575244 0.01397901]\n",
      " [0.03444144 0.00583396 0.9457424  0.01398216]]\n",
      "Iteration 1036, Accuracy 0.37909\n",
      "93.98537%change in label assignment\n",
      "0.06709887\n",
      "[[0.01459486 0.9210015  0.00718047 0.0572232 ]\n",
      " [0.06207034 0.17256    0.02299538 0.7423743 ]\n",
      " [0.04169311 0.04225539 0.01280421 0.90324736]\n",
      " ...\n",
      " [0.01287497 0.93187195 0.00641538 0.04883772]\n",
      " [0.03432553 0.00581813 0.9459156  0.01394082]\n",
      " [0.03431914 0.00581824 0.9459232  0.01393939]]\n",
      "Iteration 1037, Accuracy 0.37639\n",
      "95.22266%change in label assignment\n",
      "0.069501534\n",
      "[[0.01143732 0.9413699  0.00574797 0.04144486]\n",
      " [0.06638384 0.23347023 0.02532414 0.6748218 ]\n",
      " [0.04300101 0.05463146 0.01378607 0.88858145]\n",
      " ...\n",
      " [0.01109935 0.94397664 0.00563323 0.03929078]\n",
      " [0.03430869 0.00581694 0.9459339  0.0139404 ]\n",
      " [0.03430162 0.00581694 0.94594276 0.0139387 ]]\n",
      "Iteration 1038, Accuracy 0.37281\n",
      "98.73324%change in label assignment\n",
      "0.06435708\n",
      "[[0.01313743 0.93019414 0.00652101 0.05014742]\n",
      " [0.06173484 0.16946103 0.02285389 0.7459503 ]\n",
      " [0.0415831  0.04275929 0.01283549 0.90282214]\n",
      " ...\n",
      " [0.0132125  0.9297116  0.00656922 0.05050669]\n",
      " [0.03422341 0.00581047 0.94604665 0.01391946]\n",
      " [0.03422219 0.00581142 0.94604623 0.01392012]]\n",
      "Iteration 1039, Accuracy 0.37158\n",
      "96.64163%change in label assignment\n",
      "[[0.01219369 0.9412226  0.00631324 0.04027047]\n",
      " [0.06803779 0.2851663  0.02651793 0.620278  ]\n",
      " [0.04540885 0.06546312 0.01490099 0.87422705]\n",
      " ...\n",
      " [0.01144217 0.9439253  0.00588335 0.03874919]\n",
      " [0.0342239  0.00581057 0.94604266 0.01392287]\n",
      " [0.03422326 0.00581158 0.94604146 0.01392373]]\n",
      "Iteration 1040, Accuracy 0.37546\n",
      "98.24716%change in label assignment\n",
      "0.080890425\n",
      "[[0.03508736 0.7765483  0.01622742 0.17213684]\n",
      " [0.04467728 0.06356007 0.01480884 0.87695384]\n",
      " [0.06209221 0.03795385 0.01694596 0.883008  ]\n",
      " ...\n",
      " [0.040828   0.7290412  0.01866271 0.21146809]\n",
      " [0.03417578 0.00580034 0.9461321  0.01389177]\n",
      " [0.03416839 0.00580026 0.9461414  0.01388993]]\n",
      "Iteration 1041, Accuracy 0.37708\n",
      "94.12776%change in label assignment\n",
      "0.085773155\n",
      "[[0.03386258 0.85662615 0.01892048 0.09059075]\n",
      " [0.04360907 0.70774853 0.01942287 0.22921957]\n",
      " [0.06721122 0.24696633 0.02567489 0.6601475 ]\n",
      " ...\n",
      " [0.02732254 0.8813426  0.01502837 0.07630648]\n",
      " [0.03434142 0.00582054 0.9458776  0.01396042]\n",
      " [0.03433885 0.00582128 0.9458793  0.01396054]]\n",
      "Iteration 1042, Accuracy 0.37168\n",
      "92.4142%change in label assignment\n",
      "0.0949718\n",
      "[[0.03361977 0.7881117  0.01581101 0.16245748]\n",
      " [0.04330518 0.05603828 0.01436697 0.8862896 ]\n",
      " [0.07809427 0.04130754 0.02099182 0.8596064 ]\n",
      " ...\n",
      " [0.03629206 0.76711947 0.0169926  0.17959586]\n",
      " [0.034151   0.00580287 0.94615084 0.01389532]\n",
      " [0.03415353 0.00580445 0.94614446 0.01389753]]\n",
      "Iteration 1043, Accuracy 0.37531\n",
      "89.44371%change in label assignment\n",
      "0.08446242\n",
      "[[0.02719355 0.88190174 0.01483294 0.07607176]\n",
      " [0.05957283 0.54237825 0.02508144 0.37296748]\n",
      " [0.05994074 0.14402561 0.02130083 0.7747329 ]\n",
      " ...\n",
      " [0.02461447 0.8918507  0.01334498 0.07018982]\n",
      " [0.03435864 0.00582527 0.945842   0.01397401]\n",
      " [0.03436654 0.00582776 0.9458273  0.0139784 ]]\n",
      "Iteration 1044, Accuracy 0.37409\n",
      "93.53366%change in label assignment\n",
      "0.09277152\n",
      "[[0.04085704 0.7283417  0.0187238  0.21207744]\n",
      " [0.04222384 0.05154593 0.01367979 0.8925504 ]\n",
      " [0.08795933 0.04253114 0.02272503 0.8467845 ]\n",
      " ...\n",
      " [0.03915792 0.74292696 0.01808489 0.1998302 ]\n",
      " [0.03418437 0.00580361 0.9461123  0.01389974]\n",
      " [0.03418496 0.00580485 0.9461091  0.01390114]]\n",
      "Iteration 1045, Accuracy 0.38106\n",
      "91.12289%change in label assignment\n",
      "0.084224015\n",
      "[[0.01956151 0.91169643 0.01047554 0.05826656]\n",
      " [0.0620638  0.5017394  0.02598131 0.41021547]\n",
      " [0.05617661 0.12009162 0.01970949 0.80402225]\n",
      " ...\n",
      " [0.01963716 0.9113935  0.01053822 0.05843109]\n",
      " [0.03436772 0.00582236 0.94583654 0.01397331]\n",
      " [0.03436886 0.00582367 0.9458325  0.0139749 ]]\n",
      "Iteration 1046, Accuracy 0.37536\n",
      "94.58438%change in label assignment\n",
      "0.08899934\n",
      "[[0.04777512 0.66467476 0.021603   0.26594716]\n",
      " [0.04163119 0.04451582 0.01320628 0.9006467 ]\n",
      " [0.10541217 0.04539235 0.0265383  0.82265717]\n",
      " ...\n",
      " [0.0413559  0.72421    0.01908941 0.21534461]\n",
      " [0.03417294 0.00580407 0.94611627 0.01390669]\n",
      " [0.0341731  0.00580525 0.9461137  0.01390793]]\n",
      "Iteration 1047, Accuracy 0.37885\n",
      "91.5697%change in label assignment\n",
      "0.08507765\n",
      "[[0.02288619 0.8985061  0.01235852 0.06624912]\n",
      " [0.06494005 0.4551224  0.02667964 0.45325795]\n",
      " [0.05454256 0.10832545 0.01879769 0.8183343 ]\n",
      " ...\n",
      " [0.02130943 0.90471166 0.01146832 0.06251055]\n",
      " [0.0343955  0.00582656 0.9457959  0.01398203]\n",
      " [0.03439863 0.0058281  0.9457889  0.01398433]]\n",
      "Iteration 1048, Accuracy 0.38685\n",
      "92.55168%change in label assignment\n",
      "0.08627248\n",
      "[[0.03842397 0.7486338  0.01782497 0.19511726]\n",
      " [0.04157495 0.04642005 0.0133455  0.8986594 ]\n",
      " [0.0926183  0.04353502 0.02401324 0.83983344]\n",
      " ...\n",
      " [0.032337   0.79756063 0.01527994 0.15482242]\n",
      " [0.03418359 0.00580851 0.94609153 0.01391636]\n",
      " [0.03419055 0.00581087 0.9460782  0.01392042]]\n",
      "Iteration 1049, Accuracy 0.38111\n",
      "91.94776%change in label assignment\n",
      "0.0798516\n",
      "[[0.01375185 0.93485093 0.00714606 0.04425111]\n",
      " [0.06863612 0.31180146 0.02682819 0.5927343 ]\n",
      " [0.04647389 0.06832401 0.01516406 0.87003803]\n",
      " ...\n",
      " [0.01538323 0.92838144 0.00808965 0.04814572]\n",
      " [0.03434052 0.00582203 0.9458684  0.01396911]\n",
      " [0.03434635 0.00582416 0.94585687 0.01397264]]\n",
      "Iteration 1050, Accuracy 0.38224\n",
      "93.61713%change in label assignment\n",
      "0.084022164\n",
      "[[0.03825999 0.74975175 0.01776582 0.19422248]\n",
      " [0.04247923 0.05381959 0.01400646 0.88969475]\n",
      " [0.0753687  0.04065603 0.02032857 0.8636466 ]\n",
      " ...\n",
      " [0.03353924 0.7880342  0.01580568 0.16262081]\n",
      " [0.03419307 0.0058052  0.94608885 0.01391289]\n",
      " [0.0341971  0.00580706 0.94608    0.01391575]]\n",
      "Iteration 1051, Accuracy 0.38008\n",
      "95.40433%change in label assignment\n",
      "0.08098616\n",
      "[[0.01272681 0.93874806 0.00655598 0.04196917]\n",
      " [0.06747422 0.24908838 0.02567448 0.6577629 ]\n",
      " [0.04561287 0.064667   0.01479294 0.87492716]\n",
      " ...\n",
      " [0.01356108 0.93561345 0.00705236 0.04377317]\n",
      " [0.03436138 0.0058236  0.9458393  0.01397571]\n",
      " [0.03437149 0.00582645 0.94582105 0.013981  ]]\n",
      "Iteration 1052, Accuracy 0.37222\n",
      "95.2914%change in label assignment\n",
      "0.07919471\n",
      "[[0.04225288 0.7153579  0.01936177 0.22302744]\n",
      " [0.0412328  0.04445872 0.01309751 0.90121096]\n",
      " [0.07475552 0.04032977 0.02007135 0.8648434 ]\n",
      " ...\n",
      " [0.03758839 0.7554168  0.01748659 0.1895082 ]\n",
      " [0.03420192 0.00580483 0.9460707  0.01392248]\n",
      " [0.03420726 0.00580687 0.94606    0.01392583]]\n",
      "Iteration 1053, Accuracy 0.37433\n",
      "94.2505%change in label assignment\n",
      "0.07940921\n",
      "[[0.01785401 0.91847134 0.00952565 0.05414897]\n",
      " [0.06560082 0.4331558  0.0269826  0.4742608 ]\n",
      " [0.05990161 0.15005705 0.0216865  0.76835483]\n",
      " ...\n",
      " [0.01728665 0.9207557  0.00921853 0.05273906]\n",
      " [0.03435598 0.0058189  0.9458425  0.01398255]\n",
      " [0.03435643 0.0058201  0.9458396  0.01398386]]\n",
      "Iteration 1054, Accuracy 0.3758\n",
      "88.47646%change in label assignment\n",
      "0.08911806\n",
      "[[0.05226199 0.6178117  0.02343983 0.30648646]\n",
      " [0.04268637 0.04004452 0.0132632  0.9040059 ]\n",
      " [0.09103417 0.04330722 0.02383481 0.84182376]\n",
      " ...\n",
      " [0.04951356 0.6472244  0.02246588 0.2807961 ]\n",
      " [0.03417045 0.00579985 0.94610745 0.01392225]\n",
      " [0.034173   0.00580142 0.94610107 0.01392446]]\n",
      "Iteration 1055, Accuracy 0.36638\n",
      "90.15073%change in label assignment\n",
      "0.085365355\n",
      "[[0.04972829 0.7996924  0.02850321 0.12207608]\n",
      " [0.02881181 0.82978004 0.01323999 0.12816814]\n",
      " [0.06917272 0.3691926  0.02730959 0.5343251 ]\n",
      " ...\n",
      " [0.05379919 0.7856045  0.03115322 0.12944306]\n",
      " [0.03439278 0.0058295  0.9457634  0.01401433]\n",
      " [0.03439844 0.00583163 0.9457521  0.01401784]]\n",
      "Iteration 1056, Accuracy 0.37104\n",
      "88.96745%change in label assignment\n",
      "0.10048744\n",
      "[[0.01242856 0.9344873  0.00626572 0.04681844]\n",
      " [0.06347854 0.19477947 0.02427754 0.7174644 ]\n",
      " [0.04128873 0.0468681  0.01328592 0.89855725]\n",
      " ...\n",
      " [0.01121984 0.94287395 0.00576148 0.04014475]\n",
      " [0.03419165 0.00580895 0.9460383  0.01396109]\n",
      " [0.03419549 0.00581089 0.94602966 0.01396399]]\n",
      "Iteration 1057, Accuracy 0.38621\n",
      "93.32253%change in label assignment\n",
      "0.074716695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03309857 0.7929506  0.01527683 0.15867405]\n",
      " [0.04430474 0.06148759 0.01448753 0.8797202 ]\n",
      " [0.05954816 0.03716253 0.01619917 0.8870901 ]\n",
      " ...\n",
      " [0.0257789  0.84660846 0.01216854 0.11544412]\n",
      " [0.03420646 0.00580631 0.94602764 0.01395955]\n",
      " [0.0342159  0.00580925 0.9460101  0.01396479]]\n",
      "Iteration 1058, Accuracy 0.36706\n",
      "94.55983%change in label assignment\n",
      "0.07448424\n",
      "[[0.0118748  0.9381498  0.00595818 0.0440172 ]\n",
      " [0.06262528 0.18041484 0.02343957 0.73352027]\n",
      " [0.04237504 0.05377204 0.01374209 0.8901108 ]\n",
      " ...\n",
      " [0.01104989 0.94377923 0.00561913 0.0395518 ]\n",
      " [0.03426958 0.00581172 0.94593245 0.01398623]\n",
      " [0.03427847 0.00581448 0.9459159  0.01399117]]\n",
      "Iteration 1059, Accuracy 0.36972\n",
      "96.73%change in label assignment\n",
      "0.0721435\n",
      "[[0.04385798 0.70209354 0.01988398 0.23416458]\n",
      " [0.04155895 0.0498165  0.01339362 0.8952309 ]\n",
      " [0.06187369 0.03759508 0.01700231 0.88352895]\n",
      " ...\n",
      " [0.03877564 0.74621093 0.01786004 0.19715336]\n",
      " [0.03417054 0.0058     0.9460755  0.01395396]\n",
      " [0.03417999 0.0058029  0.946058   0.01395917]]\n",
      "Iteration 1060, Accuracy 0.36981\n",
      "95.33068%change in label assignment\n",
      "0.07144834\n",
      "[[0.01110996 0.94444644 0.00565815 0.03878553]\n",
      " [0.06702384 0.2469412  0.02580183 0.66023314]\n",
      " [0.04808366 0.07824217 0.01624025 0.8574339 ]\n",
      " ...\n",
      " [0.01130369 0.94447887 0.00582218 0.03839524]\n",
      " [0.03423896 0.00580684 0.94597036 0.01398379]\n",
      " [0.03424496 0.00580914 0.9459584  0.01398757]]\n",
      "Iteration 1061, Accuracy 0.37202\n",
      "94.21122%change in label assignment\n",
      "0.07691303\n",
      "[[0.0380923  0.751726   0.01767508 0.19250669]\n",
      " [0.04371153 0.06051797 0.01467356 0.88109696]\n",
      " [0.05140667 0.03661788 0.01491374 0.89706165]\n",
      " ...\n",
      " [0.03460132 0.7800401  0.01624363 0.16911499]\n",
      " [0.03409258 0.00578981 0.94618154 0.01393611]\n",
      " [0.03409293 0.00579113 0.9461783  0.01393756]]\n",
      "Iteration 1062, Accuracy 0.37202\n",
      "96.16046%change in label assignment\n",
      "0.07365181\n",
      "[[0.01148926 0.9433817  0.0058746  0.03925443]\n",
      " [0.06790613 0.26591125 0.02622358 0.639959  ]\n",
      " [0.0503093  0.08760877 0.01711383 0.8449681 ]\n",
      " ...\n",
      " [0.01189169 0.94228476 0.00614254 0.03968103]\n",
      " [0.03418983 0.00580054 0.9460397  0.01396993]\n",
      " [0.03419037 0.00580189 0.9460364  0.01397142]]\n",
      "Iteration 1063, Accuracy 0.36746\n",
      "96.00825%change in label assignment\n",
      "0.07727403\n",
      "[[0.04777696 0.66401005 0.02163327 0.26657966]\n",
      " [0.04119834 0.04698642 0.01330546 0.89850986]\n",
      " [0.06732576 0.03887546 0.01852402 0.8752747 ]\n",
      " ...\n",
      " [0.04182518 0.7193919  0.01931732 0.21946557]\n",
      " [0.03402565 0.00578354 0.9462809  0.01390989]\n",
      " [0.03402841 0.00578533 0.94627386 0.01391238]]\n",
      "Iteration 1064, Accuracy 0.36829\n",
      "94.61384%change in label assignment\n",
      "0.08090325\n",
      "[[0.0174732  0.92008215 0.00932963 0.05311495]\n",
      " [0.06464245 0.45600173 0.02685691 0.45249885]\n",
      " [0.0624209  0.17315307 0.02305088 0.74137515]\n",
      " ...\n",
      " [0.02002692 0.90987074 0.01081719 0.05928521]\n",
      " [0.03417859 0.00579942 0.9460461  0.01397589]\n",
      " [0.03417755 0.00580056 0.94604504 0.01397681]]\n",
      "Iteration 1065, Accuracy 0.36731\n",
      "91.45677%change in label assignment\n",
      "0.08933575\n",
      "[[0.05853758 0.54217637 0.02572219 0.37356386]\n",
      " [0.04133464 0.0424863  0.01313292 0.90304613]\n",
      " [0.07982644 0.04117613 0.02145591 0.8575415 ]\n",
      " ...\n",
      " [0.04855059 0.65697134 0.02211568 0.2723624 ]\n",
      " [0.03397685 0.00577909 0.94633234 0.01391164]\n",
      " [0.03397522 0.00578022 0.94633204 0.01391243]]\n",
      "Iteration 1066, Accuracy 0.36579\n",
      "91.38803%change in label assignment\n",
      "0.08230986\n",
      "[[0.01484384 0.930577   0.00779123 0.04678789]\n",
      " [0.06574204 0.44114563 0.02700517 0.46610713]\n",
      " [0.05938207 0.14144032 0.02125807 0.7779195 ]\n",
      " ...\n",
      " [0.02119119 0.90525454 0.01144719 0.06210703]\n",
      " [0.03416282 0.00579922 0.9460596  0.01397838]\n",
      " [0.03416366 0.00580085 0.94605523 0.01398024]]\n",
      "Iteration 1067, Accuracy 0.36883\n",
      "93.44528%change in label assignment\n",
      "0.08603044\n",
      "[[0.05108115 0.63007736 0.02294309 0.29589832]\n",
      " [0.04208663 0.05310334 0.01390513 0.8909049 ]\n",
      " [0.06766471 0.03895589 0.01866114 0.8747183 ]\n",
      " ...\n",
      " [0.03649013 0.76440305 0.01711333 0.18199351]\n",
      " [0.03395986 0.00577887 0.9463535  0.01390772]\n",
      " [0.03395995 0.00578045 0.9463503  0.01390937]]\n",
      "Iteration 1068, Accuracy 0.36824\n",
      "93.92154%change in label assignment\n",
      "0.07640263\n",
      "[[0.01149218 0.94318163 0.00587396 0.03945226]\n",
      " [0.06851204 0.31726798 0.02708114 0.5871389 ]\n",
      " [0.04967146 0.08486682 0.01687817 0.8485836 ]\n",
      " ...\n",
      " [0.01425142 0.9330261  0.00750228 0.04522026]\n",
      " [0.03411405 0.00579451 0.9461272  0.01396429]\n",
      " [0.0341199  0.00579712 0.9461146  0.01396836]]\n",
      "Iteration 1069, Accuracy 0.36775\n",
      "94.81514%change in label assignment\n",
      "0.07858055\n",
      "[[0.03807054 0.75128335 0.01771293 0.19293314]\n",
      " [0.04421898 0.06355719 0.0149896  0.8772342 ]\n",
      " [0.05817392 0.03731421 0.01649651 0.88801533]\n",
      " ...\n",
      " [0.02700099 0.836938   0.01296705 0.12309397]\n",
      " [0.03397424 0.00578159 0.9463245  0.0139196 ]\n",
      " [0.03398617 0.00578531 0.9463023  0.01392629]]\n",
      "Iteration 1070, Accuracy 0.36652\n",
      "92.30618%change in label assignment\n",
      "0.06815934\n",
      "[[0.01660792 0.9081568  0.00810724 0.06712807]\n",
      " [0.05980243 0.15148847 0.0218986  0.7668105 ]\n",
      " [0.04102199 0.04456685 0.01284779 0.9015634 ]\n",
      " ...\n",
      " [0.01186045 0.93820506 0.00596322 0.04397128]\n",
      " [0.03401617 0.00578314 0.9462678  0.0139329 ]\n",
      " [0.03402551 0.00578637 0.9462496  0.01393845]]\n",
      "Iteration 1071, Accuracy 0.37011\n",
      "97.85928%change in label assignment\n",
      "0.066676944\n",
      "[[0.02831072 0.8274997  0.01340346 0.13078614]\n",
      " [0.05047588 0.09297399 0.01772029 0.8388298 ]\n",
      " [0.04354006 0.03700063 0.01306442 0.9063949 ]\n",
      " ...\n",
      " [0.01898237 0.8921561  0.00926715 0.07959442]\n",
      " [0.03392823 0.005774   0.94639343 0.0139043 ]\n",
      " [0.03393696 0.00577713 0.94637626 0.0139096 ]]\n",
      "Iteration 1072, Accuracy 0.37055\n",
      "98.56631%change in label assignment\n",
      "0.06293193\n",
      "[[0.02941594 0.8196142  0.01385108 0.13711883]\n",
      " [0.05095265 0.09492621 0.0178644  0.8362567 ]\n",
      " [0.04296561 0.03732488 0.01290991 0.90679955]\n",
      " ...\n",
      " [0.01963645 0.88792723 0.00954151 0.08289483]\n",
      " [0.03389834 0.00577094 0.94643253 0.01389822]\n",
      " [0.03391061 0.00577471 0.9464096  0.01390504]]\n",
      "Iteration 1073, Accuracy 0.37139\n",
      "98.77252%change in label assignment\n",
      "0.06620546\n",
      "[[0.01436386 0.9220597  0.00713582 0.05644058]\n",
      " [0.05963912 0.15382087 0.02219312 0.7643469 ]\n",
      " [0.04083667 0.04698514 0.01314998 0.8990282 ]\n",
      " ...\n",
      " [0.01170551 0.9389172  0.00593329 0.04344405]\n",
      " [0.03389084 0.00577406 0.9464249  0.01391018]\n",
      " [0.03391007 0.00577899 0.9463911  0.01391986]]\n",
      "Iteration 1074, Accuracy 0.37104\n",
      "96.62199%change in label assignment\n",
      "0.06256625\n",
      "[[0.02545809 0.84823185 0.01213696 0.11417311]\n",
      " [0.05779043 0.13797475 0.0211878  0.783047  ]\n",
      " [0.04062025 0.04399499 0.01287304 0.9025118 ]\n",
      " ...\n",
      " [0.01412579 0.9235519  0.00703086 0.05529136]\n",
      " [0.03380273 0.00575953 0.9465601  0.01387767]\n",
      " [0.03381125 0.0057627  0.9465431  0.013883  ]]\n",
      "Iteration 1075, Accuracy 0.37492\n",
      "96.00825%change in label assignment\n",
      "0.066429734\n",
      "[[0.0319907  0.80008924 0.01500692 0.15291314]\n",
      " [0.05704945 0.13289274 0.02087206 0.78918576]\n",
      " [0.04057619 0.0437021  0.01287224 0.9028495 ]\n",
      " ...\n",
      " [0.01547338 0.9149329  0.00766364 0.0619301 ]\n",
      " [0.03374299 0.00575196 0.94664603 0.01385904]\n",
      " [0.03374374 0.00575379 0.94664145 0.01386114]]\n",
      "Iteration 1076, Accuracy 0.36628\n",
      "96.72509%change in label assignment\n",
      "0.064531885\n",
      "[[0.02787605 0.830741   0.01320983 0.12817316]\n",
      " [0.05624336 0.12698142 0.02043458 0.79634064]\n",
      " [0.04059869 0.04350714 0.01283814 0.9030561 ]\n",
      " ...\n",
      " [0.01446197 0.92134565 0.00719366 0.05699869]\n",
      " [0.03368647 0.00574706 0.94672525 0.01384125]\n",
      " [0.03368783 0.00574892 0.94671977 0.01384353]]\n",
      "Iteration 1077, Accuracy 0.3625\n",
      "99.32734%change in label assignment\n",
      "0.06775086\n",
      "[[0.02619472 0.84281385 0.01246973 0.11852172]\n",
      " [0.05756391 0.13658105 0.0210731  0.784782  ]\n",
      " [0.0407717  0.0463245  0.01304781 0.8998561 ]\n",
      " ...\n",
      " [0.01363389 0.9265714  0.00681112 0.05298354]\n",
      " [0.03364829 0.00574317 0.94677806 0.01383054]\n",
      " [0.03364504 0.00574423 0.9467799  0.0138309 ]]\n",
      "Iteration 1078, Accuracy 0.36412\n",
      "98.78234%change in label assignment\n",
      "0.062973626\n",
      "[[0.02345357 0.8619125  0.01127376 0.10336022]\n",
      " [0.05704747 0.13339213 0.020881   0.7886794 ]\n",
      " [0.04075464 0.04670921 0.0130987  0.8994374 ]\n",
      " ...\n",
      " [0.01326392 0.9288593  0.00665165 0.05122511]\n",
      " [0.03359468 0.00573885 0.9468493  0.01381712]\n",
      " [0.03358912 0.00573944 0.94685507 0.01381643]]\n",
      "Iteration 1079, Accuracy 0.36549\n",
      "99.28315%change in label assignment\n",
      "[[0.02042666 0.88258576 0.00989343 0.08709413]\n",
      " [0.05941382 0.15166464 0.02197899 0.76694256]\n",
      " [0.0417881  0.05265961 0.01364827 0.89190406]\n",
      " ...\n",
      " [0.01137712 0.941011   0.00578395 0.04182789]\n",
      " [0.03357692 0.00573682 0.94687545 0.01381078]\n",
      " [0.03356193 0.00573584 0.9468961  0.01380623]]\n",
      "Iteration 1080, Accuracy 0.36716\n",
      "96.96077%change in label assignment\n",
      "0.07670832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06213833 0.48348358 0.02668302 0.4276951 ]\n",
      " [0.0412343  0.03907868 0.01275712 0.90692985]\n",
      " [0.07683152 0.04026049 0.02052808 0.8623799 ]\n",
      " ...\n",
      " [0.04227878 0.71387726 0.01946445 0.22437954]\n",
      " [0.03352095 0.00572896 0.9469711  0.01377902]\n",
      " [0.03350847 0.00572836 0.9469876  0.01377547]]\n",
      "Iteration 1081, Accuracy 0.36554\n",
      "95.40433%change in label assignment\n",
      "0.08508873\n",
      "[[0.02803272 0.87830114 0.01561513 0.07805094]\n",
      " [0.04148583 0.72369635 0.01885299 0.21596487]\n",
      " [0.06685636 0.38285762 0.02749514 0.5227909 ]\n",
      " ...\n",
      " [0.03554003 0.84993637 0.0202017  0.09432192]\n",
      " [0.03373536 0.00575356 0.9466467  0.01386439]\n",
      " [0.03371217 0.00575126 0.94668007 0.01385652]]\n",
      "Iteration 1082, Accuracy 0.3681\n",
      "81.16561%change in label assignment\n",
      "0.102126546\n",
      "[[0.04308064 0.70642793 0.02010453 0.23038697]\n",
      " [0.04706814 0.07756444 0.01674828 0.8586191 ]\n",
      " [0.04721602 0.03760025 0.01450011 0.90068364]\n",
      " ...\n",
      " [0.03137125 0.80403954 0.01512661 0.14946263]\n",
      " [0.03356626 0.00573839 0.9468921  0.01380336]\n",
      " [0.03353602 0.00573483 0.9469366  0.01379254]]\n",
      "Iteration 1083, Accuracy 0.36343\n",
      "90.01326%change in label assignment\n",
      "0.07988051\n",
      "[[0.01249084 0.9383466  0.00634864 0.04281391]\n",
      " [0.06831446 0.37735072 0.02741226 0.52692264]\n",
      " [0.05770621 0.1266828  0.02033396 0.795277  ]\n",
      " ...\n",
      " [0.01333552 0.9359929  0.00689603 0.04377556]\n",
      " [0.03376142 0.00575684 0.94661134 0.01387031]\n",
      " [0.03371996 0.00575164 0.9466733  0.01385509]]\n",
      "Iteration 1084, Accuracy 0.34674\n",
      "94.91334%change in label assignment\n",
      "0.09168583\n",
      "[[0.06707653 0.30597505 0.02792131 0.5990271 ]\n",
      " [0.05563943 0.03816466 0.01650768 0.8896882 ]\n",
      " [0.15033677 0.05172345 0.03695213 0.76098764]\n",
      " ...\n",
      " [0.06291389 0.4686695  0.02773027 0.44068635]\n",
      " [0.0335141  0.00573058 0.9469843  0.01377105]\n",
      " [0.03349005 0.00572815 0.94701886 0.01376288]]\n",
      "Iteration 1085, Accuracy 0.3431\n",
      "88.01493%change in label assignment\n",
      "0.10515162\n",
      "[[0.04200243 0.82624424 0.0242023  0.10755106]\n",
      " [0.02386373 0.8596347  0.01138305 0.10511857]\n",
      " [0.06546173 0.4217754  0.02730118 0.48546165]\n",
      " ...\n",
      " [0.04880951 0.80196846 0.02857069 0.12065138]\n",
      " [0.03383025 0.00576454 0.9465062  0.01389904]\n",
      " [0.03381423 0.0057635  0.9465281  0.01389417]]\n",
      "Iteration 1086, Accuracy 0.36093\n",
      "84.54853%change in label assignment\n",
      "0.100133464\n",
      "[[0.02614911 0.8422331  0.01274551 0.11887223]\n",
      " [0.04742293 0.07979367 0.0169892  0.8557942 ]\n",
      " [0.05960483 0.03841897 0.01744238 0.8845338 ]\n",
      " ...\n",
      " [0.01464573 0.9202802  0.00745331 0.05762076]\n",
      " [0.03358426 0.00574394 0.9468541  0.01381768]\n",
      " [0.03358534 0.00574586 0.94684875 0.01381997]]\n",
      "Iteration 1087, Accuracy 0.3703\n",
      "80.44386%change in label assignment\n",
      "0.07795462\n",
      "[[0.0191077  0.8925134  0.00920952 0.07916941]\n",
      " [0.05291085 0.10336439 0.01851551 0.8252092 ]\n",
      " [0.04664988 0.03673887 0.01351451 0.90309674]\n",
      " ...\n",
      " [0.01169432 0.94033176 0.00590589 0.04206803]\n",
      " [0.03372986 0.00575471 0.9466521  0.01386332]\n",
      " [0.03374    0.00575824 0.9466323  0.0138694 ]]\n",
      "Iteration 1088, Accuracy 0.37021\n",
      "97.75617%change in label assignment\n",
      "0.0705903\n",
      "[[0.03654316 0.7624047  0.01706926 0.18398286]\n",
      " [0.04087786 0.05071234 0.01352937 0.8948805 ]\n",
      " [0.06501218 0.03823787 0.01810589 0.87864405]\n",
      " ...\n",
      " [0.01809651 0.8974338  0.00892465 0.07554503]\n",
      " [0.03367005 0.00574939 0.9467331  0.01384745]\n",
      " [0.03368523 0.0057539  0.94670504 0.01385577]]\n",
      "Iteration 1089, Accuracy 0.36976\n",
      "95.86095%change in label assignment\n",
      "0.072715506\n",
      "[[0.01749502 0.9018406  0.00855731 0.07210704]\n",
      " [0.05165803 0.10082002 0.01836014 0.8291618 ]\n",
      " [0.04193396 0.03762304 0.01278149 0.90766144]\n",
      " ...\n",
      " [0.01087812 0.94571865 0.00562734 0.03777586]\n",
      " [0.03373068 0.00575893 0.9466314  0.01387896]\n",
      " [0.03375596 0.00576521 0.94658726 0.0138915 ]]\n",
      "Iteration 1090, Accuracy 0.3758\n",
      "96.99514%change in label assignment\n",
      "0.070601925\n",
      "[[0.04743923 0.66480243 0.0215229  0.26623538]\n",
      " [0.03996965 0.0430537  0.01282746 0.9041492 ]\n",
      " [0.07381333 0.03974516 0.02006176 0.86637974]\n",
      " ...\n",
      " [0.02126259 0.8762729  0.01036677 0.09209774]\n",
      " [0.03364797 0.00574843 0.9467544  0.01384924]\n",
      " [0.03367053 0.00575426 0.9467145  0.01386069]]\n",
      "Iteration 1091, Accuracy 0.37512\n",
      "96.69073%change in label assignment\n",
      "0.070717365\n",
      "[[0.02058767 0.88140744 0.00995997 0.0880449 ]\n",
      " [0.05118768 0.0981093  0.01812176 0.8325813 ]\n",
      " [0.04145605 0.03813529 0.01270015 0.9077085 ]\n",
      " ...\n",
      " [0.01081867 0.9456652  0.00557969 0.0379364 ]\n",
      " [0.03369389 0.0057532  0.94668365 0.01386921]\n",
      " [0.03371601 0.00575891 0.94664466 0.01388043]]\n",
      "Iteration 1092, Accuracy 0.37944\n",
      "97.87892%change in label assignment\n",
      "0.0685539\n",
      "[[0.03814591 0.7488737  0.01783761 0.19514279]\n",
      " [0.04117329 0.05244497 0.01381085 0.8925709 ]\n",
      " [0.05813802 0.03726909 0.01675841 0.8878345 ]\n",
      " ...\n",
      " [0.01692117 0.90509444 0.00842501 0.06955936]\n",
      " [0.03360276 0.00574453 0.9468077  0.01384502]\n",
      " [0.03362618 0.00575048 0.9467665  0.01385681]]\n",
      "Iteration 1093, Accuracy 0.37649\n",
      "97.63343%change in label assignment\n",
      "0.06796384\n",
      "[[0.01950699 0.88887304 0.00945675 0.08216318]\n",
      " [0.05028089 0.09251793 0.0176265  0.83957464]\n",
      " [0.04140749 0.03841218 0.01267548 0.9075049 ]\n",
      " ...\n",
      " [0.01113055 0.942907   0.00566781 0.04029466]\n",
      " [0.03361338 0.00574168 0.9467957  0.0138493 ]\n",
      " [0.03362717 0.00574583 0.9467701  0.01385693]]\n",
      "Iteration 1094, Accuracy 0.3734\n",
      "97.85928%change in label assignment\n",
      "0.06722437\n",
      "[[0.03191815 0.7991622  0.01517387 0.15374584]\n",
      " [0.04187262 0.05583415 0.01417006 0.88812315]\n",
      " [0.05103774 0.03629316 0.01509106 0.89757806]\n",
      " ...\n",
      " [0.01693103 0.9050951  0.00843696 0.06953691]\n",
      " [0.03352885 0.00573403 0.94690824 0.01382892]\n",
      " [0.03354263 0.00573821 0.94688255 0.01383658]]\n",
      "Iteration 1095, Accuracy 0.37649\n",
      "98.23243%change in label assignment\n",
      "0.060708724\n",
      "[[0.01931966 0.8907734  0.00934039 0.0805665 ]\n",
      " [0.05167443 0.09755135 0.01808227 0.8326919 ]\n",
      " [0.04134454 0.04007999 0.01269391 0.9058816 ]\n",
      " ...\n",
      " [0.01139484 0.9418133  0.00578272 0.04100918]\n",
      " [0.03353742 0.00573249 0.9468963  0.01383369]\n",
      " [0.03354405 0.00573537 0.94688237 0.01383829]]\n",
      "Iteration 1096, Accuracy 0.37757\n",
      "97.64325%change in label assignment\n",
      "0.06503317\n",
      "[[0.04807326 0.6583849  0.02203604 0.2715058 ]\n",
      " [0.04018531 0.04364683 0.01313868 0.90302914]\n",
      " [0.05997895 0.03767286 0.01732734 0.88502085]\n",
      " ...\n",
      " [0.03023086 0.8122141  0.01454347 0.14301161]\n",
      " [0.03343061 0.00571858 0.94704896 0.01380173]\n",
      " [0.03342659 0.0057196  0.9470519  0.01380187]]\n",
      "Iteration 1097, Accuracy 0.3787\n",
      "96.33721%change in label assignment\n",
      "0.07271238\n",
      "[[0.01106887 0.9440432  0.00566062 0.03922735]\n",
      " [0.06485605 0.21394952 0.02488668 0.6963077 ]\n",
      " [0.04742976 0.07826111 0.01630607 0.8580031 ]\n",
      " ...\n",
      " [0.01418309 0.93324316 0.00754385 0.04502993]\n",
      " [0.03350877 0.00572911 0.94692826 0.01383397]\n",
      " [0.03350443 0.00573006 0.9469316  0.01383397]]\n",
      "Iteration 1098, Accuracy 0.38057\n",
      "94.43217%change in label assignment\n",
      "0.07683293\n",
      "[[0.04712405 0.66727865 0.02167091 0.26392642]\n",
      " [0.04042001 0.04106805 0.01305944 0.9054525 ]\n",
      " [0.06865666 0.03931568 0.01943507 0.8725926 ]\n",
      " ...\n",
      " [0.02938753 0.81832033 0.0141755  0.13811666]\n",
      " [0.03333667 0.00571471 0.947167   0.01378162]\n",
      " [0.03333917 0.00571688 0.9471594  0.01378454]]\n",
      "Iteration 1099, Accuracy 0.37556\n",
      "93.56803%change in label assignment\n",
      "0.07297393\n",
      "[[0.01336996 0.9364235  0.00704525 0.04316128]\n",
      " [0.06678948 0.25233677 0.0260476  0.65482616]\n",
      " [0.04958522 0.08814143 0.0172451  0.8450283 ]\n",
      " ...\n",
      " [0.02049566 0.9077751  0.01116729 0.06056194]\n",
      " [0.0334551  0.00572818 0.9469904  0.01382625]\n",
      " [0.03346044 0.00573081 0.9469785  0.01383029]]\n",
      "Iteration 1100, Accuracy 0.37615\n",
      "94.72676%change in label assignment\n",
      "0.079913996\n",
      "[[0.05101684 0.6271213  0.02310497 0.29875687]\n",
      " [0.04090257 0.03867721 0.01293266 0.9074876 ]\n",
      " [0.07710791 0.0405922  0.02120006 0.86109984]\n",
      " ...\n",
      " [0.03047765 0.8099773  0.01460656 0.1449385 ]\n",
      " [0.03329876 0.00570923 0.94722176 0.01377025]\n",
      " [0.03329859 0.00571096 0.94721836 0.01377205]]\n",
      "Iteration 1101, Accuracy 0.37909\n",
      "94.30942%change in label assignment\n",
      "0.076194406\n",
      "[[0.01557936 0.9275155  0.0083069  0.04859829]\n",
      " [0.06780107 0.28941607 0.02681724 0.6159656 ]\n",
      " [0.0527796  0.10433973 0.01865775 0.82422286]\n",
      " ...\n",
      " [0.02581086 0.88688016 0.01429103 0.07301787]\n",
      " [0.03342828 0.00572437 0.9470265  0.01382081]\n",
      " [0.03342652 0.00572577 0.9470258  0.01382188]]\n",
      "Iteration 1102, Accuracy 0.37629\n",
      "94.4469%change in label assignment\n",
      "0.08394611\n",
      "[[0.04075346 0.72603315 0.01906089 0.21415247]\n",
      " [0.0405574  0.04936272 0.01358742 0.8964924 ]\n",
      " [0.0580354  0.03739349 0.01690869 0.8876625 ]\n",
      " ...\n",
      " [0.01967935 0.886846   0.00977122 0.08370346]\n",
      " [0.03327838 0.00570722 0.9472458  0.01376859]\n",
      " [0.03326438 0.00570663 0.9472643  0.0137647 ]]\n",
      "Iteration 1103, Accuracy 0.37693\n",
      "94.69239%change in label assignment\n",
      "0.07170832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01328877 0.92906207 0.00662899 0.05102018]\n",
      " [0.05932935 0.1507177  0.02190945 0.76804346]\n",
      " [0.04143543 0.05145139 0.01349614 0.89361703]\n",
      " ...\n",
      " [0.01291582 0.9382882  0.00682031 0.04197569]\n",
      " [0.03333895 0.00571372 0.94715345 0.01379386]\n",
      " [0.03333071 0.00571417 0.9471627  0.01379241]]\n",
      "Iteration 1104, Accuracy 0.37168\n",
      "97.07861%change in label assignment\n",
      "0.0744494\n",
      "[[0.0488121  0.6498329  0.02228841 0.2790666 ]\n",
      " [0.0398907  0.04480514 0.01311111 0.90219307]\n",
      " [0.06268054 0.03801597 0.01793391 0.88136965]\n",
      " ...\n",
      " [0.02274383 0.86589044 0.01115845 0.10020723]\n",
      " [0.0332273  0.0057019  0.9473141  0.01375682]\n",
      " [0.03321908 0.00570236 0.94732314 0.01375541]]\n",
      "Iteration 1105, Accuracy 0.37114\n",
      "96.63181%change in label assignment\n",
      "0.07359025\n",
      "[[0.0118789  0.9420276  0.00617334 0.03992014]\n",
      " [0.06798494 0.32018757 0.02722605 0.5846014 ]\n",
      " [0.05427469 0.11343589 0.01938474 0.8129047 ]\n",
      " ...\n",
      " [0.02516172 0.8893474  0.0139088  0.07158205]\n",
      " [0.03337348 0.00571712 0.9471011  0.0138082 ]\n",
      " [0.03335518 0.00571603 0.94712603 0.01380275]]\n",
      "Iteration 1106, Accuracy 0.37251\n",
      "90.89704%change in label assignment\n",
      "0.08504967\n",
      "[[0.05113489 0.6247735  0.02334739 0.30074418]\n",
      " [0.04012288 0.04540186 0.01332966 0.9011456 ]\n",
      " [0.067214   0.03919745 0.0192531  0.87433547]\n",
      " ...\n",
      " [0.02451221 0.85351914 0.01204148 0.1099272 ]\n",
      " [0.03318222 0.00569803 0.947374   0.01374574]\n",
      " [0.03317111 0.0056981  0.94738764 0.01374323]]\n",
      "Iteration 1107, Accuracy 0.36279\n",
      "95.0901%change in label assignment\n",
      "0.07772829\n",
      "[[0.01373168 0.9348986  0.00725343 0.04411628]\n",
      " [0.06789517 0.32964328 0.02733246 0.5751291 ]\n",
      " [0.05404755 0.11249844 0.01932515 0.8141289 ]\n",
      " ...\n",
      " [0.02681319 0.882941   0.01489811 0.07534774]\n",
      " [0.03333123 0.00571367 0.9471524  0.01380275]\n",
      " [0.03332451 0.00571445 0.9471591  0.013802  ]]\n",
      "Iteration 1108, Accuracy 0.36147\n",
      "94.42726%change in label assignment\n",
      "0.08495313\n",
      "[[0.05104823 0.62555546 0.02328956 0.30010673]\n",
      " [0.03992246 0.04336525 0.01313586 0.90357643]\n",
      " [0.07292485 0.04012674 0.02054717 0.86640126]\n",
      " ...\n",
      " [0.02548304 0.84651023 0.01246978 0.11553693]\n",
      " [0.03316376 0.00569557 0.9473975  0.01374322]\n",
      " [0.03315715 0.00569638 0.9474039  0.01374255]]\n",
      "Iteration 1109, Accuracy 0.36932\n",
      "94.82987%change in label assignment\n",
      "0.082317695\n",
      "[[0.02228639 0.9006302  0.01220751 0.06487597]\n",
      " [0.06232497 0.49049777 0.02647046 0.42070675]\n",
      " [0.06339685 0.19219947 0.02405903 0.7203446 ]\n",
      " ...\n",
      " [0.03878584 0.837984   0.02221097 0.10101922]\n",
      " [0.0333451  0.00571495 0.9471219  0.01381793]\n",
      " [0.03333221 0.00571471 0.94713837 0.01381465]]\n",
      "Iteration 1110, Accuracy 0.35999\n",
      "90.10163%change in label assignment\n",
      "0.08228065\n",
      "[[0.02394139 0.85729396 0.0116877  0.10707698]\n",
      " [0.04656568 0.07832532 0.01655497 0.85855407]\n",
      " [0.04429011 0.03633345 0.01365915 0.9057173 ]\n",
      " ...\n",
      " [0.01160234 0.93915194 0.00598711 0.04325866]\n",
      " [0.03317042 0.0056967  0.9473759  0.01375694]\n",
      " [0.03315301 0.00569561 0.94739974 0.01375172]]\n",
      "Iteration 1111, Accuracy 0.3705\n",
      "94.7955%change in label assignment\n",
      "0.07016695\n",
      "[[0.01243526 0.9342574  0.006255   0.04705233]\n",
      " [0.06028014 0.16107248 0.02252627 0.7561211 ]\n",
      " [0.04168093 0.05387103 0.01374742 0.89070064]\n",
      " ...\n",
      " [0.01355315 0.9357453  0.00721436 0.04348718]\n",
      " [0.03325569 0.00570414 0.94725245 0.01378772]\n",
      " [0.03322842 0.00570141 0.9472918  0.01377842]]\n",
      "Iteration 1112, Accuracy 0.35999\n",
      "96.37158%change in label assignment\n",
      "0.06825954\n",
      "[[0.04756441 0.6618092  0.02182919 0.26879725]\n",
      " [0.0397869  0.04614786 0.01319325 0.90087193]\n",
      " [0.06247705 0.03787753 0.01791998 0.88172543]\n",
      " ...\n",
      " [0.02188542 0.87159836 0.0107794  0.09573678]\n",
      " [0.03309982 0.00568804 0.9474749  0.01373727]\n",
      " [0.03308869 0.0056881  0.9474884  0.01373474]]\n",
      "Iteration 1113, Accuracy 0.3595\n",
      "93.1065%change in label assignment\n",
      "0.077499285\n",
      "[[0.0136012  0.9355472  0.00722413 0.0436275 ]\n",
      " [0.06681971 0.38321787 0.0275816  0.52238077]\n",
      " [0.05669027 0.13153844 0.02076805 0.7910032 ]\n",
      " ...\n",
      " [0.03057686 0.86850744 0.0172412  0.08367448]\n",
      " [0.03321659 0.00570246 0.94728756 0.01379331]\n",
      " [0.0332097  0.00570341 0.9472941  0.01379274]]\n",
      "Iteration 1114, Accuracy 0.36353\n",
      "95.4878%change in label assignment\n",
      "0.07731816\n",
      "[[0.03444619 0.7785752  0.0163622  0.1706165 ]\n",
      " [0.04884076 0.08941036 0.01763918 0.84410965]\n",
      " [0.04435942 0.03617452 0.0136838  0.9057823 ]\n",
      " ...\n",
      " [0.01134497 0.9408093  0.00586866 0.04197709]\n",
      " [0.03304023 0.00568694 0.94753337 0.01373944]\n",
      " [0.03304105 0.00568945 0.9475272  0.01374236]]\n",
      "Iteration 1115, Accuracy 0.36937\n",
      "94.4469%change in label assignment\n",
      "0.067411296\n",
      "[[0.03115568 0.8062032  0.01465879 0.14798237]\n",
      " [0.05364699 0.11189546 0.01931995 0.81513757]\n",
      " [0.04064874 0.03880343 0.01261714 0.9079306 ]\n",
      " ...\n",
      " [0.01083241 0.9454775  0.0055955  0.03809455]\n",
      " [0.03308325 0.00568869 0.94747734 0.01375074]\n",
      " [0.03308531 0.00569144 0.9474691  0.01375419]]\n",
      "Iteration 1116, Accuracy 0.36584\n",
      "98.89036%change in label assignment\n",
      "0.06308484\n",
      "[[0.03072772 0.807833   0.01465941 0.14677987]\n",
      " [0.0539849  0.11795243 0.01993386 0.8081288 ]\n",
      " [0.03983853 0.03934382 0.01271836 0.90809923]\n",
      " ...\n",
      " [0.01062832 0.9462768  0.00555371 0.03754114]\n",
      " [0.0330278  0.00568424 0.9475518  0.01373616]\n",
      " [0.03303583 0.00568816 0.9475337  0.01374228]]\n",
      "Iteration 1117, Accuracy 0.36721\n",
      "97.83473%change in label assignment\n",
      "0.06628977\n",
      "[[0.0348722  0.7761483  0.0163543  0.17262518]\n",
      " [0.05468508 0.12093436 0.02006393 0.8043166 ]\n",
      " [0.03961438 0.04078003 0.01263798 0.9069676 ]\n",
      " ...\n",
      " [0.01063187 0.94637614 0.00552672 0.03746529]\n",
      " [0.03301471 0.00568004 0.9475743  0.01373087]\n",
      " [0.03301202 0.00568211 0.9475733  0.01373251]]\n",
      "Iteration 1118, Accuracy 0.36864\n",
      "97.6678%change in label assignment\n",
      "0.062074482\n",
      "[[0.0299562  0.81373775 0.01431288 0.1419932 ]\n",
      " [0.05730004 0.14115873 0.02154736 0.7799939 ]\n",
      " [0.03948937 0.04452346 0.0129584  0.9030287 ]\n",
      " ...\n",
      " [0.01085691 0.9461188  0.00572112 0.0373032 ]\n",
      " [0.03296791 0.0056764  0.94763273 0.01372302]\n",
      " [0.03296609 0.00567868 0.94763017 0.01372508]]\n",
      "Iteration 1119, Accuracy 0.36505\n",
      "98.64978%change in label assignment\n",
      "[[0.05112481 0.6268362  0.02297682 0.29906216]\n",
      " [0.04496025 0.07020461 0.01556167 0.8692735 ]\n",
      " [0.04625824 0.03547613 0.01383236 0.90443325]\n",
      " ...\n",
      " [0.01476235 0.918881   0.007389   0.05896761]\n",
      " [0.03291202 0.00566985 0.9477144  0.01370375]\n",
      " [0.03290943 0.00567193 0.94771314 0.01370543]]\n",
      "Iteration 1120, Accuracy 0.36672\n",
      "96.0328%change in label assignment\n",
      "0.07299384\n",
      "[[0.01384759 0.9247608  0.00695853 0.0544331 ]\n",
      " [0.06698191 0.29803023 0.02718394 0.60780394]\n",
      " [0.04910055 0.08993512 0.0175519  0.84341246]\n",
      " ...\n",
      " [0.01476138 0.9308111  0.00798998 0.04643751]\n",
      " [0.03297959 0.0056759  0.947613   0.01373158]\n",
      " [0.03296871 0.0056766  0.9476248  0.01372984]]\n",
      "Iteration 1121, Accuracy 0.36279\n",
      "93.70059%change in label assignment\n",
      "0.08204663\n",
      "[[0.06662641 0.32297066 0.02774511 0.5826579 ]\n",
      " [0.0403685  0.03808775 0.01284614 0.90869755]\n",
      " [0.0866731  0.04215399 0.02350775 0.8476652 ]\n",
      " ...\n",
      " [0.04197379 0.714347   0.01965823 0.224021  ]\n",
      " [0.03287518 0.00566311 0.9477736  0.01368816]\n",
      " [0.03285776 0.00566275 0.94779575 0.01368374]]\n",
      "Iteration 1122, Accuracy 0.36019\n",
      "93.81843%change in label assignment\n",
      "0.09118441\n",
      "[[0.02026052 0.9086732  0.01106972 0.05999653]\n",
      " [0.03618966 0.76828855 0.01675149 0.17877029]\n",
      " [0.06773631 0.31631967 0.02733752 0.5886065 ]\n",
      " ...\n",
      " [0.04338652 0.8212656  0.02519664 0.11015131]\n",
      " [0.03307341 0.00568749 0.94746333 0.01377573]\n",
      " [0.03305322 0.00568684 0.9474895  0.01377034]]\n",
      "Iteration 1123, Accuracy 0.36083\n",
      "89.2915%change in label assignment\n",
      "0.09993569\n",
      "[[0.0627065  0.46342915 0.02770276 0.4461616 ]\n",
      " [0.04073463 0.05110969 0.01399306 0.89416265]\n",
      " [0.08136391 0.04182934 0.02286603 0.8539407 ]\n",
      " ...\n",
      " [0.02684152 0.8367113  0.01323098 0.12321625]\n",
      " [0.03285735 0.00566579 0.94778    0.01369695]\n",
      " [0.03284287 0.00566597 0.94779736 0.01369381]]\n",
      "Iteration 1124, Accuracy 0.36412\n",
      "88.41754%change in label assignment\n",
      "0.08869422\n",
      "[[0.02081198 0.9064665  0.01136267 0.06135891]\n",
      " [0.03614319 0.76943123 0.01671301 0.17771253]\n",
      " [0.06722192 0.26403466 0.02649514 0.6422483 ]\n",
      " ...\n",
      " [0.0418622  0.8268305  0.02417908 0.10712825]\n",
      " [0.03309101 0.00568942 0.94742703 0.01379256]\n",
      " [0.03307512 0.00568944 0.9474466  0.01378889]]\n",
      "Iteration 1125, Accuracy 0.36441\n",
      "88.49119%change in label assignment\n",
      "0.095765404\n",
      "[[0.06012874 0.507489   0.02668789 0.4056944 ]\n",
      " [0.04016371 0.04982575 0.01363693 0.8963736 ]\n",
      " [0.08647389 0.04230883 0.02369299 0.84752434]\n",
      " ...\n",
      " [0.03133482 0.8026092  0.01516092 0.15089504]\n",
      " [0.03291038 0.00566976 0.947703   0.01371696]\n",
      " [0.03288214 0.00566754 0.94774234 0.01370801]]\n",
      "Iteration 1126, Accuracy 0.3623\n",
      "89.56646%change in label assignment\n",
      "0.08713493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01424583 0.93281776 0.0075735  0.04536286]\n",
      " [0.05439449 0.59763265 0.02388904 0.32408383]\n",
      " [0.05872114 0.1438736  0.02159926 0.77580607]\n",
      " ...\n",
      " [0.02945286 0.87277126 0.016537   0.08123897]\n",
      " [0.0331312  0.00569246 0.9473763  0.01380004]\n",
      " [0.03310115 0.00568986 0.9474188  0.01379022]]\n",
      "Iteration 1127, Accuracy 0.36107\n",
      "93.1605%change in label assignment\n",
      "0.096317776\n",
      "[[0.05875841 0.52844447 0.02657027 0.38622677]\n",
      " [0.04065593 0.05005044 0.0140122  0.89528143]\n",
      " [0.09412263 0.04424381 0.0260214  0.8356122 ]\n",
      " ...\n",
      " [0.03447349 0.77833396 0.01674787 0.17044464]\n",
      " [0.03291634 0.00567113 0.9476903  0.01372222]\n",
      " [0.03289117 0.00566945 0.9477248  0.01371457]]\n",
      "Iteration 1128, Accuracy 0.36152\n",
      "93.86262%change in label assignment\n",
      "0.08749966\n",
      "[[0.01737947 0.92007893 0.00935006 0.05319154]\n",
      " [0.05396721 0.6039794  0.02368667 0.3183667 ]\n",
      " [0.05755324 0.13304251 0.02090331 0.78850085]\n",
      " ...\n",
      " [0.02865187 0.8758011  0.01601425 0.07953271]\n",
      " [0.0331906  0.0056977  0.9472953  0.01381635]\n",
      " [0.0331592  0.00569485 0.9473401  0.01380594]]\n",
      "Iteration 1129, Accuracy 0.35985\n",
      "90.9756%change in label assignment\n",
      "0.09278055\n",
      "[[0.05141872 0.61963904 0.02374256 0.30519974]\n",
      " [0.04240673 0.06032884 0.01494147 0.88232297]\n",
      " [0.07823092 0.04120169 0.02212131 0.85844606]\n",
      " ...\n",
      " [0.03154859 0.80097264 0.01539218 0.15208662]\n",
      " [0.0330267  0.00568227 0.9475333  0.01375776]\n",
      " [0.03298256 0.00567721 0.94759816 0.01374201]]\n",
      "Iteration 1130, Accuracy 0.35508\n",
      "92.52222%change in label assignment\n",
      "0.08755034\n",
      "[[0.02810387 0.8778826  0.01569738 0.07831617]\n",
      " [0.0356145  0.77318424 0.01650381 0.17469752]\n",
      " [0.06509236 0.21522728 0.02504547 0.6946349 ]\n",
      " ...\n",
      " [0.03646772 0.84637034 0.02082959 0.09633234]\n",
      " [0.03325709 0.00570656 0.94718593 0.01385041]\n",
      " [0.03321045 0.00570133 0.94725436 0.01383386]]\n",
      "Iteration 1131, Accuracy 0.3545\n",
      "90.12618%change in label assignment\n",
      "0.10106039\n",
      "[[0.04073685 0.7249578  0.01945431 0.21485105]\n",
      " [0.04557421 0.07412746 0.01657216 0.86372626]\n",
      " [0.08280553 0.04232886 0.02345355 0.85141206]\n",
      " ...\n",
      " [0.02802734 0.827842   0.01385567 0.13027495]\n",
      " [0.03297957 0.00567872 0.9475895  0.01375213]\n",
      " [0.03295596 0.00567724 0.94762164 0.01374507]]\n",
      "Iteration 1132, Accuracy 0.34428\n",
      "88.86925%change in label assignment\n",
      "0.08617375\n",
      "[[0.02640251 0.88443583 0.01461346 0.07454826]\n",
      " [0.04537688 0.69135034 0.02041765 0.24285516]\n",
      " [0.05741218 0.1311795  0.02079186 0.7906165 ]\n",
      " ...\n",
      " [0.03321303 0.8585295  0.01874791 0.08950956]\n",
      " [0.0332289  0.0057041  0.94722414 0.01384287]\n",
      " [0.03320775 0.00570301 0.94725245 0.01383676]]\n",
      "Iteration 1133, Accuracy 0.35474\n",
      "94.03938%change in label assignment\n",
      "0.090707734\n",
      "[[0.02303411 0.8631405  0.01141359 0.10241183]\n",
      " [0.05539048 0.13089044 0.02118006 0.792539  ]\n",
      " [0.05715975 0.03709445 0.01696021 0.8887856 ]\n",
      " ...\n",
      " [0.015017   0.91702724 0.00767418 0.06028164]\n",
      " [0.03303619 0.00568535 0.94750226 0.01377618]\n",
      " [0.03301883 0.00568495 0.9475244  0.01377172]]\n",
      "Iteration 1134, Accuracy 0.35592\n",
      "93.38636%change in label assignment\n",
      "0.07480487\n",
      "[[0.01324315 0.9367077  0.00698795 0.04306128]\n",
      " [0.06596161 0.41432703 0.02749254 0.49221885]\n",
      " [0.0422939  0.05483837 0.01391516 0.8889526 ]\n",
      " ...\n",
      " [0.0164522  0.9238569  0.00885392 0.05083697]\n",
      " [0.03316537 0.00569768 0.9473094  0.01382757]\n",
      " [0.03315559 0.00569856 0.94731957 0.01382626]]\n",
      "Iteration 1135, Accuracy 0.36029\n",
      "97.02951%change in label assignment\n",
      "0.08016676\n",
      "[[0.05391071 0.5918719  0.02472868 0.32948875]\n",
      " [0.0400969  0.04902713 0.01376257 0.8971134 ]\n",
      " [0.09234246 0.04345619 0.02541568 0.83878577]\n",
      " ...\n",
      " [0.0447754  0.6884436  0.02112344 0.24565761]\n",
      " [0.03305587 0.00567978 0.9474716  0.01379274]\n",
      " [0.03303628 0.0056791  0.9474972  0.01378744]]\n",
      "Iteration 1136, Accuracy 0.36456\n",
      "94.5451%change in label assignment\n",
      "0.09571717\n",
      "[[0.06998927 0.7308991  0.04259285 0.15651873]\n",
      " [0.01243038 0.93881464 0.0064063  0.04234869]\n",
      " [0.0610815  0.5195604  0.02609251 0.3932656 ]\n",
      " ...\n",
      " [0.07375374 0.71868914 0.04526407 0.16229308]\n",
      " [0.03331622 0.00571577 0.94706506 0.01390302]\n",
      " [0.03330332 0.00571622 0.94708    0.0139005 ]]\n",
      "Iteration 1137, Accuracy 0.35857\n",
      "83.70403%change in label assignment\n",
      "0.09941212\n",
      "[[0.01380614 0.9343119  0.00753857 0.04434331]\n",
      " [0.06411429 0.4301949  0.02790933 0.47778141]\n",
      " [0.03975373 0.04838706 0.01354324 0.898316  ]\n",
      " ...\n",
      " [0.01596254 0.92542166 0.00881518 0.04980061]\n",
      " [0.03305383 0.00569205 0.94743514 0.01381896]\n",
      " [0.03305095 0.00569408 0.9474344  0.01382053]]\n",
      "Iteration 1138, Accuracy 0.37924\n",
      "87.04277%change in label assignment\n",
      "0.07852708\n",
      "[[0.01127443 0.9415286  0.00576622 0.0414308 ]\n",
      " [0.06501684 0.22706653 0.02552538 0.68239135]\n",
      " [0.04180892 0.03629099 0.01280576 0.9090943 ]\n",
      " ...\n",
      " [0.01069226 0.94581896 0.00553997 0.03794876]\n",
      " [0.03310537 0.00569074 0.94737244 0.01383143]\n",
      " [0.03309751 0.00569195 0.94737965 0.01383091]]\n",
      "Iteration 1139, Accuracy 0.36225\n",
      "95.97879%change in label assignment\n",
      "0.06954208\n",
      "[[0.01388715 0.92418766 0.00703499 0.0548902 ]\n",
      " [0.0589758  0.15733983 0.02265351 0.76103085]\n",
      " [0.04294614 0.03538736 0.01326069 0.90840584]\n",
      " ...\n",
      " [0.01225114 0.93456745 0.0062782  0.04690326]\n",
      " [0.03308568 0.00568711 0.9473983  0.01382891]\n",
      " [0.03308468 0.00568954 0.9473945  0.01383133]]\n",
      "Iteration 1140, Accuracy 0.36279\n",
      "98.74306%change in label assignment\n",
      "0.06768282\n",
      "[[0.01220232 0.93509954 0.00621331 0.04648488]\n",
      " [0.06072402 0.17173238 0.02336884 0.7441748 ]\n",
      " [0.04126881 0.03603839 0.01280983 0.9098829 ]\n",
      " ...\n",
      " [0.01102127 0.94268376 0.0056808  0.04061419]\n",
      " [0.03309233 0.00569114 0.9473722  0.01384431]\n",
      " [0.03311135 0.0056971  0.9473363  0.01385529]]\n",
      "Iteration 1141, Accuracy 0.3623\n",
      "95.02627%change in label assignment\n",
      "0.06493745\n",
      "[[0.01143202 0.939954   0.00587047 0.04274346]\n",
      " [0.0616034  0.18190353 0.02393114 0.73256195]\n",
      " [0.03952881 0.03795704 0.01257743 0.9099367 ]\n",
      " ...\n",
      " [0.01080808 0.94407576 0.00559937 0.03951683]\n",
      " [0.03307524 0.00568715 0.94739455 0.01384308]\n",
      " [0.0330907  0.00569252 0.9473642  0.01385258]]\n",
      "Iteration 1142, Accuracy 0.36864\n",
      "99.32243%change in label assignment\n",
      "0.06500571\n",
      "[[0.01110836 0.94205076 0.00572667 0.04111424]\n",
      " [0.0648146  0.22989284 0.02583585 0.6794568 ]\n",
      " [0.03900105 0.04361302 0.01282749 0.90455836]\n",
      " ...\n",
      " [0.01056146 0.9458717  0.00550045 0.03806629]\n",
      " [0.03304929 0.00567997 0.9474389  0.0138319 ]\n",
      " [0.03304752 0.00568231 0.94743615 0.01383407]]\n",
      "Iteration 1143, Accuracy 0.36981\n",
      "95.25703%change in label assignment\n",
      "0.06620053\n",
      "[[0.01444379 0.9207038  0.0072956  0.05755685]\n",
      " [0.05916549 0.15808599 0.02271276 0.7600358 ]\n",
      " [0.03932725 0.03820257 0.01258758 0.90988266]\n",
      " ...\n",
      " [0.01293529 0.9302439  0.00659812 0.05022267]\n",
      " [0.03299217 0.00567233 0.9475241  0.01381144]\n",
      " [0.03298182 0.00567325 0.94753486 0.01381005]]\n",
      "Iteration 1144, Accuracy 0.36392\n",
      "98.09496%change in label assignment\n",
      "0.0679984\n",
      "[[0.01265196 0.93212306 0.00644453 0.04878047]\n",
      " [0.06121998 0.17790024 0.02374608 0.7371337 ]\n",
      " [0.0388698  0.04206995 0.01270884 0.90635145]\n",
      " ...\n",
      " [0.01122888 0.9411914  0.00579343 0.04178631]\n",
      " [0.03296325 0.00567143 0.94755507 0.01381027]\n",
      " [0.03296091 0.00567367 0.9475532  0.0138122 ]]\n",
      "Iteration 1145, Accuracy 0.36412\n",
      "97.85437%change in label assignment\n",
      "0.06447176\n",
      "[[0.01960032 0.88708764 0.00970331 0.08360872]\n",
      " [0.05135189 0.10315508 0.01884394 0.8266491 ]\n",
      " [0.04144972 0.03572765 0.01292253 0.90990007]\n",
      " ...\n",
      " [0.01614211 0.9097581  0.00810767 0.06599206]\n",
      " [0.03292265 0.00566828 0.94760966 0.01379935]\n",
      " [0.03292532 0.00567137 0.9475999  0.01380339]]\n",
      "Iteration 1146, Accuracy 0.36795\n",
      "96.92149%change in label assignment\n",
      "0.06391527\n",
      "[[0.01334433 0.9277608  0.00676345 0.05213141]\n",
      " [0.0566985  0.13681073 0.02135364 0.7851371 ]\n",
      " [0.03905381 0.03935433 0.01255625 0.9090356 ]\n",
      " ...\n",
      " [0.011563   0.93901914 0.00594125 0.0434766 ]\n",
      " [0.03291189 0.00567108 0.9476148  0.01380219]\n",
      " [0.03292932 0.00567694 0.94758105 0.01381271]]\n",
      "Iteration 1147, Accuracy 0.37075\n",
      "96.89203%change in label assignment\n",
      "0.06297551\n",
      "[[0.01445638 0.9205897  0.00730182 0.05765212]\n",
      " [0.05416524 0.12009272 0.02019524 0.8055468 ]\n",
      " [0.03930183 0.03807633 0.01257571 0.91004616]\n",
      " ...\n",
      " [0.01249968 0.93296117 0.00639296 0.04814616]\n",
      " [0.03286475 0.00566544 0.9476829  0.01378687]\n",
      " [0.03287939 0.00567079 0.94765353 0.01379619]]\n",
      "Iteration 1148, Accuracy 0.37492\n",
      "99.36171%change in label assignment\n",
      "0.06307662\n",
      "[[0.01843358 0.8948227  0.00915687 0.07758678]\n",
      " [0.05086062 0.10040022 0.01858278 0.8301564 ]\n",
      " [0.0396782  0.03733443 0.01259657 0.91039085]\n",
      " ...\n",
      " [0.01355944 0.9262455  0.00688786 0.05330713]\n",
      " [0.03282841 0.00566049 0.9477362  0.01377485]\n",
      " [0.03283852 0.00566514 0.94771403 0.01378236]]\n",
      "Iteration 1149, Accuracy 0.3761\n",
      "98.62032%change in label assignment\n",
      "0.06521935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01151452 0.93923897 0.00592748 0.04331896]\n",
      " [0.05957044 0.16269056 0.02295949 0.7547796 ]\n",
      " [0.0393298  0.04736317 0.01319329 0.90011376]\n",
      " ...\n",
      " [0.01045081 0.9466475  0.00547379 0.03742789]\n",
      " [0.0328073  0.00565889 0.9477632  0.01377056]\n",
      " [0.03281315 0.00566277 0.9477479  0.01377624]]\n",
      "Iteration 1150, Accuracy 0.37625\n",
      "97.73653%change in label assignment\n",
      "0.06170178\n",
      "[[0.01734131 0.9018349  0.00866695 0.07215686]\n",
      " [0.05418222 0.12097702 0.02025742 0.8045833 ]\n",
      " [0.03871955 0.04025528 0.01260601 0.9084192 ]\n",
      " ...\n",
      " [0.01214745 0.93511224 0.00623698 0.04650327]\n",
      " [0.0327439  0.00564912 0.94786114 0.01374582]\n",
      " [0.03273015 0.00564965 0.94787693 0.01374325]]\n",
      "Iteration 1151, Accuracy 0.37291\n",
      "97.1228%change in label assignment\n",
      "0.06430861\n",
      "[[0.01100703 0.94246644 0.00571198 0.04081449]\n",
      " [0.06145234 0.18488677 0.02412638 0.7295345 ]\n",
      " [0.03933176 0.04817932 0.01331566 0.89917326]\n",
      " ...\n",
      " [0.01043141 0.94691217 0.00549141 0.03716503]\n",
      " [0.03268757 0.00564647 0.9479336  0.01373231]\n",
      " [0.03268515 0.00564891 0.9479314  0.01373449]]\n",
      "Iteration 1152, Accuracy 0.37139\n",
      "96.04262%change in label assignment\n",
      "0.061814647\n",
      "[[0.01996919 0.88436204 0.00988236 0.08578642]\n",
      " [0.04667277 0.08033006 0.01670187 0.8562953 ]\n",
      " [0.04164918 0.03554968 0.01298428 0.90981686]\n",
      " ...\n",
      " [0.01781598 0.89861023 0.00890059 0.07467318]\n",
      " [0.03263034 0.00563859 0.9480213  0.01370973]\n",
      " [0.03262661 0.00564069 0.9480214  0.01371123]]\n",
      "Iteration 1153, Accuracy 0.36633\n",
      "97.65798%change in label assignment\n",
      "0.06541824\n",
      "[[0.01205012 0.94186944 0.0064568  0.03962366]\n",
      " [0.06609096 0.26334372 0.02669353 0.6438718 ]\n",
      " [0.0456319  0.07502513 0.01615751 0.8631855 ]\n",
      " ...\n",
      " [0.012744   0.93906736 0.006873   0.04131561]\n",
      " [0.03264257 0.00564339 0.94798803 0.01372599]\n",
      " [0.03264436 0.00564652 0.9479791  0.01372992]]\n",
      "Iteration 1154, Accuracy 0.36849\n",
      "97.43703%change in label assignment\n",
      "0.06361154\n",
      "[[0.01913048 0.88971955 0.00955309 0.08159683]\n",
      " [0.04725718 0.08407429 0.01716839 0.8515001 ]\n",
      " [0.04436681 0.03505483 0.01374523 0.9068331 ]\n",
      " ...\n",
      " [0.01759359 0.8999001  0.00885195 0.07365435]\n",
      " [0.03253236 0.00563896 0.9481306  0.01369809]\n",
      " [0.03256    0.00564687 0.9480798  0.01371332]]\n",
      "Iteration 1155, Accuracy 0.37256\n",
      "92.33073%change in label assignment\n",
      "0.064437315\n",
      "[[0.01105176 0.94255316 0.00569855 0.04069663]\n",
      " [0.05818556 0.14844841 0.02204206 0.771324  ]\n",
      " [0.03898245 0.04310722 0.01275226 0.9051581 ]\n",
      " ...\n",
      " [0.01061568 0.9455258  0.00551883 0.03833959]\n",
      " [0.03256326 0.0056419  0.9480891  0.01370566]\n",
      " [0.03259269 0.00565005 0.94803566 0.01372157]]\n",
      "Iteration 1156, Accuracy 0.37256\n",
      "98.28153%change in label assignment\n",
      "0.05928587\n",
      "[[0.01284126 0.9305888  0.00656673 0.05000328]\n",
      " [0.05164852 0.10661028 0.01913901 0.8226022 ]\n",
      " [0.03902264 0.03812956 0.01259722 0.91025054]\n",
      " ...\n",
      " [0.01229525 0.93402493 0.00632249 0.04735731]\n",
      " [0.03247414 0.00563101 0.9482193  0.01367554]\n",
      " [0.03249668 0.00563785 0.94817704 0.01368841]]\n",
      "Iteration 1157, Accuracy 0.37301\n",
      "99.23406%change in label assignment\n",
      "0.063150786\n",
      "[[0.01113004 0.94166785 0.00575402 0.04144808]\n",
      " [0.05847297 0.15374734 0.02241459 0.76536506]\n",
      " [0.03928625 0.04779865 0.01321583 0.8996993 ]\n",
      " ...\n",
      " [0.01042295 0.94673795 0.00546416 0.03737485]\n",
      " [0.03243753 0.0056225  0.9482797  0.01366037]\n",
      " [0.03244273 0.00562629 0.94826514 0.01366585]]\n",
      "Iteration 1158, Accuracy 0.3735\n",
      "96.28811%change in label assignment\n",
      "0.058941346\n",
      "[[0.01453277 0.91975206 0.00737538 0.05833975]\n",
      " [0.05424136 0.12303972 0.02044003 0.80227894]\n",
      " [0.03844563 0.04167659 0.01271323 0.9071646 ]\n",
      " ...\n",
      " [0.01139687 0.93976784 0.00590876 0.04292655]\n",
      " [0.03236851 0.00561444 0.9483772  0.01363982]\n",
      " [0.0323638  0.00561654 0.9483785  0.01364115]]\n",
      "Iteration 1159, Accuracy 0.37232\n",
      "98.31099%change in label assignment\n",
      "[[0.01105759 0.9420673  0.00572851 0.04114663]\n",
      " [0.05633894 0.13738953 0.02140599 0.78486556]\n",
      " [0.03852931 0.04284068 0.01276745 0.9058625 ]\n",
      " ...\n",
      " [0.01081527 0.94365066 0.0056303  0.03990386]\n",
      " [0.03232108 0.00561228 0.94843864 0.01362805]\n",
      " [0.03232594 0.0056159  0.94842494 0.01363327]]\n",
      "Iteration 1160, Accuracy 0.37276\n",
      "97.41248%change in label assignment\n",
      "0.06973671\n",
      "[[0.0222829  0.8680779  0.01100534 0.09863388]\n",
      " [0.04408308 0.06995741 0.01565821 0.8703013 ]\n",
      " [0.04324183 0.03503898 0.01343012 0.9082891 ]\n",
      " ...\n",
      " [0.0217726  0.871515   0.01079795 0.09591451]\n",
      " [0.03230858 0.00560749 0.9484683  0.0136156 ]\n",
      " [0.0323107  0.00561066 0.94845897 0.01361968]]\n",
      "Iteration 1161, Accuracy 0.37212\n",
      "97.60397%change in label assignment\n",
      "0.068884484\n",
      "[[0.01048942 0.9464026  0.0054793  0.03762863]\n",
      " [0.06154352 0.18410224 0.02397873 0.73037547]\n",
      " [0.04137331 0.05736018 0.01423474 0.88703185]\n",
      " ...\n",
      " [0.01039962 0.9472087  0.0054615  0.03693017]\n",
      " [0.032353   0.00561365 0.94839627 0.01363703]\n",
      " [0.03235869 0.00561747 0.94838125 0.01364267]]\n",
      "Iteration 1162, Accuracy 0.36957\n",
      "98.39446%change in label assignment\n",
      "0.071907386\n",
      "[[0.04010542 0.72878295 0.01903532 0.21207632]\n",
      " [0.03884234 0.04703728 0.01323938 0.900881  ]\n",
      " [0.05261219 0.03571272 0.0158371  0.895838  ]\n",
      " ...\n",
      " [0.03687101 0.7564936  0.01770752 0.18892787]\n",
      " [0.03226796 0.00560173 0.94852847 0.01360184]\n",
      " [0.03225948 0.00560318 0.9485358  0.01360158]]\n",
      "Iteration 1163, Accuracy 0.37188\n",
      "95.32577%change in label assignment\n",
      "0.07869317\n",
      "[[0.01880961 0.9142578  0.01037907 0.05655351]\n",
      " [0.06456356 0.43053085 0.02756842 0.4773372 ]\n",
      " [0.05960235 0.1619928  0.0228215  0.7555834 ]\n",
      " ...\n",
      " [0.01821555 0.9166455  0.01004748 0.0550915 ]\n",
      " [0.03237412 0.00561671 0.9483584  0.01365071]\n",
      " [0.03236773 0.00561846 0.94836247 0.01365125]]\n",
      "Iteration 1164, Accuracy 0.36991\n",
      "94.9428%change in label assignment\n",
      "0.094283015\n",
      "[[0.06075499 0.4887259  0.02737726 0.42314184]\n",
      " [0.04712671 0.03580635 0.01481467 0.9022524 ]\n",
      " [0.11531935 0.0472508  0.03108332 0.80634654]\n",
      " ...\n",
      " [0.05538575 0.56855196 0.02560059 0.3504617 ]\n",
      " [0.03219083 0.00559744 0.9486363  0.01357545]\n",
      " [0.03218545 0.00559952 0.9486384  0.01357661]]\n",
      "Iteration 1165, Accuracy 0.37153\n",
      "93.47474%change in label assignment\n",
      "0.09460648\n",
      "[[0.0375991  0.8417855  0.02180394 0.09881151]\n",
      " [0.03455802 0.77836144 0.01632663 0.17075391]\n",
      " [0.06682073 0.30905065 0.02745718 0.59667134]\n",
      " ...\n",
      " [0.03531175 0.8502221  0.02040039 0.09406576]\n",
      " [0.03244981 0.00562496 0.9482439  0.01368134]\n",
      " [0.0324326  0.00562483 0.9482654  0.01367729]]\n",
      "Iteration 1166, Accuracy 0.36947\n",
      "87.07222%change in label assignment\n",
      "0.101020314\n",
      "[[0.02894004 0.8193788  0.01434208 0.13733907]\n",
      " [0.04393854 0.07012428 0.01611277 0.86982447]\n",
      " [0.06415535 0.03856652 0.01921086 0.87806726]\n",
      " ...\n",
      " [0.03003765 0.8111897  0.01488292 0.14388971]\n",
      " [0.03223259 0.00560434 0.94855976 0.01360329]\n",
      " [0.03222016 0.00560507 0.94857347 0.01360132]]\n",
      "Iteration 1167, Accuracy 0.36407\n",
      "91.75136%change in label assignment\n",
      "0.081381686\n",
      "[[0.01726728 0.92038184 0.00938139 0.05296949]\n",
      " [0.06638651 0.3941475  0.02777304 0.511693  ]\n",
      " [0.04843032 0.0841277  0.01704905 0.85039294]\n",
      " ...\n",
      " [0.01521032 0.9287371  0.0082056  0.04784692]\n",
      " [0.03242491 0.00562191 0.9482808  0.01367235]\n",
      " [0.03241416 0.00562285 0.948292   0.01367097]]\n",
      "Iteration 1168, Accuracy 0.36564\n",
      "96.39122%change in label assignment\n",
      "0.07997325\n",
      "[[0.03544828 0.76773846 0.01713715 0.17967609]\n",
      " [0.03882029 0.04753371 0.01340041 0.90024555]\n",
      " [0.06693234 0.03859221 0.01957373 0.8749017 ]\n",
      " ...\n",
      " [0.03334061 0.7849383  0.01624475 0.1654764 ]\n",
      " [0.03225918 0.00560692 0.9485157  0.01361823]\n",
      " [0.03225143 0.0056085  0.94852173 0.01361829]]\n",
      "Iteration 1169, Accuracy 0.36805\n",
      "94.00992%change in label assignment\n",
      "0.07964937\n",
      "[[0.02044216 0.90765977 0.01130256 0.06059545]\n",
      " [0.06300606 0.46691048 0.02710706 0.44297642]\n",
      " [0.05589807 0.12983015 0.02079282 0.79347897]\n",
      " ...\n",
      " [0.01912867 0.9129262  0.01054864 0.05739649]\n",
      " [0.03243137 0.00562387 0.94826066 0.01368401]\n",
      " [0.03241337 0.00562339 0.948284   0.01367937]]\n",
      "Iteration 1170, Accuracy 0.37109\n",
      "92.00668%change in label assignment\n",
      "0.09055112\n",
      "[[0.04341666 0.6972914  0.02072987 0.23856206]\n",
      " [0.03852234 0.04216686 0.01313379 0.9061771 ]\n",
      " [0.08403269 0.0421885  0.02397876 0.84980005]\n",
      " ...\n",
      " [0.04352535 0.69651794 0.02084602 0.23911068]\n",
      " [0.03225441 0.00560485 0.9485284  0.0136124 ]\n",
      " [0.0322375  0.00560453 0.94854975 0.0136082 ]]\n",
      "Iteration 1171, Accuracy 0.36731\n",
      "95.70383%change in label assignment\n",
      "0.089332655\n",
      "[[0.03056271 0.86815363 0.01735818 0.08392547]\n",
      " [0.04999118 0.6407679  0.02255377 0.28668723]\n",
      " [0.0616421  0.17880237 0.02365843 0.73589706]\n",
      " ...\n",
      " [0.02951746 0.87211704 0.0167467  0.08161879]\n",
      " [0.03247295 0.00562957 0.94819754 0.01369992]\n",
      " [0.03246113 0.0056305  0.9482101  0.0136983 ]]\n",
      "Iteration 1172, Accuracy 0.36382\n",
      "93.56312%change in label assignment\n",
      "0.09521928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03160863 0.7984152  0.01553115 0.15444499]\n",
      " [0.04032632 0.05494814 0.0143372  0.8903883 ]\n",
      " [0.0680349  0.03909919 0.0200953  0.8727706 ]\n",
      " ...\n",
      " [0.03030008 0.80872947 0.01497696 0.14599349]\n",
      " [0.03226252 0.00560915 0.9484988  0.01362957]\n",
      " [0.03225853 0.00561141 0.9484988  0.01363124]]\n",
      "Iteration 1173, Accuracy 0.36083\n",
      "92.24235%change in label assignment\n",
      "0.08059842\n",
      "[[0.01935051 0.9120141  0.01066684 0.05796852]\n",
      " [0.06208911 0.4824729  0.02688592 0.4285521 ]\n",
      " [0.05354279 0.11487941 0.01970485 0.81187296]\n",
      " ...\n",
      " [0.01820181 0.9166448  0.01001101 0.05514237]\n",
      " [0.03246785 0.00562742 0.94819725 0.01370746]\n",
      " [0.03245278 0.0056279  0.94821477 0.01370458]]\n",
      "Iteration 1174, Accuracy 0.3651\n",
      "91.42731%change in label assignment\n",
      "0.087448105\n",
      "[[0.02869799 0.8210435  0.0142352  0.13602337]\n",
      " [0.0471121  0.08560846 0.01766913 0.8496103 ]\n",
      " [0.05196436 0.03620161 0.01612752 0.89570653]\n",
      " ...\n",
      " [0.03170824 0.79804075 0.01565207 0.15459897]\n",
      " [0.03233294 0.00561087 0.9483982  0.01365811]\n",
      " [0.03231464 0.00561011 0.94842213 0.01365309]]\n",
      "Iteration 1175, Accuracy 0.35646\n",
      "94.43217%change in label assignment\n",
      "0.07472935\n",
      "[[0.04643289 0.8101967  0.02710459 0.1162658 ]\n",
      " [0.02531963 0.8512123  0.01214306 0.11132493]\n",
      " [0.06801588 0.28521046 0.02716118 0.61961246]\n",
      " ...\n",
      " [0.04084239 0.83018893 0.02358671 0.10538199]\n",
      " [0.03250298 0.00563294 0.948133   0.01373108]\n",
      " [0.03248276 0.00563204 0.9481598  0.01372544]]\n",
      "Iteration 1176, Accuracy 0.34792\n",
      "91.81028%change in label assignment\n",
      "0.10634789\n",
      "[[0.02112111 0.87564194 0.01066387 0.09257316]\n",
      " [0.05136412 0.10850628 0.01967986 0.82044977]\n",
      " [0.05218516 0.03603128 0.01611093 0.8956726 ]\n",
      " ...\n",
      " [0.02732965 0.831357   0.01359656 0.12771678]\n",
      " [0.03231143 0.00561038 0.9484136  0.01366453]\n",
      " [0.03229451 0.00560981 0.94843566 0.01366004]]\n",
      "Iteration 1177, Accuracy 0.34841\n",
      "92.39947%change in label assignment\n",
      "0.07945484\n",
      "[[0.02388397 0.89408135 0.01336218 0.06867248]\n",
      " [0.05398098 0.5957785  0.02417611 0.3260644 ]\n",
      " [0.05447118 0.12067588 0.02019544 0.80465746]\n",
      " ...\n",
      " [0.01900291 0.91349286 0.0104858  0.05701846]\n",
      " [0.0324608  0.00562712 0.9481824  0.01372968]\n",
      " [0.03245328 0.00562894 0.9481878  0.01373004]]\n",
      "Iteration 1178, Accuracy 0.3462\n",
      "94.33397%change in label assignment\n",
      "0.08252241\n",
      "[[0.01240801 0.9330521  0.00645866 0.04808119]\n",
      " [0.05988892 0.17511384 0.02395265 0.7410446 ]\n",
      " [0.04105745 0.03565419 0.0132712  0.9100172 ]\n",
      " ...\n",
      " [0.01766613 0.89896697 0.008999   0.07436792]\n",
      " [0.03232829 0.00561247 0.9483795  0.01367974]\n",
      " [0.03232443 0.0056149  0.94837904 0.01368165]]\n",
      "Iteration 1179, Accuracy 0.35612\n",
      "96.96077%change in label assignment\n",
      "0.071430236\n",
      "[[0.01463816 0.93123    0.00793132 0.04620053]\n",
      " [0.0638793  0.44799548 0.02746221 0.46066302]\n",
      " [0.04575551 0.07477246 0.01612949 0.8633426 ]\n",
      " ...\n",
      " [0.01216384 0.9412594  0.00650739 0.04006939]\n",
      " [0.0324269  0.00562094 0.9482315  0.01372068]\n",
      " [0.03241509 0.00562161 0.9482446  0.01371875]]\n",
      "Iteration 1180, Accuracy 0.35675\n",
      "95.44361%change in label assignment\n",
      "0.07472929\n",
      "[[0.01844284 0.89383185 0.00935717 0.07836809]\n",
      " [0.05181243 0.11098299 0.01980091 0.8174036 ]\n",
      " [0.04793806 0.03508186 0.01491527 0.9020648 ]\n",
      " ...\n",
      " [0.02588047 0.8420273  0.01287669 0.11921562]\n",
      " [0.03229912 0.00560803 0.94841295 0.01367987]\n",
      " [0.03229641 0.00561074 0.94841045 0.01368235]]\n",
      "Iteration 1181, Accuracy 0.34811\n",
      "93.54839%change in label assignment\n",
      "0.07414108\n",
      "[[0.03340683 0.85753417 0.01919824 0.08986077]\n",
      " [0.03943865 0.7387414  0.01842599 0.20339394]\n",
      " [0.06158973 0.18054098 0.02387065 0.73399866]\n",
      " ...\n",
      " [0.02522523 0.8888386  0.01422085 0.0717153 ]\n",
      " [0.03244894 0.0056237  0.9481872  0.01374014]\n",
      " [0.03243508 0.00562392 0.94820374 0.01373722]]\n",
      "Iteration 1182, Accuracy 0.35361\n",
      "90.19983%change in label assignment\n",
      "0.088680044\n",
      "[[0.03261183 0.7908134  0.01597857 0.16059628]\n",
      " [0.04298119 0.06692801 0.01565925 0.87443155]\n",
      " [0.07299596 0.03967863 0.02126686 0.8660585 ]\n",
      " ...\n",
      " [0.04058181 0.72394186 0.01955868 0.21591765]\n",
      " [0.03223848 0.00560192 0.9484958  0.0136637 ]\n",
      " [0.03223389 0.00560395 0.9484972  0.01366498]]\n",
      "Iteration 1183, Accuracy 0.34949\n",
      "88.79069%change in label assignment\n",
      "0.08697412\n",
      "[[0.03529134 0.85041803 0.02042282 0.09386777]\n",
      " [0.0456589  0.68175554 0.02104941 0.25153622]\n",
      " [0.0550429  0.12602344 0.0206468  0.79828686]\n",
      " ...\n",
      " [0.02622795 0.8848843  0.01486577 0.07402199]\n",
      " [0.032431   0.00563514 0.94817513 0.01375868]\n",
      " [0.03246554 0.00564453 0.94811267 0.01377732]]\n",
      "Iteration 1184, Accuracy 0.35656\n",
      "91.25055%change in label assignment\n",
      "0.08470622\n",
      "[[0.01495165 0.91665006 0.00766916 0.0607292 ]\n",
      " [0.04576397 0.07963925 0.0168524  0.8577444 ]\n",
      " [0.0642286  0.03761604 0.01887071 0.8792847 ]\n",
      " ...\n",
      " [0.02499592 0.8482551  0.01243832 0.11431064]\n",
      " [0.03229083 0.00561923 0.94838786 0.0137021 ]\n",
      " [0.03232761 0.00562889 0.94832194 0.01372154]]\n",
      "Iteration 1185, Accuracy 0.37325\n",
      "90.90686%change in label assignment\n",
      "0.072392195\n",
      "[[0.03040999 0.8687912  0.01739905 0.08339976]\n",
      " [0.05907488 0.52858126 0.0260985  0.3862453 ]\n",
      " [0.04790622 0.08542578 0.0172332  0.8494348 ]\n",
      " ...\n",
      " [0.0203157  0.9082161  0.01132069 0.06014749]\n",
      " [0.03241153 0.00563039 0.9482104  0.01374768]\n",
      " [0.03244401 0.00563922 0.9481515  0.0137652 ]]\n",
      "Iteration 1186, Accuracy 0.36903\n",
      "94.48618%change in label assignment\n",
      "0.07560912\n",
      "[[0.01105687 0.9417081  0.00581117 0.04142382]\n",
      " [0.05374713 0.12338386 0.02071797 0.802151  ]\n",
      " [0.04604344 0.03464031 0.014385   0.90493125]\n",
      " ...\n",
      " [0.0167529  0.9049435  0.00855816 0.06974545]\n",
      " [0.03226104 0.00561208 0.94842947 0.0136974 ]\n",
      " [0.03229122 0.00562054 0.9483742  0.013714  ]]\n",
      "Iteration 1187, Accuracy 0.37713\n",
      "96.39613%change in label assignment\n",
      "0.066519305\n",
      "[[0.02028789 0.9083268  0.01134642 0.06003893]\n",
      " [0.06419837 0.43202144 0.0277832  0.47599697]\n",
      " [0.04405384 0.06960438 0.01564759 0.87069416]\n",
      " ...\n",
      " [0.01394787 0.9341178  0.00762441 0.04430991]\n",
      " [0.03229458 0.00561075 0.94838256 0.0137121 ]\n",
      " [0.03230997 0.00561647 0.9483513  0.01372219]]\n",
      "Iteration 1188, Accuracy 0.3707\n",
      "94.54019%change in label assignment\n",
      "0.06989955\n",
      "[[0.01222527 0.9341551  0.00636524 0.04725437]\n",
      " [0.05340954 0.12092303 0.02055251 0.8051149 ]\n",
      " [0.04429749 0.03454083 0.01396456 0.9071971 ]\n",
      " ...\n",
      " [0.0187114  0.8919971  0.00949469 0.07979678]\n",
      " [0.03217243 0.00559712 0.9485566  0.0136738 ]\n",
      " [0.03218686 0.00560268 0.948527   0.0136835 ]]\n",
      "Iteration 1189, Accuracy 0.37202\n",
      "96.15555%change in label assignment\n",
      "0.07039668\n",
      "[[0.01898192 0.91363    0.01055704 0.05683099]\n",
      " [0.06468993 0.42321926 0.02786705 0.48422372]\n",
      " [0.04517718 0.07398127 0.01612289 0.86471874]\n",
      " ...\n",
      " [0.01408831 0.93357253 0.00769624 0.04464289]\n",
      " [0.03223659 0.00560584 0.9484522  0.01370543]\n",
      " [0.03225289 0.00561176 0.9484194  0.01371598]]\n",
      "Iteration 1190, Accuracy 0.36785\n",
      "97.74635%change in label assignment\n",
      "0.06396373\n",
      "[[0.01027885 0.9482582  0.0055013  0.03596158]\n",
      " [0.0636571  0.22455287 0.02598121 0.68580884]\n",
      " [0.03772218 0.03983532 0.01264605 0.9097964 ]\n",
      " ...\n",
      " [0.01086769 0.9429458  0.00571751 0.04046903]\n",
      " [0.03210661 0.00559069 0.9486403  0.01366242]\n",
      " [0.032118   0.00559577 0.9486154  0.01367089]]\n",
      "Iteration 1191, Accuracy 0.37266\n",
      "98.71852%change in label assignment\n",
      "0.06188515\n",
      "[[0.01081164 0.94671607 0.00580932 0.03666302]\n",
      " [0.06626047 0.30430993 0.0277223  0.6017073 ]\n",
      " [0.03843819 0.04637788 0.01311497 0.902069  ]\n",
      " ...\n",
      " [0.01022013 0.94840056 0.00543804 0.03594126]\n",
      " [0.03209616 0.00559348 0.94864315 0.01366725]\n",
      " [0.0321198  0.00560084 0.9485982  0.01368114]]\n",
      "Iteration 1192, Accuracy 0.3704\n",
      "97.75126%change in label assignment\n",
      "0.058746476\n",
      "[[0.01215581 0.9346316  0.00630024 0.04691231]\n",
      " [0.06325855 0.21565686 0.02562924 0.6954554 ]\n",
      " [0.03778578 0.03899019 0.01256765 0.91065633]\n",
      " ...\n",
      " [0.01204052 0.93533707 0.00625662 0.04636582]\n",
      " [0.03201693 0.00557942 0.9487664  0.01363722]\n",
      " [0.0320287  0.00558484 0.9487403  0.01364617]]\n",
      "Iteration 1193, Accuracy 0.3734\n",
      "96.57289%change in label assignment\n",
      "0.061893463\n",
      "[[0.01033887 0.9482735  0.00551433 0.03587339]\n",
      " [0.06554189 0.3867552  0.02820095 0.519502  ]\n",
      " [0.04157454 0.06030237 0.01470022 0.8834229 ]\n",
      " ...\n",
      " [0.01090463 0.94640386 0.00587672 0.03681479]\n",
      " [0.03200255 0.00558059 0.94877225 0.01364465]\n",
      " [0.03201897 0.00558693 0.94873834 0.01365574]]\n",
      "Iteration 1194, Accuracy 0.36775\n",
      "98.26189%change in label assignment\n",
      "0.06018406\n",
      "[[0.01608335 0.909519   0.00817602 0.06622162]\n",
      " [0.06195392 0.19542699 0.02482217 0.7177969 ]\n",
      " [0.03773491 0.03955598 0.01258566 0.9101234 ]\n",
      " ...\n",
      " [0.01285252 0.93019164 0.0066434  0.0503125 ]\n",
      " [0.03190729 0.0055672  0.94891566 0.01360988]\n",
      " [0.0319116  0.00557137 0.94890136 0.01361572]]\n",
      "Iteration 1195, Accuracy 0.37021\n",
      "96.92149%change in label assignment\n",
      "0.06249499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02048881 0.88055354 0.01020977 0.08874793]\n",
      " [0.0628827  0.20415334 0.02509423 0.70786977]\n",
      " [0.0383475  0.04490573 0.0129591  0.9037877 ]\n",
      " ...\n",
      " [0.01154808 0.9386337  0.00599428 0.04382397]\n",
      " [0.03189687 0.00556324 0.9489368  0.0136031 ]\n",
      " [0.03188511 0.00556376 0.94895005 0.01360105]]\n",
      "Iteration 1196, Accuracy 0.36152\n",
      "96.30284%change in label assignment\n",
      "0.06110444\n",
      "[[0.01488025 0.91715014 0.00762425 0.06034542]\n",
      " [0.06488986 0.25128967 0.02678297 0.6570375 ]\n",
      " [0.03883037 0.04933751 0.01350974 0.8983224 ]\n",
      " ...\n",
      " [0.01034753 0.9465548  0.0054878  0.0376099 ]\n",
      " [0.03183026 0.00555862 0.94902515 0.01358591]\n",
      " [0.03181975 0.00555937 0.9490365  0.01358442]]\n",
      "Iteration 1197, Accuracy 0.35886\n",
      "97.60888%change in label assignment\n",
      "0.06348418\n",
      "[[0.02426553 0.854003   0.01198735 0.10974411]\n",
      " [0.05851048 0.15735202 0.02281787 0.7613196 ]\n",
      " [0.03809142 0.03791452 0.01249921 0.91149485]\n",
      " ...\n",
      " [0.01392977 0.9233089  0.00714687 0.05561442]\n",
      " [0.03178656 0.00555398 0.9490899  0.01356961]\n",
      " [0.03177295 0.0055542  0.94910616 0.01356679]]\n",
      "Iteration 1198, Accuracy 0.35479\n",
      "96.67109%change in label assignment\n",
      "0.06124614\n",
      "[[0.01480733 0.9176692  0.00756383 0.05995965]\n",
      " [0.06446233 0.23796748 0.02630697 0.6712633 ]\n",
      " [0.03823559 0.04580033 0.01306664 0.9028974 ]\n",
      " ...\n",
      " [0.01038482 0.94620967 0.00548253 0.03792299]\n",
      " [0.03174016 0.00555416 0.9491434  0.01356219]\n",
      " [0.03173851 0.00555662 0.9491401  0.0135647 ]]\n",
      "Iteration 1199, Accuracy 0.35916\n",
      "96.53852%change in label assignment\n",
      "[[0.02197296 0.86989945 0.01094075 0.09718689]\n",
      " [0.06334692 0.21642716 0.02557151 0.69465446]\n",
      " [0.03822792 0.04576628 0.01305027 0.90295553]\n",
      " ...\n",
      " [0.01056663 0.9448919  0.00555947 0.03898194]\n",
      " [0.03172374 0.0055493  0.9491741  0.01355296]\n",
      " [0.0317073  0.00554923 0.9491944  0.01354918]]\n",
      "Iteration 1200, Accuracy 0.3649\n",
      "95.57618%change in label assignment\n",
      "0.072179\n",
      "[[0.06203077 0.46374574 0.02765964 0.44656387]\n",
      " [0.03749394 0.04151872 0.0127142  0.9082731 ]\n",
      " [0.07187077 0.03899262 0.02077815 0.8683585 ]\n",
      " ...\n",
      " [0.03619383 0.76109165 0.01755194 0.18516257]\n",
      " [0.03166346 0.00554254 0.9492698  0.01352411]\n",
      " [0.03165734 0.00554442 0.9492733  0.01352494]]\n",
      "Iteration 1201, Accuracy 0.3567\n",
      "93.83807%change in label assignment\n",
      "0.07884675\n",
      "[[0.01226379 0.94090354 0.0066318  0.04020084]\n",
      " [0.05926435 0.52307296 0.02631943 0.39134327]\n",
      " [0.05652601 0.13885663 0.02161859 0.78299874]\n",
      " ...\n",
      " [0.02302177 0.89732856 0.01301444 0.06663519]\n",
      " [0.03181185 0.00556028 0.9490424  0.01358558]\n",
      " [0.03180638 0.00556223 0.94904476 0.01358664]]\n",
      "Iteration 1202, Accuracy 0.36353\n",
      "93.91172%change in label assignment\n",
      "0.09421462\n",
      "[[0.06447966 0.39670518 0.02873917 0.5100759 ]\n",
      " [0.04074581 0.03586198 0.01350232 0.90988994]\n",
      " [0.11248212 0.04677824 0.03120806 0.80953157]\n",
      " ...\n",
      " [0.04173937 0.7123211  0.020297   0.22564252]\n",
      " [0.03162182 0.00554444 0.94931406 0.01351964]\n",
      " [0.03163215 0.00554982 0.94928974 0.01352827]]\n",
      "Iteration 1203, Accuracy 0.36152\n",
      "91.00997%change in label assignment\n",
      "0.09074402\n",
      "[[0.01608396 0.92524606 0.00882678 0.04984312]\n",
      " [0.05258799 0.61094844 0.02382522 0.3126384 ]\n",
      " [0.05910732 0.15777718 0.02278365 0.7603318 ]\n",
      " ...\n",
      " [0.03406896 0.85484177 0.01972378 0.09136548]\n",
      " [0.03183122 0.00556814 0.9489936  0.01360707]\n",
      " [0.03184281 0.00557395 0.9489668  0.01361649]]\n",
      "Iteration 1204, Accuracy 0.36186\n",
      "92.84136%change in label assignment\n",
      "0.10007903\n",
      "[[0.05751743 0.5352744  0.02644699 0.38076124]\n",
      " [0.0392621  0.05274364 0.01401628 0.893978  ]\n",
      " [0.0859533  0.04206429 0.02457592 0.84740645]\n",
      " ...\n",
      " [0.03237801 0.79158777 0.01600735 0.1600268 ]\n",
      " [0.03166027 0.00554473 0.94926834 0.01352669]\n",
      " [0.03165152 0.0055462  0.94927585 0.01352645]]\n",
      "Iteration 1205, Accuracy 0.36471\n",
      "92.64005%change in label assignment\n",
      "0.080977164\n",
      "[[0.01057523 0.9453192  0.00554619 0.03855942]\n",
      " [0.06535406 0.39252952 0.0280837  0.5140328 ]\n",
      " [0.04139893 0.05957655 0.01455403 0.88447046]\n",
      " ...\n",
      " [0.01915173 0.9127395  0.01072314 0.05738565]\n",
      " [0.03180479 0.00556695 0.94902647 0.0136017 ]\n",
      " [0.03182853 0.00557535 0.94897926 0.0136169 ]]\n",
      "Iteration 1206, Accuracy 0.35445\n",
      "92.55168%change in label assignment\n",
      "0.07983337\n",
      "[[0.05262551 0.5995374  0.02452049 0.32331663]\n",
      " [0.04512151 0.07839192 0.01678523 0.8597013 ]\n",
      " [0.06184829 0.0371652  0.01854503 0.8824415 ]\n",
      " ...\n",
      " [0.01991789 0.8833111  0.01013312 0.0866379 ]\n",
      " [0.03168848 0.00555135 0.94920695 0.01355327]\n",
      " [0.03169747 0.00555616 0.94918543 0.01356094]]\n",
      "Iteration 1207, Accuracy 0.36181\n",
      "95.65474%change in label assignment\n",
      "0.072045855\n",
      "[[0.02288457 0.8638064  0.01133541 0.10197359]\n",
      " [0.06417659 0.23022921 0.02601314 0.6795811 ]\n",
      " [0.03781911 0.04252714 0.0127133  0.9069405 ]\n",
      " ...\n",
      " [0.01147583 0.94407934 0.0062064  0.03823847]\n",
      " [0.03179505 0.00556724 0.9490304  0.01360734]\n",
      " [0.0318302  0.00557786 0.9489643  0.01362772]]\n",
      "Iteration 1208, Accuracy 0.35577\n",
      "96.25865%change in label assignment\n",
      "0.06934398\n",
      "[[0.06341764 0.43078965 0.02811849 0.47767425]\n",
      " [0.03929932 0.05295696 0.0139045  0.8938392 ]\n",
      " [0.05829693 0.03627851 0.0175477  0.88787687]\n",
      " ...\n",
      " [0.02181325 0.87034595 0.01098902 0.09685179]\n",
      " [0.03169832 0.00555603 0.94917196 0.01357372]\n",
      " [0.03172704 0.00556508 0.9491171  0.01359082]]\n",
      "Iteration 1209, Accuracy 0.35916\n",
      "96.10154%change in label assignment\n",
      "0.068863854\n",
      "[[0.0362749  0.76169014 0.01740108 0.18463385]\n",
      " [0.058415   0.15774453 0.02285277 0.76098776]\n",
      " [0.03762959 0.04265204 0.01273786 0.90698045]\n",
      " ...\n",
      " [0.01065395 0.9472569  0.0057334  0.03635569]\n",
      " [0.03177157 0.0055695  0.9490439  0.01361494]\n",
      " [0.03182087 0.0055827  0.9489549  0.01364155]]\n",
      "Iteration 1210, Accuracy 0.36014\n",
      "96.59744%change in label assignment\n",
      "0.069147564\n",
      "[[0.06053274 0.48926577 0.02735117 0.4228504 ]\n",
      " [0.04475538 0.07655598 0.01657324 0.86211544]\n",
      " [0.04713862 0.03448432 0.01480476 0.9035723 ]\n",
      " ...\n",
      " [0.01791246 0.89687675 0.00915234 0.07605843]\n",
      " [0.03167531 0.00555444 0.94919586 0.01357438]\n",
      " [0.03171074 0.00556506 0.94912934 0.01359483]]\n",
      "Iteration 1211, Accuracy 0.35823\n",
      "96.08681%change in label assignment\n",
      "0.067898795\n",
      "[[0.0557689  0.5654871  0.02534255 0.35340145]\n",
      " [0.04502267 0.07562766 0.01633465 0.86301494]\n",
      " [0.04043467 0.03469204 0.01291269 0.9119606 ]\n",
      " ...\n",
      " [0.01225524 0.9339288  0.00634467 0.04747126]\n",
      " [0.03174421 0.00556983 0.949073   0.0136129 ]\n",
      " [0.0318045  0.00558508 0.948966   0.01364441]]\n",
      "Iteration 1212, Accuracy 0.35086\n",
      "95.05082%change in label assignment\n",
      "0.06266109\n",
      "[[0.06118606 0.47993678 0.0274158  0.43146133]\n",
      " [0.04074445 0.05878637 0.01454046 0.88592875]\n",
      " [0.041917   0.03424404 0.01339918 0.9104398 ]\n",
      " ...\n",
      " [0.0161367  0.9086851  0.00826286 0.06691542]\n",
      " [0.03166879 0.0055562  0.94919205 0.013583  ]\n",
      " [0.03171021 0.00556783 0.94911593 0.01360602]]\n",
      "Iteration 1213, Accuracy 0.3571\n",
      "97.70217%change in label assignment\n",
      "0.063759565\n",
      "[[0.06025545 0.49699348 0.02706302 0.41568804]\n",
      " [0.04075399 0.05863906 0.01449794 0.886109  ]\n",
      " [0.0408132  0.03441827 0.01310627 0.9116623 ]\n",
      " ...\n",
      " [0.01533102 0.91396374 0.00785972 0.06284552]\n",
      " [0.03166368 0.00555889 0.94918895 0.01358846]\n",
      " [0.03171407 0.00557231 0.949098   0.01361561]]\n",
      "Iteration 1214, Accuracy 0.35921\n",
      "98.76761%change in label assignment\n",
      "0.0632055\n",
      "[[0.0491602  0.6401636  0.02295583 0.28772035]\n",
      " [0.04691191 0.08583815 0.01742592 0.849824  ]\n",
      " [0.03723662 0.03884307 0.01252462 0.9113957 ]\n",
      " ...\n",
      " [0.0113255  0.93962014 0.00593612 0.04311819]\n",
      " [0.03163675 0.00555358 0.9492309  0.01357879]\n",
      " [0.03167817 0.00556538 0.9491544  0.01360203]]\n",
      "Iteration 1215, Accuracy 0.35995\n",
      "96.76928%change in label assignment\n",
      "0.059730474\n",
      "[[0.06572417 0.31182438 0.02802198 0.59442943]\n",
      " [0.03723724 0.03882218 0.01251642 0.9114242 ]\n",
      " [0.05570079 0.03565206 0.0168961  0.89175105]\n",
      " ...\n",
      " [0.03216256 0.7937649  0.01575418 0.15831839]\n",
      " [0.03158946 0.00554867 0.94929403 0.0135679 ]\n",
      " [0.03163849 0.00556196 0.9492049  0.01359461]]\n",
      "Iteration 1216, Accuracy 0.35631\n",
      "96.98041%change in label assignment\n",
      "0.06883597\n",
      "[[0.01660703 0.9065186  0.0083804  0.06849397]\n",
      " [0.0655369  0.25377187 0.0267173  0.65397394]\n",
      " [0.04997334 0.09852812 0.0185625  0.83293605]\n",
      " ...\n",
      " [0.02005399 0.9091877  0.01125682 0.0595015 ]\n",
      " [0.03164519 0.00555697 0.9492031  0.01359469]\n",
      " [0.03168518 0.00556841 0.94912916 0.0136172 ]]\n",
      "Iteration 1217, Accuracy 0.35616\n",
      "96.39122%change in label assignment\n",
      "0.08097398\n",
      "[[0.06525394 0.36169747 0.028489   0.5445596 ]\n",
      " [0.03732569 0.04340109 0.01293465 0.9063386 ]\n",
      " [0.05188874 0.03518263 0.01609599 0.89683264]\n",
      " ...\n",
      " [0.02335157 0.8593412  0.01178317 0.10552414]\n",
      " [0.03147509 0.00553377 0.94946253 0.0135286 ]\n",
      " [0.03149241 0.00554032 0.949427   0.01354023]]\n",
      "Iteration 1218, Accuracy 0.36083\n",
      "95.586%change in label assignment\n",
      "0.0673152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03332691 0.78588337 0.0160843  0.16470537]\n",
      " [0.05818267 0.15492207 0.02267757 0.7642177 ]\n",
      " [0.03936562 0.05193802 0.01370331 0.8949931 ]\n",
      " ...\n",
      " [0.01084997 0.9465244  0.00585162 0.03677396]\n",
      " [0.03154909 0.00554109 0.94935304 0.01355678]\n",
      " [0.03156555 0.00554744 0.949319   0.013568  ]]\n",
      "Iteration 1219, Accuracy 0.35891\n",
      "95.83149%change in label assignment\n",
      "0.06789308\n",
      "[[0.06551882 0.32462734 0.02827771 0.5815761 ]\n",
      " [0.03707755 0.03945348 0.01263697 0.910832  ]\n",
      " [0.05881467 0.03646758 0.01788245 0.8868353 ]\n",
      " ...\n",
      " [0.02780337 0.8267868  0.0138668  0.13154301]\n",
      " [0.03144743 0.00553427 0.94949096 0.01352736]\n",
      " [0.03147507 0.00554286 0.94943833 0.01354374]]\n",
      "Iteration 1220, Accuracy 0.35528\n",
      "94.59911%change in label assignment\n",
      "0.068640344\n",
      "[[0.02767634 0.8294968  0.01353241 0.12929441]\n",
      " [0.06279942 0.20502028 0.02513506 0.7070453 ]\n",
      " [0.04344628 0.06876964 0.01560875 0.87217534]\n",
      " ...\n",
      " [0.01394816 0.93402976 0.00766846 0.04435357]\n",
      " [0.03151096 0.00553954 0.9493956  0.01355384]\n",
      " [0.03153003 0.00554645 0.9493572  0.01356632]]\n",
      "Iteration 1221, Accuracy 0.35631\n",
      "94.27014%change in label assignment\n",
      "0.07457177\n",
      "[[0.06272471 0.21977271 0.02605079 0.6914518 ]\n",
      " [0.03732153 0.03781416 0.01263513 0.91222924]\n",
      " [0.06457397 0.03769429 0.01942081 0.8783109 ]\n",
      " ...\n",
      " [0.03200597 0.7943224  0.01583567 0.15783593]\n",
      " [0.03137838 0.00552476 0.94958997 0.01350689]\n",
      " [0.03139786 0.00553196 0.9495504  0.0135198 ]]\n",
      "Iteration 1222, Accuracy 0.35543\n",
      "95.62037%change in label assignment\n",
      "0.074348524\n",
      "[[0.01954818 0.88696    0.00977588 0.08371592]\n",
      " [0.06654711 0.3240263  0.02793721 0.5814893 ]\n",
      " [0.05071962 0.10322067 0.01897517 0.82708454]\n",
      " ...\n",
      " [0.02227264 0.90024406 0.01259938 0.06488387]\n",
      " [0.03149141 0.00553816 0.949416   0.01355447]\n",
      " [0.03150897 0.00554493 0.9493797  0.01356646]]\n",
      "Iteration 1223, Accuracy 0.35553\n",
      "94.71694%change in label assignment\n",
      "0.08078334\n",
      "[[0.0647902  0.27272204 0.02750112 0.63498664]\n",
      " [0.03715248 0.0431795  0.01291116 0.9067569 ]\n",
      " [0.05842365 0.0364315  0.01782315 0.88732165]\n",
      " ...\n",
      " [0.02351641 0.8578978  0.01189066 0.10669506]\n",
      " [0.03133488 0.00551754 0.94965905 0.01348857]\n",
      " [0.03134171 0.00552254 0.9496397  0.01349605]]\n",
      "Iteration 1224, Accuracy 0.35504\n",
      "94.93298%change in label assignment\n",
      "0.07053282\n",
      "[[0.03454798 0.77605873 0.0166261  0.17276716]\n",
      " [0.06204638 0.19659281 0.02479189 0.7165689 ]\n",
      " [0.04046874 0.05681005 0.01426072 0.88846046]\n",
      " ...\n",
      " [0.01370838 0.93498486 0.0075396  0.04376715]\n",
      " [0.03142597 0.00553136 0.94950753 0.01353508]\n",
      " [0.03144391 0.00553833 0.94947034 0.0135474 ]]\n",
      "Iteration 1225, Accuracy 0.35155\n",
      "95.63019%change in label assignment\n",
      "0.07263356\n",
      "[[0.06188224 0.20706613 0.02557182 0.7054798 ]\n",
      " [0.03843284 0.03560819 0.01278859 0.9131704 ]\n",
      " [0.07469012 0.03972297 0.0219823  0.86360455]\n",
      " ...\n",
      " [0.03383006 0.77952325 0.01667662 0.1699701 ]\n",
      " [0.03132143 0.00552331 0.9496516  0.01350372]\n",
      " [0.0313502  0.00553233 0.9495966  0.01352089]]\n",
      "Iteration 1226, Accuracy 0.35371\n",
      "92.65478%change in label assignment\n",
      "0.0736879\n",
      "[[0.01733815 0.90156645 0.00874638 0.07234909]\n",
      " [0.06647558 0.31466454 0.02785636 0.5910035 ]\n",
      " [0.05028154 0.10108468 0.01879869 0.8298351 ]\n",
      " ...\n",
      " [0.02421903 0.8925134  0.01379079 0.06947675]\n",
      " [0.03142058 0.00553109 0.9495062  0.0135421 ]\n",
      " [0.03143645 0.00553771 0.94947225 0.01355356]]\n",
      "Iteration 1227, Accuracy 0.35734\n",
      "90.9265%change in label assignment\n",
      "0.0807982\n",
      "[[0.06499685 0.28603414 0.02784133 0.62112767]\n",
      " [0.03695611 0.04159848 0.01283225 0.90861315]\n",
      " [0.06253051 0.03736275 0.0190098  0.88109696]\n",
      " ...\n",
      " [0.0223889  0.86583096 0.01138889 0.10039119]\n",
      " [0.03125186 0.00551304 0.9497544  0.01348069]\n",
      " [0.03126544 0.00551946 0.9497237  0.01349139]]\n",
      "Iteration 1228, Accuracy 0.35543\n",
      "95.92969%change in label assignment\n",
      "0.070093974\n",
      "[[0.0237233  0.8579561  0.01173907 0.10658158]\n",
      " [0.06415211 0.22911225 0.02602599 0.6807096 ]\n",
      " [0.04206977 0.06328379 0.01500489 0.87964153]\n",
      " ...\n",
      " [0.01764359 0.9188358  0.00985371 0.05366693]\n",
      " [0.03135657 0.00552631 0.94958913 0.01352794]\n",
      " [0.03137625 0.00553373 0.9495488  0.0135412 ]]\n",
      "Iteration 1229, Accuracy 0.35273\n",
      "95.65965%change in label assignment\n",
      "0.0739583\n",
      "[[0.06537914 0.33653215 0.0283546  0.56973416]\n",
      " [0.03688878 0.04144529 0.0127223  0.9089436 ]\n",
      " [0.05833907 0.03630029 0.01777675 0.8875839 ]\n",
      " ...\n",
      " [0.02082106 0.876785   0.01059236 0.09180162]\n",
      " [0.03123494 0.00551165 0.9497715  0.01348183]\n",
      " [0.03124997 0.00551831 0.94973856 0.01349314]]\n",
      "Iteration 1230, Accuracy 0.35867\n",
      "94.88388%change in label assignment\n",
      "0.06531456\n",
      "[[0.03063205 0.8063115  0.01494395 0.14811255]\n",
      " [0.06284945 0.21177076 0.02544178 0.69993806]\n",
      " [0.04040859 0.05753515 0.01435742 0.8876988 ]\n",
      " ...\n",
      " [0.01558848 0.9272058  0.00867831 0.04852741]\n",
      " [0.03128882 0.00551412 0.9496952  0.01350184]\n",
      " [0.03128998 0.00551845 0.94968426 0.01350728]]\n",
      "Iteration 1231, Accuracy 0.35739\n",
      "92.39947%change in label assignment\n",
      "0.067729294\n",
      "[[0.05246841 0.6000049  0.02450502 0.32302162]\n",
      " [0.04608422 0.08406284 0.01733937 0.85251355]\n",
      " [0.04075601 0.03430687 0.01330192 0.9116352 ]\n",
      " ...\n",
      " [0.01051198 0.9448235  0.00560637 0.03905812]\n",
      " [0.03117306 0.00550375 0.9498586  0.01346467]\n",
      " [0.03117486 0.00550823 0.9498465  0.01347044]]\n",
      "Iteration 1232, Accuracy 0.35096\n",
      "98.50248%change in label assignment\n",
      "0.060095277\n",
      "[[0.05789954 0.5338165  0.02621598 0.382068  ]\n",
      " [0.04560728 0.07949049 0.01674928 0.858153  ]\n",
      " [0.03991624 0.03455298 0.01285914 0.9126717 ]\n",
      " ...\n",
      " [0.01052606 0.94490814 0.00555243 0.03901342]\n",
      " [0.03116754 0.00550276 0.94986486 0.01346486]\n",
      " [0.03117386 0.00550822 0.9498451  0.01347281]]\n",
      "Iteration 1233, Accuracy 0.35022\n",
      "97.89856%change in label assignment\n",
      "0.06191316\n",
      "[[0.04138607 0.71521145 0.01980552 0.22359703]\n",
      " [0.05478284 0.13317242 0.02139068 0.79065406]\n",
      " [0.03732883 0.04500139 0.012999   0.90467083]\n",
      " ...\n",
      " [0.01095431 0.9461057  0.0059839  0.03695609]\n",
      " [0.03115861 0.00549921 0.9498774  0.01346477]\n",
      " [0.03114497 0.00550108 0.94989    0.01346397]]\n",
      "Iteration 1234, Accuracy 0.35484\n",
      "93.98046%change in label assignment\n",
      "0.062238608\n",
      "[[0.06237118 0.45047197 0.02790854 0.45924833]\n",
      " [0.04274085 0.06875766 0.01568366 0.8728178 ]\n",
      " [0.0385501  0.03505065 0.0127398  0.91365945]\n",
      " ...\n",
      " [0.01141078 0.93880427 0.00601522 0.04376977]\n",
      " [0.03110114 0.00549247 0.9499604  0.01344595]\n",
      " [0.03107367 0.00549218 0.9499947  0.01343948]]\n",
      "Iteration 1235, Accuracy 0.3462\n",
      "96.00334%change in label assignment\n",
      "0.06331373\n",
      "[[0.05050876 0.6238978  0.02356459 0.30202883]\n",
      " [0.04789924 0.09202168 0.01801749 0.84206164]\n",
      " [0.03708516 0.03731923 0.01244994 0.91314566]\n",
      " ...\n",
      " [0.01000063 0.9488532  0.00535993 0.03578634]\n",
      " [0.0310574  0.00549204 0.95001256 0.01343797]\n",
      " [0.03105133 0.00549533 0.9500128  0.01344057]]\n",
      "Iteration 1236, Accuracy 0.33544\n",
      "96.94113%change in label assignment\n",
      "0.062289707\n",
      "[[0.05610642 0.5562364  0.0256688  0.36198843]\n",
      " [0.05013811 0.10349413 0.01902473 0.82734305]\n",
      " [0.03700517 0.04261308 0.01272229 0.9076594 ]\n",
      " ...\n",
      " [0.01007073 0.949096   0.00542189 0.03541134]\n",
      " [0.03109857 0.00549244 0.94996285 0.01344613]\n",
      " [0.03105182 0.00548899 0.95002764 0.01343158]]\n",
      "Iteration 1237, Accuracy 0.34315\n",
      "92.40929%change in label assignment\n",
      "0.06577885\n",
      "[[0.06521603 0.3202832  0.02814099 0.58635974]\n",
      " [0.03778229 0.04831801 0.01335674 0.900543  ]\n",
      " [0.04697781 0.03431754 0.01486073 0.90384394]\n",
      " ...\n",
      " [0.01587745 0.9098012  0.00821098 0.06611038]\n",
      " [0.03100089 0.00548452 0.95010185 0.01341286]\n",
      " [0.03096186 0.00548235 0.9501543  0.01340157]]\n",
      "Iteration 1238, Accuracy 0.32361\n",
      "97.44194%change in label assignment\n",
      "0.066028655\n",
      "[[0.04219203 0.7083133  0.02002179 0.22947294]\n",
      " [0.06265497 0.21299613 0.02544094 0.6989079 ]\n",
      " [0.04229815 0.06629883 0.0153212  0.8760818 ]\n",
      " ...\n",
      " [0.01729229 0.9200781  0.00970392 0.05292572]\n",
      " [0.0310932  0.00549509 0.9499654  0.01344626]\n",
      " [0.03103882 0.00549057 0.950042   0.01342874]]\n",
      "Iteration 1239, Accuracy 0.3295\n",
      "91.85938%change in label assignment\n",
      "[[0.05258216 0.12253841 0.02076341 0.8041161 ]\n",
      " [0.0462468  0.03440164 0.01475598 0.90459555]\n",
      " [0.11129226 0.04640347 0.03093906 0.8113652 ]\n",
      " ...\n",
      " [0.04149828 0.7113695  0.0201494  0.22698285]\n",
      " [0.03089009 0.0054749  0.9502664  0.01336859]\n",
      " [0.03087091 0.00547626 0.95028687 0.01336597]]\n",
      "Iteration 1240, Accuracy 0.33456\n",
      "89.45844%change in label assignment\n",
      "0.097863615\n",
      "[[0.01483404 0.9301732  0.00816434 0.04682837]\n",
      " [0.02276891 0.8648694  0.01128913 0.10107251]\n",
      " [0.06557544 0.37424603 0.02818053 0.531998  ]\n",
      " ...\n",
      " [0.05634268 0.7746168  0.03438387 0.13465667]\n",
      " [0.03113722 0.00550287 0.9498843  0.01347569]\n",
      " [0.03110551 0.00550253 0.94992375 0.01346825]]\n",
      "Iteration 1241, Accuracy 0.33137\n",
      "86.16389%change in label assignment\n",
      "0.10444459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06036565 0.1938312  0.02521571 0.72058743]\n",
      " [0.03676886 0.04036245 0.01290898 0.9099597 ]\n",
      " [0.08500933 0.04228623 0.02511624 0.8475882 ]\n",
      " ...\n",
      " [0.02264693 0.8634617  0.01161274 0.10227868]\n",
      " [0.03092603 0.00548127 0.95019776 0.01339495]\n",
      " [0.03090568 0.00548242 0.95022005 0.01339183]]\n",
      "Iteration 1242, Accuracy 0.33648\n",
      "85.81038%change in label assignment\n",
      "0.081493884\n",
      "[[0.0260157  0.84179544 0.01277704 0.11941176]\n",
      " [0.06410721 0.42921543 0.02789871 0.4787787 ]\n",
      " [0.047858   0.08852241 0.01762339 0.8459962 ]\n",
      " ...\n",
      " [0.02260496 0.89873105 0.01278458 0.0658794 ]\n",
      " [0.03115021 0.00550111 0.94987047 0.01347816]\n",
      " [0.03111285 0.00549951 0.94991964 0.01346792]]\n",
      "Iteration 1243, Accuracy 0.3325\n",
      "90.32258%change in label assignment\n",
      "0.09266799\n",
      "[[0.05123657 0.11396635 0.0204678  0.8143292 ]\n",
      " [0.04153212 0.03510757 0.01392744 0.9094328 ]\n",
      " [0.1301406  0.04958897 0.03614446 0.7841259 ]\n",
      " ...\n",
      " [0.0371557  0.75034744 0.01846564 0.19403125]\n",
      " [0.03094333 0.00548212 0.9501732  0.01340131]\n",
      " [0.03093706 0.00548613 0.950172   0.01340474]]\n",
      "Iteration 1244, Accuracy 0.32931\n",
      "88.07875%change in label assignment\n",
      "0.100940816\n",
      "[[0.01244088 0.9399367  0.00675813 0.04086427]\n",
      " [0.02682808 0.83595157 0.01315916 0.12406116]\n",
      " [0.06557576 0.27256975 0.02719933 0.63465524]\n",
      " ...\n",
      " [0.05374683 0.7836055  0.03264039 0.13000728]\n",
      " [0.03122636 0.00551265 0.94973665 0.01352429]\n",
      " [0.03120647 0.00551438 0.94975716 0.01352196]]\n",
      "Iteration 1245, Accuracy 0.33682\n",
      "87.45029%change in label assignment\n",
      "0.10333346\n",
      "[[0.06465961 0.35305774 0.02892126 0.5533614 ]\n",
      " [0.04310109 0.07205034 0.01648034 0.8683682 ]\n",
      " [0.07715417 0.04100666 0.0234308  0.85840833]\n",
      " ...\n",
      " [0.01541088 0.9129783  0.00812975 0.06348108]\n",
      " [0.03099253 0.00549162 0.9500781  0.01343771]\n",
      " [0.03099556 0.00549728 0.95006186 0.01344521]]\n",
      "Iteration 1246, Accuracy 0.33068\n",
      "90.136%change in label assignment\n",
      "0.08118343\n",
      "[[0.0349252  0.77342904 0.01678372 0.17486203]\n",
      " [0.06533431 0.2609353  0.02692357 0.64680684]\n",
      " [0.03766527 0.04168478 0.01266408 0.9079859 ]\n",
      " ...\n",
      " [0.01993228 0.9094462  0.0111834  0.05943812]\n",
      " [0.03117501 0.00551463 0.94979215 0.01351824]\n",
      " [0.03120552 0.00552529 0.94973135 0.01353788]]\n",
      "Iteration 1247, Accuracy 0.33466\n",
      "93.25379%change in label assignment\n",
      "0.07670012\n",
      "[[0.06495986 0.3275722  0.02850888 0.57895905]\n",
      " [0.03897491 0.05472629 0.01427786 0.89202094]\n",
      " [0.0688921  0.03871794 0.02088891 0.8715011 ]\n",
      " ...\n",
      " [0.01277083 0.9299138  0.00674562 0.05056978]\n",
      " [0.03110288 0.00551134 0.94988537 0.01350038]\n",
      " [0.03115272 0.00552568 0.9497928  0.01352878]]\n",
      "Iteration 1248, Accuracy 0.34782\n",
      "95.03118%change in label assignment\n",
      "0.0675684\n",
      "[[0.0590042  0.51569444 0.0266734  0.39862794]\n",
      " [0.05188358 0.11359251 0.01987785 0.814646  ]\n",
      " [0.0394074  0.03440895 0.01277914 0.9134045 ]\n",
      " ...\n",
      " [0.01195094 0.94214094 0.00653522 0.03937288]\n",
      " [0.03120055 0.00552454 0.9497289  0.01354596]\n",
      " [0.03126409 0.00554144 0.94961387 0.01358051]]\n",
      "Iteration 1249, Accuracy 0.34998\n",
      "96.93622%change in label assignment\n",
      "0.066165805\n",
      "[[0.06404926 0.25855023 0.02728461 0.6501159 ]\n",
      " [0.03717281 0.04658734 0.01325274 0.9029871 ]\n",
      " [0.05605025 0.03578823 0.01741433 0.89074713]\n",
      " ...\n",
      " [0.01268121 0.930465   0.00667024 0.05018358]\n",
      " [0.03112765 0.0055157  0.9498319  0.01352468]\n",
      " [0.03119262 0.005533   0.94971424 0.01356006]]\n",
      "Iteration 1250, Accuracy 0.35395\n",
      "98.07532%change in label assignment\n",
      "0.06787001\n",
      "[[0.05110122 0.61823773 0.02379709 0.3068639 ]\n",
      " [0.05568884 0.13970657 0.02184491 0.78275967]\n",
      " [0.03671537 0.04152551 0.01261672 0.90914243]\n",
      " ...\n",
      " [0.01332877 0.9365394  0.00736757 0.04276422]\n",
      " [0.0311552  0.00551597 0.9497875  0.01354136]\n",
      " [0.03120973 0.00553125 0.94968706 0.013572  ]]\n",
      "Iteration 1251, Accuracy 0.35332\n",
      "97.22099%change in label assignment\n",
      "0.06942883\n",
      "[[0.06294245 0.23169425 0.02656389 0.6787994 ]\n",
      " [0.03644175 0.04210703 0.01281594 0.9086353 ]\n",
      " [0.05637624 0.03586064 0.01754475 0.8902183 ]\n",
      " ...\n",
      " [0.01648808 0.9057415  0.00854813 0.06922225]\n",
      " [0.03104251 0.0054999  0.9499607  0.01349686]\n",
      " [0.03108753 0.00551364 0.9498754  0.01352351]]\n",
      "Iteration 1252, Accuracy 0.35356\n",
      "97.15226%change in label assignment\n",
      "0.06725587\n",
      "[[0.05226437 0.6049089  0.02425513 0.31857166]\n",
      " [0.0612338  0.19398461 0.02482178 0.7199598 ]\n",
      " [0.03810721 0.04953813 0.01345773 0.89889693]\n",
      " ...\n",
      " [0.01524269 0.9286269  0.00850241 0.04762802]\n",
      " [0.03109851 0.00550674 0.94987065 0.01352418]\n",
      " [0.03114063 0.00551994 0.9497899  0.01354956]]\n",
      "Iteration 1253, Accuracy 0.34595\n",
      "94.95262%change in label assignment\n",
      "0.073968515\n",
      "[[0.05891863 0.17505097 0.02419716 0.7418332 ]\n",
      " [0.03638738 0.04204106 0.01280278 0.90876883]\n",
      " [0.05623283 0.03582974 0.01751234 0.89042515]\n",
      " ...\n",
      " [0.02052212 0.87841296 0.01050895 0.09055594]\n",
      " [0.03096784 0.00548539 0.9500801  0.01346658]\n",
      " [0.03098444 0.00549394 0.950041   0.01348064]]\n",
      "Iteration 1254, Accuracy 0.34909\n",
      "95.21775%change in label assignment\n",
      "0.07267192\n",
      "[[0.02098254 0.87678427 0.01051375 0.09171949]\n",
      " [0.06219212 0.46673533 0.027587   0.44348553]\n",
      " [0.05200504 0.11323732 0.01988665 0.814871  ]\n",
      " ...\n",
      " [0.03087018 0.86665964 0.01794054 0.08452964]\n",
      " [0.03106483 0.00549955 0.94991887 0.01351678]\n",
      " [0.03108078 0.00550795 0.94988066 0.01353053]]\n",
      "Iteration 1255, Accuracy 0.33471\n",
      "93.84789%change in label assignment\n",
      "0.097446375\n",
      "[[0.04309497 0.07194269 0.01647182 0.8684905 ]\n",
      " [0.05207382 0.03530697 0.01665461 0.8959646 ]\n",
      " [0.1212798  0.04798196 0.0341569  0.7965814 ]\n",
      " ...\n",
      " [0.04834582 0.6443082  0.02335228 0.28399366]\n",
      " [0.03091855 0.00548436 0.9501286  0.01346855]\n",
      " [0.03095512 0.00549656 0.9500568  0.01349149]]\n",
      "Iteration 1256, Accuracy 0.34296\n",
      "89.17857%change in label assignment\n",
      "0.102390304\n",
      "[[0.01949904 0.9113146  0.01085191 0.0583344 ]\n",
      " [0.01474852 0.9199275  0.00750732 0.05781664]\n",
      " [0.06474991 0.4303471  0.02808204 0.47682092]\n",
      " ...\n",
      " [0.07738065 0.7058861  0.04887782 0.16785543]\n",
      " [0.03115612 0.00551067 0.9497653  0.01356792]\n",
      " [0.03115968 0.00551685 0.94974726 0.01357624]]\n",
      "Iteration 1257, Accuracy 0.34747\n",
      "80.09034%change in label assignment\n",
      "0.09641728\n",
      "[[0.03995907 0.7262219  0.01948467 0.21433431]\n",
      " [0.06451277 0.2737315  0.02768441 0.63407135]\n",
      " [0.03629501 0.03998387 0.01273386 0.91098726]\n",
      " ...\n",
      " [0.01983444 0.9094447  0.01140141 0.05931945]\n",
      " [0.03094532 0.00549267 0.9500627  0.01349925]\n",
      " [0.03096219 0.00550132 0.95002294 0.01351354]]\n",
      "Iteration 1258, Accuracy 0.36751\n",
      "86.63524%change in label assignment\n",
      "0.075300366\n",
      "[[0.06513089 0.36678454 0.02841872 0.5396658 ]\n",
      " [0.04275868 0.06920589 0.01572672 0.87230873]\n",
      " [0.05451198 0.03510571 0.016672   0.8937103 ]\n",
      " ...\n",
      " [0.01020993 0.94679266 0.00543516 0.03756228]\n",
      " [0.03094933 0.00548976 0.9500669  0.01349396]\n",
      " [0.03097429 0.00549987 0.95001405 0.0135118 ]]\n",
      "Iteration 1259, Accuracy 0.3487\n",
      "94.42726%change in label assignment\n",
      "0.06583375\n",
      "[[0.06490702 0.37836954 0.02845874 0.5282647 ]\n",
      " [0.04379441 0.07361559 0.01622814 0.86636186]\n",
      " [0.0451932  0.0335821  0.01430628 0.9069184 ]\n",
      " ...\n",
      " [0.00991432 0.94904417 0.00531615 0.03572533]\n",
      " [0.03098055 0.00549374 0.9500158  0.01350997]\n",
      " [0.03101673 0.005506   0.9499443  0.01353293]]\n",
      "Iteration 1260, Accuracy 0.34512\n",
      "98.22752%change in label assignment\n",
      "0.064542286\n",
      "[[0.06279384 0.22416718 0.02621541 0.68682355]\n",
      " [0.03728629 0.04696738 0.01322581 0.90252054]\n",
      " [0.05083067 0.03442016 0.01590431 0.8988449 ]\n",
      " ...\n",
      " [0.0125224  0.93153805 0.00657585 0.04936365]\n",
      " [0.03094918 0.00548972 0.95005655 0.01350455]\n",
      " [0.03098693 0.00550233 0.94998246 0.0135283 ]]\n",
      "Iteration 1261, Accuracy 0.34801\n",
      "98.8609%change in label assignment\n",
      "0.06293331\n",
      "[[0.06481185 0.38399652 0.02849335 0.52269834]\n",
      " [0.04462397 0.07720685 0.01663768 0.8615315 ]\n",
      " [0.03788352 0.03460862 0.01257513 0.91493267]\n",
      " ...\n",
      " [0.00990592 0.9498347  0.00535048 0.03490894]\n",
      " [0.0309555  0.00549024 0.9500415  0.01351285]\n",
      " [0.03098956 0.00550206 0.94997364 0.01353482]]\n",
      "Iteration 1262, Accuracy 0.34679\n",
      "98.13915%change in label assignment\n",
      "0.06251243\n",
      "[[0.06524262 0.33531427 0.02852631 0.5709168 ]\n",
      " [0.03909282 0.05514394 0.01423201 0.8915312 ]\n",
      " [0.04386676 0.03352928 0.01419981 0.9084041 ]\n",
      " ...\n",
      " [0.01120677 0.9399425  0.00595137 0.0428994 ]\n",
      " [0.03090577 0.00548879 0.95009977 0.01350569]\n",
      " [0.03095081 0.00550264 0.95001394 0.0135326 ]]\n",
      "Iteration 1263, Accuracy 0.3462\n",
      "97.62361%change in label assignment\n",
      "0.062265452\n",
      "[[0.06554843 0.33512357 0.02836899 0.570959  ]\n",
      " [0.04072597 0.06062286 0.01479368 0.8838574 ]\n",
      " [0.03942141 0.03376171 0.01291553 0.9139014 ]\n",
      " ...\n",
      " [0.01033327 0.9458462  0.00549525 0.03832529]\n",
      " [0.0308958  0.00548703 0.95011336 0.01350381]\n",
      " [0.03094091 0.00550099 0.9500272  0.01353087]]\n",
      "Iteration 1264, Accuracy 0.3517\n",
      "98.99347%change in label assignment\n",
      "0.0624189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.063935   0.4064029  0.02858041 0.50108165]\n",
      " [0.03969476 0.0576469  0.01453714 0.8881212 ]\n",
      " [0.04066247 0.03355418 0.01340406 0.91237926]\n",
      " ...\n",
      " [0.01074912 0.94290775 0.00573166 0.04061139]\n",
      " [0.03087319 0.00549004 0.95012903 0.01350769]\n",
      " [0.03092945 0.0055058  0.95002526 0.01353947]]\n",
      "Iteration 1265, Accuracy 0.35057\n",
      "96.37158%change in label assignment\n",
      "0.06229465\n",
      "[[0.06507284 0.37639055 0.02851338 0.5300232 ]\n",
      " [0.03830395 0.05040535 0.01359555 0.8976951 ]\n",
      " [0.04194727 0.03335036 0.01352346 0.9111789 ]\n",
      " ...\n",
      " [0.01145877 0.93852997 0.00602852 0.04398274]\n",
      " [0.03089045 0.00549674 0.9500894  0.01352343]\n",
      " [0.03095773 0.0055144  0.9499679  0.01355997]]\n",
      "Iteration 1266, Accuracy 0.35926\n",
      "95.8364%change in label assignment\n",
      "0.059731137\n",
      "[[0.06547963 0.31319833 0.02823128 0.5930907 ]\n",
      " [0.03939668 0.05554985 0.01422919 0.89082426]\n",
      " [0.03833759 0.03411974 0.01273113 0.91481155]\n",
      " ...\n",
      " [0.01111068 0.9406226  0.00588119 0.04238551]\n",
      " [0.03080256 0.00547701 0.9502369  0.01348356]\n",
      " [0.03084382 0.00549006 0.9501574  0.0135087 ]]\n",
      "Iteration 1267, Accuracy 0.36034\n",
      "96.89694%change in label assignment\n",
      "0.061032463\n",
      "[[0.06531321 0.34437937 0.02852857 0.56177884]\n",
      " [0.03867317 0.05302607 0.01394251 0.8943583 ]\n",
      " [0.03901881 0.03378757 0.01293275 0.9142609 ]\n",
      " ...\n",
      " [0.01121178 0.93991345 0.00594041 0.04293434]\n",
      " [0.03077162 0.00547853 0.9502679  0.01348189]\n",
      " [0.03082177 0.00549315 0.9501741  0.01351094]]\n",
      "Iteration 1268, Accuracy 0.35145\n",
      "98.08514%change in label assignment\n",
      "0.059982587\n",
      "[[0.06529904 0.35267943 0.02853183 0.55348974]\n",
      " [0.03810514 0.05031942 0.01360529 0.89797014]\n",
      " [0.0390749  0.03375217 0.01290518 0.9142677 ]\n",
      " ...\n",
      " [0.01187951 0.9356707  0.00625648 0.04619331]\n",
      " [0.03072939 0.00547135 0.95033365 0.01346564]\n",
      " [0.0307702  0.00548409 0.95025533 0.01349031]]\n",
      "Iteration 1269, Accuracy 0.35631\n",
      "99.33716%change in label assignment\n",
      "0.061727144\n",
      "[[0.06530375 0.30126017 0.02806337 0.6053727 ]\n",
      " [0.03770007 0.0487048  0.01341571 0.90017945]\n",
      " [0.03925397 0.03369366 0.01296532 0.91408706]\n",
      " ...\n",
      " [0.01173622 0.9365417  0.00619149 0.04553066]\n",
      " [0.03069474 0.00547003 0.95037735 0.01345793]\n",
      " [0.03073757 0.00548318 0.9502957  0.01348356]]\n",
      "Iteration 1270, Accuracy 0.35587\n",
      "97.53032%change in label assignment\n",
      "0.057464592\n",
      "[[0.06120305 0.19690986 0.02510406 0.7167831 ]\n",
      " [0.03761235 0.0478102  0.01329462 0.90128285]\n",
      " [0.03844656 0.03404356 0.01273357 0.9147763 ]\n",
      " ...\n",
      " [0.01230041 0.9329644  0.00645903 0.04827622]\n",
      " [0.03062156 0.00545462 0.9505008  0.0134231 ]\n",
      " [0.03063774 0.00546289 0.9504626  0.01343684]]\n",
      "Iteration 1271, Accuracy 0.36053\n",
      "96.09172%change in label assignment\n",
      "0.0595405\n",
      "[[0.05806411 0.16497111 0.02360932 0.75335544]\n",
      " [0.03665892 0.04454008 0.012973   0.905828  ]\n",
      " [0.04133208 0.03342778 0.0135741  0.911666  ]\n",
      " ...\n",
      " [0.01316354 0.9272846  0.00691818 0.05263373]\n",
      " [0.03057663 0.00546002 0.9505408  0.0134226 ]\n",
      " [0.03061864 0.0054732  0.95046    0.0134481 ]]\n",
      "Iteration 1272, Accuracy 0.36319\n",
      "97.03442%change in label assignment\n",
      "0.056665182\n",
      "[[0.06379016 0.24380869 0.02680019 0.66560096]\n",
      " [0.03879032 0.05350426 0.01396892 0.89373654]\n",
      " [0.03802159 0.03422156 0.01268824 0.9150686 ]\n",
      " ...\n",
      " [0.01071361 0.9430989  0.00569504 0.04049245]\n",
      " [0.03056049 0.0054602  0.9505583  0.01342104]\n",
      " [0.03060649 0.00547406 0.9504712  0.01344828]]\n",
      "Iteration 1273, Accuracy 0.36083\n",
      "98.17352%change in label assignment\n",
      "0.058070138\n",
      "[[0.06109998 0.19856074 0.02521782 0.71512145]\n",
      " [0.03662851 0.04400831 0.01289519 0.90646803]\n",
      " [0.04219209 0.03335923 0.01374792 0.9107008 ]\n",
      " ...\n",
      " [0.01272296 0.9301343  0.00668575 0.05045694]\n",
      " [0.03054859 0.00546749 0.9505542  0.01342975]\n",
      " [0.0306107  0.00548421 0.95044106 0.01346407]]\n",
      "Iteration 1274, Accuracy 0.35695\n",
      "94.5451%change in label assignment\n",
      "0.0559008\n",
      "[[0.06274384 0.2221069  0.0260956  0.68905365]\n",
      " [0.03800027 0.05002564 0.01355495 0.89841914]\n",
      " [0.03926628 0.03367992 0.01296437 0.9140895 ]\n",
      " ...\n",
      " [0.01120784 0.93989444 0.00593088 0.04296685]\n",
      " [0.03051801 0.00546472 0.95059645 0.01342081]\n",
      " [0.03058161 0.00548176 0.9504807  0.01345588]]\n",
      "Iteration 1275, Accuracy 0.35646\n",
      "99.43536%change in label assignment\n",
      "0.056711156\n",
      "[[0.05861674 0.16908588 0.02376602 0.74853134]\n",
      " [0.03797338 0.05009085 0.01356446 0.8983713 ]\n",
      " [0.03816412 0.03411216 0.01272487 0.9149988 ]\n",
      " ...\n",
      " [0.01175577 0.93632454 0.00620579 0.04571388]\n",
      " [0.03042043 0.00544557 0.9507569  0.01337711]\n",
      " [0.03046254 0.0054588  0.95067596 0.01340273]]\n",
      "Iteration 1276, Accuracy 0.35783\n",
      "97.36338%change in label assignment\n",
      "0.056600444\n",
      "[[0.06204368 0.21469226 0.02590183 0.6973622 ]\n",
      " [0.03875836 0.05425719 0.01409036 0.8928941 ]\n",
      " [0.03748998 0.03454194 0.01266844 0.91529965]\n",
      " ...\n",
      " [0.01061097 0.94365114 0.00567171 0.04006616]\n",
      " [0.03035529 0.00544017 0.9508445  0.01335996]\n",
      " [0.03039499 0.00545301 0.9507674  0.01338458]]\n",
      "Iteration 1277, Accuracy 0.3594\n",
      "98.30608%change in label assignment\n",
      "0.05832117\n",
      "[[0.05856414 0.17119445 0.02394211 0.7462993 ]\n",
      " [0.03662296 0.0449294  0.01301245 0.9054352 ]\n",
      " [0.04317853 0.03343884 0.01408765 0.90929496]\n",
      " ...\n",
      " [0.01296764 0.9284454  0.0068251  0.05176188]\n",
      " [0.03035081 0.00545011 0.950826   0.01337312]\n",
      " [0.03041458 0.00546747 0.95070934 0.01340864]]\n",
      "Iteration 1278, Accuracy 0.35612\n",
      "96.27829%change in label assignment\n",
      "0.05512926\n",
      "[[0.0599318  0.18448576 0.02456142 0.73102105]\n",
      " [0.03787097 0.05013731 0.01357821 0.8984135 ]\n",
      " [0.04110907 0.03337825 0.01347779 0.9120349 ]\n",
      " ...\n",
      " [0.01148522 0.9380169  0.00607713 0.04442079]\n",
      " [0.03033941 0.00545341 0.9508302  0.01337702]\n",
      " [0.03041286 0.00547271 0.95069724 0.01341709]]\n",
      "Iteration 1279, Accuracy 0.35985\n",
      "97.89856%change in label assignment\n",
      "[[0.06461906 0.38589382 0.02850821 0.52097887]\n",
      " [0.04298294 0.07074466 0.01595333 0.87031907]\n",
      " [0.03601545 0.03841488 0.01242884 0.91314083]\n",
      " ...\n",
      " [0.00983366 0.94997346 0.00532452 0.03486834]\n",
      " [0.03024649 0.00543076 0.9509903  0.01333244]\n",
      " [0.03028518 0.00544331 0.950915   0.01335652]]\n",
      "Iteration 1280, Accuracy 0.36206\n",
      "96.61708%change in label assignment\n",
      "0.06497732\n",
      "[[0.06172602 0.21344584 0.02596593 0.69886225]\n",
      " [0.03620015 0.03633573 0.01246955 0.9149946 ]\n",
      " [0.05172408 0.03477714 0.01647905 0.89701974]\n",
      " ...\n",
      " [0.01944039 0.8855443  0.01004266 0.08497261]\n",
      " [0.0302062  0.00543019 0.95104164 0.013322  ]\n",
      " [0.03025481 0.00544456 0.9509501  0.0133505 ]]\n",
      "Iteration 1281, Accuracy 0.36525\n",
      "98.04586%change in label assignment\n",
      "0.0628158\n",
      "[[0.06537961 0.35220504 0.02844138 0.553974  ]\n",
      " [0.04089695 0.0613488  0.01487545 0.88287884]\n",
      " [0.03629468 0.03707871 0.01236937 0.9142572 ]\n",
      " ...\n",
      " [0.00999235 0.9483201  0.00535359 0.03633392]\n",
      " [0.0302466  0.00543657 0.950973   0.0133439 ]\n",
      " [0.03030002 0.0054519  0.9508735  0.01337466]]\n",
      "Iteration 1282, Accuracy 0.36481\n",
      "98.54176%change in label assignment\n",
      "0.06190349\n",
      "[[0.05215859 0.12066661 0.02073867 0.8064361 ]\n",
      " [0.03778497 0.03423062 0.01271176 0.9152727 ]\n",
      " [0.05304125 0.03498883 0.0168059  0.895164  ]\n",
      " ...\n",
      " [0.02194089 0.86820966 0.0112396  0.09860985]\n",
      " [0.0301995  0.00543642 0.9510301  0.01333395]\n",
      " [0.03026326 0.00545378 0.9509135  0.01336951]]\n",
      "Iteration 1283, Accuracy 0.36594\n",
      "96.42559%change in label assignment\n",
      "0.064029545\n",
      "[[0.06410973 0.40739393 0.02839332 0.500103  ]\n",
      " [0.04757673 0.09138179 0.01806252 0.84297895]\n",
      " [0.03753797 0.04879152 0.01341661 0.90025395]\n",
      " ...\n",
      " [0.01073234 0.94708353 0.00588421 0.03629984]\n",
      " [0.03020388 0.00543264 0.9510278  0.01333566]\n",
      " [0.03025529 0.00544779 0.9509312  0.01336578]]\n",
      "Iteration 1284, Accuracy 0.36161\n",
      "94.36343%change in label assignment\n",
      "0.07001895\n",
      "[[0.05810115 0.16974273 0.02403612 0.74812   ]\n",
      " [0.03577175 0.03835831 0.0125637  0.91330624]\n",
      " [0.0475167  0.03416244 0.01546134 0.9028595 ]\n",
      " ...\n",
      " [0.02099942 0.87463987 0.01084613 0.09351464]\n",
      " [0.03008476 0.00541472 0.95121497 0.01328561]\n",
      " [0.03011981 0.00542681 0.95114505 0.01330832]]\n",
      "Iteration 1285, Accuracy 0.36466\n",
      "94.87406%change in label assignment\n",
      "0.063097715\n",
      "[[0.06234214 0.45118058 0.02798562 0.45849165]\n",
      " [0.04731091 0.09049147 0.01797523 0.84422237]\n",
      " [0.03670468 0.0450368  0.01298835 0.90527016]\n",
      " ...\n",
      " [0.01027851 0.94874936 0.00560749 0.03536458]\n",
      " [0.03014247 0.0054273  0.9511087  0.01332147]\n",
      " [0.03019201 0.00544216 0.951015   0.01335082]]\n",
      "Iteration 1286, Accuracy 0.35995\n",
      "97.25045%change in label assignment\n",
      "0.06739975\n",
      "[[0.05167653 0.11882439 0.02063365 0.8088654 ]\n",
      " [0.04008908 0.03350662 0.01334837 0.91305584]\n",
      " [0.06585685 0.03781977 0.02031381 0.87600964]\n",
      " ...\n",
      " [0.025829   0.839852   0.01314378 0.12117522]\n",
      " [0.0301004  0.00543194 0.9511549  0.01331284]\n",
      " [0.03017161 0.00545104 0.9510252  0.0133522 ]]\n",
      "Iteration 1287, Accuracy 0.36549\n",
      "94.0001%change in label assignment\n",
      "0.06766972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06214255 0.45849523 0.02785316 0.4515091 ]\n",
      " [0.04893798 0.09789326 0.01865305 0.83451575]\n",
      " [0.03797373 0.05040585 0.0135881  0.8980323 ]\n",
      " ...\n",
      " [0.01282479 0.9385587  0.00711405 0.04150243]\n",
      " [0.03016837 0.00544163 0.9510412  0.01334883]\n",
      " [0.03024289 0.00546139 0.950906   0.01338979]]\n",
      "Iteration 1288, Accuracy 0.36049\n",
      "97.63343%change in label assignment\n",
      "0.066569865\n",
      "[[0.04904662 0.10307258 0.0192832  0.8285976 ]\n",
      " [0.03905531 0.03362301 0.01305827 0.91426337]\n",
      " [0.05872726 0.03623848 0.01839721 0.8866371 ]\n",
      " ...\n",
      " [0.02226149 0.8656814  0.01142001 0.10063717]\n",
      " [0.03007225 0.0054331  0.9511791  0.01331555]\n",
      " [0.03014841 0.00545319 0.9510411  0.01335726]]\n",
      "Iteration 1289, Accuracy 0.36574\n",
      "96.10154%change in label assignment\n",
      "0.06284328\n",
      "[[0.06184043 0.46195212 0.02783091 0.44837654]\n",
      " [0.04781049 0.0929815  0.01820863 0.84099936]\n",
      " [0.03757598 0.04936831 0.01347635 0.8995794 ]\n",
      " ...\n",
      " [0.01317999 0.93708044 0.0073429  0.04239668]\n",
      " [0.03010704 0.00543761 0.9511175  0.01333778]\n",
      " [0.03018135 0.00545736 0.95098263 0.0133787 ]]\n",
      "Iteration 1290, Accuracy 0.35955\n",
      "96.95095%change in label assignment\n",
      "0.069283105\n",
      "[[0.05718465 0.16204084 0.02359542 0.7571791 ]\n",
      " [0.03753866 0.0343395  0.01280346 0.91531837]\n",
      " [0.05861571 0.03636079 0.01851319 0.8865103 ]\n",
      " ...\n",
      " [0.01941319 0.8854422  0.01007712 0.08506747]\n",
      " [0.03004099 0.00543376 0.9512116  0.01331365]\n",
      " [0.03012285 0.00545499 0.9510641  0.01335806]]\n",
      "Iteration 1291, Accuracy 0.36461\n",
      "96.50906%change in label assignment\n",
      "0.0641885\n",
      "[[0.0639341  0.41273728 0.02833316 0.49499545]\n",
      " [0.04166932 0.0648375  0.01526718 0.878226  ]\n",
      " [0.03603283 0.03842876 0.01239485 0.9131436 ]\n",
      " ...\n",
      " [0.01013659 0.94909644 0.0055026  0.03526436]\n",
      " [0.03008413 0.0054369  0.95114547 0.01333345]\n",
      " [0.03016314 0.00545755 0.9510028  0.01337654]]\n",
      "Iteration 1292, Accuracy 0.36412\n",
      "98.63014%change in label assignment\n",
      "0.061315943\n",
      "[[0.05858958 0.17558369 0.02431027 0.7415165 ]\n",
      " [0.03684055 0.03478618 0.01261922 0.91575396]\n",
      " [0.05214221 0.03492993 0.01673108 0.8961968 ]\n",
      " ...\n",
      " [0.01976123 0.88309306 0.01023067 0.08691498]\n",
      " [0.03000396 0.00542869 0.95125955 0.01330774]\n",
      " [0.03008243 0.00544927 0.9511177  0.01335062]]\n",
      "Iteration 1293, Accuracy 0.36702\n",
      "98.31099%change in label assignment\n",
      "0.061411414\n",
      "[[0.05660947 0.54701906 0.02611878 0.37025267]\n",
      " [0.04523439 0.08111574 0.01705282 0.85659707]\n",
      " [0.03651766 0.04480126 0.0129633  0.90571773]\n",
      " ...\n",
      " [0.01012294 0.9492948  0.00551841 0.03506383]\n",
      " [0.02998498 0.00541809 0.9512987  0.01329827]\n",
      " [0.03004157 0.00543435 0.95119303 0.01333099]]\n",
      "Iteration 1294, Accuracy 0.36515\n",
      "94.01974%change in label assignment\n",
      "0.06659874\n",
      "[[0.05912853 0.18171662 0.0246578  0.7344971 ]\n",
      " [0.03781337 0.03405777 0.0128541  0.9152748 ]\n",
      " [0.05424289 0.03534534 0.0173143  0.89309746]\n",
      " ...\n",
      " [0.02794971 0.82384366 0.01418259 0.134024  ]\n",
      " [0.02988541 0.00540356 0.9514533  0.01325776]\n",
      " [0.02993345 0.00541824 0.9513617  0.01328662]]\n",
      "Iteration 1295, Accuracy 0.36859\n",
      "97.48613%change in label assignment\n",
      "0.064208195\n",
      "[[0.01691375 0.9040595  0.00863455 0.07039212]\n",
      " [0.06522492 0.26287672 0.02722128 0.6446771 ]\n",
      " [0.05350133 0.12419481 0.02084384 0.80145997]\n",
      " ...\n",
      " [0.02984086 0.87045485 0.01737957 0.08232473]\n",
      " [0.02994389 0.00541229 0.95135397 0.0132899 ]\n",
      " [0.02998625 0.00542584 0.95127183 0.01331612]]\n",
      "Iteration 1296, Accuracy 0.3624\n",
      "94.60402%change in label assignment\n",
      "0.093140915\n",
      "[[0.052582   0.12550117 0.02129144 0.8006254 ]\n",
      " [0.04716022 0.03400769 0.01539485 0.9034372 ]\n",
      " [0.08186845 0.04126643 0.02477125 0.8520939 ]\n",
      " ...\n",
      " [0.03618494 0.75755453 0.01807576 0.18818477]\n",
      " [0.02983495 0.00540498 0.9515011  0.013259  ]\n",
      " [0.0298931  0.00542172 0.9513925  0.01329267]]\n",
      "Iteration 1297, Accuracy 0.36967\n",
      "91.01488%change in label assignment\n",
      "0.072425336\n",
      "[[0.03461472 0.77451587 0.01686311 0.17400627]\n",
      " [0.05961568 0.17376137 0.02397197 0.74265105]\n",
      " [0.04521375 0.0802141  0.01696753 0.8576047 ]\n",
      " ...\n",
      " [0.0206124  0.90668315 0.01175502 0.06094943]\n",
      " [0.02991637 0.00541013 0.95138156 0.01329184]\n",
      " [0.02996259 0.00542464 0.95129263 0.01332013]]\n",
      "Iteration 1298, Accuracy 0.36225\n",
      "93.84298%change in label assignment\n",
      "0.078211054\n",
      "[[0.06072894 0.20277365 0.02565312 0.7108443 ]\n",
      " [0.03695967 0.03465071 0.01271445 0.91567516]\n",
      " [0.06072511 0.03677465 0.0191303  0.8833699 ]\n",
      " ...\n",
      " [0.02018957 0.8800853  0.01047368 0.08925141]\n",
      " [0.02980234 0.00540238 0.9515413  0.01325396]\n",
      " [0.02985832 0.00541893 0.9514358  0.01328694]]\n",
      "Iteration 1299, Accuracy 0.36716\n",
      "95.6842%change in label assignment\n",
      "0.06325135\n",
      "[[0.04987864 0.63199556 0.023395   0.29473075]\n",
      " [0.04862594 0.09585721 0.01848681 0.83703005]\n",
      " [0.03745328 0.04769097 0.01327819 0.9015776 ]\n",
      " ...\n",
      " [0.01185748 0.9425267  0.00653561 0.03908024]\n",
      " [0.02982625 0.00539267 0.95152843 0.01325264]\n",
      " [0.02984903 0.00540271 0.95147806 0.01327018]]\n",
      "Iteration 1300, Accuracy 0.36328\n",
      "93.90681%change in label assignment\n",
      "0.068059556\n",
      "[[0.05565671 0.14840081 0.02279279 0.77314967]\n",
      " [0.03872888 0.0336038  0.01308518 0.9145822 ]\n",
      " [0.05803363 0.0360903  0.01834066 0.8875354 ]\n",
      " ...\n",
      " [0.02832793 0.8208927  0.01437769 0.13640168]\n",
      " [0.0297194  0.00537856 0.95169026 0.01321183]\n",
      " [0.02973356 0.0053871  0.95165366 0.01322561]]\n",
      "Iteration 1301, Accuracy 0.36004\n",
      "96.46978%change in label assignment\n",
      "0.06835164\n",
      "[[0.04401448 0.6916558  0.02096173 0.24336801]\n",
      " [0.06029234 0.18190868 0.02437276 0.7334263 ]\n",
      " [0.04783378 0.09301612 0.01822465 0.8409255 ]\n",
      " ...\n",
      " [0.01948073 0.9112097  0.01108872 0.05822085]\n",
      " [0.02981057 0.00538417 0.95156014 0.01324512]\n",
      " [0.02979647 0.00538773 0.9515693  0.01324646]]\n",
      "Iteration 1302, Accuracy 0.35204\n",
      "90.95105%change in label assignment\n",
      "0.082767166\n",
      "[[0.05351623 0.13302472 0.02191412 0.7915449 ]\n",
      " [0.04106762 0.03365412 0.01386819 0.91141003]\n",
      " [0.07257567 0.03954417 0.02250548 0.8653747 ]\n",
      " ...\n",
      " [0.02980821 0.8092586  0.0151962  0.14573705]\n",
      " [0.02965087 0.00537105 0.9517868  0.01319126]\n",
      " [0.0296496  0.00537717 0.9517746  0.01319861]]\n",
      "Iteration 1303, Accuracy 0.34453\n",
      "94.75622%change in label assignment\n",
      "0.073738776\n",
      "[[0.04094703 0.72009975 0.01964666 0.21930653]\n",
      " [0.05882804 0.16636796 0.02356109 0.75124294]\n",
      " [0.04365283 0.07332007 0.01622231 0.8668047 ]\n",
      " ...\n",
      " [0.01926461 0.91206485 0.01094748 0.05772305]\n",
      " [0.02976795 0.00538315 0.9516089  0.01324003]\n",
      " [0.02976222 0.00538857 0.9516037  0.01324554]]\n",
      "Iteration 1304, Accuracy 0.34669\n",
      "95.25212%change in label assignment\n",
      "0.08110401\n",
      "[[0.05009717 0.11096836 0.02017027 0.81876415]\n",
      " [0.04782681 0.03424831 0.01568503 0.9022398 ]\n",
      " [0.09413758 0.04372323 0.02805441 0.8340847 ]\n",
      " ...\n",
      " [0.03393603 0.77584183 0.01711944 0.17310266]\n",
      " [0.02962046 0.00537176 0.9518211  0.01318676]\n",
      " [0.02963214 0.00538039 0.95178735 0.01320011]]\n",
      "Iteration 1305, Accuracy 0.34492\n",
      "93.69077%change in label assignment\n",
      "0.08045339\n",
      "[[0.02022809 0.88190174 0.01021571 0.08765445]\n",
      " [0.0653132  0.26727265 0.02731656 0.64009756]\n",
      " [0.0497591  0.10152175 0.01898666 0.8297325 ]\n",
      " ...\n",
      " [0.03494341 0.851035   0.02059689 0.09342468]\n",
      " [0.02977661 0.00539284 0.9515749  0.01325563]\n",
      " [0.02979687 0.00540298 0.9515274  0.01327278]]\n",
      "Iteration 1306, Accuracy 0.34821\n",
      "96.22919%change in label assignment\n",
      "0.0886012\n",
      "[[0.06079143 0.20728609 0.02588351 0.706039  ]\n",
      " [0.0373412  0.03431235 0.01287324 0.9154733 ]\n",
      " [0.07319013 0.03957846 0.02258023 0.8646512 ]\n",
      " ...\n",
      " [0.01681054 0.90287834 0.00885237 0.07145876]\n",
      " [0.02964554 0.00538572 0.9517542  0.01321449]\n",
      " [0.02968852 0.00540033 0.9516689  0.01324222]]\n",
      "Iteration 1307, Accuracy 0.35528\n",
      "94.72185%change in label assignment\n",
      "0.065835565\n",
      "[[0.04741939 0.655326   0.02251463 0.27474   ]\n",
      " [0.04971894 0.1042725  0.01923544 0.82677317]\n",
      " [0.03576919 0.03891145 0.01239774 0.91292155]\n",
      " ...\n",
      " [0.01415572 0.9329754  0.00793259 0.04493633]\n",
      " [0.02973825 0.00539491 0.95161283 0.01325405]\n",
      " [0.02978412 0.00541008 0.95152265 0.01328314]]\n",
      "Iteration 1308, Accuracy 0.35523\n",
      "95.59091%change in label assignment\n",
      "0.06693017\n",
      "[[0.06040888 0.19995403 0.02548794 0.7141491 ]\n",
      " [0.03638989 0.03473516 0.01255817 0.91631675]\n",
      " [0.06604349 0.03779877 0.02050138 0.87565637]\n",
      " ...\n",
      " [0.0183932  0.89223844 0.00958061 0.07978774]\n",
      " [0.02967337 0.00539291 0.951694   0.01323967]\n",
      " [0.02973614 0.00541151 0.9515756  0.01327677]]\n",
      "Iteration 1309, Accuracy 0.36107\n",
      "95.36996%change in label assignment\n",
      "0.06688239\n",
      "[[0.05635894 0.54980916 0.02606352 0.3677684 ]\n",
      " [0.04802212 0.09516378 0.01843442 0.8383797 ]\n",
      " [0.03573454 0.0401768  0.01248969 0.911599  ]\n",
      " ...\n",
      " [0.01270895 0.9390237  0.00707747 0.04118996]\n",
      " [0.02973131 0.00540127 0.95159525 0.01327211]\n",
      " [0.02979677 0.00542036 0.9514724  0.01331047]]\n",
      "Iteration 1310, Accuracy 0.35739\n",
      "97.92802%change in label assignment\n",
      "0.060915805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06044519 0.19898838 0.02538392 0.71518254]\n",
      " [0.03535873 0.03715745 0.01237745 0.9151063 ]\n",
      " [0.05271106 0.0347954  0.01683568 0.89565784]\n",
      " ...\n",
      " [0.01541222 0.9122099  0.00808489 0.064293  ]\n",
      " [0.02963776 0.00539188 0.9517293  0.01324108]\n",
      " [0.02970466 0.00541137 0.9516037  0.01328025]]\n",
      "Iteration 1311, Accuracy 0.36211\n",
      "96.54834%change in label assignment\n",
      "0.06038436\n",
      "[[0.06512511 0.31644237 0.02824916 0.5901833 ]\n",
      " [0.04041582 0.06146837 0.01489879 0.88321704]\n",
      " [0.03579623 0.03620785 0.01230066 0.91569525]\n",
      " ...\n",
      " [0.00975739 0.94969803 0.00527215 0.03527242]\n",
      " [0.02959989 0.00537627 0.95180553 0.01321832]\n",
      " [0.02963059 0.00538878 0.9517399  0.01324076]]\n",
      "Iteration 1312, Accuracy 0.3595\n",
      "95.95915%change in label assignment\n",
      "0.05764684\n",
      "[[0.05510868 0.14432524 0.02245318 0.7781129 ]\n",
      " [0.03529654 0.03976369 0.01252041 0.9124194 ]\n",
      " [0.04195485 0.03310762 0.01393595 0.91100156]\n",
      " ...\n",
      " [0.01592793 0.9087981  0.00834741 0.06692658]\n",
      " [0.02952284 0.00536224 0.95192844 0.01318657]\n",
      " [0.02953106 0.0053708  0.9518991  0.01319911]]\n",
      "Iteration 1313, Accuracy 0.35209\n",
      "95.71856%change in label assignment\n",
      "0.060175315\n",
      "[[0.06478742 0.29851803 0.02805918 0.6086353 ]\n",
      " [0.04395505 0.07725783 0.01668551 0.86210155]\n",
      " [0.03544215 0.04069911 0.01255682 0.91130185]\n",
      " ...\n",
      " [0.00997978 0.9499033  0.00547728 0.03463968]\n",
      " [0.02951832 0.00536472 0.9519224  0.01319464]\n",
      " [0.02952947 0.00537422 0.95188725 0.01320901]]\n",
      "Iteration 1314, Accuracy 0.34178\n",
      "97.41739%change in label assignment\n",
      "0.060231484\n",
      "[[0.05462974 0.14115307 0.02223992 0.78197724]\n",
      " [0.03520311 0.0381882  0.01241784 0.9141908 ]\n",
      " [0.04751496 0.03381693 0.01546544 0.90320265]\n",
      " ...\n",
      " [0.01642815 0.90541637 0.00859732 0.06955815]\n",
      " [0.02944473 0.00536521 0.9520109  0.01317906]\n",
      " [0.02948433 0.00538024 0.95192873 0.01320663]]\n",
      "Iteration 1315, Accuracy 0.34438\n",
      "96.0328%change in label assignment\n",
      "0.060512386\n",
      "[[0.06479213 0.28980574 0.02787557 0.61752653]\n",
      " [0.04546458 0.08354942 0.01731931 0.8536666 ]\n",
      " [0.03564242 0.04151008 0.01261129 0.91023624]\n",
      " ...\n",
      " [0.01040856 0.94834745 0.00572673 0.03551725]\n",
      " [0.02945546 0.00536052 0.9520035  0.01318055]\n",
      " [0.02947045 0.00537105 0.9519614  0.01319708]]\n",
      "Iteration 1316, Accuracy 0.34963\n",
      "94.37816%change in label assignment\n",
      "0.062370844\n",
      "[[0.04930765 0.10600415 0.01952877 0.8251594 ]\n",
      " [0.03520858 0.0393015  0.01246946 0.91302043]\n",
      " [0.04573521 0.03350817 0.01495678 0.90579987]\n",
      " ...\n",
      " [0.01452284 0.91793203 0.00765273 0.05989236]\n",
      " [0.02937803 0.00535126 0.95212144 0.01314923]\n",
      " [0.02938594 0.00536077 0.9520904  0.0131629 ]]\n",
      "Iteration 1317, Accuracy 0.34482\n",
      "97.85437%change in label assignment\n",
      "0.061617993\n",
      "[[0.06139379 0.20689213 0.0255365  0.70617753]\n",
      " [0.04447903 0.07917321 0.01685537 0.85949236]\n",
      " [0.03550747 0.04066471 0.01252433 0.91130346]\n",
      " ...\n",
      " [0.01025554 0.94890326 0.00563615 0.03520502]\n",
      " [0.02940029 0.00535291 0.95208573 0.013161  ]\n",
      " [0.02939923 0.005361   0.9520688  0.01317093]]\n",
      "Iteration 1318, Accuracy 0.34099\n",
      "97.43703%change in label assignment\n",
      "0.061338864\n",
      "[[0.03605661 0.04560095 0.01308857 0.90525395]\n",
      " [0.03552175 0.03589875 0.01234573 0.91623384]\n",
      " [0.04820149 0.03392476 0.01563774 0.902236  ]\n",
      " ...\n",
      " [0.01987011 0.8819791  0.01028996 0.08786087]\n",
      " [0.02932534 0.0053449  0.95219517 0.01313461]\n",
      " [0.02931973 0.00535262 0.95218456 0.01314305]]\n",
      "Iteration 1319, Accuracy 0.33643\n",
      "96.0328%change in label assignment\n",
      "[[0.06201878 0.21661302 0.0259029  0.69546527]\n",
      " [0.05117563 0.11401924 0.02007425 0.8147309 ]\n",
      " [0.03774083 0.05191866 0.0137672  0.8965733 ]\n",
      " ...\n",
      " [0.01397658 0.93369144 0.00786259 0.0444694 ]\n",
      " [0.02935105 0.00535247 0.9521396  0.01315689]\n",
      " [0.02935963 0.00536276 0.9521059  0.01317172]]\n",
      "Iteration 1320, Accuracy 0.3353\n",
      "96.37649%change in label assignment\n",
      "0.07781352\n",
      "[[0.0410617  0.03326615 0.01383032 0.9118418 ]\n",
      " [0.05204662 0.0348287  0.01679426 0.8963304 ]\n",
      " [0.10026213 0.04472243 0.02958485 0.82543063]\n",
      " ...\n",
      " [0.03556815 0.76132524 0.01784952 0.18525709]\n",
      " [0.02926496 0.00534385 0.9522752  0.01311596]\n",
      " [0.02928761 0.00535706 0.9522179  0.01313752]]\n",
      "Iteration 1321, Accuracy 0.33623\n",
      "95.38469%change in label assignment\n",
      "0.08530107\n",
      "[[0.06453454 0.39396563 0.02844989 0.5130499 ]\n",
      " [0.06503492 0.27436107 0.02749967 0.6331044 ]\n",
      " [0.05265202 0.12186781 0.02067676 0.80480343]\n",
      " ...\n",
      " [0.03588658 0.84733206 0.02129161 0.09548973]\n",
      " [0.02941793 0.00536434 0.9520287  0.01318903]\n",
      " [0.02945096 0.00537972 0.95195365 0.01321568]]\n",
      "Iteration 1322, Accuracy 0.33579\n",
      "96.29302%change in label assignment\n",
      "0.09686838\n",
      "[[0.05940695 0.03675212 0.01909002 0.8847509 ]\n",
      " [0.05960841 0.03676372 0.01911804 0.88450986]\n",
      " [0.11703905 0.04778778 0.03426188 0.8009113 ]\n",
      " ...\n",
      " [0.02751273 0.82610327 0.01416516 0.13221888]\n",
      " [0.02927741 0.00535902 0.95222175 0.01314173]\n",
      " [0.02934912 0.00538197 0.9520825  0.01318636]]\n",
      "Iteration 1323, Accuracy 0.34006\n",
      "89.08038%change in label assignment\n",
      "0.082203686\n",
      "[[0.05849054 0.16747041 0.02362599 0.75041306]\n",
      " [0.05305047 0.1235017  0.0207911  0.80265677]\n",
      " [0.03853    0.05385219 0.01398164 0.89363617]\n",
      " ...\n",
      " [0.02723894 0.8802433  0.01585497 0.07666276]\n",
      " [0.02946336 0.00538691 0.9519182  0.01323148]\n",
      " [0.02956166 0.00541483 0.95173514 0.01328831]]\n",
      "Iteration 1324, Accuracy 0.34379\n",
      "95.89041%change in label assignment\n",
      "0.08053638\n",
      "[[0.04259545 0.03338041 0.01432646 0.9096977 ]\n",
      " [0.03923835 0.03337059 0.01340905 0.913982  ]\n",
      " [0.0734705  0.03974425 0.02289808 0.8638872 ]\n",
      " ...\n",
      " [0.01402908 0.92098516 0.0074819  0.05750383]\n",
      " [0.02932505 0.00536889 0.95213145 0.01317452]\n",
      " [0.02941513 0.00539563 0.95196116 0.01322803]]\n",
      "Iteration 1325, Accuracy 0.35189\n",
      "96.58271%change in label assignment\n",
      "0.06682649\n",
      "[[0.04018478 0.06087273 0.01480432 0.88413817]\n",
      " [0.04462723 0.07960811 0.01688195 0.8588827 ]\n",
      " [0.03543265 0.03966198 0.01241957 0.9124858 ]\n",
      " ...\n",
      " [0.01287259 0.93826985 0.00718361 0.04167395]\n",
      " [0.02937346 0.00536321 0.95207536 0.01318792]\n",
      " [0.02942916 0.00538344 0.9519618  0.01322568]]\n",
      "Iteration 1326, Accuracy 0.34335\n",
      "94.36343%change in label assignment\n",
      "0.07043614\n",
      "[[0.0385288  0.03349165 0.01325873 0.91472083]\n",
      " [0.03518878 0.03648121 0.01251627 0.91581374]\n",
      " [0.05513173 0.0357215  0.0179217  0.89122504]\n",
      " ...\n",
      " [0.01487643 0.91542184 0.00791213 0.06178964]\n",
      " [0.02927097 0.00535043 0.9522314  0.01314718]\n",
      " [0.02931557 0.00536906 0.95213485 0.01318047]]\n",
      "Iteration 1327, Accuracy 0.33672\n",
      "95.04591%change in label assignment\n",
      "0.066238925\n",
      "[[0.04677049 0.08929279 0.01785286 0.8460838 ]\n",
      " [0.04954794 0.10330483 0.01914632 0.82800084]\n",
      " [0.03700826 0.04802614 0.01328385 0.9016817 ]\n",
      " ...\n",
      " [0.01136869 0.94441634 0.00627044 0.03794454]\n",
      " [0.0293523  0.00535    0.9521263  0.01317145]\n",
      " [0.02934937 0.00535955 0.95210826 0.01318283]]\n",
      "Iteration 1328, Accuracy 0.32803\n",
      "91.46659%change in label assignment\n",
      "0.07221966\n",
      "[[0.03800983 0.03385884 0.01321392 0.91491747]\n",
      " [0.03850395 0.03365147 0.01331177 0.9145328 ]\n",
      " [0.07026625 0.03918415 0.02218537 0.8683643 ]\n",
      " ...\n",
      " [0.0240952  0.8515408  0.01250029 0.11186372]\n",
      " [0.02921532 0.00534386 0.9523086  0.01313224]\n",
      " [0.02924734 0.00535994 0.9522332  0.01315954]]\n",
      "Iteration 1329, Accuracy 0.31355\n",
      "94.42726%change in label assignment\n",
      "0.07277195\n",
      "[[0.0653285  0.3285044  0.02830521 0.5778619 ]\n",
      " [0.06281249 0.2233163  0.02610337 0.68776786]\n",
      " [0.04387997 0.07627193 0.01653456 0.86331356]\n",
      " ...\n",
      " [0.02554521 0.88687545 0.01480712 0.07277223]\n",
      " [0.02934461 0.00536282 0.95209664 0.01319593]\n",
      " [0.0293959  0.00538222 0.9519902  0.0132317 ]]\n",
      "Iteration 1330, Accuracy 0.32582\n",
      "94.85933%change in label assignment\n",
      "0.088373914\n",
      "[[0.03702256 0.03442665 0.01302376 0.91552705]\n",
      " [0.04453993 0.03380018 0.01500697 0.90665287]\n",
      " [0.11311938 0.04719698 0.03337087 0.80631274]\n",
      " ...\n",
      " [0.03101833 0.79834056 0.01585398 0.15478708]\n",
      " [0.02920783 0.00534657 0.9523146  0.01313107]\n",
      " [0.02926018 0.0053665  0.9522056  0.0131677 ]]\n",
      "Iteration 1331, Accuracy 0.33299\n",
      "95.9346%change in label assignment\n",
      "0.08655333\n",
      "[[0.06101604 0.48031685 0.02755413 0.431113  ]\n",
      " [0.06563279 0.31709036 0.02818646 0.5890904 ]\n",
      " [0.04894378 0.09960981 0.0188282  0.8326182 ]\n",
      " ...\n",
      " [0.03530286 0.8495241  0.02088855 0.0942845 ]\n",
      " [0.02937968 0.00536934 0.95203364 0.01321742]\n",
      " [0.02943957 0.00539053 0.9519125  0.0132574 ]]\n",
      "Iteration 1332, Accuracy 0.34242\n",
      "95.20793%change in label assignment\n",
      "0.09566461\n",
      "[[0.03585569 0.04621631 0.01337091 0.9045571 ]\n",
      " [0.04149413 0.03335745 0.01411344 0.911035  ]\n",
      " [0.09491    0.04402631 0.0286541  0.8324096 ]\n",
      " ...\n",
      " [0.02289397 0.86008364 0.01191971 0.10510268]\n",
      " [0.02921337 0.00534824 0.95229304 0.01314539]\n",
      " [0.02926129 0.00536719 0.95219165 0.01317984]]\n",
      "Iteration 1333, Accuracy 0.33446\n",
      "94.81514%change in label assignment\n",
      "0.07699283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06381095 0.40626714 0.02842686 0.5014951 ]\n",
      " [0.05774542 0.16249837 0.02336563 0.75639063]\n",
      " [0.03711774 0.04867498 0.01335848 0.90084887]\n",
      " ...\n",
      " [0.02108256 0.90455735 0.01210352 0.06225662]\n",
      " [0.0293932  0.00537557 0.951999   0.01323224]\n",
      " [0.02947473 0.00540068 0.9518426  0.01328195]]\n",
      "Iteration 1334, Accuracy 0.33034\n",
      "94.87897%change in label assignment\n",
      "0.08033305\n",
      "[[0.03495796 0.03748944 0.01261336 0.9149393 ]\n",
      " [0.04736505 0.03401229 0.01574052 0.90288216]\n",
      " [0.09784258 0.04448045 0.02942387 0.82825315]\n",
      " ...\n",
      " [0.02916856 0.8131059  0.01495385 0.14277174]\n",
      " [0.02928048 0.00536328 0.95216686 0.01318943]\n",
      " [0.02936848 0.00538978 0.9519995  0.01324228]]\n",
      "Iteration 1335, Accuracy 0.34006\n",
      "93.9019%change in label assignment\n",
      "0.075361624\n",
      "[[0.03989503 0.73111993 0.01915576 0.20982935]\n",
      " [0.06479247 0.40427125 0.02844042 0.5024959 ]\n",
      " [0.05238828 0.11663061 0.02029904 0.81068206]\n",
      " ...\n",
      " [0.04759916 0.80487645 0.02876793 0.11875646]\n",
      " [0.02940168 0.00537699 0.95197767 0.0132437 ]\n",
      " [0.02947189 0.00539986 0.95184016 0.01328812]]\n",
      "Iteration 1336, Accuracy 0.33363\n",
      "92.60078%change in label assignment\n",
      "0.10629699\n",
      "[[0.04061447 0.06648117 0.01576393 0.87714034]\n",
      " [0.03690017 0.03386113 0.01289492 0.91634375]\n",
      " [0.07900734 0.04083695 0.02449213 0.8556635 ]\n",
      " ...\n",
      " [0.01664559 0.90358937 0.00881763 0.07094744]\n",
      " [0.0292435  0.00535788 0.95221186 0.01318671]\n",
      " [0.02931313 0.00538081 0.952075   0.01323104]]\n",
      "Iteration 1337, Accuracy 0.34075\n",
      "95.1932%change in label assignment\n",
      "0.06743476\n",
      "[[0.06230245 0.21897498 0.02599573 0.69272685]\n",
      " [0.04255204 0.07024989 0.01589185 0.87130624]\n",
      " [0.03675288 0.03438143 0.0124053  0.91646045]\n",
      " ...\n",
      " [0.01126283 0.9448683  0.00621316 0.03765577]\n",
      " [0.02934842 0.00536782 0.95205384 0.01322999]\n",
      " [0.02941887 0.00539074 0.9519158  0.01327454]]\n",
      "Iteration 1338, Accuracy 0.33397\n",
      "98.24225%change in label assignment\n",
      "0.066139325\n",
      "[[0.04971655 0.11066283 0.02002865 0.81959194]\n",
      " [0.03469788 0.03701124 0.01234949 0.91594136]\n",
      " [0.05067923 0.03432944 0.01651539 0.89847594]\n",
      " ...\n",
      " [0.01241705 0.93134856 0.00664222 0.04959217]\n",
      " [0.0292586  0.00535862 0.9521793  0.0132035 ]\n",
      " [0.02932558 0.00538089 0.9520471  0.01324647]]\n",
      "Iteration 1339, Accuracy 0.33579\n",
      "97.27009%change in label assignment\n",
      "0.06080801\n",
      "[[0.06293462 0.23879987 0.02672055 0.67154497]\n",
      " [0.03981295 0.06070555 0.01480508 0.8846764 ]\n",
      " [0.03593462 0.03434429 0.01230794 0.91741323]\n",
      " ...\n",
      " [0.00993425 0.9500948  0.0054511  0.03451983]\n",
      " [0.02928721 0.00535857 0.9521395  0.01321467]\n",
      " [0.02934261 0.00537844 0.95202684 0.01325206]]\n",
      "Iteration 1340, Accuracy 0.33692\n",
      "97.96239%change in label assignment\n",
      "0.06302446\n",
      "[[0.04276068 0.07554327 0.01667838 0.8650177 ]\n",
      " [0.03635516 0.03364992 0.01262619 0.9173687 ]\n",
      " [0.05751705 0.03584253 0.01846824 0.88817215]\n",
      " ...\n",
      " [0.01864794 0.8900649  0.00976114 0.08152601]\n",
      " [0.02921572 0.00535472 0.9522322  0.01319731]\n",
      " [0.02928416 0.00537712 0.9520979  0.0132408 ]]\n",
      "Iteration 1341, Accuracy 0.33702\n",
      "97.00005%change in label assignment\n",
      "0.0645985\n",
      "[[0.06438302 0.3781703  0.02861406 0.5288326 ]\n",
      " [0.04852129 0.09976471 0.01889023 0.8328237 ]\n",
      " [0.03574248 0.04385983 0.01281604 0.9075816 ]\n",
      " ...\n",
      " [0.01485994 0.9300648  0.00839364 0.04668167]\n",
      " [0.02928827 0.00536761 0.9521079  0.01323624]\n",
      " [0.0293661  0.00539164 0.95195836 0.0132839 ]]\n",
      "Iteration 1342, Accuracy 0.33849\n",
      "96.39613%change in label assignment\n",
      "0.068912916\n",
      "[[0.0532049  0.13420716 0.02193092 0.79065704]\n",
      " [0.03457448 0.03882012 0.01250056 0.9141048 ]\n",
      " [0.0487967  0.03401498 0.01610641 0.9010819 ]\n",
      " ...\n",
      " [0.01344986 0.9246389  0.00718523 0.05472602]\n",
      " [0.02915684 0.00534304 0.95232385 0.01317624]\n",
      " [0.02920016 0.00536063 0.9522311  0.01320809]]\n",
      "Iteration 1343, Accuracy 0.34379\n",
      "95.12938%change in label assignment\n",
      "0.062013894\n",
      "[[0.06343762 0.24918468 0.02702389 0.6603538 ]\n",
      " [0.04070814 0.0641397  0.01521168 0.8799405 ]\n",
      " [0.0354993  0.03507213 0.01223372 0.91719484]\n",
      " ...\n",
      " [0.0099355  0.95007104 0.00544716 0.03454627]\n",
      " [0.02919276 0.00535003 0.9522599  0.01319736]\n",
      " [0.02924631 0.00536954 0.9521503  0.01323392]]\n",
      "Iteration 1344, Accuracy 0.33672\n",
      "97.52541%change in label assignment\n",
      "0.06370189\n",
      "[[0.04493104 0.08583575 0.01780166 0.85143155]\n",
      " [0.035241   0.03481255 0.01244684 0.9174996 ]\n",
      " [0.05874247 0.03618551 0.01890584 0.8861662 ]\n",
      " ...\n",
      " [0.01642566 0.90503    0.00867707 0.06986728]\n",
      " [0.0291801  0.0053654  0.9522393  0.0132152 ]\n",
      " [0.02928784 0.00539545 0.9520396  0.01327713]]\n",
      "Iteration 1345, Accuracy 0.33805\n",
      "94.88388%change in label assignment\n",
      "0.064464904\n",
      "[[0.064731   0.37105885 0.02861078 0.5355994 ]\n",
      " [0.05193279 0.11817196 0.02045587 0.80943936]\n",
      " [0.03624003 0.045504   0.01300862 0.90524733]\n",
      " ...\n",
      " [0.01707288 0.9209552  0.0096961  0.05227587]\n",
      " [0.02926887 0.00538049 0.95208794 0.01326268]\n",
      " [0.02938828 0.00541279 0.95186883 0.01333008]]\n",
      "Iteration 1346, Accuracy 0.34664\n",
      "97.11298%change in label assignment\n",
      "0.06967171\n",
      "[[0.03679315 0.05104405 0.01381814 0.89834464]\n",
      " [0.03806233 0.03287545 0.0130649  0.9159973 ]\n",
      " [0.06749821 0.03806561 0.02125787 0.8731783 ]\n",
      " ...\n",
      " [0.02159367 0.86948895 0.01122773 0.09768956]\n",
      " [0.02912703 0.0053584  0.9523133  0.01320126]\n",
      " [0.0292338  0.00538851 0.9521146  0.01326306]]\n",
      "Iteration 1347, Accuracy 0.35194\n",
      "96.36667%change in label assignment\n",
      "0.06664342\n",
      "[[0.06474391 0.2944297  0.02802128 0.6128051 ]\n",
      " [0.04657453 0.08969845 0.01796011 0.8457669 ]\n",
      " [0.03513119 0.0403435  0.01246311 0.91206217]\n",
      " ...\n",
      " [0.0142311  0.93268144 0.00802495 0.0450625 ]\n",
      " [0.02924418 0.00538137 0.95211166 0.01326279]\n",
      " [0.02937146 0.00541517 0.9518795  0.01333386]]\n",
      "Iteration 1348, Accuracy 0.3432\n",
      "95.95915%change in label assignment\n",
      "0.06556272\n",
      "[[0.0458004  0.08993009 0.01816216 0.8461073 ]\n",
      " [0.03655982 0.03327353 0.01265642 0.91751015]\n",
      " [0.05867392 0.0360145  0.01879835 0.88651323]\n",
      " ...\n",
      " [0.01751558 0.8976998  0.00920794 0.07557672]\n",
      " [0.02910072 0.00535842 0.95233834 0.01320253]\n",
      " [0.02920395 0.00538726 0.95214677 0.01326198]]\n",
      "Iteration 1349, Accuracy 0.35454\n",
      "96.83311%change in label assignment\n",
      "0.064706005\n",
      "[[0.06465617 0.29194227 0.02798773 0.6154138 ]\n",
      " [0.04533944 0.08403747 0.0173985  0.8532245 ]\n",
      " [0.03520653 0.04136186 0.0125576  0.91087395]\n",
      " ...\n",
      " [0.01358521 0.9353575  0.0076507  0.0434066 ]\n",
      " [0.02910535 0.00535102 0.95234466 0.01319898]\n",
      " [0.0291823  0.00537485 0.95219654 0.01324627]]\n",
      "Iteration 1350, Accuracy 0.35199\n",
      "96.45996%change in label assignment\n",
      "0.061653603\n",
      "[[0.0358251  0.04691919 0.01324565 0.90401006]\n",
      " [0.03545111 0.03410222 0.01236254 0.9180842 ]\n",
      " [0.04948231 0.03394473 0.01620144 0.9003715 ]\n",
      " ...\n",
      " [0.01540035 0.9118119  0.00814305 0.06464471]\n",
      " [0.02895854 0.00532849 0.95257366 0.01313932]\n",
      " [0.02901281 0.00534838 0.95246226 0.01317659]]\n",
      "Iteration 1351, Accuracy 0.35214\n",
      "94.96735%change in label assignment\n",
      "0.060577344\n",
      "[[0.05219138 0.12483315 0.02103632 0.8019391 ]\n",
      " [0.03903182 0.05899054 0.01463655 0.88734114]\n",
      " [0.0348679  0.03518439 0.01222266 0.9177251 ]\n",
      " ...\n",
      " [0.00984222 0.95051533 0.005444   0.03419843]\n",
      " [0.02896249 0.00532818 0.9525662  0.01314317]\n",
      " [0.02900846 0.00534642 0.95246863 0.01317653]]\n",
      "Iteration 1352, Accuracy 0.35125\n",
      "95.67438%change in label assignment\n",
      "0.05775039\n",
      "[[0.03507541 0.04341468 0.01282513 0.9086848 ]\n",
      " [0.03739926 0.03283789 0.01279201 0.91697085]\n",
      " [0.05530108 0.0351709  0.01779179 0.89173627]\n",
      " ...\n",
      " [0.02018699 0.8793374  0.01050525 0.08997041]\n",
      " [0.0289011  0.00532883 0.95263755 0.0131325 ]\n",
      " [0.02897302 0.00535216 0.95249665 0.01317806]]\n",
      "Iteration 1353, Accuracy 0.34384\n",
      "93.93627%change in label assignment\n",
      "0.06208658\n",
      "[[0.05905617 0.18184274 0.02452359 0.73457754]\n",
      " [0.04497379 0.08353729 0.01735866 0.8541303 ]\n",
      " [0.03523166 0.0431627  0.01275194 0.9088537 ]\n",
      " ...\n",
      " [0.01351298 0.9356117  0.0076382  0.04323711]\n",
      " [0.02893048 0.00533524 0.9525816  0.01315276]\n",
      " [0.02900433 0.00535906 0.95243716 0.01319942]]\n",
      "Iteration 1354, Accuracy 0.35322\n",
      "96.75455%change in label assignment\n",
      "0.06128837\n",
      "[[0.04055621 0.06662479 0.01563499 0.877184  ]\n",
      " [0.0343556  0.03752384 0.01232745 0.9157931 ]\n",
      " [0.04421939 0.03304824 0.01481487 0.9079175 ]\n",
      " ...\n",
      " [0.01208205 0.93334657 0.00649132 0.04808007]\n",
      " [0.02881425 0.00532212 0.95275074 0.01311295]\n",
      " [0.02888187 0.00534483 0.9526166  0.01315681]]\n",
      "Iteration 1355, Accuracy 0.34954\n",
      "98.24716%change in label assignment\n",
      "0.05734032\n",
      "[[0.05450949 0.14168552 0.02223378 0.7815712 ]\n",
      " [0.03824339 0.0559733  0.0142711  0.8915122 ]\n",
      " [0.03554431 0.03391687 0.01233372 0.9182051 ]\n",
      " ...\n",
      " [0.00964042 0.95114    0.00531225 0.03390729]\n",
      " [0.02880089 0.0053155  0.95277953 0.01310413]\n",
      " [0.02884896 0.00533438 0.9526778  0.0131388 ]]\n",
      "Iteration 1356, Accuracy 0.34787\n",
      "95.58109%change in label assignment\n",
      "0.057218667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05251983 0.12915158 0.02143857 0.79689   ]\n",
      " [0.03511241 0.04376186 0.01285839 0.9082673 ]\n",
      " [0.04013445 0.03255815 0.01361079 0.91369665]\n",
      " ...\n",
      " [0.00994614 0.94728535 0.00541326 0.03735526]\n",
      " [0.02873524 0.00530788 0.9528757  0.01308119]\n",
      " [0.02877931 0.00532612 0.95278037 0.01311415]]\n",
      "Iteration 1357, Accuracy 0.34914\n",
      "98.08514%change in label assignment\n",
      "0.057950847\n",
      "[[0.05634636 0.15815257 0.02329805 0.7622031 ]\n",
      " [0.03463193 0.03994112 0.01243002 0.9129969 ]\n",
      " [0.04408412 0.03285185 0.01459873 0.9084653 ]\n",
      " ...\n",
      " [0.01061755 0.9428755  0.00571788 0.04078909]\n",
      " [0.0287851  0.00533403 0.95275253 0.01312843]\n",
      " [0.02888877 0.00536348 0.9525589  0.01318886]]\n",
      "Iteration 1358, Accuracy 0.3487\n",
      "93.23415%change in label assignment\n",
      "0.05387311\n",
      "[[0.06062413 0.20562902 0.02564664 0.7081002 ]\n",
      " [0.03533921 0.04393959 0.01283835 0.90788287]\n",
      " [0.0400125  0.03248632 0.01346873 0.9140325 ]\n",
      " ...\n",
      " [0.00980122 0.94836247 0.00531955 0.03651673]\n",
      " [0.02876235 0.00533305 0.95278025 0.01312433]\n",
      " [0.0288658  0.00536231 0.95258737 0.0131845 ]]\n",
      "Iteration 1359, Accuracy 0.35847\n",
      "98.54176%change in label assignment\n",
      "[[0.06093068 0.20952646 0.0258004  0.70374244]\n",
      " [0.03716979 0.05174475 0.01376184 0.89732367]\n",
      " [0.03585574 0.03354077 0.01240828 0.9181952 ]\n",
      " ...\n",
      " [0.00948334 0.95142126 0.00520653 0.03388888]\n",
      " [0.02865566 0.00530466 0.9529711  0.01306865]\n",
      " [0.02871027 0.00532467 0.95285887 0.01310623]]\n",
      "Iteration 1360, Accuracy 0.36039\n",
      "97.16699%change in label assignment\n",
      "0.062949605\n",
      "[[0.04175008 0.07143623 0.01613452 0.8706792 ]\n",
      " [0.0347632  0.03501666 0.01222316 0.91799694]\n",
      " [0.04577219 0.03316126 0.0151683  0.9058982 ]\n",
      " ...\n",
      " [0.01369828 0.9228828  0.00728449 0.05613441]\n",
      " [0.02863934 0.00530652 0.95298624 0.01306798]\n",
      " [0.02871139 0.00533026 0.9528442  0.01311417]]\n",
      "Iteration 1361, Accuracy 0.35558\n",
      "97.6187%change in label assignment\n",
      "0.061452027\n",
      "[[0.05668323 0.1594882  0.02334707 0.76048154]\n",
      " [0.03969409 0.0615775  0.01494067 0.88378775]\n",
      " [0.03441046 0.03756871 0.01224951 0.91577137]\n",
      " ...\n",
      " [0.00958595 0.9513032  0.00527633 0.03383454]\n",
      " [0.02864947 0.00530455 0.95297325 0.01307271]\n",
      " [0.02870975 0.00532606 0.9528507  0.01311348]]\n",
      "Iteration 1362, Accuracy 0.35238\n",
      "96.09172%change in label assignment\n",
      "0.06324335\n",
      "[[0.04703415 0.09710427 0.01883394 0.83702767]\n",
      " [0.03436361 0.03597405 0.01226432 0.91739804]\n",
      " [0.0437078  0.03292396 0.01470187 0.9086663 ]\n",
      " ...\n",
      " [0.01691624 0.901496   0.00893472 0.07265302]\n",
      " [0.02857235 0.00529226 0.9530955  0.01303991]\n",
      " [0.02861615 0.00531049 0.9530005  0.01307284]]\n",
      "Iteration 1363, Accuracy 0.35145\n",
      "96.31266%change in label assignment\n",
      "0.06332118\n",
      "[[0.06473345 0.33622894 0.0285312  0.5705064 ]\n",
      " [0.04987662 0.10849064 0.01968491 0.82194775]\n",
      " [0.03878171 0.05793634 0.0144986  0.88878334]\n",
      " ...\n",
      " [0.01157305 0.9436727  0.0064697  0.0382845 ]\n",
      " [0.02860411 0.00528994 0.9530564  0.01304957]\n",
      " [0.02861993 0.00530313 0.95300704 0.01306988]]\n",
      "Iteration 1364, Accuracy 0.346\n",
      "94.62366%change in label assignment\n",
      "0.07472782\n",
      "[[0.04038009 0.0670405  0.01585186 0.87672764]\n",
      " [0.03902282 0.03291768 0.01355309 0.91450644]\n",
      " [0.07529105 0.04010664 0.02379926 0.860803  ]\n",
      " ...\n",
      " [0.03459493 0.76797086 0.0176549  0.1797794 ]\n",
      " [0.02849766 0.00527506 0.9532306  0.01299665]\n",
      " [0.02849763 0.00528548 0.953207   0.0130099 ]]\n",
      "Iteration 1365, Accuracy 0.33854\n",
      "93.69077%change in label assignment\n",
      "0.080083966\n",
      "[[0.03531542 0.76605827 0.0174097  0.18121658]\n",
      " [0.06500097 0.33444571 0.02848879 0.5720645 ]\n",
      " [0.05229876 0.12361933 0.0209014  0.80318046]\n",
      " ...\n",
      " [0.02850407 0.8751778  0.01677805 0.07954013]\n",
      " [0.02861644 0.00529443 0.95302516 0.01306403]\n",
      " [0.02863422 0.00530787 0.95297277 0.01308513]]\n",
      "Iteration 1366, Accuracy 0.34065\n",
      "96.12609%change in label assignment\n",
      "0.09510767\n",
      "[[0.03751878 0.05533466 0.01454979 0.8925968 ]\n",
      " [0.05214043 0.0350696  0.01741247 0.8953775 ]\n",
      " [0.12130237 0.0485292  0.0360641  0.7941043 ]\n",
      " ...\n",
      " [0.04152507 0.70535535 0.02095027 0.23216929]\n",
      " [0.02845682 0.00527502 0.9532795  0.01298868]\n",
      " [0.02846915 0.00528767 0.95323575 0.01300749]]\n",
      "Iteration 1367, Accuracy 0.34011\n",
      "96.09172%change in label assignment\n",
      "0.0963388\n",
      "[[0.01369877 0.9241357  0.00717435 0.05499122]\n",
      " [0.04767321 0.65291226 0.0227077  0.27670687]\n",
      " [0.06109136 0.20251572 0.02542579 0.7109671 ]\n",
      " ...\n",
      " [0.04738086 0.8051899  0.02892024 0.11850898]\n",
      " [0.02867475 0.005296   0.95294696 0.01308234]\n",
      " [0.02865438 0.00530218 0.9529577  0.01308574]]\n",
      "Iteration 1368, Accuracy 0.34173\n",
      "90.16546%change in label assignment\n",
      "0.10391947\n",
      "[[0.04858036 0.10795645 0.02010971 0.82335347]\n",
      " [0.03839223 0.03334378 0.0135668  0.9146971 ]\n",
      " [0.11018589 0.04691573 0.03336979 0.8095286 ]\n",
      " ...\n",
      " [0.02598531 0.83650106 0.01359893 0.12391469]\n",
      " [0.02845909 0.00528241 0.9532516  0.01300687]\n",
      " [0.02848656 0.00529775 0.9531831  0.01303249]]\n",
      "Iteration 1369, Accuracy 0.33019\n",
      "90.31767%change in label assignment\n",
      "0.0823636\n",
      "[[0.02509415 0.8460049  0.01267596 0.11622497]\n",
      " [0.06409587 0.3943103  0.02858261 0.5130112 ]\n",
      " [0.04287482 0.0730864  0.01623576 0.86780304]\n",
      " ...\n",
      " [0.03245353 0.86000323 0.01923459 0.08830868]\n",
      " [0.02862164 0.00529901 0.95299834 0.01308097]\n",
      " [0.028641   0.00531266 0.9529436  0.0131027 ]]\n",
      "Iteration 1370, Accuracy 0.3431\n",
      "95.34541%change in label assignment\n",
      "0.08994689\n",
      "[[0.05261644 0.13429353 0.02206803 0.791022  ]\n",
      " [0.03503155 0.03448649 0.01258704 0.9178949 ]\n",
      " [0.09447352 0.04393727 0.02896086 0.83262837]\n",
      " ...\n",
      " [0.0200748  0.8794479  0.01061798 0.08985937]\n",
      " [0.02849489 0.00528612 0.9531946  0.0130243 ]\n",
      " [0.02852659 0.00530237 0.953119   0.01305206]]\n",
      "Iteration 1371, Accuracy 0.33785\n",
      "94.75131%change in label assignment\n",
      "0.07647899\n",
      "[[0.05215812 0.5994295  0.02466908 0.32374334]\n",
      " [0.05885204 0.17825481 0.02429827 0.7385949 ]\n",
      " [0.03533692 0.04216907 0.01263263 0.9098614 ]\n",
      " ...\n",
      " [0.01835106 0.9155078  0.01051857 0.05562258]\n",
      " [0.02863468 0.0053052  0.9529674  0.01309279]\n",
      " [0.0286825  0.00532449 0.95286506 0.01312804]]\n",
      "Iteration 1372, Accuracy 0.33716\n",
      "97.44194%change in label assignment\n",
      "0.07503249\n",
      "[[0.05801113 0.18299788 0.02503017 0.73396075]\n",
      " [0.03404815 0.03799617 0.0125432  0.9154124 ]\n",
      " [0.06525738 0.03787155 0.02111612 0.87575495]\n",
      " ...\n",
      " [0.01586619 0.90832376 0.00849832 0.06731166]\n",
      " [0.02853078 0.00529778 0.9531079  0.01306358]\n",
      " [0.02859113 0.00531955 0.9529846  0.01310478]]\n",
      "Iteration 1373, Accuracy 0.33603\n",
      "96.3814%change in label assignment\n",
      "0.06751139\n",
      "[[0.06168237 0.45228103 0.02815934 0.45787722]\n",
      " [0.05035762 0.11177593 0.01996703 0.81789947]\n",
      " [0.03474586 0.03839797 0.01228924 0.91456693]\n",
      " ...\n",
      " [0.01217128 0.94114774 0.00681342 0.03986756]\n",
      " [0.02862716 0.00530838 0.9529543  0.01311021]\n",
      " [0.02869181 0.00533105 0.95282364 0.01315355]]\n",
      "Iteration 1374, Accuracy 0.34114\n",
      "97.82491%change in label assignment\n",
      "0.06882952\n",
      "[[0.04481174 0.08731223 0.01805231 0.84982365]\n",
      " [0.03904284 0.03259291 0.01354691 0.91481733]\n",
      " [0.06941053 0.03864276 0.02218076 0.86976594]\n",
      " ...\n",
      " [0.02337171 0.8560893  0.01222058 0.10831836]\n",
      " [0.02857627 0.00531359 0.9530031  0.01310699]\n",
      " [0.02867466 0.00534281 0.95281637 0.01316618]]\n",
      "Iteration 1375, Accuracy 0.33692\n",
      "91.70226%change in label assignment\n",
      "0.06691211\n",
      "[[0.0380027  0.7449852  0.01853997 0.19847205]\n",
      " [0.06402207 0.24891698 0.02706045 0.6600005 ]\n",
      " [0.0423889  0.06992103 0.01592173 0.87176836]\n",
      " ...\n",
      " [0.03226387 0.8608982  0.0190556  0.08778232]\n",
      " [0.02871348 0.00534138 0.95276487 0.01318026]\n",
      " [0.02883335 0.00537466 0.95254254 0.01324949]]\n",
      "Iteration 1376, Accuracy 0.34767\n",
      "95.93951%change in label assignment\n",
      "0.09033991\n",
      "[[0.05336696 0.13885985 0.02240551 0.78536767]\n",
      " [0.03555285 0.03342135 0.01264069 0.91838515]\n",
      " [0.06095008 0.0366859  0.01984248 0.8825215 ]\n",
      " ...\n",
      " [0.01853764 0.89029634 0.00982025 0.08134577]\n",
      " [0.0285135  0.00530154 0.9530944  0.01309051]\n",
      " [0.02859052 0.00532665 0.95294315 0.01313975]]\n",
      "Iteration 1377, Accuracy 0.35179\n",
      "95.61546%change in label assignment\n",
      "0.06441313\n",
      "[[0.06460363 0.35991982 0.02869015 0.54678637]\n",
      " [0.04252103 0.07197927 0.01614261 0.8693571 ]\n",
      " [0.03477045 0.03826026 0.01228415 0.91468513]\n",
      " ...\n",
      " [0.01073381 0.94706166 0.00595001 0.03625447]\n",
      " [0.02857799 0.00530347 0.9530072  0.01311138]\n",
      " [0.0286396  0.00532558 0.9528814  0.01315338]]\n",
      "Iteration 1378, Accuracy 0.34487\n",
      "96.51888%change in label assignment\n",
      "0.0647559\n",
      "[[0.05966095 0.20191687 0.02583013 0.712592  ]\n",
      " [0.03400332 0.03986575 0.0125626  0.9135684 ]\n",
      " [0.04287755 0.03276411 0.01464853 0.9097098 ]\n",
      " ...\n",
      " [0.01335102 0.9249257  0.00718875 0.05453454]\n",
      " [0.02846384 0.00528804 0.95317745 0.01307065]\n",
      " [0.02850794 0.00530686 0.9530806  0.01310455]]\n",
      "Iteration 1379, Accuracy 0.34841\n",
      "96.00334%change in label assignment\n",
      "0.06008109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06327908 0.40148598 0.02870194 0.5065329 ]\n",
      " [0.03913626 0.06014725 0.01477444 0.88594204]\n",
      " [0.03539395 0.03349104 0.01228154 0.91883343]\n",
      " ...\n",
      " [0.00951371 0.9514401  0.00522727 0.03381895]\n",
      " [0.02855143 0.00531244 0.9530129  0.01312322]\n",
      " [0.02864167 0.00533989 0.95284003 0.01317844]]\n",
      "Iteration 1380, Accuracy 0.33859\n",
      "96.34703%change in label assignment\n",
      "0.060557593\n",
      "[[0.06157934 0.23144242 0.02683564 0.6801426 ]\n",
      " [0.03393462 0.03935145 0.01242595 0.9142879 ]\n",
      " [0.0431471  0.03259262 0.01459161 0.9096687 ]\n",
      " ...\n",
      " [0.01457659 0.9169136  0.00777195 0.0607378 ]\n",
      " [0.02844493 0.00528619 0.95320046 0.01306841]\n",
      " [0.02849331 0.00530575 0.9530967  0.01310422]]\n",
      "Iteration 1381, Accuracy 0.34315\n",
      "95.28158%change in label assignment\n",
      "0.060087383\n",
      "[[0.06069348 0.4653692  0.02810097 0.44583634]\n",
      " [0.04224962 0.07360055 0.01634845 0.86780137]\n",
      " [0.03434512 0.03481304 0.01213409 0.91870785]\n",
      " ...\n",
      " [0.00984259 0.9506317  0.00546495 0.03406082]\n",
      " [0.02851059 0.00531033 0.95306015 0.01311891]\n",
      " [0.02860582 0.00533894 0.95287853 0.01317677]]\n",
      "Iteration 1382, Accuracy 0.33549\n",
      "95.72838%change in label assignment\n",
      "0.060458474\n",
      "[[0.06385348 0.31313276 0.02860655 0.59440726]\n",
      " [0.03657209 0.0518463  0.01384552 0.89773613]\n",
      " [0.03930887 0.03217662 0.01348573 0.91502875]\n",
      " ...\n",
      " [0.01091754 0.9406368  0.00591458 0.04253102]\n",
      " [0.02841097 0.0052862  0.9532353  0.01306751]\n",
      " [0.0284716  0.00530854 0.9531103  0.01310962]]\n",
      "Iteration 1383, Accuracy 0.34895\n",
      "95.88059%change in label assignment\n",
      "0.059701882\n",
      "[[0.05705565 0.5285083  0.02686927 0.38756675]\n",
      " [0.04580005 0.09006577 0.01807249 0.84606165]\n",
      " [0.03401845 0.03621294 0.01213624 0.9176324 ]\n",
      " ...\n",
      " [0.00958119 0.95153    0.00530734 0.03358153]\n",
      " [0.02839234 0.00527446 0.95328206 0.01305109]\n",
      " [0.02841176 0.00528865 0.95322603 0.01307359]]\n",
      "Iteration 1384, Accuracy 0.34566\n",
      "95.20302%change in label assignment\n",
      "0.062315777\n",
      "[[0.06322231 0.28138313 0.02818788 0.6272067 ]\n",
      " [0.03571124 0.04886432 0.01352744 0.901897  ]\n",
      " [0.04559433 0.03306712 0.01536495 0.9059736 ]\n",
      " ...\n",
      " [0.01423611 0.91904855 0.00762344 0.05909187]\n",
      " [0.02832491 0.00526721 0.95338225 0.01302557]\n",
      " [0.02834651 0.00528212 0.9533219  0.01304947]]\n",
      "Iteration 1385, Accuracy 0.33996\n",
      "96.25865%change in label assignment\n",
      "0.06346045\n",
      "[[0.03662409 0.754225   0.01808763 0.19106321]\n",
      " [0.06239089 0.22853076 0.02648631 0.6825921 ]\n",
      " [0.0376869  0.05370321 0.01399385 0.894616  ]\n",
      " ...\n",
      " [0.01782232 0.91777754 0.01022534 0.05417478]\n",
      " [0.02837426 0.00527581 0.9532949  0.01305506]\n",
      " [0.02839999 0.00529151 0.9532275  0.0130809 ]]\n",
      "Iteration 1386, Accuracy 0.35494\n",
      "97.41248%change in label assignment\n",
      "0.07302436\n",
      "[[0.05862857 0.18950622 0.02524515 0.72662   ]\n",
      " [0.03437017 0.03423528 0.01231749 0.9190771 ]\n",
      " [0.07051489 0.03869314 0.02239032 0.8684016 ]\n",
      " ...\n",
      " [0.02500004 0.8439042  0.01300409 0.11809168]\n",
      " [0.02826361 0.00526006 0.9534724  0.01300407]\n",
      " [0.02828253 0.00527466 0.95341575 0.01302696]]\n",
      "Iteration 1387, Accuracy 0.34571\n",
      "94.01974%change in label assignment\n",
      "0.0731057\n",
      "[[0.02420673 0.8518489  0.01231973 0.11162462]\n",
      " [0.06373003 0.2585336  0.02735798 0.6503784 ]\n",
      " [0.03918643 0.05937916 0.01468253 0.8867519 ]\n",
      " ...\n",
      " [0.02277082 0.8977232  0.01325442 0.06625152]\n",
      " [0.02839553 0.00529291 0.9532272  0.01308439]\n",
      " [0.02846983 0.00531788 0.9530793  0.01313297]]\n",
      "Iteration 1388, Accuracy 0.33544\n",
      "92.1687%change in label assignment\n",
      "0.07470246\n",
      "[[0.05779513 0.18095218 0.02486539 0.7363874 ]\n",
      " [0.03510852 0.03330614 0.01251334 0.91907203]\n",
      " [0.06870715 0.03837886 0.0220103  0.8709037 ]\n",
      " ...\n",
      " [0.02110607 0.8720693  0.01111487 0.09570985]\n",
      " [0.02827463 0.00528112 0.95340484 0.01303943]\n",
      " [0.02835901 0.00530846 0.95323926 0.01309325]]\n",
      "Iteration 1389, Accuracy 0.34414\n",
      "95.16375%change in label assignment\n",
      "0.069878906\n",
      "[[0.04204752 0.7051605  0.02051927 0.23227273]\n",
      " [0.05663184 0.15767333 0.02324311 0.7624517 ]\n",
      " [0.03569418 0.04577412 0.01302891 0.90550286]\n",
      " ...\n",
      " [0.01777189 0.9179125  0.01021197 0.05410364]\n",
      " [0.02839047 0.0053012  0.9532117  0.01309658]\n",
      " [0.02848847 0.0053308  0.9530244  0.01315639]]\n",
      "Iteration 1390, Accuracy 0.34148\n",
      "97.03442%change in label assignment\n",
      "0.06540884\n",
      "[[0.06352238 0.3030674  0.02851161 0.6048986 ]\n",
      " [0.03416031 0.04217773 0.01269454 0.9109674 ]\n",
      " [0.04468143 0.03283205 0.01508484 0.90740174]\n",
      " ...\n",
      " [0.01117829 0.93881387 0.00607692 0.04393096]\n",
      " [0.02821958 0.00526784 0.9534951  0.01301742]\n",
      " [0.02827178 0.0052888  0.95338345 0.01305594]]\n",
      "Iteration 1391, Accuracy 0.3463\n",
      "96.90185%change in label assignment\n",
      "0.05865045\n",
      "[[0.06314209 0.394343   0.02878969 0.5137252 ]\n",
      " [0.03924965 0.06198243 0.01500336 0.88376456]\n",
      " [0.03605856 0.03257913 0.0125081  0.9188543 ]\n",
      " ...\n",
      " [0.00935273 0.9520427  0.0051689  0.03343565]\n",
      " [0.02824311 0.00527725 0.9534415  0.01303812]\n",
      " [0.02831801 0.00530274 0.9532917  0.01308752]]\n",
      "Iteration 1392, Accuracy 0.33844\n",
      "98.75779%change in label assignment\n",
      "0.05581956\n",
      "[[0.06306512 0.27116883 0.02785601 0.63791007]\n",
      " [0.03442623 0.04308606 0.01275006 0.90973765]\n",
      " [0.04109183 0.03218358 0.01396781 0.9127568 ]\n",
      " ...\n",
      " [0.0114194  0.9372753  0.00617425 0.04513103]\n",
      " [0.02816816 0.00526059 0.95356774 0.01300353]\n",
      " [0.02821522 0.00528066 0.9534644  0.01303977]]\n",
      "Iteration 1393, Accuracy 0.34099\n",
      "97.5205%change in label assignment\n",
      "0.05697213\n",
      "[[0.05680752 0.5295934  0.02688289 0.3867162 ]\n",
      " [0.04472699 0.08598388 0.01768216 0.85160697]\n",
      " [0.03381266 0.03554997 0.01211159 0.91852576]\n",
      " ...\n",
      " [0.0102014  0.94924337 0.00572042 0.03483479]\n",
      " [0.02816021 0.00526237 0.9535678  0.01300965]\n",
      " [0.02820987 0.00528291 0.95346016 0.0130471 ]]\n",
      "Iteration 1394, Accuracy 0.33603\n",
      "97.13753%change in label assignment\n",
      "0.057435133\n",
      "[[0.06373855 0.3148077  0.02859793 0.59285575]\n",
      " [0.03412639 0.04173873 0.01260436 0.9115305 ]\n",
      " [0.04223597 0.03232418 0.01431115 0.9111287 ]\n",
      " ...\n",
      " [0.01211961 0.9327365  0.00653072 0.04861319]\n",
      " [0.0280877  0.00525457 0.95367336 0.0129844 ]\n",
      " [0.02813631 0.00527487 0.95356756 0.01302128]]\n",
      "Iteration 1395, Accuracy 0.34305\n",
      "98.35518%change in label assignment\n",
      "0.057607103\n",
      "[[0.05960821 0.48385873 0.02781141 0.42872164]\n",
      " [0.0382186  0.05772953 0.01449207 0.88955975]\n",
      " [0.0348396  0.03342898 0.01222078 0.91951066]\n",
      " ...\n",
      " [0.00932636 0.9517342  0.0051309  0.03380859]\n",
      " [0.02807998 0.00525539 0.9536774  0.01298728]\n",
      " [0.02812942 0.0052757  0.9535705  0.01302439]]\n",
      "Iteration 1396, Accuracy 0.33844\n",
      "98.0164%change in label assignment\n",
      "0.056362372\n",
      "[[0.06089399 0.45207858 0.02839813 0.45862928]\n",
      " [0.03920348 0.06277823 0.01514386 0.8828744 ]\n",
      " [0.03443467 0.03357654 0.0122354  0.9197534 ]\n",
      " ...\n",
      " [0.00930194 0.95193434 0.00515592 0.03360784]\n",
      " [0.02803513 0.00523712 0.9537728  0.01295497]\n",
      " [0.02802583 0.00524651 0.9537627  0.01296494]]\n",
      "Iteration 1397, Accuracy 0.33888\n",
      "92.25708%change in label assignment\n",
      "0.058976047\n",
      "[[0.06365392 0.3101145  0.02850937 0.5977222 ]\n",
      " [0.0340117  0.04129395 0.01253934 0.91215503]\n",
      " [0.04380295 0.03256495 0.01474332 0.9088888 ]\n",
      " ...\n",
      " [0.01266445 0.9291605  0.00680603 0.05136898]\n",
      " [0.0279721  0.00523635 0.953852   0.01293963]\n",
      " [0.02798951 0.00525086 0.95379746 0.01296212]]\n",
      "Iteration 1398, Accuracy 0.32911\n",
      "96.131%change in label assignment\n",
      "0.05751677\n",
      "[[0.05711277 0.5250113  0.02694158 0.3909343 ]\n",
      " [0.04063988 0.06780361 0.01567921 0.8758773 ]\n",
      " [0.03567189 0.03274207 0.01241355 0.91917247]\n",
      " ...\n",
      " [0.00931672 0.9520931  0.00514508 0.03344505]\n",
      " [0.02799129 0.00525409 0.9537858  0.01296876]\n",
      " [0.02805769 0.00527781 0.95365053 0.01301402]]\n",
      "Iteration 1399, Accuracy 0.3324\n",
      "95.57618%change in label assignment\n",
      "[[0.06223959 0.41835892 0.02869286 0.49070853]\n",
      " [0.03371692 0.03825317 0.0122363  0.9157936 ]\n",
      " [0.04415472 0.03259664 0.01478423 0.90846443]\n",
      " ...\n",
      " [0.01342032 0.9242899  0.00717509 0.05511471]\n",
      " [0.0279745  0.00526269 0.9537846  0.01297818]\n",
      " [0.02806238 0.00529    0.9536148  0.01303285]]\n",
      "Iteration 1400, Accuracy 0.34423\n",
      "93.09668%change in label assignment\n",
      "0.06316317\n",
      "[[0.03698544 0.7476771  0.01852241 0.1968151 ]\n",
      " [0.05029275 0.11703774 0.02051046 0.812159  ]\n",
      " [0.0335501  0.03755066 0.01219415 0.91670513]\n",
      " ...\n",
      " [0.00929132 0.9517533  0.00512983 0.03382562]\n",
      " [0.02798205 0.0052554  0.95378566 0.01297683]\n",
      " [0.02805014 0.00527874 0.9536491  0.01302203]]\n",
      "Iteration 1401, Accuracy 0.3485\n",
      "91.78082%change in label assignment\n",
      "0.06680775\n",
      "[[0.06330459 0.29133987 0.02831624 0.6170394 ]\n",
      " [0.03712595 0.03209526 0.01290887 0.9178699 ]\n",
      " [0.06115587 0.03641546 0.01973612 0.88269264]\n",
      " ...\n",
      " [0.02977381 0.8069784  0.01528448 0.14796333]\n",
      " [0.02799123 0.00527079 0.9537382  0.01299977]\n",
      " [0.02809348 0.00530056 0.95354503 0.01306084]]\n",
      "Iteration 1402, Accuracy 0.34414\n",
      "91.72681%change in label assignment\n",
      "0.06988496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02561973 0.84065497 0.01307079 0.12065451]\n",
      " [0.04967686 0.10991633 0.01987207 0.82053477]\n",
      " [0.03767274 0.05528063 0.01419172 0.8928549 ]\n",
      " ...\n",
      " [0.01351614 0.9355571  0.00767919 0.04324758]\n",
      " [0.02814509 0.00530512 0.95346117 0.01308857]\n",
      " [0.02827969 0.00534142 0.95321345 0.01316534]]\n",
      "Iteration 1403, Accuracy 0.34674\n",
      "95.72347%change in label assignment\n",
      "0.074609436\n",
      "[[0.06190926 0.249986   0.0275769  0.6605278 ]\n",
      " [0.04010795 0.03222018 0.01391746 0.9137544 ]\n",
      " [0.06293448 0.03709006 0.02055537 0.87942004]\n",
      " ...\n",
      " [0.03085027 0.79792637 0.01593568 0.15528767]\n",
      " [0.02797322 0.00527242 0.95375144 0.01300293]\n",
      " [0.02807994 0.00530311 0.9535507  0.01306622]]\n",
      "Iteration 1404, Accuracy 0.35641\n",
      "96.70545%change in label assignment\n",
      "0.07313348\n",
      "[[0.01006959 0.9468408  0.00544638 0.03764315]\n",
      " [0.06416922 0.37190202 0.02878065 0.5351481 ]\n",
      " [0.05116129 0.11898591 0.0206262  0.80922663]\n",
      " ...\n",
      " [0.02351381 0.8946564  0.01376938 0.06806046]\n",
      " [0.02800507 0.0052602  0.95372677 0.01300798]\n",
      " [0.02806977 0.00528276 0.953596   0.01305151]]\n",
      "Iteration 1405, Accuracy 0.35037\n",
      "89.86105%change in label assignment\n",
      "0.09263964\n",
      "[[0.06000149 0.21520157 0.02664525 0.6981517 ]\n",
      " [0.05228519 0.03487444 0.01770959 0.89513075]\n",
      " [0.10297277 0.04562456 0.03194586 0.81945676]\n",
      " ...\n",
      " [0.04275898 0.69164723 0.02169746 0.24389635]\n",
      " [0.02786033 0.00525209 0.95393234 0.01295528]\n",
      " [0.02794481 0.00527866 0.95376825 0.01300829]]\n",
      "Iteration 1406, Accuracy 0.34173\n",
      "92.84627%change in label assignment\n",
      "0.08813754\n",
      "[[0.00982065 0.9501433  0.00539269 0.03464339]\n",
      " [0.0639572  0.38529864 0.02875469 0.5219895 ]\n",
      " [0.04835768 0.10205802 0.01918535 0.8303989 ]\n",
      " ...\n",
      " [0.03090419 0.8657875  0.01838773 0.08492061]\n",
      " [0.02809438 0.00529253 0.9535381  0.013075  ]\n",
      " [0.02820837 0.00532498 0.9533243  0.01314231]]\n",
      "Iteration 1407, Accuracy 0.34448\n",
      "95.75784%change in label assignment\n",
      "0.09209472\n",
      "[[0.06179078 0.41563445 0.02913286 0.49344185]\n",
      " [0.03487016 0.03322072 0.01260728 0.9193018 ]\n",
      " [0.07291782 0.03950549 0.02352239 0.8640543 ]\n",
      " ...\n",
      " [0.02470155 0.84541833 0.01300393 0.11687625]\n",
      " [0.0278846  0.00525648 0.95388454 0.01297439]\n",
      " [0.02796703 0.0052825  0.95372415 0.01302628]]\n",
      "Iteration 1408, Accuracy 0.35278\n",
      "95.06555%change in label assignment\n",
      "0.0674855\n",
      "[[0.02351636 0.8560967  0.01205486 0.1083321 ]\n",
      " [0.05292711 0.1304786  0.02146601 0.7951282 ]\n",
      " [0.03448847 0.04053828 0.01244392 0.91252935]\n",
      " ...\n",
      " [0.01121424 0.9450836  0.00628866 0.03741345]\n",
      " [0.02796444 0.00525671 0.9537812  0.01299763]\n",
      " [0.0280254  0.00527823 0.9536574  0.01303897]]\n",
      "Iteration 1409, Accuracy 0.35013\n",
      "96.28811%change in label assignment\n",
      "0.06778929\n",
      "[[0.06308024 0.3020794  0.02875131 0.60608906]\n",
      " [0.03966268 0.03224865 0.01388761 0.91420114]\n",
      " [0.06626213 0.03794321 0.02162083 0.8741739 ]\n",
      " ...\n",
      " [0.02968304 0.80699503 0.01543432 0.14788763]\n",
      " [0.02785039 0.005246   0.953944   0.01295964]\n",
      " [0.02791523 0.00526845 0.95381325 0.01300303]]\n",
      "Iteration 1410, Accuracy 0.35204\n",
      "97.53523%change in label assignment\n",
      "0.07151003\n",
      "[[0.01309399 0.92697453 0.00695505 0.05297642]\n",
      " [0.06070339 0.20880961 0.02581237 0.7046746 ]\n",
      " [0.03837604 0.05830492 0.0145544  0.8887646 ]\n",
      " ...\n",
      " [0.01748818 0.91898435 0.01009089 0.05343656]\n",
      " [0.02801282 0.00527937 0.95365775 0.01305009]\n",
      " [0.02811074 0.00530843 0.95347136 0.01310948]]\n",
      "Iteration 1411, Accuracy 0.35096\n",
      "94.55001%change in label assignment\n",
      "0.077266455\n",
      "[[0.06134694 0.42889038 0.02898022 0.48078245]\n",
      " [0.03803072 0.03215409 0.01341644 0.9163987 ]\n",
      " [0.06588048 0.03783388 0.02151327 0.87477237]\n",
      " ...\n",
      " [0.02500129 0.84312814 0.01312813 0.1187425 ]\n",
      " [0.02790031 0.00526974 0.95382035 0.01300957]\n",
      " [0.0280022  0.00529948 0.95362765 0.01307066]]\n",
      "Iteration 1412, Accuracy 0.34747\n",
      "94.72676%change in label assignment\n",
      "0.06715767\n",
      "[[0.01956923 0.8840569  0.01013584 0.086238  ]\n",
      " [0.05282453 0.12950517 0.02139756 0.7962727 ]\n",
      " [0.03544737 0.04568983 0.01300896 0.9058538 ]\n",
      " ...\n",
      " [0.01376865 0.93444484 0.00781637 0.04397012]\n",
      " [0.0279914  0.00527844 0.9536857  0.01304445]\n",
      " [0.02809202 0.00530813 0.95349467 0.01310525]]\n",
      "Iteration 1413, Accuracy 0.35538\n",
      "96.82329%change in label assignment\n",
      "0.07114452\n",
      "[[0.05971825 0.46723804 0.02847075 0.444573  ]\n",
      " [0.03460744 0.03305955 0.01249657 0.9198364 ]\n",
      " [0.05380391 0.03497972 0.01805006 0.89316624]\n",
      " ...\n",
      " [0.02005482 0.87899226 0.01066123 0.09029164]\n",
      " [0.02783273 0.00525552 0.95393044 0.01298131]\n",
      " [0.02791581 0.00528164 0.9537689  0.01303357]]\n",
      "Iteration 1414, Accuracy 0.35307\n",
      "97.18172%change in label assignment\n",
      "0.06360569\n",
      "[[0.02523897 0.84376097 0.01286452 0.11813555]\n",
      " [0.04687882 0.09426174 0.01844508 0.8404144 ]\n",
      " [0.0344162  0.04039463 0.01242318 0.912766  ]\n",
      " ...\n",
      " [0.01143149 0.9441654  0.00640623 0.03799691]\n",
      " [0.02792758 0.00527176 0.9537725  0.01302824]\n",
      " [0.0280222  0.00530018 0.9535916  0.01308607]]\n",
      "Iteration 1415, Accuracy 0.34978\n",
      "98.11951%change in label assignment\n",
      "0.06893289\n",
      "[[0.06327943 0.32794654 0.02912666 0.57964736]\n",
      " [0.04555232 0.0330589  0.01563072 0.9057581 ]\n",
      " [0.06901924 0.03853671 0.02247219 0.8699719 ]\n",
      " ...\n",
      " [0.04091    0.7097315  0.020764   0.2285945 ]\n",
      " [0.02776831 0.00523419 0.9540467  0.01295083]\n",
      " [0.02782108 0.00525413 0.95393646 0.01298834]]\n",
      "Iteration 1416, Accuracy 0.35165\n",
      "95.62037%change in label assignment\n",
      "0.079842426\n",
      "[[0.01830358 0.91572994 0.01045933 0.05550717]\n",
      " [0.0593195  0.5068178  0.02731916 0.40654352]\n",
      " [0.05897035 0.17623071 0.02432999 0.7404689 ]\n",
      " ...\n",
      " [0.04504484 0.8135509  0.02744059 0.11396365]\n",
      " [0.02790634 0.00525809 0.9538164  0.01301912]\n",
      " [0.02796109 0.00527836 0.953703   0.01305758]]\n",
      "Iteration 1417, Accuracy 0.35278\n",
      "95.19811%change in label assignment\n",
      "0.0975166\n",
      "[[0.03429998 0.76888996 0.01758485 0.1792252 ]\n",
      " [0.03838436 0.06153389 0.01516381 0.88491786]\n",
      " [0.0423826  0.03252597 0.01470956 0.91038185]\n",
      " ...\n",
      " [0.01020173 0.945161   0.00565428 0.03898302]\n",
      " [0.0276964  0.00522572 0.95414776 0.01293012]\n",
      " [0.02772059 0.00523992 0.9540857  0.01295385]]\n",
      "Iteration 1418, Accuracy 0.35268\n",
      "96.59744%change in label assignment\n",
      "0.062306613\n",
      "[[0.03111944 0.79775083 0.01571075 0.15541892]\n",
      " [0.04230282 0.07459123 0.0164285  0.86667746]\n",
      " [0.03771459 0.03208818 0.01287304 0.91732424]\n",
      " ...\n",
      " [0.00971809 0.94864035 0.00529716 0.03634436]\n",
      " [0.0277646  0.00522849 0.95405895 0.0129479 ]\n",
      " [0.02778665 0.00524214 0.95400083 0.01297043]]\n",
      "Iteration 1419, Accuracy 0.34909\n",
      "98.81671%change in label assignment\n",
      "0.058640845\n",
      "[[0.03813877 0.73577356 0.01924871 0.20683894]\n",
      " [0.03922315 0.06457926 0.01541041 0.88078713]\n",
      " [0.03990378 0.03189133 0.01380041 0.9144045 ]\n",
      " ...\n",
      " [0.01160196 0.9358142  0.00631565 0.04626824]\n",
      " [0.0277165  0.00522485 0.9541207  0.01293793]\n",
      " [0.0277411  0.00523923 0.95405763 0.01296199]]\n",
      "Iteration 1420, Accuracy 0.34811\n",
      "98.07041%change in label assignment\n",
      "0.059001856\n",
      "[[0.03405626 0.77238744 0.0172088  0.1763475 ]\n",
      " [0.03611413 0.05138598 0.01370718 0.8987927 ]\n",
      " [0.04137826 0.03196013 0.01404036 0.9126212 ]\n",
      " ...\n",
      " [0.01239    0.93086404 0.00666399 0.05008198]\n",
      " [0.02774313 0.00524087 0.954048   0.01296803]\n",
      " [0.02780187 0.00526156 0.9539287  0.0130079 ]]\n",
      "Iteration 1421, Accuracy 0.34247\n",
      "96.55325%change in label assignment\n",
      "0.057630017\n",
      "[[0.02059804 0.87565196 0.01077263 0.09297747]\n",
      " [0.04303586 0.07999849 0.01708961 0.85987604]\n",
      " [0.03429046 0.03289029 0.01218389 0.9206354 ]\n",
      " ...\n",
      " [0.00934081 0.9508693  0.00514905 0.03464082]\n",
      " [0.02773268 0.00523985 0.9540575  0.01296998]\n",
      " [0.02778854 0.00525988 0.95394325 0.01300837]]\n",
      "Iteration 1422, Accuracy 0.34654\n",
      "98.60068%change in label assignment\n",
      "0.058134545\n",
      "[[0.03296018 0.7811744  0.01675127 0.16911411]\n",
      " [0.0363965  0.05293932 0.01392915 0.8967351 ]\n",
      " [0.03831809 0.03168396 0.01325543 0.9167425 ]\n",
      " ...\n",
      " [0.01260025 0.92941874 0.00678979 0.05119121]\n",
      " [0.02768678 0.00523259 0.9541277  0.01295286]\n",
      " [0.02773707 0.00525149 0.95402294 0.01298854]]\n",
      "Iteration 1423, Accuracy 0.34507\n",
      "98.37973%change in label assignment\n",
      "0.05743468\n",
      "[[0.02305423 0.8581749  0.01196387 0.10680699]\n",
      " [0.04150414 0.07305504 0.01631163 0.86912924]\n",
      " [0.03399061 0.03325019 0.01210065 0.9206585 ]\n",
      " ...\n",
      " [0.00964851 0.94867563 0.00528785 0.03638797]\n",
      " [0.02768006 0.00523567 0.95412576 0.01295855]\n",
      " [0.02773589 0.00525561 0.9540116  0.01299683]]\n",
      "Iteration 1424, Accuracy 0.34585\n",
      "98.81671%change in label assignment\n",
      "0.058961302\n",
      "[[0.03204882 0.78829336 0.01637224 0.16328561]\n",
      " [0.03522359 0.0487712  0.01343174 0.9025735 ]\n",
      " [0.03851221 0.03169868 0.01338103 0.91640806]\n",
      " ...\n",
      " [0.01361393 0.9227328  0.00732627 0.056327  ]\n",
      " [0.02763255 0.00522963 0.9541962  0.01294165]\n",
      " [0.02768441 0.00524856 0.95408934 0.01297773]]\n",
      "Iteration 1425, Accuracy 0.3489\n",
      "98.39937%change in label assignment\n",
      "0.058400773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01273888 0.928785   0.00681522 0.05166084]\n",
      " [0.05613888 0.16140568 0.02356191 0.7588936 ]\n",
      " [0.03437384 0.04403123 0.01279631 0.9087986 ]\n",
      " ...\n",
      " [0.01025117 0.94910276 0.00576693 0.03487916]\n",
      " [0.02762753 0.00521021 0.95424384 0.01291841]\n",
      " [0.02762595 0.00521886 0.9542261  0.01292919]]\n",
      "Iteration 1426, Accuracy 0.3462\n",
      "90.83812%change in label assignment\n",
      "0.06422425\n",
      "[[0.04963038 0.6175562  0.02449166 0.30832174]\n",
      " [0.03339192 0.04121168 0.01261443 0.91278195]\n",
      " [0.05271873 0.03451261 0.01767061 0.89509803]\n",
      " ...\n",
      " [0.02407873 0.8498041  0.01265424 0.113463  ]\n",
      " [0.02752625 0.00520228 0.9543873  0.01288407]\n",
      " [0.02753778 0.00521365 0.95434725 0.01290129]]\n",
      "Iteration 1427, Accuracy 0.32725\n",
      "97.06388%change in label assignment\n",
      "0.067016855\n",
      "[[0.01073504 0.94709814 0.00602247 0.03614438]\n",
      " [0.06408209 0.31874663 0.02852757 0.58864367]\n",
      " [0.03880748 0.06089592 0.01486293 0.8854337 ]\n",
      " ...\n",
      " [0.01789768 0.9172773  0.01037027 0.05445477]\n",
      " [0.02762571 0.0052306  0.9541941  0.01294957]\n",
      " [0.02767366 0.0052488  0.9540937  0.01298386]]\n",
      "Iteration 1428, Accuracy 0.33211\n",
      "94.26032%change in label assignment\n",
      "0.07394591\n",
      "[[0.05274767 0.5781634  0.02583969 0.34324923]\n",
      " [0.03333945 0.04114886 0.01262551 0.91288614]\n",
      " [0.06195072 0.03677973 0.02038226 0.88088727]\n",
      " ...\n",
      " [0.02560948 0.8381985  0.01343323 0.12275881]\n",
      " [0.02749093 0.0052045  0.9544247  0.01287984]\n",
      " [0.02751949 0.00521951 0.95435536 0.0129056 ]]\n",
      "Iteration 1429, Accuracy 0.34281\n",
      "96.01316%change in label assignment\n",
      "0.07217817\n",
      "[[0.01043954 0.94821954 0.0058258  0.03551506]\n",
      " [0.06259692 0.42026007 0.02861919 0.4885239 ]\n",
      " [0.04154933 0.0714377  0.01609405 0.87091887]\n",
      " ...\n",
      " [0.02405512 0.8924002  0.01414645 0.06939833]\n",
      " [0.02761603 0.00523499 0.9541905  0.01295837]\n",
      " [0.02767954 0.00525713 0.95406204 0.01300128]]\n",
      "Iteration 1430, Accuracy 0.33328\n",
      "95.02136%change in label assignment\n",
      "0.072262615\n",
      "[[0.04320667 0.6865261  0.02165833 0.24860895]\n",
      " [0.03521934 0.04965423 0.01360486 0.9015216 ]\n",
      " [0.05741699 0.03563223 0.01902257 0.88792825]\n",
      " ...\n",
      " [0.01731832 0.89774936 0.00926324 0.07566901]\n",
      " [0.02753621 0.00523912 0.95428306 0.01294165]\n",
      " [0.02762908 0.00526719 0.95410496 0.01299877]]\n",
      "Iteration 1431, Accuracy 0.34369\n",
      "91.8201%change in label assignment\n",
      "0.06309833\n",
      "[[0.01845892 0.8909155  0.00965829 0.08096728]\n",
      " [0.05198964 0.12770516 0.02129774 0.7990075 ]\n",
      " [0.03373193 0.03455769 0.01201109 0.9196993 ]\n",
      " ...\n",
      " [0.00965454 0.95133996 0.00538348 0.03362207]\n",
      " [0.02758324 0.00524417 0.9542085  0.0129641 ]\n",
      " [0.02767574 0.00527218 0.954031   0.01302108]]\n",
      "Iteration 1432, Accuracy 0.34674\n",
      "97.38302%change in label assignment\n",
      "0.05613945\n",
      "[[0.03210464 0.7875257  0.01641778 0.1639519 ]\n",
      " [0.03783974 0.05933489 0.01473541 0.8880899 ]\n",
      " [0.0377604  0.03161817 0.01316984 0.91745156]\n",
      " ...\n",
      " [0.01222018 0.9316963  0.00662612 0.04945737]\n",
      " [0.02742316 0.00519504 0.9545143  0.01286754]\n",
      " [0.0274385  0.00520702 0.95446813 0.01288644]]\n",
      "Iteration 1433, Accuracy 0.34728\n",
      "94.74149%change in label assignment\n",
      "0.05676486\n",
      "[[0.02261405 0.8611122  0.01176676 0.10450711]\n",
      " [0.04824482 0.10662945 0.01964054 0.82548517]\n",
      " [0.03310524 0.03518472 0.01199008 0.91971993]\n",
      " ...\n",
      " [0.00915029 0.9528205  0.00509021 0.03293901]\n",
      " [0.02740362 0.00519493 0.9545332  0.01286826]\n",
      " [0.02741903 0.00520724 0.95448613 0.01288761]]\n",
      "Iteration 1434, Accuracy 0.34148\n",
      "98.85108%change in label assignment\n",
      "0.055562053\n",
      "[[0.03416665 0.770257   0.01738118 0.17819524]\n",
      " [0.03971846 0.06699428 0.01564507 0.8776421 ]\n",
      " [0.03651567 0.03168442 0.01283208 0.91896784]\n",
      " ...\n",
      " [0.01118218 0.9383867  0.00609344 0.04433776]\n",
      " [0.02733752 0.00519337 0.95461553 0.01285361]\n",
      " [0.02736575 0.00520835 0.9545466  0.0128793 ]]\n",
      "Iteration 1435, Accuracy 0.34158\n",
      "97.85437%change in label assignment\n",
      "0.05562222\n",
      "[[0.02939566 0.80981016 0.01504657 0.14574765]\n",
      " [0.04329569 0.08168302 0.01723173 0.85778964]\n",
      " [0.03435025 0.03258754 0.01218827 0.92087394]\n",
      " ...\n",
      " [0.0096281  0.9486579  0.00528689 0.03642707]\n",
      " [0.02732247 0.00519236 0.95463526 0.01284992]\n",
      " [0.02735174 0.00520777 0.95456403 0.01287641]]\n",
      "Iteration 1436, Accuracy 0.34487\n",
      "99.47955%change in label assignment\n",
      "0.05462748\n",
      "[[0.03177314 0.790222   0.01623025 0.16177456]\n",
      " [0.04652707 0.09844961 0.018924   0.8360993 ]\n",
      " [0.03382192 0.03289703 0.01212799 0.92115307]\n",
      " ...\n",
      " [0.00930087 0.95091593 0.00514685 0.03463634]\n",
      " [0.02728497 0.0051922  0.9546798  0.0128431 ]\n",
      " [0.02732175 0.00520926 0.95459557 0.01287342]]\n",
      "Iteration 1437, Accuracy 0.34517\n",
      "99.13586%change in label assignment\n",
      "0.05586692\n",
      "[[0.03788463 0.73749745 0.01910811 0.2055098 ]\n",
      " [0.04147886 0.07471875 0.01651154 0.86729085]\n",
      " [0.03813181 0.03161523 0.01329504 0.916958  ]\n",
      " ...\n",
      " [0.0110937  0.9389043  0.00604594 0.0439561 ]\n",
      " [0.02734455 0.00522753 0.9545245  0.01290342]\n",
      " [0.02744351 0.00525709 0.9543354  0.01296392]]\n",
      "Iteration 1438, Accuracy 0.34286\n",
      "94.84951%change in label assignment\n",
      "0.053526632\n",
      "[[0.02333887 0.8557726  0.01211174 0.10877683]\n",
      " [0.04955563 0.11453986 0.02028356 0.815621  ]\n",
      " [0.03332988 0.03416246 0.01198428 0.92052335]\n",
      " ...\n",
      " [0.00916551 0.95283806 0.00510035 0.03289609]\n",
      " [0.02725939 0.00520077 0.95468956 0.01285032]\n",
      " [0.02732071 0.00522273 0.95456403 0.01289252]]\n",
      "Iteration 1439, Accuracy 0.34836\n",
      "96.83802%change in label assignment\n",
      "[[0.02156739 0.8680008  0.01132907 0.09910281]\n",
      " [0.04552118 0.09406455 0.01853669 0.8418775 ]\n",
      " [0.03444887 0.03228365 0.01232949 0.92093796]\n",
      " ...\n",
      " [0.00938168 0.9502639  0.00519942 0.03515496]\n",
      " [0.02717393 0.00518236 0.9548335  0.01281018]\n",
      " [0.02721074 0.00519943 0.9547493  0.01284053]]\n",
      "Iteration 1440, Accuracy 0.34517\n",
      "97.97221%change in label assignment\n",
      "0.061797515\n",
      "[[0.0411708  0.70717376 0.0205779  0.23107763]\n",
      " [0.03509106 0.04880397 0.01337258 0.9027324 ]\n",
      " [0.04410668 0.03237414 0.01495559 0.90856355]\n",
      " ...\n",
      " [0.01647871 0.90350235 0.00877169 0.07124729]\n",
      " [0.02717711 0.00518347 0.9548269  0.01281249]\n",
      " [0.02722219 0.00520228 0.9547286  0.01284693]]\n",
      "Iteration 1441, Accuracy 0.34193\n",
      "98.92964%change in label assignment\n",
      "0.06119502\n",
      "[[0.00997283 0.94633394 0.0054522  0.03824106]\n",
      " [0.05978386 0.20840964 0.02589981 0.70590675]\n",
      " [0.03363951 0.04241905 0.01260206 0.9113394 ]\n",
      " ...\n",
      " [0.00923733 0.9528078  0.00516738 0.03278751]\n",
      " [0.02719163 0.0051756  0.9548201  0.01281271]\n",
      " [0.02720621 0.00518783 0.9547742  0.0128318 ]]\n",
      "Iteration 1442, Accuracy 0.34129\n",
      "93.52384%change in label assignment\n",
      "0.06869002\n",
      "[[0.04918299 0.62013334 0.02444372 0.30623987]\n",
      " [0.03258015 0.03763615 0.01227374 0.91751   ]\n",
      " [0.06921986 0.03857796 0.02264071 0.8695615 ]\n",
      " ...\n",
      " [0.03124975 0.7931397  0.0162309  0.15937963]\n",
      " [0.02715532 0.00519216 0.95483005 0.0128225 ]\n",
      " [0.02722444 0.00521577 0.9546911  0.01286858]]\n",
      "Iteration 1443, Accuracy 0.33235\n",
      "93.17032%change in label assignment\n",
      "0.075673535\n",
      "[[0.0187623  0.9137065  0.01085996 0.0566713 ]\n",
      " [0.05651756 0.53718305 0.02665033 0.37964898]\n",
      " [0.04852779 0.1055217  0.01950178 0.82644874]\n",
      " ...\n",
      " [0.02658601 0.8823488  0.01573053 0.07533468]\n",
      " [0.02727872 0.00520953 0.95462674 0.01288499]\n",
      " [0.02734879 0.00523333 0.9544862  0.01293162]]\n",
      "Iteration 1444, Accuracy 0.34148\n",
      "96.2341%change in label assignment\n",
      "0.09631042\n",
      "[[0.05107512 0.5958489  0.02547521 0.3276007 ]\n",
      " [0.03277797 0.03635308 0.01241014 0.9184589 ]\n",
      " [0.09652729 0.04460952 0.03066672 0.82819647]\n",
      " ...\n",
      " [0.03834629 0.73112833 0.01978852 0.21073687]\n",
      " [0.02711213 0.00518231 0.9549082  0.01279731]\n",
      " [0.02716866 0.00520351 0.95479035 0.01283742]]\n",
      "Iteration 1445, Accuracy 0.34453\n",
      "95.28649%change in label assignment\n",
      "0.086118355\n",
      "[[0.0307595  0.866083   0.01838669 0.08477082]\n",
      " [0.0332117  0.7807809  0.01670399 0.16930337]\n",
      " [0.0528538  0.13361046 0.02173591 0.79179984]\n",
      " ...\n",
      " [0.03151283 0.86315924 0.01888803 0.0864399 ]\n",
      " [0.02724812 0.00518871 0.9547067  0.01285646]\n",
      " [0.02727363 0.00520374 0.954641   0.01288168]]\n",
      "Iteration 1446, Accuracy 0.33844\n",
      "89.51245%change in label assignment\n",
      "0.102164745\n",
      "[[0.04360886 0.67982036 0.02227129 0.2542995 ]\n",
      " [0.03376044 0.04423338 0.01322898 0.9087772 ]\n",
      " [0.10695623 0.04663339 0.0337557  0.8126546 ]\n",
      " ...\n",
      " [0.03783887 0.7356692  0.01962526 0.20686664]\n",
      " [0.02711378 0.0051853  0.9548939  0.01280703]\n",
      " [0.02717548 0.00520782 0.95476663 0.01285004]]\n",
      "Iteration 1447, Accuracy 0.33382\n",
      "91.57461%change in label assignment\n",
      "0.09034295\n",
      "[[0.04040302 0.8299073  0.02457485 0.10511482]\n",
      " [0.02363561 0.8552126  0.0121434  0.1090084 ]\n",
      " [0.05211726 0.1267984  0.02123131 0.79985297]\n",
      " ...\n",
      " [0.03651512 0.84427387 0.02206049 0.09715048]\n",
      " [0.02727709 0.00519295 0.954658   0.01287198]\n",
      " [0.02730076 0.00520745 0.9545957  0.01289613]]\n",
      "Iteration 1448, Accuracy 0.34212\n",
      "90.20965%change in label assignment\n",
      "0.098224245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01659364 0.9023123  0.00901292 0.07208115]\n",
      " [0.04810028 0.11135197 0.02044367 0.82010406]\n",
      " [0.07027038 0.03918154 0.02332988 0.86721826]\n",
      " ...\n",
      " [0.02072654 0.87343997 0.01114431 0.09468914]\n",
      " [0.02711579 0.0051783  0.95489836 0.01280758]\n",
      " [0.02714946 0.00519461 0.9548196  0.01283625]]\n",
      "Iteration 1449, Accuracy 0.33515\n",
      "94.47636%change in label assignment\n",
      "0.074347645\n",
      "[[0.02026135 0.90750146 0.01179777 0.06043945]\n",
      " [0.05774153 0.5151734  0.02718475 0.39990032]\n",
      " [0.03452083 0.04371313 0.01275764 0.90900844]\n",
      " ...\n",
      " [0.01605813 0.92475057 0.00923948 0.0499518 ]\n",
      " [0.0272767  0.00520856 0.95461744 0.01289725]\n",
      " [0.02733946 0.00523071 0.9544896  0.01294014]]\n",
      "Iteration 1450, Accuracy 0.33859\n",
      "96.45505%change in label assignment\n",
      "0.07298696\n",
      "[[0.01894073 0.88612    0.0101454  0.08479395]\n",
      " [0.03863766 0.06471778 0.01555364 0.88109094]\n",
      " [0.06654935 0.03795426 0.02196114 0.8735353 ]\n",
      " ...\n",
      " [0.02164017 0.8669161  0.01152722 0.09991648]\n",
      " [0.02721857 0.00520864 0.95469075 0.01288202]\n",
      " [0.02729797 0.00523409 0.95453507 0.01293291]]\n",
      "Iteration 1451, Accuracy 0.3432\n",
      "93.22924%change in label assignment\n",
      "0.069705784\n",
      "[[0.0136643  0.9347867  0.00781128 0.04373771]\n",
      " [0.06195129 0.24233723 0.02703875 0.66867274]\n",
      " [0.03325527 0.03812839 0.01214761 0.91646874]\n",
      " ...\n",
      " [0.01162118 0.9433562  0.00658737 0.0384353 ]\n",
      " [0.02738063 0.00524263 0.95440304 0.01297369]\n",
      " [0.02748681 0.00527343 0.9542021  0.01303764]]\n",
      "Iteration 1452, Accuracy 0.34143\n",
      "97.03442%change in label assignment\n",
      "0.06736369\n",
      "[[0.01106574 0.9389391  0.00609822 0.04389692]\n",
      " [0.0460807  0.0991679  0.01922976 0.83552164]\n",
      " [0.04036059 0.03186187 0.01425403 0.9135235 ]\n",
      " ...\n",
      " [0.01444185 0.91682565 0.00785183 0.06088068]\n",
      " [0.02719103 0.00519725 0.9547347  0.01287701]\n",
      " [0.02725098 0.00521869 0.95461196 0.01291838]]\n",
      "Iteration 1453, Accuracy 0.34939\n",
      "92.72352%change in label assignment\n",
      "0.061168257\n",
      "[[0.00954591 0.95176345 0.00534049 0.03335015]\n",
      " [0.05552514 0.15934898 0.02348599 0.7616399 ]\n",
      " [0.03308805 0.03402615 0.01193828 0.92094755]\n",
      " ...\n",
      " [0.00948616 0.9497482  0.0052128  0.03555282]\n",
      " [0.02723912 0.00520554 0.95464927 0.01290604]\n",
      " [0.02730401 0.00522776 0.9545186  0.01294956]]\n",
      "Iteration 1454, Accuracy 0.3381\n",
      "99.15059%change in label assignment\n",
      "0.06050875\n",
      "[[0.01001641 0.94582015 0.0055409  0.03862255]\n",
      " [0.05022859 0.1230424  0.02123944 0.8054896 ]\n",
      " [0.03561134 0.03157002 0.01279583 0.92002285]\n",
      " ...\n",
      " [0.01435258 0.9174891  0.00777969 0.06037859]\n",
      " [0.02716201 0.00518749 0.9547815  0.01286903]\n",
      " [0.02720897 0.00520636 0.9546805  0.01290415]]\n",
      "Iteration 1455, Accuracy 0.33937\n",
      "97.33883%change in label assignment\n",
      "0.054078024\n",
      "[[0.00948504 0.95186716 0.00528067 0.03336713]\n",
      " [0.05984627 0.20565431 0.02580039 0.70869905]\n",
      " [0.03310168 0.0348719  0.01195225 0.9200742 ]\n",
      " ...\n",
      " [0.01017815 0.9452687  0.00554309 0.03901004]\n",
      " [0.02715785 0.00518216 0.95479405 0.01286593]\n",
      " [0.02719357 0.00519897 0.9547116  0.01289582]]\n",
      "Iteration 1456, Accuracy 0.33152\n",
      "97.70217%change in label assignment\n",
      "0.057833772\n",
      "[[0.01739806 0.8970458  0.00934551 0.0762107 ]\n",
      " [0.04152294 0.07645617 0.01692984 0.86509097]\n",
      " [0.04279999 0.03216207 0.01495445 0.91008353]\n",
      " ...\n",
      " [0.02578285 0.83657664 0.01357584 0.12406466]\n",
      " [0.02708974 0.00517017 0.95490086 0.01283925]\n",
      " [0.02711655 0.00518532 0.95483315 0.01286496]]\n",
      "Iteration 1457, Accuracy 0.32793\n",
      "96.68582%change in label assignment\n",
      "0.064342104\n",
      "[[0.01354864 0.9353587  0.00775043 0.04334219]\n",
      " [0.06384563 0.3198857  0.02864384 0.5876248 ]\n",
      " [0.03626271 0.05211535 0.01379745 0.8978245 ]\n",
      " ...\n",
      " [0.00998123 0.9501594  0.00560088 0.03425852]\n",
      " [0.02713977 0.00517271 0.9548285  0.012859  ]\n",
      " [0.02714886 0.0051841  0.9547913  0.01287583]]\n",
      "Iteration 1458, Accuracy 0.32332\n",
      "96.16537%change in label assignment\n",
      "0.06980676\n",
      "[[0.02443438 0.8464373  0.01295346 0.11617482]\n",
      " [0.03523549 0.05128302 0.01395924 0.89952224]\n",
      " [0.06016142 0.03644484 0.02028985 0.8831039 ]\n",
      " ...\n",
      " [0.03285826 0.7796434  0.017123   0.17037538]\n",
      " [0.02705357 0.00518386 0.9549126  0.01284994]\n",
      " [0.02711692 0.00520643 0.95478314 0.01289357]]\n",
      "Iteration 1459, Accuracy 0.32779\n",
      "93.1065%change in label assignment\n",
      "0.07237723\n",
      "[[0.02595524 0.88488203 0.01533542 0.07382731]\n",
      " [0.05758787 0.5209169  0.02711214 0.3943831 ]\n",
      " [0.04190272 0.07316282 0.01632441 0.8686101 ]\n",
      " ...\n",
      " [0.02078287 0.90549684 0.0121316  0.0615887 ]\n",
      " [0.02724575 0.00522471 0.95457745 0.01295206]\n",
      " [0.02734518 0.0052548  0.95438623 0.01301369]]\n",
      "Iteration 1460, Accuracy 0.33157\n",
      "94.53528%change in label assignment\n",
      "0.0795535\n",
      "[[0.01551996 0.90958107 0.00840341 0.06649555]\n",
      " [0.04196426 0.07908092 0.01719266 0.86176217]\n",
      " [0.04968711 0.03371452 0.01703935 0.899559  ]\n",
      " ...\n",
      " [0.0204383  0.87555605 0.01092829 0.09307739]\n",
      " [0.02702889 0.00517841 0.9549517  0.01284101]\n",
      " [0.02708412 0.00519953 0.9548355  0.01288093]]\n",
      "Iteration 1461, Accuracy 0.34104\n",
      "95.09992%change in label assignment\n",
      "0.06307766\n",
      "[[0.01545385 0.9273325  0.00890105 0.04831258]\n",
      " [0.06372652 0.3549179  0.02885882 0.5524968 ]\n",
      " [0.03434142 0.04403202 0.01278748 0.90883905]\n",
      " ...\n",
      " [0.01206637 0.9415252  0.00686141 0.03954702]\n",
      " [0.02711178 0.00519009 0.9548206  0.01287756]\n",
      " [0.02717019 0.00521189 0.95469886 0.0129191 ]]\n",
      "Iteration 1462, Accuracy 0.34222\n",
      "97.79054%change in label assignment\n",
      "0.06581249\n",
      "[[0.0128411  0.9273164  0.00703385 0.05280861]\n",
      " [0.04565163 0.09731762 0.01907966 0.83795106]\n",
      " [0.04618002 0.03293707 0.016048   0.9048349 ]\n",
      " ...\n",
      " [0.01819691 0.89126474 0.00980468 0.08073369]\n",
      " [0.02699845 0.00517725 0.9549856  0.01283867]\n",
      " [0.02705537 0.00519878 0.9548663  0.01287952]]\n",
      "Iteration 1463, Accuracy 0.34031\n",
      "98.58595%change in label assignment\n",
      "0.06280476\n",
      "[[0.01378186 0.93431187 0.00790705 0.04399925]\n",
      " [0.06297096 0.27376506 0.0278919  0.6353721 ]\n",
      " [0.03296771 0.03728164 0.01206426 0.9176864 ]\n",
      " ...\n",
      " [0.0100309  0.9499318  0.00564365 0.03439364]\n",
      " [0.02715401 0.00521837 0.95469797 0.01292961]\n",
      " [0.02726371 0.00525102 0.9544884  0.0129969 ]]\n",
      "Iteration 1464, Accuracy 0.33977\n",
      "95.51726%change in label assignment\n",
      "0.061632622\n",
      "[[0.00971881 0.947706   0.00541135 0.03716381]\n",
      " [0.04640003 0.10135738 0.01941661 0.832826  ]\n",
      " [0.04339571 0.03224351 0.01516689 0.90919393]\n",
      " ...\n",
      " [0.01540567 0.9102719  0.00835269 0.06596974]\n",
      " [0.02709421 0.00521588 0.95477504 0.01291482]\n",
      " [0.02720889 0.00524931 0.95455754 0.01298426]]\n",
      "Iteration 1465, Accuracy 0.34846\n",
      "96.25374%change in label assignment\n",
      "0.05905509\n",
      "[[0.01712184 0.9203977  0.00993606 0.05254437]\n",
      " [0.06355619 0.3033731  0.02842778 0.60464287]\n",
      " [0.03313636 0.038948   0.01221016 0.91570556]\n",
      " ...\n",
      " [0.01073538 0.94708717 0.00606911 0.03610833]\n",
      " [0.0271867  0.00523504 0.9546143  0.01296395]\n",
      " [0.02731458 0.0052711  0.95437455 0.01303984]]\n",
      "Iteration 1466, Accuracy 0.34409\n",
      "98.14406%change in label assignment\n",
      "0.060891505\n",
      "[[0.01108174 0.93873    0.00609218 0.04409612]\n",
      " [0.0397471  0.06946159 0.01605215 0.87473917]\n",
      " [0.0480607  0.03318439 0.01648847 0.90226644]\n",
      " ...\n",
      " [0.01979058 0.88013285 0.0105645  0.08951213]\n",
      " [0.02718537 0.00524959 0.9545803  0.01298468]\n",
      " [0.02732681 0.00528779 0.954319   0.01306641]]\n",
      "Iteration 1467, Accuracy 0.35013\n",
      "94.77095%change in label assignment\n",
      "0.06093764\n",
      "[[0.02106616 0.90423125 0.01239256 0.0623101 ]\n",
      " [0.06356619 0.34540418 0.02886891 0.5621608 ]\n",
      " [0.03486186 0.0475548  0.01321224 0.9043711 ]\n",
      " ...\n",
      " [0.0125918  0.93933004 0.00720912 0.04086913]\n",
      " [0.02705324 0.00520336 0.95484465 0.01289876]\n",
      " [0.02713975 0.00523012 0.9546769  0.01295319]]\n",
      "Iteration 1468, Accuracy 0.35224\n",
      "95.60073%change in label assignment\n",
      "0.06436462\n",
      "[[0.01126203 0.93755287 0.00621631 0.04496884]\n",
      " [0.03974211 0.06984229 0.01616521 0.8742504 ]\n",
      " [0.04699655 0.03306192 0.01631133 0.9036302 ]\n",
      " ...\n",
      " [0.0200521  0.87816715 0.0107502  0.09103045]\n",
      " [0.0269388  0.00519147 0.9550099  0.01285985]\n",
      " [0.0270271  0.00521867 0.95483893 0.01291525]]\n",
      "Iteration 1469, Accuracy 0.35337\n",
      "97.74635%change in label assignment\n",
      "0.063312836\n",
      "[[0.02447685 0.8905319  0.01452481 0.07046636]\n",
      " [0.06167877 0.4294236  0.02868064 0.48021707]\n",
      " [0.03582977 0.05144319 0.01370249 0.89902455]\n",
      " ...\n",
      " [0.01544251 0.9273415  0.00894173 0.04827427]\n",
      " [0.02694906 0.0051845  0.95500904 0.01285737]\n",
      " [0.0270198  0.00520829 0.95486754 0.01290437]]\n",
      "Iteration 1470, Accuracy 0.35283\n",
      "95.36996%change in label assignment\n",
      "0.0625208\n",
      "[[0.00909445 0.9521316  0.0050994  0.03367455]\n",
      " [0.051365   0.13216782 0.02190149 0.79456574]\n",
      " [0.03814938 0.03129079 0.01355281 0.9170071 ]\n",
      " ...\n",
      " [0.01181679 0.9338929  0.00648699 0.04780333]\n",
      " [0.02677504 0.00514485 0.95531315 0.01276695]\n",
      " [0.02680331 0.0051602  0.9552432  0.01279327]]\n",
      "Iteration 1471, Accuracy 0.3514\n",
      "96.71527%change in label assignment\n",
      "0.056893595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01181332 0.94256383 0.00676983 0.03885296]\n",
      " [0.06222171 0.26292783 0.02771961 0.6471309 ]\n",
      " [0.03236147 0.03637363 0.01196606 0.91929877]\n",
      " ...\n",
      " [0.0093788  0.9525126  0.00528769 0.03282081]\n",
      " [0.02678283 0.00513551 0.9553226  0.01275905]\n",
      " [0.02678085 0.00514493 0.95530343 0.0127708 ]]\n",
      "Iteration 1472, Accuracy 0.33991\n",
      "96.48942%change in label assignment\n",
      "0.056056734\n",
      "[[0.00957749 0.9485407  0.00532766 0.03655418]\n",
      " [0.05001121 0.12291109 0.02118738 0.8058904 ]\n",
      " [0.03789203 0.03124222 0.01347123 0.9173945 ]\n",
      " ...\n",
      " [0.01304596 0.92585295 0.00712225 0.05397881]\n",
      " [0.02671876 0.00512419 0.9554242  0.01273299]\n",
      " [0.02670238 0.00513054 0.9554297  0.01273746]]\n",
      "Iteration 1473, Accuracy 0.33289\n",
      "97.45176%change in label assignment\n",
      "0.057423584\n",
      "[[0.01579932 0.925783   0.00919762 0.04922002]\n",
      " [0.06318501 0.35047767 0.02894403 0.5573933 ]\n",
      " [0.03323081 0.04222131 0.01255478 0.9119931 ]\n",
      " ...\n",
      " [0.011407   0.94426835 0.00652899 0.03779564]\n",
      " [0.02673219 0.00512771 0.955395   0.01274508]\n",
      " [0.0267157  0.00513412 0.9554006  0.01274961]]\n",
      "Iteration 1474, Accuracy 0.32823\n",
      "96.74964%change in label assignment\n",
      "0.059247024\n",
      "[[0.00906353 0.95238394 0.0050933  0.03345927]\n",
      " [0.05138735 0.13306169 0.02198715 0.79356384]\n",
      " [0.04068903 0.03165454 0.01436458 0.9132919 ]\n",
      " ...\n",
      " [0.01300403 0.92608005 0.00711043 0.05380542]\n",
      " [0.02666029 0.00514902 0.95543355 0.0127571 ]\n",
      " [0.02671694 0.00516998 0.9553159  0.01279719]]\n",
      "Iteration 1475, Accuracy 0.32945\n",
      "94.32906%change in label assignment\n",
      "0.058387633\n",
      "[[0.01375596 0.93438715 0.00792597 0.04393091]\n",
      " [0.06244292 0.26485938 0.02773366 0.6449641 ]\n",
      " [0.03263101 0.03449329 0.01189055 0.9209851 ]\n",
      " ...\n",
      " [0.00934701 0.9525656  0.00524659 0.03284086]\n",
      " [0.02672899 0.00516867 0.95530194 0.01280048]\n",
      " [0.02681    0.00519484 0.9551423  0.01285292]]\n",
      "Iteration 1476, Accuracy 0.35003\n",
      "96.88712%change in label assignment\n",
      "0.054296643\n",
      "[[0.00955794 0.95182574 0.00542336 0.03319288]\n",
      " [0.05445667 0.1551292  0.02334197 0.7670721 ]\n",
      " [0.03508718 0.03127446 0.01255248 0.9210859 ]\n",
      " ...\n",
      " [0.00997982 0.94583607 0.00550729 0.03867682]\n",
      " [0.02663627 0.0051495  0.9554592  0.01275499]\n",
      " [0.02670067 0.00517227 0.95532775 0.01279923]]\n",
      "Iteration 1477, Accuracy 0.35243\n",
      "98.23734%change in label assignment\n",
      "0.055681445\n",
      "[[0.01133536 0.9445606  0.00648912 0.03761488]\n",
      " [0.05963626 0.21147408 0.02610126 0.7027884 ]\n",
      " [0.03250805 0.03365913 0.01187826 0.9219545 ]\n",
      " ...\n",
      " [0.00895965 0.95343155 0.00500572 0.03260313]\n",
      " [0.02659157 0.00513659 0.9555413  0.01273057]\n",
      " [0.02664078 0.00515669 0.9554347  0.01276787]]\n",
      "Iteration 1478, Accuracy 0.3486\n",
      "97.87401%change in label assignment\n",
      "0.05346831\n",
      "[[0.00908184 0.95346177 0.00512663 0.03232971]\n",
      " [0.05595726 0.16986114 0.0242102  0.7499714 ]\n",
      " [0.03330889 0.03199062 0.01210823 0.92259234]\n",
      " ...\n",
      " [0.00940346 0.9496576  0.00522495 0.035714  ]\n",
      " [0.02651996 0.00511215 0.9556835  0.01268438]\n",
      " [0.02652674 0.00512411 0.95564777 0.01270142]]\n",
      "Iteration 1479, Accuracy 0.34512\n",
      "96.22919%change in label assignment\n",
      "[[0.00962818 0.9515531  0.00546364 0.03335505]\n",
      " [0.0580203  0.19170919 0.02527585 0.72499466]\n",
      " [0.03257773 0.03301497 0.01191299 0.9224943 ]\n",
      " ...\n",
      " [0.00894298 0.95330036 0.00500533 0.03275136]\n",
      " [0.02649603 0.00510469 0.955728   0.01267123]\n",
      " [0.02648251 0.00511281 0.95572597 0.01267864]]\n",
      "Iteration 1480, Accuracy 0.33461\n",
      "98.69888%change in label assignment\n",
      "0.06221259\n",
      "[[0.01904354 0.8851921  0.01016104 0.08560327]\n",
      " [0.03403028 0.04729525 0.01321621 0.9054583 ]\n",
      " [0.05091351 0.0337946  0.01728402 0.89800787]\n",
      " ...\n",
      " [0.02197053 0.86423194 0.01165349 0.10214403]\n",
      " [0.02645719 0.00510199 0.955783   0.01265777]\n",
      " [0.02645639 0.00511287 0.9557591  0.01267165]]\n",
      "Iteration 1481, Accuracy 0.33186\n",
      "96.21446%change in label assignment\n",
      "0.064635046\n",
      "[[0.02163434 0.9018092  0.01279359 0.06376287]\n",
      " [0.06226799 0.4048262  0.02888503 0.50402075]\n",
      " [0.0382826  0.06198217 0.0150111  0.8847241 ]\n",
      " ...\n",
      " [0.01494927 0.92933446 0.00867503 0.0470413 ]\n",
      " [0.02653036 0.00510997 0.9556656  0.01269401]\n",
      " [0.02651797 0.00511821 0.95566195 0.01270184]]\n",
      "Iteration 1482, Accuracy 0.33927\n",
      "94.87406%change in label assignment\n",
      "0.081035584\n",
      "[[0.03025334 0.7999405  0.01596209 0.15384403]\n",
      " [0.03209932 0.03442771 0.01221066 0.9212623 ]\n",
      " [0.08496571 0.04227581 0.02782483 0.8449336 ]\n",
      " ...\n",
      " [0.04038276 0.7102753  0.02088609 0.2284559 ]\n",
      " [0.02641187 0.00509498 0.9558534  0.0126398 ]\n",
      " [0.02639955 0.00510309 0.95584995 0.01264745]]\n",
      "Iteration 1483, Accuracy 0.33078\n",
      "97.00005%change in label assignment\n",
      "0.088947624\n",
      "[[0.05116288 0.7911417  0.03188534 0.12581006]\n",
      " [0.03555898 0.7612697  0.01783124 0.18534008]\n",
      " [0.05563129 0.15615004 0.02328939 0.7649293 ]\n",
      " ...\n",
      " [0.03812677 0.83816904 0.02320719 0.10049699]\n",
      " [0.02657276 0.00511937 0.9555856  0.0127223 ]\n",
      " [0.02656392 0.00512807 0.9555764  0.01273157]]\n",
      "Iteration 1484, Accuracy 0.33353\n",
      "95.17847%change in label assignment\n",
      "0.1087506\n",
      "[[0.02410324 0.84794956 0.01295793 0.11498923]\n",
      " [0.03243201 0.04055467 0.01274669 0.91426665]\n",
      " [0.10372732 0.0460971  0.03334602 0.8168296 ]\n",
      " ...\n",
      " [0.03562832 0.7539468  0.01869778 0.1917271 ]\n",
      " [0.02640514 0.00509699 0.9558583  0.0126396 ]\n",
      " [0.02639406 0.00510508 0.9558533  0.01264752]]\n",
      "Iteration 1485, Accuracy 0.33176\n",
      "95.52708%change in label assignment\n",
      "0.08752343\n",
      "[[0.04095021 0.8275271  0.02515485 0.10636773]\n",
      " [0.04800863 0.63864374 0.02350604 0.2898416 ]\n",
      " [0.04079119 0.07122482 0.01608885 0.87189513]\n",
      " ...\n",
      " [0.02880551 0.87335753 0.01727803 0.08055891]\n",
      " [0.02656857 0.00513144 0.9555615  0.01273852]\n",
      " [0.02659458 0.00514664 0.9554945  0.01276422]]\n",
      "Iteration 1486, Accuracy 0.33225\n",
      "95.46816%change in label assignment\n",
      "0.08749965\n",
      "[[0.0093914  0.9501931  0.00532359 0.03509183]\n",
      " [0.05054686 0.1301487  0.02196785 0.79733664]\n",
      " [0.05468284 0.03521314 0.018925   0.89117897]\n",
      " ...\n",
      " [0.01341307 0.92324376 0.00741015 0.05593301]\n",
      " [0.0264493  0.0051187  0.95574194 0.01269008]\n",
      " [0.02648101 0.00513533 0.95566475 0.0127189 ]]\n",
      "Iteration 1487, Accuracy 0.34045\n",
      "97.10316%change in label assignment\n",
      "0.06703802\n",
      "[[0.02057472 0.90612304 0.01207547 0.06122678]\n",
      " [0.06163589 0.42830637 0.02868498 0.48137283]\n",
      " [0.03285655 0.03712608 0.01205344 0.91796386]\n",
      " ...\n",
      " [0.0126107  0.93913084 0.00721044 0.04104799]\n",
      " [0.02655849 0.00513334 0.95556426 0.01274395]\n",
      " [0.02659811 0.00515156 0.9554737  0.01277667]]\n",
      "Iteration 1488, Accuracy 0.338\n",
      "97.76108%change in label assignment\n",
      "0.06713952\n",
      "[[0.00941322 0.9495774  0.00528526 0.03572411]\n",
      " [0.05132059 0.13489988 0.02222284 0.7915567 ]\n",
      " [0.04556499 0.03269524 0.01599218 0.9057476 ]\n",
      " ...\n",
      " [0.01459234 0.91536933 0.00797778 0.06206051]\n",
      " [0.02646152 0.00511923 0.95571244 0.0127068 ]\n",
      " [0.02649263 0.00513577 0.95563626 0.01273539]]\n",
      "Iteration 1489, Accuracy 0.34104\n",
      "97.25536%change in label assignment\n",
      "0.061405245\n",
      "[[0.01587958 0.92543536 0.00922923 0.04945584]\n",
      " [0.0632078  0.3407147  0.02888093 0.5671966 ]\n",
      " [0.0323723  0.0370406  0.01199751 0.9185896 ]\n",
      " ...\n",
      " [0.00985962 0.95061076 0.00557173 0.0339579 ]\n",
      " [0.02651538 0.00512217 0.9556336  0.01272892]\n",
      " [0.02653783 0.00513682 0.95557225 0.01275309]]\n",
      "Iteration 1490, Accuracy 0.33255\n",
      "97.60397%change in label assignment\n",
      "0.06363107\n",
      "[[0.01023688 0.94402266 0.00571388 0.04002664]\n",
      " [0.0491676  0.12023319 0.02113951 0.8094597 ]\n",
      " [0.04501855 0.03261112 0.0158845  0.90648586]\n",
      " ...\n",
      " [0.01764981 0.8945605  0.00957351 0.07821618]\n",
      " [0.02644333 0.00511684 0.95573443 0.01270538]\n",
      " [0.02647494 0.00513358 0.9556571  0.01273435]]\n",
      "Iteration 1491, Accuracy 0.3324\n",
      "97.6678%change in label assignment\n",
      "0.064488016\n",
      "[[0.01575607 0.9259736  0.00914168 0.04912865]\n",
      " [0.0628823  0.2855682  0.02819801 0.62335145]\n",
      " [0.03238123 0.03676342 0.01198062 0.91887474]\n",
      " ...\n",
      " [0.01019166 0.94927096 0.0057662  0.03477117]\n",
      " [0.02660181 0.00516593 0.95541894 0.01281327]\n",
      " [0.02669909 0.00519645 0.95522916 0.01287527]]\n",
      "Iteration 1492, Accuracy 0.33353\n",
      "94.21122%change in label assignment\n",
      "0.0621888\n",
      "[[0.00903504 0.9524142  0.00510057 0.03345016]\n",
      " [0.0532873  0.1499256  0.02324975 0.77353734]\n",
      " [0.03785308 0.0311783  0.01365483 0.9173138 ]\n",
      " ...\n",
      " [0.01333128 0.9237567  0.00731497 0.05559705]\n",
      " [0.0264807  0.00513843 0.9556287  0.01275221]\n",
      " [0.02655267 0.00516376 0.955482   0.01280163]]\n",
      "Iteration 1493, Accuracy 0.35091\n",
      "95.53199%change in label assignment\n",
      "0.061823927\n",
      "[[0.01172056 0.94296116 0.00670395 0.0386143 ]\n",
      " [0.0589458  0.20041561 0.02565368 0.71498495]\n",
      " [0.03232028 0.03417586 0.01184138 0.92166245]\n",
      " ...\n",
      " [0.00921382 0.9514044  0.00510142 0.03428041]\n",
      " [0.0265136  0.00514214 0.95557386 0.01277041]\n",
      " [0.02658689 0.0051678  0.9554248  0.01282059]]\n",
      "Iteration 1494, Accuracy 0.34885\n",
      "98.12442%change in label assignment\n",
      "0.05839489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00910059 0.9531567  0.00518229 0.03256043]\n",
      " [0.0513627  0.13445318 0.02220537 0.7919787 ]\n",
      " [0.03629445 0.03105416 0.01318108 0.91947037]\n",
      " ...\n",
      " [0.011911   0.933109   0.00657439 0.0484056 ]\n",
      " [0.02662028 0.00518828 0.9553336  0.01285785]\n",
      " [0.02674952 0.00522514 0.9550899  0.01293546]]\n",
      "Iteration 1495, Accuracy 0.34747\n",
      "94.84951%change in label assignment\n",
      "0.05236846\n",
      "[[0.0126665  0.9389784  0.00725612 0.04109906]\n",
      " [0.06049157 0.22088043 0.02647687 0.6921511 ]\n",
      " [0.03261248 0.03392491 0.01187541 0.9215873 ]\n",
      " ...\n",
      " [0.00919429 0.9521428  0.00510089 0.03356212]\n",
      " [0.02659138 0.00517688 0.9553911  0.01284055]\n",
      " [0.02670761 0.00521101 0.9551697  0.01291164]]\n",
      "Iteration 1496, Accuracy 0.35651\n",
      "99.02784%change in label assignment\n",
      "0.05645013\n",
      "[[0.01126989 0.93732876 0.0062375  0.04516387]\n",
      " [0.03991679 0.07155468 0.01640463 0.87212384]\n",
      " [0.04269101 0.03190805 0.01513253 0.91026837]\n",
      " ...\n",
      " [0.02091948 0.87173486 0.0112258  0.09611988]\n",
      " [0.02652336 0.0051672  0.9554918  0.0128177 ]\n",
      " [0.02663893 0.00520129 0.9552711  0.01288859]]\n",
      "Iteration 1497, Accuracy 0.35734\n",
      "96.17028%change in label assignment\n",
      "0.0616724\n",
      "[[0.01947225 0.91070414 0.01144009 0.0583835 ]\n",
      " [0.06346525 0.3258815  0.02882534 0.5818279 ]\n",
      " [0.03521916 0.05006472 0.01354555 0.9011706 ]\n",
      " ...\n",
      " [0.0115336  0.94379485 0.00659042 0.03808116]\n",
      " [0.02648768 0.00514888 0.95557314 0.01279027]\n",
      " [0.02657586 0.00517747 0.955399   0.01284774]]\n",
      "Iteration 1498, Accuracy 0.35415\n",
      "96.85275%change in label assignment\n",
      "0.0664461\n",
      "[[0.01230489 0.9304945  0.00678688 0.05041368]\n",
      " [0.03745049 0.06188168 0.01523004 0.8854378 ]\n",
      " [0.04564448 0.0325833  0.01606692 0.9057053 ]\n",
      " ...\n",
      " [0.02244928 0.86041456 0.01202112 0.10511495]\n",
      " [0.02637439 0.00513571 0.95574385 0.01274609]\n",
      " [0.02645942 0.00516341 0.95557547 0.01280165]]\n",
      "Iteration 1499, Accuracy 0.35214\n",
      "97.85437%change in label assignment\n",
      "0.0633108\n",
      "[[0.02806684 0.8763321  0.0168555  0.0787456 ]\n",
      " [0.06197578 0.41560084 0.02889317 0.49353024]\n",
      " [0.03670504 0.05595487 0.01429019 0.89304996]\n",
      " ...\n",
      " [0.01663333 0.92235345 0.0097071  0.0513061 ]\n",
      " [0.02647678 0.0051599  0.9555578  0.01280556]\n",
      " [0.0265773  0.00519067 0.95536333 0.01286873]]\n",
      "Iteration 1500, Accuracy 0.3487\n",
      "96.33721%change in label assignment\n",
      "0.07015128\n",
      "[[0.01067574 0.9410807  0.00593464 0.04230894]\n",
      " [0.03726631 0.06122812 0.01513025 0.8863753 ]\n",
      " [0.05050084 0.03374116 0.01752075 0.8982372 ]\n",
      " ...\n",
      " [0.02048652 0.8746001  0.01101933 0.09389401]\n",
      " [0.02639954 0.00515531 0.9556649  0.01278025]\n",
      " [0.02650807 0.00518763 0.9554571  0.01284726]]\n",
      "Iteration 1501, Accuracy 0.35813\n",
      "94.95753%change in label assignment\n",
      "0.06369108\n",
      "[[0.02867907 0.8739703  0.01722854 0.08012205]\n",
      " [0.06272733 0.3924068  0.02899957 0.5158663 ]\n",
      " [0.03584766 0.05218552 0.01381245 0.8981543 ]\n",
      " ...\n",
      " [0.01699381 0.9208652  0.00991737 0.05222369]\n",
      " [0.02637598 0.00513643 0.95573324 0.01275437]\n",
      " [0.02645373 0.00516243 0.95557773 0.01280608]]\n",
      "Iteration 1502, Accuracy 0.35793\n",
      "96.95586%change in label assignment\n",
      "0.06744898\n",
      "[[0.00950307 0.9488712  0.00534032 0.03628539]\n",
      " [0.04090659 0.07649405 0.01695628 0.865643  ]\n",
      " [0.04661334 0.03280304 0.01637214 0.9042115 ]\n",
      " ...\n",
      " [0.01658631 0.90185976 0.0090325  0.07252148]\n",
      " [0.0262797  0.00512853 0.9558682  0.01272357]\n",
      " [0.0263646  0.00515612 0.9557003  0.01277898]]\n",
      "Iteration 1503, Accuracy 0.35936\n",
      "96.44032%change in label assignment\n",
      "0.05994964\n",
      "[[0.02004228 0.90828365 0.01182823 0.05984583]\n",
      " [0.06237714 0.26763427 0.02789084 0.6420978 ]\n",
      " [0.03279218 0.04072464 0.01237314 0.91411   ]\n",
      " ...\n",
      " [0.01142793 0.9442029  0.00654821 0.03782091]\n",
      " [0.02627716 0.00511864 0.9558901  0.01271405]\n",
      " [0.02634226 0.00514215 0.95575595 0.01275962]]\n",
      "Iteration 1504, Accuracy 0.35612\n",
      "97.03933%change in label assignment\n",
      "0.061813623\n",
      "[[0.00906195 0.95200515 0.00511805 0.03381494]\n",
      " [0.04170667 0.08014312 0.01733227 0.8608179 ]\n",
      " [0.04425443 0.03219638 0.01562424 0.90792495]\n",
      " ...\n",
      " [0.01514214 0.911613   0.00827518 0.06496966]\n",
      " [0.02620786 0.00511613 0.95598096 0.01269508]\n",
      " [0.02628337 0.00514178 0.955829   0.01274582]]\n",
      "Iteration 1505, Accuracy 0.3545\n",
      "97.2259%change in label assignment\n",
      "0.058808945\n",
      "[[0.02448912 0.8903995  0.01459154 0.07051989]\n",
      " [0.06298395 0.37510002 0.02903986 0.5328762 ]\n",
      " [0.03396713 0.04529139 0.01293077 0.9078107 ]\n",
      " ...\n",
      " [0.01453718 0.93109083 0.00842641 0.0459456 ]\n",
      " [0.02620637 0.00509984 0.95601743 0.01267631]\n",
      " [0.02624915 0.00511908 0.95592064 0.01271118]]\n",
      "Iteration 1506, Accuracy 0.357\n",
      "95.1392%change in label assignment\n",
      "0.06456474\n",
      "[[0.01073653 0.9406337  0.00597296 0.04265678]\n",
      " [0.03850449 0.06641881 0.01576047 0.8793162 ]\n",
      " [0.04911511 0.03337075 0.01711807 0.9003961 ]\n",
      " ...\n",
      " [0.01923378 0.8834094  0.01039266 0.08696412]\n",
      " [0.02612117 0.00509671 0.95612776 0.01265433]\n",
      " [0.0261803  0.0051194  0.95600283 0.01269744]]\n",
      "Iteration 1507, Accuracy 0.35395\n",
      "95.79221%change in label assignment\n",
      "0.061482675\n",
      "[[0.0261907  0.8835785  0.01571197 0.07451885]\n",
      " [0.06219132 0.40183955 0.02900048 0.5069686 ]\n",
      " [0.03449902 0.04828631 0.01330564 0.903909  ]\n",
      " ...\n",
      " [0.01618819 0.9241408  0.00946602 0.050205  ]\n",
      " [0.02617529 0.00510538 0.9560368  0.0126825 ]\n",
      " [0.0262358  0.00512863 0.95590883 0.01272669]]\n",
      "Iteration 1508, Accuracy 0.35597\n",
      "97.19645%change in label assignment\n",
      "0.06634992\n",
      "[[0.00932334 0.9500732  0.00525896 0.03534454]\n",
      " [0.04135666 0.07893495 0.01723685 0.8624715 ]\n",
      " [0.04834547 0.03324335 0.01696138 0.9014498 ]\n",
      " ...\n",
      " [0.0167465  0.9006494  0.00913182 0.07347219]\n",
      " [0.02610522 0.00510773 0.95611626 0.01267082]\n",
      " [0.02618223 0.00513382 0.95596147 0.0127225 ]]\n",
      "Iteration 1509, Accuracy 0.35435\n",
      "96.58271%change in label assignment\n",
      "0.062416155\n",
      "[[0.02962699 0.8702097  0.0178701  0.08229315]\n",
      " [0.06198958 0.41434795 0.02890521 0.49475726]\n",
      " [0.03503025 0.04957567 0.01347622 0.9019178 ]\n",
      " ...\n",
      " [0.01706212 0.920534   0.00998132 0.05242254]\n",
      " [0.02608898 0.00507808 0.9562008  0.01263214]\n",
      " [0.02610331 0.00509072 0.9561541  0.01265185]]\n",
      "Iteration 1510, Accuracy 0.35783\n",
      "93.73005%change in label assignment\n",
      "0.062413182\n",
      "[[0.00979296 0.95074475 0.00564132 0.03382092]\n",
      " [0.04796202 0.11290044 0.0204596  0.81867796]\n",
      " [0.03913321 0.0310795  0.01402051 0.91576684]\n",
      " ...\n",
      " [0.01081087 0.94008785 0.00601004 0.0430912 ]\n",
      " [0.02597592 0.00506706 0.9563641  0.01259294]\n",
      " [0.02599207 0.00507949 0.9563157  0.01261278]]\n",
      "Iteration 1511, Accuracy 0.35459\n",
      "97.35847%change in label assignment\n",
      "0.056006283\n",
      "[[0.01873519 0.91349876 0.01108091 0.05668512]\n",
      " [0.06109563 0.2442975  0.02732911 0.66727775]\n",
      " [0.03170462 0.03442167 0.01179522 0.9220785 ]\n",
      " ...\n",
      " [0.00994859 0.9503446  0.00568599 0.03402078]\n",
      " [0.02599026 0.00506881 0.95634013 0.01260079]\n",
      " [0.02600574 0.00508125 0.95629245 0.01262051]]\n",
      "Iteration 1512, Accuracy 0.35651\n",
      "98.38955%change in label assignment\n",
      "0.054581363\n",
      "[[0.00912476 0.95342046 0.00521116 0.03224361]\n",
      " [0.04627702 0.10274514 0.01950866 0.8314691 ]\n",
      " [0.03778053 0.03080521 0.01354021 0.9178741 ]\n",
      " ...\n",
      " [0.01178662 0.9337301  0.00650144 0.04798184]\n",
      " [0.02594058 0.00505009 0.95644146 0.0125679 ]\n",
      " [0.02592443 0.00505666 0.9564463  0.01257264]]\n",
      "Iteration 1513, Accuracy 0.35886\n",
      "96.68582%change in label assignment\n",
      "0.055693656\n",
      "[[0.02145995 0.90236056 0.01279396 0.0633855 ]\n",
      " [0.06261498 0.2987229  0.02857373 0.6100884 ]\n",
      " [0.03184299 0.03805754 0.01207154 0.918028  ]\n",
      " ...\n",
      " [0.01123992 0.9449513  0.0064813  0.03732745]\n",
      " [0.02592194 0.00506144 0.9564327  0.01258381]\n",
      " [0.02593512 0.00507389 0.956388   0.012603  ]]\n",
      "Iteration 1514, Accuracy 0.35008\n",
      "97.34374%change in label assignment\n",
      "0.055255204\n",
      "[[0.01007808 0.94974273 0.00579775 0.0343814 ]\n",
      " [0.05192263 0.13859412 0.02236925 0.78711396]\n",
      " [0.0352172  0.03067026 0.0127442  0.92136836]\n",
      " ...\n",
      " [0.00996857 0.94557154 0.00554257 0.03891728]\n",
      " [0.0258546  0.00504762 0.95654565 0.01255208]\n",
      " [0.02585221 0.00505734 0.9565264  0.0125641 ]]\n",
      "Iteration 1515, Accuracy 0.35813\n",
      "97.1228%change in label assignment\n",
      "0.05428879\n",
      "[[0.01385219 0.9338461  0.00808645 0.04421521]\n",
      " [0.05847896 0.20187365 0.02583188 0.7138155 ]\n",
      " [0.03207486 0.03231768 0.0118358  0.9237717 ]\n",
      " ...\n",
      " [0.00883663 0.95437396 0.00499589 0.03179354]\n",
      " [0.02584069 0.00504739 0.9565617  0.01255016]\n",
      " [0.02583922 0.00505739 0.95654064 0.01256274]]\n",
      "Iteration 1516, Accuracy 0.35307\n",
      "98.57613%change in label assignment\n",
      "0.052651152\n",
      "[[0.01054996 0.9477759  0.00608616 0.03558794]\n",
      " [0.05228126 0.14145698 0.02255189 0.7837099 ]\n",
      " [0.03599803 0.03065479 0.01296492 0.9203823 ]\n",
      " ...\n",
      " [0.00993877 0.9457433  0.0055247  0.03879321]\n",
      " [0.02581611 0.00506984 0.95653903 0.01257499]\n",
      " [0.02586484 0.00508943 0.956434   0.01261171]]\n",
      "Iteration 1517, Accuracy 0.3513\n",
      "94.34379%change in label assignment\n",
      "0.055405963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01631211 0.9234746  0.00960624 0.05060701]\n",
      " [0.05961315 0.21970895 0.02654473 0.69413316]\n",
      " [0.03170787 0.03301276 0.01178368 0.92349565]\n",
      " ...\n",
      " [0.00906543 0.9537473  0.00515271 0.03203459]\n",
      " [0.02581034 0.00507129 0.9565409  0.01257752]\n",
      " [0.02586143 0.00509142 0.9564316  0.01261549]]\n",
      "Iteration 1518, Accuracy 0.35985\n",
      "97.275%change in label assignment\n",
      "0.051602483\n",
      "[[0.01099668 0.94588166 0.00636404 0.0367576 ]\n",
      " [0.05219918 0.14107135 0.02253027 0.7841992 ]\n",
      " [0.03495372 0.03066945 0.01267926 0.92169756]\n",
      " ...\n",
      " [0.00974021 0.9470284  0.00542571 0.03780568]\n",
      " [0.02576566 0.00506904 0.9565984  0.01256697]\n",
      " [0.02581981 0.00508967 0.95648426 0.01260628]]\n",
      "Iteration 1519, Accuracy 0.36392\n",
      "98.85108%change in label assignment\n",
      "[[0.01658079 0.92237633 0.00976756 0.0512753 ]\n",
      " [0.05996356 0.22480562 0.02672112 0.6885097 ]\n",
      " [0.03144772 0.03487324 0.01180535 0.9218737 ]\n",
      " ...\n",
      " [0.00946559 0.95227844 0.00540243 0.0328535 ]\n",
      " [0.02570368 0.00503814 0.9567422  0.01251593]\n",
      " [0.02570941 0.00504908 0.95671016 0.01253141]]\n",
      "Iteration 1520, Accuracy 0.36535\n",
      "95.96406%change in label assignment\n",
      "0.062127706\n",
      "[[0.01465876 0.9146913  0.00800628 0.06264358]\n",
      " [0.03130425 0.03739823 0.01206587 0.9192316 ]\n",
      " [0.05628048 0.03510536 0.01925424 0.8893599 ]\n",
      " ...\n",
      " [0.02616813 0.8319604  0.01388787 0.12798356]\n",
      " [0.02565937 0.00503444 0.95680636 0.01249988]\n",
      " [0.02567443 0.00504753 0.9567578  0.0125203 ]]\n",
      "Iteration 1521, Accuracy 0.35901\n",
      "97.05897%change in label assignment\n",
      "0.0684871\n",
      "[[0.03885788 0.8351857  0.02391525 0.10204111]\n",
      " [0.05535351 0.54411405 0.02671249 0.37382   ]\n",
      " [0.04440387 0.08940201 0.01810324 0.8480909 ]\n",
      " ...\n",
      " [0.02302346 0.89618236 0.01371384 0.06708022]\n",
      " [0.02574187 0.00503984 0.95668113 0.01253718]\n",
      " [0.02573749 0.00504928 0.9566648  0.01254838]]\n",
      "Iteration 1522, Accuracy 0.35346\n",
      "93.07704%change in label assignment\n",
      "0.08953643\n",
      "[[0.02469376 0.8429356  0.01330194 0.11906863]\n",
      " [0.03342751 0.03144233 0.01259966 0.92253053]\n",
      " [0.08843    0.04301983 0.02932671 0.83922344]\n",
      " ...\n",
      " [0.04003333 0.71210784 0.02089815 0.22696066]\n",
      " [0.02561064 0.00502629 0.9568826  0.01248039]\n",
      " [0.02561599 0.00503774 0.9568498  0.01249641]]\n",
      "Iteration 1523, Accuracy 0.35278\n",
      "94.03447%change in label assignment\n",
      "0.08904765\n",
      "[[0.05234613 0.7868037  0.0328563  0.12799384]\n",
      " [0.05167202 0.5978029  0.02507062 0.32545444]\n",
      " [0.0507681  0.12242787 0.0210403  0.8057637 ]\n",
      " ...\n",
      " [0.03605984 0.8457397  0.02197756 0.09622286]\n",
      " [0.02575606 0.00504774 0.9566411  0.01255511]\n",
      " [0.02576151 0.00505956 0.9566072  0.01257171]]\n",
      "Iteration 1524, Accuracy 0.34875\n",
      "95.56145%change in label assignment\n",
      "0.100169316\n",
      "[[0.01651183 0.9019709  0.00909132 0.07242586]\n",
      " [0.03332297 0.03152439 0.01259023 0.9225624 ]\n",
      " [0.08677396 0.04274193 0.02888713 0.84159696]\n",
      " ...\n",
      " [0.02786273 0.8181301  0.01493892 0.13906835]\n",
      " [0.02561242 0.00503905 0.95684934 0.0124992 ]\n",
      " [0.02564182 0.0050558  0.95677465 0.0125277 ]]\n",
      "Iteration 1525, Accuracy 0.35297\n",
      "93.49929%change in label assignment\n",
      "0.07635884\n",
      "[[0.03319297 0.8563234  0.02022623 0.09025746]\n",
      " [0.06307316 0.32165638 0.028792   0.5864785 ]\n",
      " [0.03458916 0.04913146 0.0134027  0.9028767 ]\n",
      " ...\n",
      " [0.02088694 0.9046946  0.01238872 0.06202971]\n",
      " [0.02576439 0.00506968 0.9565796  0.01258627]\n",
      " [0.02581898 0.0050917  0.9564618  0.01262756]]\n",
      "Iteration 1526, Accuracy 0.35504\n",
      "95.52708%change in label assignment\n",
      "0.074752085\n",
      "[[0.00925601 0.9503376  0.00523876 0.03516763]\n",
      " [0.03155727 0.03989872 0.01240895 0.916135  ]\n",
      " [0.05165859 0.03410298 0.01808756 0.8961509 ]\n",
      " ...\n",
      " [0.01550984 0.9087713  0.00851776 0.06720114]\n",
      " [0.02564703 0.00504741 0.9567762  0.01252937]\n",
      " [0.02568327 0.00506525 0.95669055 0.01256093]]\n",
      "Iteration 1527, Accuracy 0.35837\n",
      "96.05735%change in label assignment\n",
      "0.06238098\n",
      "[[0.02003876 0.9081742  0.0118626  0.0599244 ]\n",
      " [0.05630629 0.17277986 0.0243504  0.74656343]\n",
      " [0.03190601 0.03754746 0.01200534 0.91854113]\n",
      " ...\n",
      " [0.01083934 0.94660866 0.00621235 0.03633961]\n",
      " [0.02570188 0.00504282 0.95671153 0.01254385]\n",
      " [0.02571245 0.00505521 0.95667    0.01256238]]\n",
      "Iteration 1528, Accuracy 0.35651\n",
      "96.40104%change in label assignment\n",
      "0.0636047\n",
      "[[0.00925988 0.95029384 0.00524119 0.03520504]\n",
      " [0.03290777 0.04589379 0.01316569 0.9080327 ]\n",
      " [0.0430429  0.03194482 0.01545338 0.9095589 ]\n",
      " ...\n",
      " [0.01672453 0.90047765 0.00915432 0.07364353]\n",
      " [0.02561239 0.00502726 0.9568532  0.01250712]\n",
      " [0.02560505 0.00503607 0.956842   0.01251679]]\n",
      "Iteration 1529, Accuracy 0.35616\n",
      "96.78892%change in label assignment\n",
      "0.060317904\n",
      "[[0.01679401 0.9215523  0.0098541  0.0517996 ]\n",
      " [0.04897214 0.11591719 0.02052047 0.81459016]\n",
      " [0.03159366 0.03592191 0.01185465 0.92062974]\n",
      " ...\n",
      " [0.00978886 0.9509524  0.00557277 0.0336859 ]\n",
      " [0.02566837 0.00505396 0.95671815 0.01255947]\n",
      " [0.02570874 0.00507299 0.95662475 0.01259358]]\n",
      "Iteration 1530, Accuracy 0.35027\n",
      "96.0328%change in label assignment\n",
      "0.06079143\n",
      "[[0.00883793 0.9538306  0.00504205 0.03228943]\n",
      " [0.03760108 0.06407343 0.01546309 0.8828623 ]\n",
      " [0.03833283 0.03092188 0.01394322 0.91680205]\n",
      " ...\n",
      " [0.01301032 0.9254899  0.0071928  0.05430697]\n",
      " [0.02560122 0.00504195 0.9568277  0.01252917]\n",
      " [0.02563465 0.00505989 0.95674527 0.01256022]]\n",
      "Iteration 1531, Accuracy 0.35744\n",
      "96.75946%change in label assignment\n",
      "0.057935476\n",
      "[[0.00973187 0.9512059  0.00554179 0.03352043]\n",
      " [0.03673275 0.05825638 0.01456585 0.890445  ]\n",
      " [0.0334837  0.03096931 0.01214778 0.9233992 ]\n",
      " ...\n",
      " [0.00974749 0.94721764 0.00539193 0.03764293]\n",
      " [0.02568297 0.00507167 0.9566559  0.01258943]\n",
      " [0.02575761 0.00509811 0.9565031  0.01264117]]\n",
      "Iteration 1532, Accuracy 0.34978\n",
      "95.29631%change in label assignment\n",
      "0.055301294\n",
      "[[0.00963581 0.95144546 0.00556072 0.03335797]\n",
      " [0.03987186 0.07326522 0.0165155  0.87034744]\n",
      " [0.0334697  0.0307498  0.01243243 0.92334807]\n",
      " ...\n",
      " [0.00985079 0.9461614  0.0055184  0.03846939]\n",
      " [0.0256913  0.0050858  0.9566085  0.01261444]\n",
      " [0.02578808 0.00511708 0.95641714 0.01267766]]\n",
      "Iteration 1533, Accuracy 0.35916\n",
      "96.05244%change in label assignment\n",
      "0.057029072\n",
      "[[0.0088604  0.95444334 0.00502029 0.03167598]\n",
      " [0.0370484  0.06040696 0.01485571 0.8876889 ]\n",
      " [0.03395125 0.03058306 0.01236094 0.9231047 ]\n",
      " ...\n",
      " [0.01170682 0.93425375 0.00642678 0.04761261]\n",
      " [0.02564076 0.00506658 0.9567132  0.01257952]\n",
      " [0.02571698 0.00509361 0.95655704 0.01263241]]\n",
      "Iteration 1534, Accuracy 0.35499\n",
      "96.95586%change in label assignment\n",
      "0.05587201\n",
      "[[0.01420642 0.9322855  0.00834577 0.04516236]\n",
      " [0.05193849 0.13964945 0.0224428  0.78596926]\n",
      " [0.03105772 0.03619951 0.01189787 0.9208449 ]\n",
      " ...\n",
      " [0.0088395  0.9545441  0.00503458 0.0315818 ]\n",
      " [0.02560661 0.00505895 0.9567678  0.0125667 ]\n",
      " [0.02567082 0.00508331 0.9566325  0.01261333]]\n",
      "Iteration 1535, Accuracy 0.34949\n",
      "98.16861%change in label assignment\n",
      "0.056390513\n",
      "[[0.01761045 0.89456624 0.0095235  0.07829977]\n",
      " [0.03096687 0.03344032 0.01174692 0.9238458 ]\n",
      " [0.04950529 0.03323188 0.01723307 0.9000298 ]\n",
      " ...\n",
      " [0.03287577 0.7776195  0.01718116 0.17232358]\n",
      " [0.02552048 0.00503092 0.9569357  0.01251299]\n",
      " [0.02555464 0.00504897 0.9568521  0.01254436]]\n",
      "Iteration 1536, Accuracy 0.35081\n",
      "96.81347%change in label assignment\n",
      "0.07428748\n",
      "[[0.05824012 0.766411   0.03701048 0.13833836]\n",
      " [0.0440877  0.68090427 0.02187394 0.25313407]\n",
      " [0.05630549 0.1672283  0.02406689 0.7523993 ]\n",
      " ...\n",
      " [0.04255792 0.821731   0.02632301 0.10938808]\n",
      " [0.02558895 0.00504343 0.9568144  0.0125532 ]\n",
      " [0.02561502 0.0050601  0.95674396 0.01258094]]\n",
      "Iteration 1537, Accuracy 0.34576\n",
      "96.2341%change in label assignment\n",
      "0.096222915\n",
      "[[0.00949585 0.9487123  0.00538335 0.03640855]\n",
      " [0.03319771 0.04724794 0.01338517 0.9061692 ]\n",
      " [0.04980323 0.0336707  0.01765341 0.8988727 ]\n",
      " ...\n",
      " [0.01550176 0.9088045  0.00855022 0.0671435 ]\n",
      " [0.02543699 0.00502715 0.95704293 0.01249289]\n",
      " [0.02547    0.00504552 0.9569601  0.0125244 ]]\n",
      "Iteration 1538, Accuracy 0.34497\n",
      "94.00992%change in label assignment\n",
      "0.061654545\n",
      "[[0.01471022 0.9302836  0.00856137 0.0464448 ]\n",
      " [0.04846641 0.1119198  0.02018303 0.81943077]\n",
      " [0.03248648 0.03225953 0.01185371 0.9234003 ]\n",
      " ...\n",
      " [0.00955074 0.95185703 0.005416   0.03317625]\n",
      " [0.02559559 0.00507064 0.9567468  0.01258698]\n",
      " [0.02568443 0.00510078 0.9565679  0.01264691]]\n",
      "Iteration 1539, Accuracy 0.35302\n",
      "95.586%change in label assignment\n",
      "0.056888066\n",
      "[[0.00910084 0.9511701  0.00512843 0.03460068]\n",
      " [0.03087019 0.03591714 0.01189678 0.9213159 ]\n",
      " [0.04345121 0.03177162 0.01543263 0.90934455]\n",
      " ...\n",
      " [0.01496726 0.9124243  0.00819692 0.06441143]\n",
      " [0.02553827 0.00506645 0.9568232  0.01257205]\n",
      " [0.02563125 0.00509749 0.95663714 0.01263411]]\n",
      "Iteration 1540, Accuracy 0.35896\n",
      "97.88874%change in label assignment\n",
      "0.055889633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01041902 0.9484276  0.00597364 0.03517975]\n",
      " [0.03444851 0.04943069 0.01343449 0.9026863 ]\n",
      " [0.03231336 0.0316882  0.01184201 0.9241564 ]\n",
      " ...\n",
      " [0.00889754 0.95302284 0.00497579 0.03310382]\n",
      " [0.02548923 0.00504381 0.9569327  0.01253424]\n",
      " [0.0255492  0.00506813 0.95680285 0.01257985]]\n",
      "Iteration 1541, Accuracy 0.36117\n",
      "96.51888%change in label assignment\n",
      "0.05436635\n",
      "[[0.0093026  0.9497703  0.00523988 0.03568728]\n",
      " [0.03094418 0.03301626 0.01177566 0.9242639 ]\n",
      " [0.03831157 0.03073702 0.01390176 0.91704965]\n",
      " ...\n",
      " [0.01459075 0.9149168  0.00801524 0.06247718]\n",
      " [0.02541371 0.00503457 0.9570417  0.01250997]\n",
      " [0.02546959 0.00505814 0.95691866 0.01255363]]\n",
      "Iteration 1542, Accuracy 0.36682\n",
      "98.74797%change in label assignment\n",
      "0.05531893\n",
      "[[0.0131357  0.9368912  0.00765261 0.04232051]\n",
      " [0.03698504 0.05988347 0.01477566 0.88835585]\n",
      " [0.03118415 0.03368085 0.01169538 0.92343956]\n",
      " ...\n",
      " [0.00892744 0.9543002  0.00506876 0.03170354]\n",
      " [0.02548214 0.0050597  0.95689803 0.01256019]\n",
      " [0.02556789 0.00508934 0.95672405 0.01261878]]\n",
      "Iteration 1543, Accuracy 0.36397\n",
      "97.25536%change in label assignment\n",
      "0.056193016\n",
      "[[0.00911689 0.9509926  0.00513235 0.03475817]\n",
      " [0.03128771 0.03202891 0.01177715 0.92490625]\n",
      " [0.03856537 0.03072014 0.01392629 0.91678816]\n",
      " ...\n",
      " [0.01461456 0.9147414  0.00800913 0.06263483]\n",
      " [0.02538416 0.00503826 0.9570678  0.01250977]\n",
      " [0.02545181 0.0050642  0.95692474 0.01255928]]\n",
      "Iteration 1544, Accuracy 0.36265\n",
      "98.33063%change in label assignment\n",
      "0.05669289\n",
      "[[0.00921603 0.9532947  0.00524625 0.03224299]\n",
      " [0.03174812 0.0386573  0.01208553 0.917509  ]\n",
      " [0.03172956 0.03201862 0.01174411 0.92450774]\n",
      " ...\n",
      " [0.00920378 0.9506154  0.00513042 0.03505038]\n",
      " [0.02535534 0.00502332 0.957135   0.01248631]\n",
      " [0.02539327 0.00504251 0.9570442  0.01252008]]\n",
      "Iteration 1545, Accuracy 0.36407\n",
      "97.36338%change in label assignment\n",
      "0.053530727\n",
      "[[0.00956296 0.94796073 0.00536094 0.03711538]\n",
      " [0.03127359 0.03205176 0.01176435 0.9249103 ]\n",
      " [0.03459339 0.03036637 0.01271753 0.9223227 ]\n",
      " ...\n",
      " [0.01331271 0.9233985  0.00733225 0.05595655]\n",
      " [0.02529209 0.00500052 0.95726144 0.01244588]\n",
      " [0.02528788 0.00501108 0.95724255 0.01245851]]\n",
      "Iteration 1546, Accuracy 0.36967\n",
      "97.31919%change in label assignment\n",
      "0.053933367\n",
      "[[0.01108129 0.9455528  0.00642497 0.03694084]\n",
      " [0.03244725 0.04343421 0.01266008 0.91145843]\n",
      " [0.03098409 0.0330333  0.0116799  0.9243027 ]\n",
      " ...\n",
      " [0.00866535 0.95459104 0.00490105 0.03184264]\n",
      " [0.02528063 0.00501265 0.9572457  0.01246101]\n",
      " [0.02530533 0.00502876 0.9571783  0.01248767]]\n",
      "Iteration 1547, Accuracy 0.36937\n",
      "96.40595%change in label assignment\n",
      "0.054003555\n",
      "[[0.00881558 0.9545595  0.00504251 0.0315824 ]\n",
      " [0.03072686 0.03557777 0.01182294 0.92187244]\n",
      " [0.03393979 0.03039604 0.01253384 0.92313033]\n",
      " ...\n",
      " [0.01052096 0.9416292  0.00586467 0.04198523]\n",
      " [0.02523671 0.00501405 0.95729494 0.0124543 ]\n",
      " [0.02527468 0.00503322 0.95720404 0.01248804]]\n",
      "Iteration 1548, Accuracy 0.3652\n",
      "98.69888%change in label assignment\n",
      "0.05372528\n",
      "[[0.01237584 0.9400335  0.00722135 0.04036933]\n",
      " [0.03613596 0.05782753 0.01452674 0.8915098 ]\n",
      " [0.0311827  0.03217851 0.01170887 0.9249299 ]\n",
      " ...\n",
      " [0.00868267 0.9549504  0.00493137 0.03143555]\n",
      " [0.02525973 0.00502761 0.9572336  0.01247903]\n",
      " [0.02531903 0.00505124 0.9571061  0.01252359]]\n",
      "Iteration 1549, Accuracy 0.36319\n",
      "95.36505%change in label assignment\n",
      "0.05564957\n",
      "[[0.00867392 0.9543276  0.0049106  0.03208784]\n",
      " [0.03076577 0.03564958 0.01180482 0.9217799 ]\n",
      " [0.03803094 0.0305892  0.01372682 0.9176531 ]\n",
      " ...\n",
      " [0.01252153 0.9285911  0.0068993  0.05198805]\n",
      " [0.02522405 0.00502564 0.95728266 0.01246765]\n",
      " [0.02528735 0.00504996 0.9571486  0.01251402]]\n",
      "Iteration 1550, Accuracy 0.35376\n",
      "97.96239%change in label assignment\n",
      "0.05331896\n",
      "[[0.01255698 0.939299   0.00731716 0.04082692]\n",
      " [0.03577747 0.05602842 0.01427719 0.89391685]\n",
      " [0.03129513 0.03218336 0.01169538 0.92482615]\n",
      " ...\n",
      " [0.00870099 0.95494515 0.00493326 0.03142061]\n",
      " [0.02522217 0.00503089 0.95727056 0.01247632]\n",
      " [0.02528971 0.0050558  0.95713    0.01252449]]\n",
      "Iteration 1551, Accuracy 0.35391\n",
      "98.23243%change in label assignment\n",
      "0.05383815\n",
      "[[0.01141288 0.9440349  0.00665858 0.03789365]\n",
      " [0.03679476 0.06112499 0.01497181 0.88710845]\n",
      " [0.03181068 0.03112515 0.01192458 0.9251396 ]\n",
      " ...\n",
      " [0.00866024 0.9546247  0.00492594 0.03178906]\n",
      " [0.0251313  0.00500691 0.95743626 0.01242554]\n",
      " [0.02517432 0.00502699 0.9573371  0.01246164]]\n",
      "Iteration 1552, Accuracy 0.3568\n",
      "95.01154%change in label assignment\n",
      "0.050839774\n",
      "[[0.00890422 0.95440984 0.00508305 0.03160282]\n",
      " [0.03317868 0.04684618 0.01309378 0.90688133]\n",
      " [0.03400287 0.03033187 0.01245638 0.9232089 ]\n",
      " ...\n",
      " [0.00978637 0.94645905 0.00545568 0.03829892]\n",
      " [0.02508532 0.00498276 0.9575469  0.01238503]\n",
      " [0.02508811 0.00499485 0.9575158  0.01240127]]\n",
      "Iteration 1553, Accuracy 0.35675\n",
      "97.04424%change in label assignment\n",
      "0.05304481\n",
      "[[0.01229737 0.94035727 0.00717861 0.04016673]\n",
      " [0.04127353 0.07915422 0.01705038 0.8625219 ]\n",
      " [0.03090046 0.03274605 0.01167054 0.9246829 ]\n",
      " ...\n",
      " [0.00888977 0.95446396 0.00507874 0.03156753]\n",
      " [0.02505847 0.0049824  0.9575769  0.0123822 ]\n",
      " [0.02506386 0.00499511 0.9575411  0.01239985]]\n",
      "Iteration 1554, Accuracy 0.34836\n",
      "99.32243%change in label assignment\n",
      "0.05283751\n",
      "[[0.00863261 0.9547911  0.00489794 0.03167827]\n",
      " [0.03258954 0.04496677 0.01286384 0.9095799 ]\n",
      " [0.03592374 0.03031769 0.01308267 0.9206759 ]\n",
      " ...\n",
      " [0.01059869 0.94109523 0.00589205 0.04241407]\n",
      " [0.02500818 0.00497776 0.9576494  0.01236458]\n",
      " [0.02501723 0.00499151 0.9576069  0.01238441]]\n",
      "Iteration 1555, Accuracy 0.34998\n",
      "98.9591%change in label assignment\n",
      "0.054724514\n",
      "[[0.00971152 0.95131207 0.00557649 0.03339993]\n",
      " [0.03604399 0.05727774 0.01442802 0.89225024]\n",
      " [0.03206268 0.03108379 0.01186934 0.9249841 ]\n",
      " ...\n",
      " [0.0087233  0.95388854 0.00491118 0.03247698]\n",
      " [0.02499315 0.00497978 0.95766175 0.01236536]\n",
      " [0.02500826 0.00499459 0.9576091  0.012388  ]]\n",
      "Iteration 1556, Accuracy 0.34664\n",
      "98.36991%change in label assignment\n",
      "0.051950444\n",
      "[[0.00871934 0.95493305 0.00497267 0.03137495]\n",
      " [0.03245807 0.04450971 0.01279788 0.91023433]\n",
      " [0.03530768 0.03028686 0.01289672 0.9215088 ]\n",
      " ...\n",
      " [0.01009259 0.9443687  0.00562749 0.03991125]\n",
      " [0.0249924  0.00500488 0.95760393 0.01239876]\n",
      " [0.02505603 0.00502957 0.95746875 0.01244568]]\n",
      "Iteration 1557, Accuracy 0.34649\n",
      "96.04753%change in label assignment\n",
      "0.053672887\n",
      "[[0.01122089 0.94495076 0.00650512 0.03732333]\n",
      " [0.0387373  0.06808372 0.01575087 0.8774281 ]\n",
      " [0.03105471 0.03239999 0.01165771 0.92488754]\n",
      " ...\n",
      " [0.00876691 0.9548365  0.00498724 0.03140939]\n",
      " [0.02495182 0.00499023 0.9576858  0.01237206]\n",
      " [0.02499628 0.00501119 0.9575829  0.01240962]]\n",
      "Iteration 1558, Accuracy 0.35204\n",
      "98.96401%change in label assignment\n",
      "0.05197034\n",
      "[[0.00863764 0.954874   0.00491973 0.03156868]\n",
      " [0.03302906 0.04714668 0.01315403 0.9066702 ]\n",
      " [0.03453333 0.03028771 0.01272344 0.9224555 ]\n",
      " ...\n",
      " [0.00978948 0.94630545 0.00548389 0.03842124]\n",
      " [0.02489368 0.00497916 0.9577813  0.01234583]\n",
      " [0.02492976 0.0049988  0.9576918  0.01237969]]\n",
      "Iteration 1559, Accuracy 0.35155\n",
      "97.44685%change in label assignment\n",
      "[[0.00883993 0.95459783 0.00502144 0.03154078]\n",
      " [0.0351854  0.05384903 0.01396923 0.8969963 ]\n",
      " [0.03138696 0.03191396 0.01169244 0.9250067 ]\n",
      " ...\n",
      " [0.00865217 0.9547525  0.00488626 0.0317091 ]\n",
      " [0.02491496 0.00495637 0.9578058  0.01232281]\n",
      " [0.02487538 0.00496092 0.95784456 0.01231918]]\n",
      "Iteration 1560, Accuracy 0.34963\n",
      "95.73329%change in label assignment\n",
      "0.058444243\n",
      "[[0.01160688 0.9343943  0.0064571  0.04754177]\n",
      " [0.03097929 0.03192087 0.01177166 0.9253282 ]\n",
      " [0.04045231 0.03117696 0.01466363 0.9137071 ]\n",
      " ...\n",
      " [0.01378955 0.9199687  0.00761527 0.05862644]\n",
      " [0.02483374 0.0049583  0.9579004  0.01230752]\n",
      " [0.02483415 0.0049712  0.9578704  0.01232418]]\n",
      "Iteration 1561, Accuracy 0.33746\n",
      "96.72018%change in label assignment\n",
      "0.058204256\n",
      "[[0.01152935 0.9436633  0.00665665 0.03815067]\n",
      " [0.04186755 0.08020338 0.01710154 0.86082745]\n",
      " [0.03152202 0.03898141 0.01208318 0.91741335]\n",
      " ...\n",
      " [0.00996376 0.95024085 0.00570564 0.03408976]\n",
      " [0.02487535 0.00496454 0.9578299  0.01233024]\n",
      " [0.02487934 0.00497818 0.9577937  0.01234872]]\n",
      "Iteration 1562, Accuracy 0.3462\n",
      "97.0737%change in label assignment\n",
      "0.06399256\n",
      "[[0.01343154 0.9223344  0.00746513 0.056769  ]\n",
      " [0.03134817 0.04119375 0.01252497 0.91493315]\n",
      " [0.04457981 0.03235409 0.0161436  0.9069225 ]\n",
      " ...\n",
      " [0.01710945 0.8973093  0.00940986 0.07617142]\n",
      " [0.02480512 0.0049582  0.95793223 0.01230435]\n",
      " [0.02481599 0.00497391 0.95788294 0.01232712]]\n",
      "Iteration 1563, Accuracy 0.33883\n",
      "96.80365%change in label assignment\n",
      "0.062723204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01676449 0.9216048  0.0098305  0.05180016]\n",
      " [0.05846578 0.19878119 0.02559594 0.7171571 ]\n",
      " [0.03415658 0.04885543 0.01332938 0.90365857]\n",
      " ...\n",
      " [0.01387426 0.9337185  0.00806325 0.04434394]\n",
      " [0.02486434 0.00496736 0.95783174 0.01233664]\n",
      " [0.02487926 0.00498392 0.95777535 0.01236149]]\n",
      "Iteration 1564, Accuracy 0.3433\n",
      "98.09005%change in label assignment\n",
      "0.07153738\n",
      "[[0.03739808 0.73511624 0.0196158  0.20786987]\n",
      " [0.04389171 0.03209333 0.01583131 0.9081837 ]\n",
      " [0.07667234 0.04051028 0.02608075 0.85673666]\n",
      " ...\n",
      " [0.03383214 0.76719886 0.0179157  0.18105328]\n",
      " [0.0247803  0.00496491 0.9579499  0.01230497]\n",
      " [0.02481951 0.00498647 0.957852   0.012342  ]]\n",
      "Iteration 1565, Accuracy 0.34011\n",
      "92.25217%change in label assignment\n",
      "0.0803472\n",
      "[[0.03527789 0.8482856  0.02163366 0.0948028 ]\n",
      " [0.06127703 0.4222647  0.02885441 0.48760387]\n",
      " [0.04506969 0.09547556 0.01865818 0.8407966 ]\n",
      " ...\n",
      " [0.03053351 0.86637497 0.01856036 0.0845312 ]\n",
      " [0.0248828  0.00497193 0.9577936  0.01235174]\n",
      " [0.02489749 0.00498809 0.95773834 0.01237605]]\n",
      "Iteration 1566, Accuracy 0.34418\n",
      "92.14415%change in label assignment\n",
      "0.09167686\n",
      "[[0.02361466 0.8499243  0.01284142 0.11361957]\n",
      " [0.03343226 0.03077846 0.01269744 0.9230918 ]\n",
      " [0.07400142 0.04012165 0.02554405 0.86033297]\n",
      " ...\n",
      " [0.02412209 0.84609115 0.01311921 0.11666758]\n",
      " [0.02476615 0.00496832 0.9579547  0.01231082]\n",
      " [0.02481204 0.00499153 0.95784485 0.01235159]]\n",
      "Iteration 1567, Accuracy 0.33932\n",
      "93.86262%change in label assignment\n",
      "0.07487992\n",
      "[[0.02806028 0.875967   0.01694404 0.07902865]\n",
      " [0.06242689 0.28906772 0.02830941 0.620196  ]\n",
      " [0.03561932 0.05463825 0.01406552 0.8956769 ]\n",
      " ...\n",
      " [0.02658893 0.8817244  0.01601587 0.07567084]\n",
      " [0.02497327 0.00502562 0.9575565  0.01244461]\n",
      " [0.02508602 0.00506313 0.95733094 0.01251986]]\n",
      "Iteration 1568, Accuracy 0.34104\n",
      "94.6973%change in label assignment\n",
      "0.07549845\n",
      "[[0.01731407 0.8956944  0.00948619 0.07750531]\n",
      " [0.03025382 0.03380829 0.01173961 0.92419827]\n",
      " [0.05302376 0.03449379 0.0187506  0.8937319 ]\n",
      " ...\n",
      " [0.01387691 0.91920215 0.00769391 0.05922711]\n",
      " [0.02482016 0.00499221 0.9578257  0.01236183]\n",
      " [0.02490373 0.00502346 0.95765084 0.01242199]]\n",
      "Iteration 1569, Accuracy 0.35612\n",
      "96.37649%change in label assignment\n",
      "0.061039146\n",
      "[[0.01124821 0.9448123  0.00649264 0.03744684]\n",
      " [0.04785479 0.11187094 0.02013742 0.82013685]\n",
      " [0.03093446 0.03595045 0.01175925 0.92135584]\n",
      " ...\n",
      " [0.01165379 0.9430878  0.00674755 0.03851083]\n",
      " [0.02483217 0.00497986 0.9578318  0.01235613]\n",
      " [0.02488155 0.00500337 0.9577169  0.01239819]]\n",
      "Iteration 1570, Accuracy 0.3433\n",
      "96.37158%change in label assignment\n",
      "0.062847205\n",
      "[[0.01531025 0.90952355 0.00844903 0.06671714]\n",
      " [0.0314745  0.03106113 0.01199657 0.92546785]\n",
      " [0.05272941 0.03445436 0.01873416 0.89408207]\n",
      " ...\n",
      " [0.01557767 0.9077015  0.00859993 0.06812096]\n",
      " [0.0248756  0.00501851 0.9576895  0.01241642]\n",
      " [0.02499163 0.00505617 0.95745957 0.01249262]]\n",
      "Iteration 1571, Accuracy 0.3377\n",
      "93.98046%change in label assignment\n",
      "0.06284688\n",
      "[[0.0120662  0.9413483  0.00699032 0.03959514]\n",
      " [0.04227141 0.08266387 0.01736014 0.8577046 ]\n",
      " [0.03094044 0.03649055 0.01180595 0.920763  ]\n",
      " ...\n",
      " [0.012459   0.93967456 0.00723767 0.04062867]\n",
      " [0.02497732 0.00504138 0.9575037  0.01247762]\n",
      " [0.02511714 0.00508473 0.9572312  0.01256693]]\n",
      "Iteration 1572, Accuracy 0.35199\n",
      "97.36338%change in label assignment\n",
      "0.06237766\n",
      "[[0.01420236 0.91704357 0.00785363 0.06090045]\n",
      " [0.0312697  0.03108229 0.01190273 0.92574525]\n",
      " [0.0466546  0.03274319 0.01677433 0.90382785]\n",
      " ...\n",
      " [0.01562345 0.9074098  0.00861031 0.06835639]\n",
      " [0.02487318 0.00502198 0.9576753  0.01242951]\n",
      " [0.02499945 0.00506225 0.95742667 0.01251157]]\n",
      "Iteration 1573, Accuracy 0.35592\n",
      "98.38955%change in label assignment\n",
      "0.059329677\n",
      "[[0.01171015 0.9428909  0.0067828  0.03861615]\n",
      " [0.04245288 0.08379152 0.01749456 0.85626096]\n",
      " [0.03121992 0.038835   0.01204776 0.9178973 ]\n",
      " ...\n",
      " [0.01070962 0.9471317  0.00617736 0.03598129]\n",
      " [0.02485481 0.00500655 0.9577251  0.01241354]\n",
      " [0.0249512  0.00503998 0.95752907 0.01247971]]\n",
      "Iteration 1574, Accuracy 0.35445\n",
      "96.45996%change in label assignment\n",
      "0.0625663\n",
      "[[0.01720941 0.8964995  0.00945184 0.07683921]\n",
      " [0.03086639 0.0315537  0.01185713 0.9257228 ]\n",
      " [0.04847801 0.03326699 0.01742746 0.90082747]\n",
      " ...\n",
      " [0.0203875  0.87392    0.01111763 0.0945749 ]\n",
      " [0.02483848 0.00501602 0.95772344 0.01242206]\n",
      " [0.02495815 0.0050545  0.95748705 0.01250029]]\n",
      "Iteration 1575, Accuracy 0.3513\n",
      "97.08352%change in label assignment\n",
      "0.05920606\n",
      "[[0.03588724 0.8461599  0.02194888 0.09600399]\n",
      " [0.06317615 0.34190023 0.02897852 0.5659451 ]\n",
      " [0.04146438 0.07714283 0.01678726 0.86460555]\n",
      " ...\n",
      " [0.0306361  0.86612403 0.01855292 0.08468692]\n",
      " [0.02485322 0.00501367 0.95770407 0.01242903]\n",
      " [0.02495588 0.00504839 0.9574974  0.01249838]]\n",
      "Iteration 1576, Accuracy 0.3544\n",
      "95.61055%change in label assignment\n",
      "0.09058964\n",
      "[[0.02162138 0.8649056  0.01177255 0.10170041]\n",
      " [0.03191781 0.03074461 0.01218336 0.92515415]\n",
      " [0.06464362 0.03758113 0.02257824 0.87519693]\n",
      " ...\n",
      " [0.02689925 0.82466066 0.01449058 0.13394952]\n",
      " [0.02474473 0.00499462 0.9578825  0.01237808]\n",
      " [0.02484365 0.00502857 0.9576823  0.0124455 ]]\n",
      "Iteration 1577, Accuracy 0.35955\n",
      "97.09825%change in label assignment\n",
      "0.07167496\n",
      "[[0.02579238 0.8850292  0.01549301 0.07368536]\n",
      " [0.05696582 0.18039848 0.02476774 0.7378679 ]\n",
      " [0.03640186 0.05753336 0.01445705 0.89160776]\n",
      " ...\n",
      " [0.02109336 0.90384316 0.01253788 0.06252562]\n",
      " [0.02485132 0.00501884 0.9576838  0.01244602]\n",
      " [0.02496061 0.00505473 0.9574661  0.01251853]]\n",
      "Iteration 1578, Accuracy 0.36073\n",
      "95.64001%change in label assignment\n",
      "0.073532596\n",
      "[[0.01633125 0.9025993  0.00896326 0.07210624]\n",
      " [0.03462834 0.03007611 0.01285614 0.9224394 ]\n",
      " [0.05125878 0.03385076 0.01818294 0.8967075 ]\n",
      " ...\n",
      " [0.01908824 0.88328165 0.01041006 0.08722008]\n",
      " [0.02468484 0.00498122 0.9579807  0.01235321]\n",
      " [0.02476268 0.00501009 0.9578182  0.01240905]]\n",
      "Iteration 1579, Accuracy 0.36063\n",
      "94.91825%change in label assignment\n",
      "0.06128068\n",
      "[[0.01639136 0.9231444  0.00964892 0.05081533]\n",
      " [0.04389887 0.08986869 0.01812645 0.848106  ]\n",
      " [0.03208828 0.04202573 0.01243496 0.9134511 ]\n",
      " ...\n",
      " [0.01426009 0.9320915  0.00834274 0.04530575]\n",
      " [0.02469577 0.00496964 0.95798904 0.01234558]\n",
      " [0.02474593 0.00499277 0.9578739  0.01238742]]\n",
      "Iteration 1580, Accuracy 0.3514\n",
      "96.59744%change in label assignment\n",
      "0.0653332\n",
      "[[0.01490091 0.912306   0.00823859 0.06455451]\n",
      " [0.03553127 0.0301669  0.01319884 0.92110306]\n",
      " [0.05063081 0.03375585 0.01808263 0.8975307 ]\n",
      " ...\n",
      " [0.01872625 0.88577384 0.01025297 0.085247  ]\n",
      " [0.02463752 0.00497483 0.9580483  0.01233936]\n",
      " [0.02471458 0.00500393 0.95788616 0.01239533]]\n",
      "Iteration 1581, Accuracy 0.34713\n",
      "95.54181%change in label assignment\n",
      "0.060018353\n",
      "[[0.01969491 0.9094588  0.01170589 0.05914037]\n",
      " [0.04303398 0.08585465 0.01771519 0.85339606]\n",
      " [0.03179011 0.04090293 0.01229395 0.915013  ]\n",
      " ...\n",
      " [0.0153711  0.927397   0.0090284  0.04820346]\n",
      " [0.02469438 0.00498504 0.9579506  0.01237003]\n",
      " [0.02477016 0.00501338 0.9577917  0.01242476]]\n",
      "Iteration 1582, Accuracy 0.35312\n",
      "97.60888%change in label assignment\n",
      "0.06398329\n",
      "[[0.01248664 0.92843294 0.00696104 0.0521194 ]\n",
      " [0.03333094 0.03010367 0.01251672 0.92404866]\n",
      " [0.04668923 0.03268061 0.01683045 0.9037998 ]\n",
      " ...\n",
      " [0.01561824 0.9074003  0.00862377 0.06835766]\n",
      " [0.02459906 0.00497196 0.9580968  0.01233225]\n",
      " [0.02467581 0.00500098 0.9579351  0.01238806]]\n",
      "Iteration 1583, Accuracy 0.35734\n",
      "98.30608%change in label assignment\n",
      "0.057084385\n",
      "[[0.01217604 0.94091326 0.00708701 0.0398237 ]\n",
      " [0.03692794 0.06103183 0.01488371 0.8871565 ]\n",
      " [0.03049374 0.03477483 0.01164193 0.9230895 ]\n",
      " ...\n",
      " [0.01084596 0.9465686  0.00627854 0.0363069 ]\n",
      " [0.02463832 0.00498158 0.9580212  0.01235893]\n",
      " [0.02472265 0.00501242 0.95784605 0.01241893]]\n",
      "Iteration 1584, Accuracy 0.35322\n",
      "98.88054%change in label assignment\n",
      "0.058925312\n",
      "[[0.01137395 0.93573123 0.00635679 0.046538  ]\n",
      " [0.03215242 0.03024345 0.01212355 0.9254805 ]\n",
      " [0.04405216 0.03190764 0.01593374 0.90810645]\n",
      " ...\n",
      " [0.01452166 0.9148393  0.00802801 0.06261102]\n",
      " [0.02453998 0.00496167 0.9581881  0.01231026]\n",
      " [0.02460783 0.00498877 0.9580419  0.01236145]]\n",
      "Iteration 1585, Accuracy 0.35621\n",
      "98.68415%change in label assignment\n",
      "0.05674866\n",
      "[[0.01661633 0.9221912  0.00979927 0.05139318]\n",
      " [0.04066899 0.07558248 0.01660399 0.8671445 ]\n",
      " [0.03130299 0.03915018 0.01207544 0.9174714 ]\n",
      " ...\n",
      " [0.0132492  0.9363657  0.00773111 0.04265394]\n",
      " [0.0245476  0.00495293 0.9581952  0.01230425]\n",
      " [0.0245914  0.00497479 0.95809084 0.01234292]]\n",
      "Iteration 1586, Accuracy 0.35484\n",
      "97.32901%change in label assignment\n",
      "0.061073624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01565887 0.9071169  0.00862421 0.06859999]\n",
      " [0.03635842 0.03015526 0.01342138 0.92006487]\n",
      " [0.04858512 0.03312426 0.0174007  0.90088993]\n",
      " ...\n",
      " [0.01892811 0.88430625 0.01034249 0.08642317]\n",
      " [0.02448116 0.00495235 0.9582774  0.01228907]\n",
      " [0.02454241 0.00497807 0.95814264 0.01233689]]\n",
      "Iteration 1587, Accuracy 0.35391\n",
      "96.61708%change in label assignment\n",
      "0.059138954\n",
      "[[0.01664154 0.922057   0.00983392 0.05146756]\n",
      " [0.04197336 0.08196259 0.01730799 0.85875607]\n",
      " [0.03164126 0.04127672 0.01232762 0.91475445]\n",
      " ...\n",
      " [0.0144755  0.93114835 0.00850191 0.04587425]\n",
      " [0.02451764 0.00495754 0.95821565 0.01230912]\n",
      " [0.02457525 0.0049825  0.9580871  0.01235512]]\n",
      "Iteration 1588, Accuracy 0.35734\n",
      "98.35518%change in label assignment\n",
      "0.061876796\n",
      "[[0.01438105 0.915731   0.00797499 0.06191296]\n",
      " [0.03464903 0.03004251 0.01295659 0.92235196]\n",
      " [0.04705967 0.03280615 0.01701525 0.9031189 ]\n",
      " ...\n",
      " [0.01686211 0.89875084 0.00929189 0.07509516]\n",
      " [0.02443455 0.00495183 0.9583299  0.01228369]\n",
      " [0.02450047 0.00497857 0.9581869  0.01233397]]\n",
      "Iteration 1589, Accuracy 0.35955\n",
      "97.62361%change in label assignment\n",
      "0.060313188\n",
      "[[0.01742683 0.91878057 0.01031827 0.05347438]\n",
      " [0.04366973 0.08996222 0.01813745 0.84823054]\n",
      " [0.03124019 0.0395705  0.01211654 0.9170728 ]\n",
      " ...\n",
      " [0.01520417 0.92806894 0.00894857 0.04777826]\n",
      " [0.02449836 0.00496931 0.958207   0.01232537]\n",
      " [0.02457894 0.00499925 0.9580385  0.01238335]]\n",
      "Iteration 1590, Accuracy 0.35607\n",
      "97.26518%change in label assignment\n",
      "0.05761185\n",
      "[[0.00985905 0.9455862  0.00554421 0.03901054]\n",
      " [0.0300886  0.03612761 0.01178746 0.9219963 ]\n",
      " [0.03584683 0.0300593  0.01327928 0.9208146 ]\n",
      " ...\n",
      " [0.00924246 0.94967943 0.00522812 0.03584999]\n",
      " [0.024353   0.00491725 0.9585057  0.01222409]\n",
      " [0.02435964 0.00493205 0.95846367 0.01224466]]\n",
      "Iteration 1591, Accuracy 0.35916\n",
      "94.7955%change in label assignment\n",
      "0.052598644\n",
      "[[0.0087106  0.9551949  0.00500041 0.03109409]\n",
      " [0.03568802 0.05831151 0.01456982 0.8914307 ]\n",
      " [0.032116   0.03010321 0.0120483  0.92573255]\n",
      " ...\n",
      " [0.00872799 0.95511943 0.00501611 0.03113646]\n",
      " [0.02440755 0.00496197 0.95833254 0.01229794]\n",
      " [0.02449755 0.00499469 0.958146   0.01236175]]\n",
      "Iteration 1592, Accuracy 0.34708\n",
      "95.53199%change in label assignment\n",
      "0.050181378\n",
      "[[0.00953821 0.9477341  0.00535628 0.03737139]\n",
      " [0.03101841 0.04064234 0.01227169 0.91606754]\n",
      " [0.03935561 0.03060242 0.01431465 0.9157273 ]\n",
      " ...\n",
      " [0.01008138 0.94415885 0.00564625 0.04011353]\n",
      " [0.02447523 0.00499438 0.95817167 0.01235866]\n",
      " [0.02460959 0.0050368  0.9579079  0.01244569]]\n",
      "Iteration 1593, Accuracy 0.36093\n",
      "97.34374%change in label assignment\n",
      "0.051256105\n",
      "[[0.00931812 0.95297116 0.00535682 0.03235388]\n",
      " [0.03997033 0.07453991 0.01650932 0.8689804 ]\n",
      " [0.03054471 0.03160568 0.01156136 0.92628825]\n",
      " ...\n",
      " [0.00923132 0.9533161  0.00530798 0.03214459]\n",
      " [0.02441958 0.00497897 0.9582709  0.01233047]\n",
      " [0.02453689 0.00501772 0.9580367  0.01240866]]\n",
      "Iteration 1594, Accuracy 0.36662\n",
      "98.18825%change in label assignment\n",
      "0.050042786\n",
      "[[0.01112956 0.9374008  0.00618241 0.04528725]\n",
      " [0.03038026 0.03721054 0.01185861 0.9205506 ]\n",
      " [0.03591318 0.02995038 0.01319803 0.9209385 ]\n",
      " ...\n",
      " [0.01051747 0.9413583  0.00586648 0.0422577 ]\n",
      " [0.02430414 0.00495117 0.95847666 0.0122681 ]\n",
      " [0.02439669 0.00498472 0.958285   0.0123336 ]]\n",
      "Iteration 1595, Accuracy 0.36648\n",
      "97.94766%change in label assignment\n",
      "0.05115651\n",
      "[[0.0084964  0.95565104 0.0048312  0.03102139]\n",
      " [0.03321321 0.0483674  0.01325123 0.9051682 ]\n",
      " [0.03090177 0.03088329 0.01165594 0.926559  ]\n",
      " ...\n",
      " [0.00848275 0.95564336 0.00482513 0.03104872]\n",
      " [0.02422959 0.00492515 0.95862633 0.01221886]\n",
      " [0.02428148 0.00494945 0.95850646 0.0122626 ]]\n",
      "Iteration 1596, Accuracy 0.36171\n",
      "98.36009%change in label assignment\n",
      "0.049773555\n",
      "[[0.00886639 0.95224535 0.00500826 0.03388003]\n",
      " [0.03114324 0.04102354 0.01230794 0.9155253 ]\n",
      " [0.03190566 0.03011012 0.01199383 0.9259904 ]\n",
      " ...\n",
      " [0.00872862 0.95321715 0.00494425 0.03310999]\n",
      " [0.02417004 0.00489785 0.9587612  0.01217091]\n",
      " [0.02417091 0.00491115 0.9587298  0.01218811]]\n",
      "Iteration 1597, Accuracy 0.35911\n",
      "96.25374%change in label assignment\n",
      "0.05243982\n",
      "[[0.0084671  0.9555573  0.00483612 0.03113946]\n",
      " [0.03160009 0.04319966 0.01259154 0.9126087 ]\n",
      " [0.03227527 0.0299859  0.01212982 0.925609  ]\n",
      " ...\n",
      " [0.00858856 0.9542627  0.00488407 0.03226472]\n",
      " [0.02413091 0.00489737 0.95880854 0.01216317]\n",
      " [0.02413836 0.00491186 0.9587663  0.01218349]]\n",
      "Iteration 1598, Accuracy 0.3514\n",
      "97.68744%change in label assignment\n",
      "0.04985338\n",
      "[[0.01009732 0.9441148  0.00563252 0.04015539]\n",
      " [0.03041177 0.03690903 0.01180332 0.92087585]\n",
      " [0.0326872  0.02992469 0.0121623  0.9252258 ]\n",
      " ...\n",
      " [0.00901829 0.9512204  0.00507675 0.03468465]\n",
      " [0.02416428 0.00488474 0.9587956  0.01215532]\n",
      " [0.02411638 0.00488867 0.95884633 0.0121486 ]]\n",
      "Iteration 1599, Accuracy 0.35567\n",
      "96.44032%change in label assignment\n",
      "[[0.01264544 0.92734975 0.00699064 0.05301414]\n",
      " [0.02998866 0.03433718 0.01158577 0.92408836]\n",
      " [0.03424612 0.0298229  0.01269772 0.92323333]\n",
      " ...\n",
      " [0.00958484 0.9473547  0.00538671 0.03767378]\n",
      " [0.0241153  0.00488163 0.9588623  0.01214081]\n",
      " [0.02407234 0.00488682 0.95890397 0.0121369 ]]\n",
      "Iteration 1600, Accuracy 0.3406\n",
      "99.05239%change in label assignment\n",
      "0.055483893\n",
      "[[0.01034278 0.94252276 0.00575797 0.04137645]\n",
      " [0.03167533 0.04280082 0.01250323 0.9130206 ]\n",
      " [0.03037015 0.03147381 0.01154573 0.9266103 ]\n",
      " ...\n",
      " [0.00846588 0.9556815  0.00482134 0.03103126]\n",
      " [0.02410894 0.00488258 0.9588643  0.01214435]\n",
      " [0.02407132 0.00488901 0.95889634 0.01214333]]\n",
      "Iteration 1601, Accuracy 0.34094\n",
      "99.2586%change in label assignment\n",
      "0.05522945\n",
      "[[0.01560791 0.9073676  0.00856861 0.06845592]\n",
      " [0.0299027  0.0354719  0.01168875 0.9229367 ]\n",
      " [0.03496949 0.02987206 0.01300063 0.9221579 ]\n",
      " ...\n",
      " [0.01143878 0.9351875  0.00638106 0.04699268]\n",
      " [0.0240688  0.00487939 0.9589188  0.01213304]\n",
      " [0.02403765 0.00488727 0.9589397  0.01213543]]\n",
      "Iteration 1602, Accuracy 0.34104\n",
      "99.06221%change in label assignment\n",
      "0.056023896\n",
      "[[0.00876853 0.9550238  0.00501222 0.03119544]\n",
      " [0.04454592 0.09633849 0.01876998 0.84034556]\n",
      " [0.0304292  0.03745737 0.01185018 0.92026323]\n",
      " ...\n",
      " [0.00932191 0.9529272  0.00536696 0.03238399]\n",
      " [0.02411576 0.00488098 0.95885223 0.01215101]\n",
      " [0.02406387 0.0048844  0.9589091  0.01214263]]\n",
      "Iteration 1603, Accuracy 0.33913\n",
      "96.44032%change in label assignment\n",
      "0.061108828\n",
      "[[0.03653654 0.74197924 0.01921373 0.20227048]\n",
      " [0.03187463 0.03008729 0.01208446 0.92595357]\n",
      " [0.04800278 0.03296209 0.01726666 0.9017685 ]\n",
      " ...\n",
      " [0.02596484 0.83139354 0.01399001 0.12865162]\n",
      " [0.02414535 0.00487706 0.95882773 0.01214983]\n",
      " [0.02405514 0.00487288 0.95894974 0.01212233]]\n",
      "Iteration 1604, Accuracy 0.33255\n",
      "95.62528%change in label assignment\n",
      "0.069127604\n",
      "[[0.02190261 0.90041673 0.01310639 0.06457422]\n",
      " [0.06211845 0.37895197 0.02910991 0.52981967]\n",
      " [0.04606083 0.10287345 0.01936582 0.83169985]\n",
      " ...\n",
      " [0.02544786 0.8861773  0.01536883 0.07300598]\n",
      " [0.02427533 0.00489423 0.9586163  0.01221406]\n",
      " [0.02416815 0.00488617 0.95876825 0.01217745]]\n",
      "Iteration 1605, Accuracy 0.32705\n",
      "94.97717%change in label assignment\n",
      "0.09054487\n",
      "[[0.03963071 0.7114794  0.02102179 0.22786811]\n",
      " [0.03011069 0.03256479 0.01192833 0.9253961 ]\n",
      " [0.07058018 0.03944067 0.02493656 0.86504257]\n",
      " ...\n",
      " [0.0312513  0.78789    0.01690685 0.16395178]\n",
      " [0.02419095 0.00488264 0.95875597 0.01217038]\n",
      " [0.02406535 0.0048718  0.9589371  0.01212566]]\n",
      "Iteration 1606, Accuracy 0.33328\n",
      "94.73167%change in label assignment\n",
      "0.08139867\n",
      "[[0.03190665 0.86102134 0.01945096 0.08762108]\n",
      " [0.05344838 0.5664794  0.02610996 0.35396224]\n",
      " [0.04530691 0.09718626 0.01882847 0.8386784 ]\n",
      " ...\n",
      " [0.03377593 0.853856   0.02068356 0.0916845 ]\n",
      " [0.02418578 0.00488835 0.95873976 0.01218611]\n",
      " [0.02410187 0.00488683 0.95884776 0.01216355]]\n",
      "Iteration 1607, Accuracy 0.34409\n",
      "95.57618%change in label assignment\n",
      "0.09536066\n",
      "[[0.03007824 0.7976196  0.01623513 0.15606706]\n",
      " [0.03077555 0.0311498  0.0120183  0.9260564 ]\n",
      " [0.08238208 0.04206879 0.02843162 0.84711754]\n",
      " ...\n",
      " [0.02777787 0.8165098  0.01508912 0.14062321]\n",
      " [0.02407779 0.00487304 0.958917   0.01213223]\n",
      " [0.02398849 0.00487019 0.9590347  0.01210662]]\n",
      "Iteration 1608, Accuracy 0.33255\n",
      "95.92478%change in label assignment\n",
      "0.077962734\n",
      "[[0.02467707 0.88917524 0.01486326 0.07128444]\n",
      " [0.056421   0.51778334 0.02738258 0.39841303]\n",
      " [0.03620121 0.05817712 0.01449266 0.89112896]\n",
      " ...\n",
      " [0.0285856  0.8737241  0.0173797  0.0803106 ]\n",
      " [0.02410947 0.004887   0.9588365  0.01216714]\n",
      " [0.02406808 0.0048953  0.9588691  0.01216758]]\n",
      "Iteration 1609, Accuracy 0.32597\n",
      "94.54019%change in label assignment\n",
      "0.0829157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02226115 0.85929304 0.01215081 0.10629497]\n",
      " [0.03309632 0.05062733 0.01368338 0.902593  ]\n",
      " [0.06159282 0.03676595 0.02168644 0.8799548 ]\n",
      " ...\n",
      " [0.0167675  0.8989747  0.0092916  0.07496626]\n",
      " [0.024074   0.00487428 0.9589146  0.01213718]\n",
      " [0.0239964  0.00487459 0.9590105  0.01211848]]\n",
      "Iteration 1610, Accuracy 0.32651\n",
      "93.72514%change in label assignment\n",
      "0.06723253\n",
      "[[0.01537195 0.9272819  0.00904749 0.04829869]\n",
      " [0.06195517 0.3707444  0.02910219 0.5381983 ]\n",
      " [0.0322753  0.04408072 0.01265151 0.9109925 ]\n",
      " ...\n",
      " [0.02055215 0.9057817  0.01228826 0.06137796]\n",
      " [0.02406825 0.00488714 0.95888084 0.01216374]\n",
      " [0.02404753 0.00489961 0.9588782  0.01217458]]\n",
      "Iteration 1611, Accuracy 0.32474\n",
      "94.6433%change in label assignment\n",
      "0.07215489\n",
      "[[0.02832124 0.8120646  0.01528621 0.14432801]\n",
      " [0.03233906 0.04788618 0.01338379 0.90639096]\n",
      " [0.05922472 0.03627685 0.02114356 0.8833548 ]\n",
      " ...\n",
      " [0.01388851 0.9187412  0.00778036 0.05958997]\n",
      " [0.02398511 0.00491051 0.9589311  0.01217332]\n",
      " [0.02407206 0.0049465  0.95874083 0.01224062]]\n",
      "Iteration 1612, Accuracy 0.33049\n",
      "93.19487%change in label assignment\n",
      "0.06252007\n",
      "[[0.008846   0.9541928  0.00500324 0.03195796]\n",
      " [0.06016472 0.23742735 0.02708097 0.675327  ]\n",
      " [0.03057791 0.03509827 0.0116394  0.92268443]\n",
      " ...\n",
      " [0.01509806 0.9284414  0.00887366 0.04758689]\n",
      " [0.02403735 0.00490408 0.9588768  0.01218177]\n",
      " [0.02408941 0.00493265 0.95874685 0.01223107]]\n",
      "Iteration 1613, Accuracy 0.3381\n",
      "97.45176%change in label assignment\n",
      "0.06537757\n",
      "[[0.01984147 0.87710696 0.01088037 0.09217118]\n",
      " [0.03722545 0.06684551 0.01576545 0.8801636 ]\n",
      " [0.04633132 0.03260363 0.01689504 0.90417   ]\n",
      " ...\n",
      " [0.01144937 0.93492866 0.0064532  0.04716877]\n",
      " [0.02395795 0.00489794 0.9589871  0.01215698]\n",
      " [0.02401903 0.00492858 0.9588412  0.01221112]]\n",
      "Iteration 1614, Accuracy 0.33682\n",
      "96.3814%change in label assignment\n",
      "0.058064513\n",
      "[[0.01174632 0.94267505 0.00682145 0.03875715]\n",
      " [0.06216489 0.34013584 0.02901576 0.56868345]\n",
      " [0.0305158  0.03656    0.01174678 0.9211774 ]\n",
      " ...\n",
      " [0.01478868 0.92974955 0.00869894 0.04676281]\n",
      " [0.02399806 0.00489894 0.95892805 0.01217498]\n",
      " [0.02404437 0.00492615 0.9588083  0.01222114]]\n",
      "Iteration 1615, Accuracy 0.33608\n",
      "95.96406%change in label assignment\n",
      "0.06804532\n",
      "[[0.0431225  0.6763362  0.022592   0.25794926]\n",
      " [0.02958867 0.03299883 0.01173722 0.92567533]\n",
      " [0.0753339  0.04025267 0.02623999 0.8581735 ]\n",
      " ...\n",
      " [0.03760583 0.73132604 0.01995658 0.21111152]\n",
      " [0.02392813 0.00488813 0.95903516 0.01214858]\n",
      " [0.02398011 0.00491659 0.9589056  0.0121977 ]]\n",
      "Iteration 1616, Accuracy 0.33608\n",
      "97.53523%change in label assignment\n",
      "0.09071222\n",
      "[[0.05389684 0.78117764 0.03402761 0.13089785]\n",
      " [0.01241834 0.93102854 0.00673531 0.04981786]\n",
      " [0.06067651 0.22664477 0.02678367 0.685895  ]\n",
      " ...\n",
      " [0.06590206 0.74031514 0.04249102 0.15129177]\n",
      " [0.02404645 0.0048962  0.958857   0.01220036]\n",
      " [0.02404633 0.00491405 0.95881647 0.01222318]]\n",
      "Iteration 1617, Accuracy 0.34266\n",
      "94.16703%change in label assignment\n",
      "0.10449131\n",
      "[[0.01354633 0.92098325 0.00756418 0.05790629]\n",
      " [0.05516926 0.18230502 0.02519585 0.7373299 ]\n",
      " [0.04185425 0.03132567 0.01540317 0.9114169 ]\n",
      " ...\n",
      " [0.0086639  0.95471317 0.00502689 0.03159606]\n",
      " [0.0239268  0.0048713  0.9590701  0.01213177]\n",
      " [0.02389142 0.00488238 0.959089   0.01213716]]\n",
      "Iteration 1618, Accuracy 0.33196\n",
      "95.93951%change in label assignment\n",
      "0.058567796\n",
      "[[0.00899212 0.95156676 0.00504882 0.03439232]\n",
      " [0.06131389 0.28061935 0.02827816 0.6297886 ]\n",
      " [0.03293975 0.02983459 0.01214755 0.9250781 ]\n",
      " ...\n",
      " [0.00947524 0.95228016 0.00546427 0.03278031]\n",
      " [0.02400103 0.00487345 0.95896834 0.01215725]\n",
      " [0.0239452  0.00487971 0.9590237  0.01215143]]\n",
      "Iteration 1619, Accuracy 0.32017\n",
      "97.92311%change in label assignment\n",
      "0.05654911\n",
      "[[0.01794942 0.8908635  0.00983364 0.08135342]\n",
      " [0.05077028 0.14010422 0.02256388 0.7865616 ]\n",
      " [0.04221879 0.03119127 0.01536153 0.9112284 ]\n",
      " ...\n",
      " [0.01003491 0.9442107  0.00566264 0.0400917 ]\n",
      " [0.02392255 0.00487076 0.95906794 0.01213882]\n",
      " [0.02390098 0.00488459 0.9590633  0.01215112]]\n",
      "Iteration 1620, Accuracy 0.3272\n",
      "98.44356%change in label assignment\n",
      "0.05540636\n",
      "[[0.00970764 0.9466307  0.0054274  0.03823429]\n",
      " [0.05856341 0.21806441 0.02652989 0.6968423 ]\n",
      " [0.03139458 0.0300527  0.01175735 0.92679536]\n",
      " ...\n",
      " [0.0085627  0.9558052  0.00490497 0.03072712]\n",
      " [0.02398532 0.00487071 0.95898426 0.01215978]\n",
      " [0.02392475 0.0048758  0.9590482  0.0121513 ]]\n",
      "Iteration 1621, Accuracy 0.32238\n",
      "95.71365%change in label assignment\n",
      "0.05537829\n",
      "[[0.02214305 0.8605614  0.01200301 0.10529249]\n",
      " [0.04212953 0.08779874 0.01806063 0.85201114]\n",
      " [0.04209699 0.03113476 0.0153311  0.91143715]\n",
      " ...\n",
      " [0.01261419 0.92729455 0.00703169 0.05305954]\n",
      " [0.02389036 0.00486711 0.9591079  0.01213473]\n",
      " [0.02386803 0.00488127 0.95910347 0.01214726]]\n",
      "Iteration 1622, Accuracy 0.34114\n",
      "95.38469%change in label assignment\n",
      "0.056368936\n",
      "[[0.00860246 0.9556519  0.00491474 0.03083087]\n",
      " [0.06194668 0.3357373  0.02908047 0.5732356 ]\n",
      " [0.03017493 0.03681872 0.01175023 0.9212561 ]\n",
      " ...\n",
      " [0.01241252 0.9398749  0.00728207 0.04043054]\n",
      " [0.0239506  0.00487005 0.9590204  0.01215897]\n",
      " [0.0238964  0.00487743 0.95907116 0.01215496]]\n",
      "Iteration 1623, Accuracy 0.32209\n",
      "95.14411%change in label assignment\n",
      "0.061799113\n",
      "[[0.02720892 0.8212338  0.01466405 0.1368932 ]\n",
      " [0.04491953 0.10316338 0.01964925 0.83226776]\n",
      " [0.0437127  0.03169707 0.01600971 0.9085805 ]\n",
      " ...\n",
      " [0.01334698 0.92233604 0.00746098 0.05685597]\n",
      " [0.02385069 0.00486148 0.95916516 0.01212264]\n",
      " [0.02381731 0.00487403 0.9591783  0.01213035]]\n",
      "Iteration 1624, Accuracy 0.33967\n",
      "97.67271%change in label assignment\n",
      "0.059722118\n",
      "[[0.00957411 0.9478693  0.00533497 0.03722158]\n",
      " [0.06218567 0.32548252 0.02895904 0.5833728 ]\n",
      " [0.03044774 0.03701413 0.01178333 0.9207548 ]\n",
      " ...\n",
      " [0.0114502  0.94401234 0.00666172 0.03787573]\n",
      " [0.0239509  0.00486699 0.9590203  0.01216181]\n",
      " [0.02387289 0.00486959 0.95911175 0.01214574]]\n",
      "Iteration 1625, Accuracy 0.34281\n",
      "96.131%change in label assignment\n",
      "0.06290962\n",
      "[[0.03895595 0.71835816 0.02056126 0.22212465]\n",
      " [0.0358795  0.06154256 0.01516644 0.8874115 ]\n",
      " [0.05563836 0.0350814  0.01997053 0.88930964]\n",
      " ...\n",
      " [0.02155092 0.8646873  0.01181583 0.10194593]\n",
      " [0.02379065 0.00486026 0.9592364  0.01211265]\n",
      " [0.02378573 0.00487924 0.9591994  0.0121356 ]]\n",
      "Iteration 1626, Accuracy 0.34566\n",
      "94.56474%change in label assignment\n",
      "0.06593554\n",
      "[[0.01133974 0.944469   0.00656553 0.03762576]\n",
      " [0.05942361 0.45995772 0.02855399 0.45206466]\n",
      " [0.03341774 0.04845994 0.01324664 0.90487576]\n",
      " ...\n",
      " [0.01980353 0.908979   0.01181278 0.05940469]\n",
      " [0.02386781 0.00489845 0.95904607 0.01218772]\n",
      " [0.02394451 0.00493339 0.95887065 0.01225136]]\n",
      "Iteration 1627, Accuracy 0.32543\n",
      "94.10321%change in label assignment\n",
      "0.06801375\n",
      "[[0.02155006 0.8646942  0.01175637 0.10199939]\n",
      " [0.04385568 0.09746502 0.01910357 0.83957577]\n",
      " [0.04258244 0.03136669 0.01565088 0.9104    ]\n",
      " ...\n",
      " [0.011887   0.93201774 0.00667832 0.04941697]\n",
      " [0.02375746 0.00487806 0.9592301  0.01213437]\n",
      " [0.02381445 0.00490865 0.9590894  0.01218751]]\n",
      "Iteration 1628, Accuracy 0.33044\n",
      "98.09987%change in label assignment\n",
      "0.055997763\n",
      "[[0.00851086 0.95529544 0.00482333 0.03137035]\n",
      " [0.05974312 0.23658739 0.02716839 0.6765011 ]\n",
      " [0.02999612 0.03205679 0.01142478 0.92652225]\n",
      " ...\n",
      " [0.00978747 0.95105034 0.00566062 0.03350163]\n",
      " [0.02379383 0.00488309 0.95916927 0.01215378]\n",
      " [0.02385035 0.0049132  0.9590302  0.01220623]]\n",
      "Iteration 1629, Accuracy 0.33029\n",
      "98.12933%change in label assignment\n",
      "0.057232417\n",
      "[[0.02582086 0.8323919  0.01390263 0.12788458]\n",
      " [0.04383378 0.09681648 0.01897864 0.84037113]\n",
      " [0.0377116  0.03006164 0.01397924 0.9182475 ]\n",
      " ...\n",
      " [0.01125644 0.9361681  0.00631953 0.04625594]\n",
      " [0.02373258 0.00486072 0.95929646 0.01211024]\n",
      " [0.02374605 0.00488263 0.9592298  0.01214149]]\n",
      "Iteration 1630, Accuracy 0.33343\n",
      "97.05897%change in label assignment\n",
      "0.052006178\n",
      "[[0.01440519 0.9156229  0.00790449 0.06206742]\n",
      " [0.05673829 0.19413999 0.02554867 0.723573  ]\n",
      " [0.03032457 0.03046873 0.01151792 0.9276888 ]\n",
      " ...\n",
      " [0.00848827 0.9561352  0.00487195 0.03050456]\n",
      " [0.02373431 0.00485833 0.9592968  0.01211057]\n",
      " [0.02373396 0.00487748 0.95925367 0.01213487]]\n",
      "Iteration 1631, Accuracy 0.32813\n",
      "99.04257%change in label assignment\n",
      "0.053889047\n",
      "[[0.03004014 0.7985857  0.0159786  0.15539557]\n",
      " [0.05074709 0.1398437  0.0225025  0.7869067 ]\n",
      " [0.03432468 0.02945221 0.01283765 0.92338544]\n",
      " ...\n",
      " [0.00918365 0.949747   0.0052022  0.03586712]\n",
      " [0.02367159 0.00486014 0.9593696  0.01209867]\n",
      " [0.02370431 0.00488673 0.95926833 0.01214066]]\n",
      "Iteration 1632, Accuracy 0.32759\n",
      "98.14406%change in label assignment\n",
      "0.051525157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02309543 0.8537235  0.01241586 0.11076515]\n",
      " [0.05803442 0.2146406  0.02646029 0.7008647 ]\n",
      " [0.03173414 0.02953451 0.01195491 0.92677647]\n",
      " ...\n",
      " [0.00832302 0.95638865 0.00476614 0.03052218]\n",
      " [0.02368838 0.00488486 0.9592898  0.01213693]\n",
      " [0.02379975 0.00492988 0.95904833 0.01222201]]\n",
      "Iteration 1633, Accuracy 0.3269\n",
      "96.96568%change in label assignment\n",
      "0.052425683\n",
      "[[0.02067622 0.8714757  0.0111982  0.09664982]\n",
      " [0.0577482  0.21122816 0.02634535 0.70467824]\n",
      " [0.03154238 0.02955084 0.01191824 0.9269885 ]\n",
      " ...\n",
      " [0.00831034 0.95630074 0.0047614  0.03062745]\n",
      " [0.02362684 0.00485207 0.9594397  0.0120814 ]\n",
      " [0.02365471 0.00487961 0.95934224 0.01212338]]\n",
      "Iteration 1634, Accuracy 0.33908\n",
      "96.24392%change in label assignment\n",
      "0.050802365\n",
      "[[0.02183839 0.8629615  0.01179127 0.10340884]\n",
      " [0.05079746 0.13932048 0.02242042 0.7874617 ]\n",
      " [0.0370215  0.02981414 0.01361224 0.91955215]\n",
      " ...\n",
      " [0.01003188 0.9442126  0.0056325  0.04012295]\n",
      " [0.02367495 0.00490079 0.95926785 0.01215647]\n",
      " [0.02383823 0.00495758 0.9589348  0.01226943]]\n",
      "Iteration 1635, Accuracy 0.33736\n",
      "92.30618%change in label assignment\n",
      "0.052470487\n",
      "[[0.01002994 0.94434243 0.00561127 0.04001629]\n",
      " [0.05922057 0.23377928 0.02713161 0.67986846]\n",
      " [0.03050571 0.03016079 0.01157562 0.9277579 ]\n",
      " ...\n",
      " [0.00846935 0.9562331  0.00486767 0.03042992]\n",
      " [0.02369974 0.00491558 0.9591998  0.01218485]\n",
      " [0.02388258 0.00497612 0.9588338  0.01230752]]\n",
      "Iteration 1636, Accuracy 0.34399\n",
      "97.45176%change in label assignment\n",
      "0.049596146\n",
      "[[0.01394572 0.91839254 0.00770941 0.05995237]\n",
      " [0.05625429 0.1923127  0.02553825 0.72589475]\n",
      " [0.03216619 0.02939329 0.01213994 0.92630064]\n",
      " ...\n",
      " [0.00841472 0.9551175  0.00480522 0.03166263]\n",
      " [0.0236002  0.00489058 0.9593794  0.01212982]\n",
      " [0.02375779 0.00494653 0.95905536 0.01224032]]\n",
      "Iteration 1637, Accuracy 0.35027\n",
      "98.50739%change in label assignment\n",
      "0.051117424\n",
      "[[0.013774   0.9196178  0.00760291 0.05900533]\n",
      " [0.05884381 0.229534   0.02701589 0.68460625]\n",
      " [0.030376   0.03007532 0.01158435 0.9279644 ]\n",
      " ...\n",
      " [0.00836199 0.9564913  0.00480812 0.0303386 ]\n",
      " [0.02351034 0.00485744 0.9595646  0.01206761]\n",
      " [0.02360278 0.00489966 0.9593532  0.01214437]]\n",
      "Iteration 1638, Accuracy 0.34752\n",
      "97.29955%change in label assignment\n",
      "0.049985025\n",
      "[[0.01889376 0.88417727 0.01029468 0.08663426]\n",
      " [0.0505138  0.13790014 0.02233353 0.78925246]\n",
      " [0.03546865 0.0295372  0.01316539 0.9218288 ]\n",
      " ...\n",
      " [0.01035536 0.9420418  0.00581382 0.04178902]\n",
      " [0.0235245  0.00488174 0.9594903  0.01210347]\n",
      " [0.02368777 0.00493987 0.9591542  0.01221814]]\n",
      "Iteration 1639, Accuracy 0.34448\n",
      "94.98699%change in label assignment\n",
      "[[0.008502   0.95458573 0.00482166 0.03209066]\n",
      " [0.0611109  0.28748462 0.02850468 0.62289983]\n",
      " [0.02950152 0.0318305  0.01136944 0.9272985 ]\n",
      " ...\n",
      " [0.00905506 0.95407945 0.00524314 0.03162243]\n",
      " [0.02353272 0.00489337 0.95944935 0.01212455]\n",
      " [0.02371621 0.00495577 0.95907825 0.01224974]]\n",
      "Iteration 1640, Accuracy 0.34782\n",
      "97.11298%change in label assignment\n",
      "0.0601436\n",
      "[[0.02926155 0.8045101  0.01566237 0.15056595]\n",
      " [0.04292771 0.09282796 0.01857035 0.8456739 ]\n",
      " [0.04377832 0.03153673 0.01596691 0.908718  ]\n",
      " ...\n",
      " [0.01772841 0.89215577 0.0097548  0.08036099]\n",
      " [0.02352731 0.00489572 0.95945036 0.01212659]\n",
      " [0.02372804 0.00496218 0.95904857 0.01226124]]\n",
      "Iteration 1641, Accuracy 0.35332\n",
      "97.6678%change in label assignment\n",
      "0.05917197\n",
      "[[0.00918569 0.95355046 0.00528605 0.03197779]\n",
      " [0.05795684 0.48290533 0.02824476 0.43089294]\n",
      " [0.03205727 0.04519593 0.01279027 0.9099565 ]\n",
      " ...\n",
      " [0.01165895 0.9431277  0.00681732 0.03839605]\n",
      " [0.02350822 0.00488107 0.95949805 0.01211263]\n",
      " [0.0236741  0.00494034 0.9591561  0.01222951]]\n",
      "Iteration 1642, Accuracy 0.35184\n",
      "97.15717%change in label assignment\n",
      "0.06646258\n",
      "[[0.03057391 0.79335934 0.01639458 0.15967217]\n",
      " [0.04448872 0.10157041 0.01949084 0.83445   ]\n",
      " [0.045158   0.03198158 0.01651289 0.9063475 ]\n",
      " ...\n",
      " [0.02559704 0.83363634 0.01388831 0.12687829]\n",
      " [0.02337158 0.00483058 0.95978796 0.01200992]\n",
      " [0.02342871 0.00486526 0.9596379  0.01206821]]\n",
      "Iteration 1643, Accuracy 0.34968\n",
      "94.70712%change in label assignment\n",
      "0.064402975\n",
      "[[0.01351727 0.93519205 0.0079421  0.04334856]\n",
      " [0.04949814 0.61140317 0.02476088 0.3143378 ]\n",
      " [0.03605857 0.05953071 0.01467715 0.88973355]\n",
      " ...\n",
      " [0.01382643 0.9338696  0.00814136 0.04416261]\n",
      " [0.02342087 0.00484015 0.9596959  0.0120431 ]\n",
      " [0.02347666 0.00487444 0.95954823 0.01210063]]\n",
      "Iteration 1644, Accuracy 0.33181\n",
      "97.62361%change in label assignment\n",
      "0.07656303\n",
      "[[0.03174229 0.78290474 0.01711621 0.1682367 ]\n",
      " [0.04008748 0.08043601 0.01743898 0.8620376 ]\n",
      " [0.06488348 0.0376214  0.02308779 0.87440735]\n",
      " ...\n",
      " [0.03263095 0.77520305 0.01758994 0.17457607]\n",
      " [0.02334039 0.00483705 0.95981276 0.01200985]\n",
      " [0.02342798 0.00487825 0.9596098  0.01208403]]\n",
      "Iteration 1645, Accuracy 0.33333\n",
      "97.47631%change in label assignment\n",
      "0.07286283\n",
      "[[0.02324794 0.8949094  0.01402683 0.06781589]\n",
      " [0.03519749 0.75768465 0.01819868 0.18891922]\n",
      " [0.03904095 0.0709597  0.01607648 0.8739229 ]\n",
      " ...\n",
      " [0.01858434 0.91391075 0.01109237 0.0564125 ]\n",
      " [0.02341408 0.00484021 0.9596986  0.01204702]\n",
      " [0.02346549 0.00487305 0.9595598  0.01210162]]\n",
      "Iteration 1646, Accuracy 0.33652\n",
      "96.02298%change in label assignment\n",
      "0.08185261\n",
      "[[0.01945501 0.87953806 0.01078986 0.09021705]\n",
      " [0.04888019 0.13036115 0.02209211 0.7986665 ]\n",
      " [0.0598652  0.0363329  0.02153411 0.8822677 ]\n",
      " ...\n",
      " [0.02638135 0.8270385  0.01442256 0.13215758]\n",
      " [0.02330963 0.0048276  0.95986575 0.01199705]\n",
      " [0.02336682 0.00486175 0.9597168  0.01205467]]\n",
      "Iteration 1647, Accuracy 0.32823\n",
      "97.63343%change in label assignment\n",
      "0.069597825\n",
      "[[0.0276644  0.8773752  0.01681412 0.07814629]\n",
      " [0.02528392 0.8392162  0.0133347  0.12216521]\n",
      " [0.03804627 0.06586381 0.01547313 0.8806168 ]\n",
      " ...\n",
      " [0.01739982 0.9188163  0.0103225  0.0534614 ]\n",
      " [0.02347158 0.00482312 0.9596648  0.0120404 ]\n",
      " [0.02341221 0.00483024 0.95972306 0.01203449]]\n",
      "Iteration 1648, Accuracy 0.32523\n",
      "92.5124%change in label assignment\n",
      "0.079712614\n",
      "[[0.02307494 0.85263395 0.01270158 0.11158949]\n",
      " [0.04333882 0.09689702 0.01919863 0.84056556]\n",
      " [0.06439982 0.03758119 0.02303184 0.8749872 ]\n",
      " ...\n",
      " [0.03071289 0.79142344 0.01665965 0.16120398]\n",
      " [0.02331591 0.00480862 0.959895   0.01198043]\n",
      " [0.02327736 0.00481986 0.95991784 0.0119849 ]]\n",
      "Iteration 1649, Accuracy 0.33279\n",
      "93.8037%change in label assignment\n",
      "0.07195038\n",
      "[[0.02524698 0.8868233  0.01530457 0.07262516]\n",
      " [0.03735135 0.7380016  0.01922574 0.20542128]\n",
      " [0.03458524 0.05369652 0.01391246 0.8978058 ]\n",
      " ...\n",
      " [0.01756509 0.9180624  0.01045618 0.05391636]\n",
      " [0.02339007 0.00482987 0.9597471  0.01203296]\n",
      " [0.02339557 0.00485113 0.959692   0.01206132]]\n",
      "Iteration 1650, Accuracy 0.327\n",
      "96.04262%change in label assignment\n",
      "0.07582538\n",
      "[[0.01699586 0.8969435  0.0094834  0.07657721]\n",
      " [0.04805077 0.12518147 0.02165503 0.8051128 ]\n",
      " [0.05969382 0.03629019 0.0214793  0.8825367 ]\n",
      " ...\n",
      " [0.02434835 0.842763   0.01336696 0.11952174]\n",
      " [0.02329195 0.00481458 0.959909   0.01198445]\n",
      " [0.02329013 0.00483423 0.9598668  0.01200886]]\n",
      "Iteration 1651, Accuracy 0.32666\n",
      "97.75126%change in label assignment\n",
      "0.06722872\n",
      "[[0.01987152 0.9085273  0.01190358 0.05969765]\n",
      " [0.05032936 0.59990054 0.02513638 0.3246337 ]\n",
      " [0.0312999  0.04179027 0.01232696 0.91458285]\n",
      " ...\n",
      " [0.01307284 0.93698794 0.00767711 0.04226202]\n",
      " [0.02337039 0.0048401  0.95974505 0.01204445]\n",
      " [0.0234194  0.00487088 0.95961386 0.01209586]]\n",
      "Iteration 1652, Accuracy 0.3269\n",
      "97.17681%change in label assignment\n",
      "0.0674906\n",
      "[[0.01425233 0.9158802  0.00798338 0.06188411]\n",
      " [0.04870677 0.1292747  0.02189205 0.8001265 ]\n",
      " [0.04517012 0.03209046 0.01665289 0.9060865 ]\n",
      " ...\n",
      " [0.02065752 0.87062734 0.01138848 0.09732662]\n",
      " [0.02329242 0.00481387 0.95990074 0.01199295]\n",
      " [0.0232822  0.00483145 0.9598738  0.01201262]]\n",
      "Iteration 1653, Accuracy 0.33314\n",
      "96.66127%change in label assignment\n",
      "0.060094096\n",
      "[[0.0107889  0.9467492  0.00629799 0.03616382]\n",
      " [0.06142756 0.3659872  0.02921752 0.5433676 ]\n",
      " [0.02945728 0.03299728 0.01136687 0.9261785 ]\n",
      " ...\n",
      " [0.00839722 0.95622504 0.00480287 0.03057483]\n",
      " [0.02333944 0.00482597 0.95980996 0.01202472]\n",
      " [0.02335188 0.00484837 0.9597435  0.01205626]]\n",
      "Iteration 1654, Accuracy 0.32651\n",
      "96.60235%change in label assignment\n",
      "0.05763631\n",
      "[[0.01294474 0.9247042  0.00727959 0.05507146]\n",
      " [0.05065535 0.1435949  0.02294028 0.7828095 ]\n",
      " [0.04213281 0.0312123  0.01564965 0.91100526]\n",
      " ...\n",
      " [0.02110197 0.8673869  0.01161526 0.09989592]\n",
      " [0.02327776 0.00481815 0.9599023  0.01200179]\n",
      " [0.02329247 0.00484113 0.9598318  0.01203461]]\n",
      "Iteration 1655, Accuracy 0.32302\n",
      "99.18987%change in label assignment\n",
      "0.05588202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02424657 0.8909077  0.01462146 0.07022426]\n",
      " [0.04289344 0.68568695 0.02174604 0.24967358]\n",
      " [0.03386815 0.05054827 0.01350613 0.9020774 ]\n",
      " ...\n",
      " [0.01566288 0.9260684  0.0092417  0.04902703]\n",
      " [0.02331759 0.00483361 0.9598129  0.01203585]\n",
      " [0.02335613 0.00486153 0.9597013  0.01208099]]\n",
      "Iteration 1656, Accuracy 0.32376\n",
      "97.09334%change in label assignment\n",
      "0.07550324\n",
      "[[0.02304225 0.8529108  0.01263832 0.11140858]\n",
      " [0.03847608 0.07373835 0.01666356 0.871122  ]\n",
      " [0.05743584 0.03557999 0.02075707 0.88622713]\n",
      " ...\n",
      " [0.03395541 0.7633574  0.0182314  0.18445584]\n",
      " [0.0232277  0.00482093 0.9599509  0.01200044]\n",
      " [0.02327105 0.00484991 0.9598309  0.0120481 ]]\n",
      "Iteration 1657, Accuracy 0.32936\n",
      "96.16046%change in label assignment\n",
      "0.069494046\n",
      "[[0.02218043 0.89918494 0.01335597 0.0652786 ]\n",
      " [0.05292766 0.56655496 0.02621218 0.35430524]\n",
      " [0.03421097 0.05269316 0.01377521 0.89932066]\n",
      " ...\n",
      " [0.01509501 0.92846364 0.00892325 0.04751817]\n",
      " [0.02332925 0.00485026 0.95975256 0.01206792]\n",
      " [0.02341195 0.00488793 0.95956355 0.01213663]]\n",
      "Iteration 1658, Accuracy 0.33432\n",
      "97.35356%change in label assignment\n",
      "0.06857698\n",
      "[[0.0096671  0.94633836 0.00552669 0.03846782]\n",
      " [0.04909647 0.13241816 0.02215736 0.796328  ]\n",
      " [0.04302013 0.03149514 0.01600864 0.90947604]\n",
      " ...\n",
      " [0.01638608 0.9011547  0.00914024 0.07331889]\n",
      " [0.02326516 0.00485914 0.9598126  0.01206307]\n",
      " [0.02339585 0.00490785 0.9595384  0.01215787]]\n",
      "Iteration 1659, Accuracy 0.33849\n",
      "96.3323%change in label assignment\n",
      "0.055703156\n",
      "[[0.01551532 0.9266256  0.00921703 0.04864202]\n",
      " [0.06014251 0.42392528 0.02902436 0.4869079 ]\n",
      " [0.02942828 0.03339536 0.01138012 0.9257962 ]\n",
      " ...\n",
      " [0.00919356 0.9534797  0.00531399 0.03201278]\n",
      " [0.02329318 0.00485635 0.95978117 0.01206924]\n",
      " [0.02340746 0.00490138 0.95953596 0.01215524]]\n",
      "Iteration 1660, Accuracy 0.33834\n",
      "97.00496%change in label assignment\n",
      "0.058374465\n",
      "[[0.00962595 0.94656044 0.00547881 0.03833476]\n",
      " [0.04818492 0.12538745 0.02153818 0.80488944]\n",
      " [0.04336291 0.0314266  0.0159949  0.90921557]\n",
      " ...\n",
      " [0.01798692 0.88995606 0.00994601 0.082111  ]\n",
      " [0.0233035  0.00487959 0.9597134  0.01210347]\n",
      " [0.02347538 0.0049376  0.9593665  0.0122205 ]]\n",
      "Iteration 1661, Accuracy 0.3376\n",
      "96.26356%change in label assignment\n",
      "0.05542581\n",
      "[[0.02047884 0.9060191  0.0123378  0.06116423]\n",
      " [0.05358471 0.5529945  0.02664434 0.36677644]\n",
      " [0.03072611 0.04101063 0.01220243 0.9160608 ]\n",
      " ...\n",
      " [0.01148879 0.9437981  0.00673402 0.037979  ]\n",
      " [0.02321514 0.00483088 0.95992976 0.01202428]\n",
      " [0.02327728 0.00486359 0.9597777  0.01208147]]\n",
      "Iteration 1662, Accuracy 0.34389\n",
      "94.6433%change in label assignment\n",
      "0.061862316\n",
      "[[0.00995458 0.9443978  0.00565536 0.03999222]\n",
      " [0.05047074 0.14193644 0.02277008 0.78482264]\n",
      " [0.04072654 0.03069466 0.01513048 0.91344833]\n",
      " ...\n",
      " [0.01868821 0.88495284 0.01031738 0.08604151]\n",
      " [0.0231234  0.00480724 0.9600975  0.01197188]\n",
      " [0.02314965 0.00483178 0.960009   0.01200955]]\n",
      "Iteration 1663, Accuracy 0.33505\n",
      "96.03771%change in label assignment\n",
      "0.05712874\n",
      "[[0.01580388 0.9253794  0.00942977 0.04938701]\n",
      " [0.05983809 0.42717156 0.02904509 0.4839453 ]\n",
      " [0.02911198 0.03394185 0.01138962 0.92555654]\n",
      " ...\n",
      " [0.00921783 0.95343345 0.00535214 0.0319966 ]\n",
      " [0.02330966 0.00489416 0.9596619  0.01213427]\n",
      " [0.02350474 0.00495764 0.9592734  0.01226419]]\n",
      "Iteration 1664, Accuracy 0.33505\n",
      "92.9101%change in label assignment\n",
      "0.056311592\n",
      "[[0.00879542 0.95205814 0.00502049 0.034126  ]\n",
      " [0.05329324 0.16565289 0.02421957 0.7568343 ]\n",
      " [0.0373593  0.02977425 0.01395525 0.9189112 ]\n",
      " ...\n",
      " [0.01420751 0.9161841  0.00791461 0.06169378]\n",
      " [0.02318562 0.0048614  0.95989186 0.01206116]\n",
      " [0.02334124 0.00491576 0.95957357 0.01216947]]\n",
      "Iteration 1665, Accuracy 0.34733\n",
      "98.98856%change in label assignment\n",
      "0.05360101\n",
      "[[0.01682681 0.92108124 0.01007229 0.05201969]\n",
      " [0.05816273 0.46971592 0.02850864 0.44361264]\n",
      " [0.02923848 0.03505041 0.01148459 0.92422646]\n",
      " ...\n",
      " [0.00972815 0.95130813 0.00567024 0.03329348]\n",
      " [0.02319945 0.0048649  0.95986295 0.01207275]\n",
      " [0.02335108 0.00491804 0.95955235 0.01217853]]\n",
      "Iteration 1666, Accuracy 0.34482\n",
      "97.67762%change in label assignment\n",
      "0.054479603\n",
      "[[0.00858707 0.95349276 0.00491943 0.0330007 ]\n",
      " [0.05462277 0.17968042 0.02502411 0.74067265]\n",
      " [0.0374536  0.02980096 0.01401456 0.91873085]\n",
      " ...\n",
      " [0.01436909 0.91506135 0.00801142 0.06255817]\n",
      " [0.0231522  0.0048625  0.9599263  0.01205894]\n",
      " [0.02331566 0.00491842 0.9595947  0.01217122]]\n",
      "Iteration 1667, Accuracy 0.3487\n",
      "98.21771%change in label assignment\n",
      "0.052833628\n",
      "[[0.01373755 0.9341443  0.00814739 0.04397079]\n",
      " [0.05844796 0.4629741  0.02861949 0.4499585 ]\n",
      " [0.02920165 0.03507959 0.01148439 0.92423433]\n",
      " ...\n",
      " [0.00913157 0.9538001  0.00530233 0.03176601]\n",
      " [0.02308781 0.00483606 0.96006334 0.01201278]\n",
      " [0.02320581 0.00488231 0.9598106  0.01210129]]\n",
      "Iteration 1668, Accuracy 0.34634\n",
      "97.84946%change in label assignment\n",
      "0.054624453\n",
      "[[0.00897066 0.95084935 0.00512498 0.03505499]\n",
      " [0.05672606 0.20627381 0.02629695 0.7107032 ]\n",
      " [0.0353767  0.02937221 0.01335841 0.9218927 ]\n",
      " ...\n",
      " [0.01365684 0.91984177 0.00764032 0.05886103]\n",
      " [0.02300266 0.00481856 0.9602084  0.0119704 ]\n",
      " [0.02310748 0.00486251 0.9599774  0.0120526 ]]\n",
      "Iteration 1669, Accuracy 0.34232\n",
      "98.63996%change in label assignment\n",
      "0.05404377\n",
      "[[0.01435471 0.9315106  0.00853041 0.0456043 ]\n",
      " [0.05817061 0.46952203 0.02850729 0.44380015]\n",
      " [0.02924178 0.03525392 0.01149799 0.92400634]\n",
      " ...\n",
      " [0.00930232 0.9530912  0.00540808 0.03219844]\n",
      " [0.02299188 0.00481166 0.96023476 0.01196169]\n",
      " [0.0230735  0.00485026 0.96004504 0.01203124]]\n",
      "Iteration 1670, Accuracy 0.33908\n",
      "98.82162%change in label assignment\n",
      "0.05194251\n",
      "[[0.00820109 0.9568486  0.00474634 0.03020388]\n",
      " [0.05970437 0.26403812 0.02817472 0.6480828 ]\n",
      " [0.03267992 0.02900434 0.01245201 0.9258637 ]\n",
      " ...\n",
      " [0.01027749 0.9422063  0.00582064 0.04169555]\n",
      " [0.02291873 0.00480184 0.9603477  0.01193174]\n",
      " [0.02299832 0.00484029 0.96016085 0.01200054]]\n",
      "Iteration 1671, Accuracy 0.34104\n",
      "98.67433%change in label assignment\n",
      "0.050092608\n",
      "[[0.0111715  0.94508713 0.00658409 0.03715725]\n",
      " [0.05998758 0.41528457 0.02917843 0.4955494 ]\n",
      " [0.02894404 0.0313547  0.01125345 0.9284478 ]\n",
      " ...\n",
      " [0.00824243 0.9570535  0.00475358 0.02995055]\n",
      " [0.0228951  0.00478741 0.9604084  0.011909  ]\n",
      " [0.02293359 0.00481635 0.9602948  0.01195528]]\n",
      "Iteration 1672, Accuracy 0.34124\n",
      "97.46649%change in label assignment\n",
      "0.049339615\n",
      "[[0.00820249 0.95703155 0.00474729 0.03001867]\n",
      " [0.0594562  0.25573796 0.02792322 0.65688264]\n",
      " [0.03324292 0.02900828 0.01259205 0.9251567 ]\n",
      " ...\n",
      " [0.01080072 0.9387853  0.00609146 0.04432248]\n",
      " [0.02286822 0.00480141 0.9604088  0.01192156]\n",
      " [0.02296056 0.00484241 0.96020025 0.01199681]]\n",
      "Iteration 1673, Accuracy 0.33819\n",
      "97.39775%change in label assignment\n",
      "0.04999347\n",
      "[[0.01169372 0.9428577  0.00690032 0.03854834]\n",
      " [0.0604034  0.40144378 0.02924522 0.50890756]\n",
      " [0.02916061 0.03078646 0.01127335 0.9287796 ]\n",
      " ...\n",
      " [0.00818853 0.95706475 0.00470573 0.03004104]\n",
      " [0.02290855 0.00482671 0.96029854 0.01196619]\n",
      " [0.02304775 0.00487784 0.9600082  0.01206622]]\n",
      "Iteration 1674, Accuracy 0.34423\n",
      "97.17681%change in label assignment\n",
      "0.047593106\n",
      "[[0.00863275 0.9557667  0.0050172  0.03058339]\n",
      " [0.06002795 0.2673262  0.02813982 0.6445059 ]\n",
      " [0.03151532 0.02900891 0.01197341 0.92750233]\n",
      " ...\n",
      " [0.00987567 0.94489366 0.00557456 0.03965614]\n",
      " [0.0228682  0.00482569 0.9603506  0.01195557]\n",
      " [0.0230124  0.00487769 0.960052   0.01205794]]\n",
      "Iteration 1675, Accuracy 0.34968\n",
      "97.58433%change in label assignment\n",
      "0.049100295\n",
      "[[0.00828673 0.9569701  0.00478042 0.02996275]\n",
      " [0.06083482 0.29441553 0.02864038 0.61610925]\n",
      " [0.0293993  0.03016748 0.0113304  0.9291028 ]\n",
      " ...\n",
      " [0.00854481 0.95379186 0.00486333 0.03279998]\n",
      " [0.02275596 0.00477651 0.96060157 0.01186593]\n",
      " [0.02280757 0.00480897 0.9604635  0.01191994]]\n",
      "Iteration 1676, Accuracy 0.3486\n",
      "97.20136%change in label assignment\n",
      "0.048200823\n",
      "[[0.00861688 0.9557652  0.00502134 0.03059657]\n",
      " [0.05966937 0.2615174  0.02802632 0.65078694]\n",
      " [0.03119646 0.02902295 0.01193523 0.92784536]\n",
      " ...\n",
      " [0.00956753 0.94682556 0.00542704 0.03817986]\n",
      " [0.02284207 0.00483767 0.9603534  0.01196689]\n",
      " [0.02301569 0.00489658 0.96000224 0.01208548]]\n",
      "Iteration 1677, Accuracy 0.34414\n",
      "95.6351%change in label assignment\n",
      "0.04882868\n",
      "[[0.01117928 0.9450177  0.00659579 0.03720724]\n",
      " [0.06108527 0.34414735 0.02921013 0.56555724]\n",
      " [0.02901739 0.03071158 0.01125918 0.9290119 ]\n",
      " ...\n",
      " [0.00815377 0.95698214 0.00468757 0.03017651]\n",
      " [0.0227404  0.00480358 0.9605566  0.01189941]\n",
      " [0.02285872 0.0048497  0.9603038  0.01198771]]\n",
      "Iteration 1678, Accuracy 0.34924\n",
      "98.53194%change in label assignment\n",
      "0.046733532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00868682 0.95552504 0.00505893 0.03072923]\n",
      " [0.05843551 0.23200586 0.02711218 0.6824465 ]\n",
      " [0.03167529 0.02896687 0.01204587 0.92731196]\n",
      " ...\n",
      " [0.01015342 0.94300354 0.00573062 0.04111248]\n",
      " [0.02273909 0.0048177  0.96052516 0.01191809]\n",
      " [0.02288288 0.00486895 0.96022874 0.01201938]]\n",
      "Iteration 1679, Accuracy 0.3514\n",
      "96.45996%change in label assignment\n",
      "[[0.01715795 0.919543   0.01032651 0.0529725 ]\n",
      " [0.05999617 0.41264862 0.02915974 0.49819544]\n",
      " [0.02876231 0.03387542 0.01134104 0.9260212 ]\n",
      " ...\n",
      " [0.00880868 0.9550697  0.00512567 0.03099597]\n",
      " [0.02264931 0.00478779 0.96070296 0.01185998]\n",
      " [0.02273842 0.00482582 0.9605051  0.0119306 ]]\n",
      "Iteration 1680, Accuracy 0.35337\n",
      "98.32081%change in label assignment\n",
      "0.06148945\n",
      "[[0.0106396  0.93971926 0.00602905 0.04361211]\n",
      " [0.04767441 0.1234197  0.02131434 0.80759156]\n",
      " [0.04552633 0.03200569 0.01672484 0.9057432 ]\n",
      " ...\n",
      " [0.02377651 0.8467825  0.01299042 0.11645056]\n",
      " [0.02260153 0.00477409 0.9607939  0.01183049]\n",
      " [0.0226759  0.0048085  0.9606229  0.01189274]]\n",
      "Iteration 1681, Accuracy 0.35828\n",
      "97.01478%change in label assignment\n",
      "0.057856653\n",
      "[[0.02121895 0.9028956  0.01284217 0.06304325]\n",
      " [0.04631397 0.6438392  0.02356707 0.28627974]\n",
      " [0.03096981 0.04302326 0.01243345 0.91357344]\n",
      " ...\n",
      " [0.00937204 0.9527909  0.00544782 0.03238918]\n",
      " [0.02261259 0.0047616  0.96079946 0.01182637]\n",
      " [0.02264474 0.00478649 0.96070284 0.01186586]]\n",
      "Iteration 1682, Accuracy 0.35273\n",
      "95.37978%change in label assignment\n",
      "0.067516096\n",
      "[[0.01599078 0.9036318  0.00894452 0.07143282]\n",
      " [0.03844162 0.07487178 0.01672318 0.86996335]\n",
      " [0.05643398 0.03530002 0.0205174  0.88774854]\n",
      " ...\n",
      " [0.03485013 0.7541536  0.01872545 0.19227085]\n",
      " [0.02255825 0.00476824 0.96085477 0.01181871]\n",
      " [0.02262737 0.00480124 0.96069354 0.01187783]]\n",
      "Iteration 1683, Accuracy 0.35003\n",
      "95.60564%change in label assignment\n",
      "0.06950211\n",
      "[[0.03484039 0.8494883  0.02159989 0.09407142]\n",
      " [0.04430198 0.6683304  0.02252703 0.26484063]\n",
      " [0.03772169 0.06747667 0.01561926 0.8791824 ]\n",
      " ...\n",
      " [0.01661518 0.9219978  0.00989701 0.05149001]\n",
      " [0.02266028 0.00480033 0.960644   0.0118954 ]\n",
      " [0.02276284 0.00484069 0.96042395 0.0119725 ]]\n",
      "Iteration 1684, Accuracy 0.34914\n",
      "97.40757%change in label assignment\n",
      "0.07941897\n",
      "[[0.01227809 0.92885053 0.00696148 0.05190989]\n",
      " [0.03931049 0.07897999 0.01719776 0.8645118 ]\n",
      " [0.05781136 0.03573609 0.0210049  0.8854476 ]\n",
      " ...\n",
      " [0.02932905 0.80198324 0.01595986 0.15272792]\n",
      " [0.02256925 0.00478595 0.9607987  0.01184613]\n",
      " [0.02267024 0.00482609 0.96058124 0.01192244]]\n",
      "Iteration 1685, Accuracy 0.35587\n",
      "97.93293%change in label assignment\n",
      "0.063491866\n",
      "[[0.02318584 0.8948899  0.01409271 0.06783149]\n",
      " [0.05812952 0.46980092 0.0284595  0.44361007]\n",
      " [0.03027185 0.04051232 0.0120945  0.91712135]\n",
      " ...\n",
      " [0.01025441 0.9490294  0.00599078 0.03472539]\n",
      " [0.02272948 0.00483628 0.9604733  0.01196091]\n",
      " [0.02288873 0.00488978 0.9601523  0.01206915]]\n",
      "Iteration 1686, Accuracy 0.35067\n",
      "97.23081%change in label assignment\n",
      "0.06414395\n",
      "[[0.00882748 0.95168597 0.00507458 0.03441194]\n",
      " [0.05200657 0.15754476 0.02377783 0.76667076]\n",
      " [0.04044659 0.03061168 0.01513811 0.91380364]\n",
      " ...\n",
      " [0.02144629 0.8642656  0.01181222 0.10247582]\n",
      " [0.02254927 0.00477925 0.9608307  0.01184081]\n",
      " [0.02262515 0.00481151 0.9606625  0.01190081]]\n",
      "Iteration 1687, Accuracy 0.3596\n",
      "93.89208%change in label assignment\n",
      "0.055659924\n",
      "[[0.01942948 0.9101824  0.01172528 0.05866277]\n",
      " [0.0477273  0.6263207  0.02425873 0.30169326]\n",
      " [0.02993344 0.0395708  0.01196857 0.9185272 ]\n",
      " ...\n",
      " [0.00858672 0.95592225 0.00496439 0.03052668]\n",
      " [0.0225668  0.0047786  0.9608022  0.0118524 ]\n",
      " [0.022628   0.00480655 0.9606621  0.01190325]]\n",
      "Iteration 1688, Accuracy 0.34718\n",
      "96.00334%change in label assignment\n",
      "0.06177257\n",
      "[[0.01060126 0.9399147  0.00602372 0.04346036]\n",
      " [0.05123948 0.15095854 0.02335522 0.7744467 ]\n",
      " [0.04368087 0.03150306 0.01620849 0.9086076 ]\n",
      " ...\n",
      " [0.02784864 0.8143488  0.01512635 0.14267617]\n",
      " [0.02249938 0.00476895 0.9609108  0.01182085]\n",
      " [0.02255653 0.00479542 0.96077925 0.01186875]]\n",
      "Iteration 1689, Accuracy 0.346\n",
      "97.35847%change in label assignment\n",
      "0.060502335\n",
      "[[0.02025319 0.9068424  0.01222153 0.06068288]\n",
      " [0.05520126 0.5252539  0.02734571 0.39219907]\n",
      " [0.0307175  0.04222856 0.01231584 0.91473806]\n",
      " ...\n",
      " [0.00898315 0.9543638  0.00520156 0.03145147]\n",
      " [0.02258455 0.00479917 0.9607288  0.01188745]\n",
      " [0.02267439 0.00483351 0.9605383  0.01195373]]\n",
      "Iteration 1690, Accuracy 0.33849\n",
      "96.76437%change in label assignment\n",
      "0.062669076\n",
      "[[0.01127274 0.93546766 0.0063993  0.04686028]\n",
      " [0.047073   0.12072625 0.0211638  0.81103694]\n",
      " [0.04667738 0.03242977 0.01726763 0.90362525]\n",
      " ...\n",
      " [0.02773891 0.81509066 0.01510282 0.14206754]\n",
      " [0.02253508 0.00479602 0.96080136 0.01186754]\n",
      " [0.02263422 0.00483276 0.9605938  0.01193918]]\n",
      "Iteration 1691, Accuracy 0.34433\n",
      "98.63996%change in label assignment\n",
      "0.060733955\n",
      "[[0.02367868 0.8929446  0.01438584 0.06899089]\n",
      " [0.04921712 0.6105816  0.02478905 0.3154123 ]\n",
      " [0.03242093 0.04811588 0.01310944 0.9063538 ]\n",
      " ...\n",
      " [0.00991283 0.9504831  0.0057692  0.03383486]\n",
      " [0.02253084 0.00478195 0.9608318  0.01185541]\n",
      " [0.02259547 0.00480965 0.960688   0.01190684]]\n",
      "Iteration 1692, Accuracy 0.34129\n",
      "96.65145%change in label assignment\n",
      "0.06580516\n",
      "[[0.01205882 0.9302564  0.00684524 0.05083954]\n",
      " [0.04547977 0.11125342 0.02041126 0.82285553]\n",
      " [0.0478284  0.03285448 0.01774998 0.9015671 ]\n",
      " ...\n",
      " [0.02924796 0.8025248  0.01592653 0.15230069]\n",
      " [0.02247466 0.00478372 0.9608992  0.0118425 ]\n",
      " [0.02256372 0.00481827 0.9607092  0.0119088 ]]\n",
      "Iteration 1693, Accuracy 0.34001\n",
      "96.96568%change in label assignment\n",
      "0.06181959\n",
      "[[0.02101502 0.90372574 0.01269283 0.06256639]\n",
      " [0.05421006 0.54199076 0.0269128  0.37688646]\n",
      " [0.0310592  0.04321126 0.01244776 0.9132818 ]\n",
      " ...\n",
      " [0.00937313 0.95273566 0.0054373  0.03245386]\n",
      " [0.02254169 0.00479622 0.9607798  0.01188228]\n",
      " [0.0226347  0.00483156 0.9605831  0.01195065]]\n",
      "Iteration 1694, Accuracy 0.3407\n",
      "97.5205%change in label assignment\n",
      "0.06469258\n",
      "[[0.01394168 0.91760033 0.00785841 0.06059967]\n",
      " [0.0445702  0.10584632 0.01992055 0.8296629 ]\n",
      " [0.04901112 0.03314483 0.01811241 0.89973164]\n",
      " ...\n",
      " [0.03204812 0.77873343 0.01734511 0.17187338]\n",
      " [0.02249797 0.0047953  0.96083856 0.01186816]\n",
      " [0.02260355 0.00483369 0.96061915 0.0119436 ]]\n",
      "Iteration 1695, Accuracy 0.34458\n",
      "97.37811%change in label assignment\n",
      "0.0642732\n",
      "[[0.05660386 0.77103925 0.0362961  0.13606079]\n",
      " [0.01092156 0.9394355  0.00603377 0.04360918]\n",
      " [0.04846079 0.1185378  0.02072821 0.8122732 ]\n",
      " ...\n",
      " [0.0347955  0.8497534  0.02148994 0.09396113]\n",
      " [0.02253121 0.00480069 0.9607773  0.01189081]\n",
      " [0.02262672 0.00483675 0.9605757  0.01196078]]\n",
      "Iteration 1696, Accuracy 0.34197\n",
      "96.64654%change in label assignment\n",
      "0.099011354\n",
      "[[0.00837411 0.95502055 0.00486355 0.03174188]\n",
      " [0.05999982 0.29479286 0.02895375 0.6162536 ]\n",
      " [0.0409503  0.03077104 0.01538603 0.9128926 ]\n",
      " ...\n",
      " [0.01798071 0.88940567 0.01002402 0.08258969]\n",
      " [0.0223925  0.00476474 0.9610333  0.01180945]\n",
      " [0.022462   0.00479504 0.96087754 0.01186538]]\n",
      "Iteration 1697, Accuracy 0.34801\n",
      "96.17028%change in label assignment\n",
      "0.056082375\n",
      "[[0.0147723  0.929713   0.00877984 0.04673485]\n",
      " [0.0542913  0.5396578  0.02698456 0.37906638]\n",
      " [0.02904892 0.03200207 0.01122502 0.927724  ]\n",
      " ...\n",
      " [0.00827585 0.95629907 0.00472313 0.03070195]\n",
      " [0.02242939 0.00477031 0.9609701  0.01183026]\n",
      " [0.02249536 0.00479892 0.9608226  0.01188319]]\n",
      "Iteration 1698, Accuracy 0.34011\n",
      "97.9182%change in label assignment\n",
      "0.054843\n",
      "[[0.00941354 0.9524297  0.00555461 0.03260216]\n",
      " [0.05771947 0.46290615 0.0288057  0.45056868]\n",
      " [0.02991401 0.02900002 0.01162276 0.92946315]\n",
      " ...\n",
      " [0.01173418 0.9323985  0.00662499 0.04924233]\n",
      " [0.02234093 0.00473214 0.9611653  0.01176158]\n",
      " [0.02235099 0.00474687 0.96111953 0.01178256]]\n",
      "Iteration 1699, Accuracy 0.33849\n",
      "93.11632%change in label assignment\n",
      "0.05091953\n",
      "[[0.01057121 0.9476354  0.00623156 0.03556179]\n",
      " [0.05674049 0.48982498 0.02819361 0.42524093]\n",
      " [0.0293     0.02955841 0.01131101 0.9298306 ]\n",
      " ...\n",
      " [0.01028563 0.9420769  0.00579495 0.04184254]\n",
      " [0.02233529 0.00474163 0.96114886 0.01177413]\n",
      " [0.02236376 0.00476044 0.9610708  0.01180501]]\n",
      "Iteration 1700, Accuracy 0.32504\n",
      "98.75779%change in label assignment\n",
      "0.051740274\n",
      "[[0.0080753  0.9571434  0.00465986 0.03012143]\n",
      " [0.05945699 0.2619819  0.02804291 0.6505182 ]\n",
      " [0.03616211 0.0293055  0.01355275 0.9209796 ]\n",
      " ...\n",
      " [0.01813884 0.88852507 0.01000103 0.08333509]\n",
      " [0.02237066 0.00477492 0.96102804 0.01182636]\n",
      " [0.02245116 0.00480634 0.9608559  0.01188658]]\n",
      "Iteration 1701, Accuracy 0.32999\n",
      "92.95429%change in label assignment\n",
      "0.05175695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0190897  0.91155374 0.01153601 0.05782049]\n",
      " [0.04743965 0.6287142  0.02417397 0.29967216]\n",
      " [0.02912901 0.03680793 0.01159623 0.9224669 ]\n",
      " ...\n",
      " [0.00837503 0.95679456 0.00484542 0.029985  ]\n",
      " [0.02235179 0.00476564 0.9610683  0.0118143 ]\n",
      " [0.02241604 0.00479308 0.9609255  0.01186532]]\n",
      "Iteration 1702, Accuracy 0.3379\n",
      "98.03604%change in label assignment\n",
      "0.055781297\n",
      "[[0.00814525 0.95651704 0.0047138  0.03062394]\n",
      " [0.05931267 0.26441738 0.02819661 0.6480733 ]\n",
      " [0.03849999 0.02994743 0.01445107 0.9171015 ]\n",
      " ...\n",
      " [0.01944814 0.8788787  0.01073984 0.09093329]\n",
      " [0.02237156 0.00479106 0.96098596 0.01185148]\n",
      " [0.02248042 0.00482981 0.9607615  0.01192826]]\n",
      "Iteration 1703, Accuracy 0.33559\n",
      "94.75131%change in label assignment\n",
      "0.05365674\n",
      "[[0.02349867 0.893515   0.0143515  0.06863494]\n",
      " [0.03779851 0.7295591  0.01971003 0.21293236]\n",
      " [0.02978221 0.03980938 0.01196775 0.9184407 ]\n",
      " ...\n",
      " [0.00953834 0.95208895 0.00557665 0.03279608]\n",
      " [0.02235167 0.00478148 0.9610276  0.01183922]\n",
      " [0.02244508 0.00481662 0.9608308  0.01190748]]\n",
      "Iteration 1704, Accuracy 0.34384\n",
      "96.60235%change in label assignment\n",
      "0.05879661\n",
      "[[0.00825686 0.9555762  0.00477447 0.0313924 ]\n",
      " [0.05935809 0.26856318 0.02831534 0.6437634 ]\n",
      " [0.041274   0.03072845 0.01539933 0.91259825]\n",
      " ...\n",
      " [0.02035518 0.8721216  0.01123616 0.09628706]\n",
      " [0.02229796 0.0047761  0.9611102  0.01181575]\n",
      " [0.02239634 0.00481248 0.96090436 0.01188681]]\n",
      "Iteration 1705, Accuracy 0.34507\n",
      "97.54505%change in label assignment\n",
      "0.05738551\n",
      "[[0.0327778  0.85719943 0.02030459 0.08971813]\n",
      " [0.02985066 0.8008301  0.0157354  0.15358382]\n",
      " [0.03232653 0.04799195 0.01307127 0.9066103 ]\n",
      " ...\n",
      " [0.01352208 0.9350322  0.0080038  0.04344186]\n",
      " [0.02229608 0.00476877 0.96112436 0.01181074]\n",
      " [0.02237065 0.0047986  0.96096337 0.01186744]]\n",
      "Iteration 1706, Accuracy 0.34713\n",
      "97.29955%change in label assignment\n",
      "0.061868556\n",
      "[[0.00818405 0.9568889  0.00477522 0.03015191]\n",
      " [0.05970472 0.2823643  0.02864549 0.62928545]\n",
      " [0.04078694 0.03061878 0.01527126 0.91332304]\n",
      " ...\n",
      " [0.01809134 0.8885184  0.01005791 0.08333237]\n",
      " [0.02223244 0.00476392 0.9612161  0.01178758]\n",
      " [0.0223165  0.00479637 0.9610371  0.01185   ]]\n",
      "Iteration 1707, Accuracy 0.34983\n",
      "98.05077%change in label assignment\n",
      "0.054069843\n",
      "[[0.02665413 0.8808749  0.01638578 0.07608522]\n",
      " [0.03570537 0.7489264  0.01869005 0.19667816]\n",
      " [0.02972821 0.03950908 0.01191649 0.9188462 ]\n",
      " ...\n",
      " [0.01063857 0.94736356 0.00625636 0.03574147]\n",
      " [0.02220099 0.00474363 0.96129686 0.01175857]\n",
      " [0.02224885 0.00476664 0.96118474 0.01179975]]\n",
      "Iteration 1708, Accuracy 0.3461\n",
      "95.51235%change in label assignment\n",
      "0.05746481\n",
      "[[0.00846501 0.95610255 0.00497175 0.0304607 ]\n",
      " [0.0601809  0.36181244 0.02952438 0.54848224]\n",
      " [0.03560356 0.02929078 0.01356249 0.9215432 ]\n",
      " ...\n",
      " [0.01467113 0.9124324  0.00823987 0.06465665]\n",
      " [0.02211272 0.0047184  0.9614651  0.01170379]\n",
      " [0.0221395  0.00473696 0.96138966 0.01173387]]\n",
      "Iteration 1709, Accuracy 0.3462\n",
      "95.46816%change in label assignment\n",
      "0.054686982\n",
      "[[0.02311309 0.895122   0.01406931 0.0676956 ]\n",
      " [0.03584917 0.7482244  0.01871143 0.19721505]\n",
      " [0.02945665 0.03779992 0.01170223 0.9210412 ]\n",
      " ...\n",
      " [0.00983239 0.9508354  0.00574262 0.03358959]\n",
      " [0.0221204  0.0047145  0.96145964 0.01170537]\n",
      " [0.02213348 0.00472921 0.9614102  0.01172706]]\n",
      "Iteration 1710, Accuracy 0.34124\n",
      "96.09172%change in label assignment\n",
      "0.05307651\n",
      "[[0.01075829 0.9465919  0.00640737 0.03624247]\n",
      " [0.05264188 0.55262494 0.02686581 0.36786726]\n",
      " [0.0298636  0.02885826 0.01164961 0.9296285 ]\n",
      " ...\n",
      " [0.00913305 0.94941425 0.00523555 0.0362172 ]\n",
      " [0.02207436 0.00469331 0.96156377 0.01166848]\n",
      " [0.02205366 0.00470019 0.9615746  0.01167151]]\n",
      "Iteration 1711, Accuracy 0.33412\n",
      "94.75622%change in label assignment\n",
      "0.050541945\n",
      "[[0.01493694 0.92879784 0.00897087 0.04729434]\n",
      " [0.04279716 0.6774698  0.02221235 0.25752065]\n",
      " [0.02836684 0.03048274 0.01111507 0.93003535]\n",
      " ...\n",
      " [0.00803294 0.9575806  0.00464001 0.02974645]\n",
      " [0.02204159 0.00470251 0.9615833  0.01167259]\n",
      " [0.02204778 0.00471606 0.96154517 0.01169099]]\n",
      "Iteration 1712, Accuracy 0.3269\n",
      "97.59906%change in label assignment\n",
      "0.048153706\n",
      "[[0.01113452 0.9451134  0.00660904 0.03714305]\n",
      " [0.04774551 0.6207032  0.0245442  0.3070071 ]\n",
      " [0.02926983 0.02913897 0.01136281 0.9302284 ]\n",
      " ...\n",
      " [0.00863393 0.9527609  0.00493731 0.03366795]\n",
      " [0.02201912 0.0046941  0.9616294  0.01165745]\n",
      " [0.02201461 0.00470544 0.9616097  0.01167027]]\n",
      "Iteration 1713, Accuracy 0.33657\n",
      "97.58433%change in label assignment\n",
      "0.0490759\n",
      "[[0.01469443 0.9298621  0.00881135 0.04663215]\n",
      " [0.0398903  0.7075567  0.02081538 0.2317376 ]\n",
      " [0.02824107 0.03118963 0.0110936  0.92947567]\n",
      " ...\n",
      " [0.00803361 0.9577254  0.00464112 0.0295999 ]\n",
      " [0.02199881 0.00469382 0.96165365 0.01165372]\n",
      " [0.02199713 0.00470545 0.9616297  0.01166764]]\n",
      "Iteration 1714, Accuracy 0.33343\n",
      "99.38626%change in label assignment\n",
      "0.04806308\n",
      "[[0.01098231 0.94578874 0.00650746 0.03672152]\n",
      " [0.05032317 0.5880348  0.02565272 0.33598936]\n",
      " [0.03011337 0.02880159 0.01158881 0.9294962 ]\n",
      " ...\n",
      " [0.0091378  0.9493941  0.00519794 0.03627015]\n",
      " [0.02195996 0.00469929 0.96169025 0.01165055]\n",
      " [0.02197659 0.00471499 0.96163404 0.01167434]]\n",
      "Iteration 1715, Accuracy 0.33137\n",
      "97.58433%change in label assignment\n",
      "0.049124256\n",
      "[[0.0144736  0.9307698  0.00867794 0.04607867]\n",
      " [0.04339916 0.6709277  0.02247904 0.26319405]\n",
      " [0.02833901 0.03051309 0.01109939 0.9300486 ]\n",
      " ...\n",
      " [0.00801899 0.9574641  0.00462341 0.02989346]\n",
      " [0.0219389  0.00469311 0.9617289  0.01163914]\n",
      " [0.02194573 0.00470613 0.96169114 0.01165702]]\n",
      "Iteration 1716, Accuracy 0.32818\n",
      "98.91491%change in label assignment\n",
      "0.048175357\n",
      "[[0.01243424 0.9394798  0.0074148  0.04067117]\n",
      " [0.04668834 0.633183   0.02403332 0.2960953 ]\n",
      " [0.02878688 0.02953563 0.01121468 0.93046284]\n",
      " ...\n",
      " [0.00814245 0.9562286  0.00468163 0.03094739]\n",
      " [0.02191613 0.00468287 0.9617802  0.01162075]\n",
      " [0.02190713 0.00469175 0.9617718  0.01162931]]\n",
      "Iteration 1717, Accuracy 0.32479\n",
      "97.39284%change in label assignment\n",
      "0.04979957\n",
      "[[0.01166391 0.94278026 0.00693982 0.03861604]\n",
      " [0.0491834  0.60170424 0.02518695 0.3239254 ]\n",
      " [0.03041702 0.02869324 0.01170914 0.92918056]\n",
      " ...\n",
      " [0.0088786  0.9510404  0.00506663 0.03501437]\n",
      " [0.02187445 0.00469364 0.96180737 0.01162459]\n",
      " [0.02189232 0.00470927 0.96174973 0.0116486 ]]\n",
      "Iteration 1718, Accuracy 0.33181\n",
      "94.82987%change in label assignment\n",
      "0.04719621\n",
      "[[0.01561852 0.9258464  0.00940156 0.04913348]\n",
      " [0.04214853 0.6836188  0.02190186 0.2523308 ]\n",
      " [0.02854281 0.02991378 0.0111392  0.93040425]\n",
      " ...\n",
      " [0.00800533 0.95759594 0.00462201 0.02977677]\n",
      " [0.0218619  0.00469795 0.9618127  0.01162754]\n",
      " [0.02188986 0.00471664 0.9617355  0.01165801]]\n",
      "Iteration 1719, Accuracy 0.33171\n",
      "97.63343%change in label assignment\n",
      "[[0.01338886 0.93531    0.00802134 0.04327985]\n",
      " [0.04567725 0.64396745 0.02361558 0.2867397 ]\n",
      " [0.02933522 0.02901024 0.01139231 0.9302623 ]\n",
      " ...\n",
      " [0.00834661 0.9546476  0.00479038 0.03221544]\n",
      " [0.02182896 0.00469374 0.96186215 0.01161511]\n",
      " [0.02185573 0.00471209 0.96178734 0.01164482]]\n",
      "Iteration 1720, Accuracy 0.33137\n",
      "97.45176%change in label assignment\n",
      "0.059264086\n",
      "[[0.00898543 0.9503543  0.00511629 0.03554396]\n",
      " [0.0594855  0.27264023 0.02820952 0.6396647 ]\n",
      " [0.04414958 0.0314638  0.0161914  0.9081952 ]\n",
      " ...\n",
      " [0.02110332 0.8665377  0.01155047 0.10080852]\n",
      " [0.02185884 0.00471118 0.96178347 0.01164655]\n",
      " [0.0219172  0.0047375  0.9616508  0.0116945 ]]\n",
      "Iteration 1721, Accuracy 0.33859\n",
      "95.85604%change in label assignment\n",
      "0.05848675\n",
      "[[0.01954745 0.9095533  0.01185192 0.05904728]\n",
      " [0.04146855 0.6918562  0.02152916 0.24514604]\n",
      " [0.02889375 0.03749491 0.011627   0.9219844 ]\n",
      " ...\n",
      " [0.00925268 0.95328486 0.00541337 0.03204908]\n",
      " [0.02196204 0.00475029 0.9615542  0.01173351]\n",
      " [0.02206958 0.00478956 0.96133024 0.01181061]]\n",
      "Iteration 1722, Accuracy 0.3435\n",
      "96.30775%change in label assignment\n",
      "0.06281014\n",
      "[[0.01397211 0.9170814  0.00783521 0.06111125]\n",
      " [0.05055632 0.1483685  0.02308499 0.7779902 ]\n",
      " [0.05098984 0.03356306 0.01864332 0.89680374]\n",
      " ...\n",
      " [0.03106468 0.78647316 0.0167658  0.1656964 ]\n",
      " [0.02186625 0.00471927 0.9617492  0.0116653 ]\n",
      " [0.02193498 0.00474764 0.9615988  0.01171855]]\n",
      "Iteration 1723, Accuracy 0.35582\n",
      "96.87239%change in label assignment\n",
      "0.0677168\n",
      "[[0.03562047 0.8462108  0.02224007 0.09592863]\n",
      " [0.02031979 0.87372094 0.01098748 0.09497179]\n",
      " [0.03884526 0.07461242 0.01639575 0.8701465 ]\n",
      " ...\n",
      " [0.0166238  0.9218167  0.00996537 0.05159415]\n",
      " [0.02188296 0.00471613 0.9617236  0.01167723]\n",
      " [0.02193631 0.00474089 0.9616008  0.01172202]]\n",
      "Iteration 1724, Accuracy 0.3463\n",
      "94.56965%change in label assignment\n",
      "0.08431828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01620748 0.90144485 0.00913974 0.07320796]\n",
      " [0.05468355 0.19272396 0.02580532 0.7267872 ]\n",
      " [0.05976096 0.03637278 0.02186437 0.8820019 ]\n",
      " ...\n",
      " [0.03670824 0.73442054 0.0198329  0.20903838]\n",
      " [0.02178949 0.00469269 0.96190715 0.01161072]\n",
      " [0.02182884 0.00471398 0.96180975 0.01164742]]\n",
      "Iteration 1725, Accuracy 0.34787\n",
      "94.27996%change in label assignment\n",
      "0.07474308\n",
      "[[0.0438914  0.8155014  0.02778625 0.11282086]\n",
      " [0.00912893 0.9501012  0.00514165 0.03562821]\n",
      " [0.04408495 0.09910428 0.01895605 0.8378547 ]\n",
      " ...\n",
      " [0.021286   0.9024858  0.01290229 0.06332594]\n",
      " [0.02185692 0.00470634 0.9617719  0.01166481]\n",
      " [0.02189705 0.00472788 0.96167296 0.0117021 ]]\n",
      "Iteration 1726, Accuracy 0.35327\n",
      "96.86257%change in label assignment\n",
      "0.09027716\n",
      "[[0.01348896 0.92024857 0.00766864 0.05859382]\n",
      " [0.05866843 0.2649408  0.0284087  0.64798206]\n",
      " [0.06322425 0.03732512 0.0230133  0.8764374 ]\n",
      " ...\n",
      " [0.03291113 0.7694574  0.01791343 0.17971809]\n",
      " [0.02178791 0.00470247 0.9618804  0.01162927]\n",
      " [0.02184161 0.00472727 0.961757   0.01167413]]\n",
      "Iteration 1727, Accuracy 0.34723\n",
      "95.10974%change in label assignment\n",
      "0.07490183\n",
      "[[0.04740749 0.80283415 0.03014398 0.11961434]\n",
      " [0.00846308 0.9558855  0.00484127 0.0308102 ]\n",
      " [0.0400475  0.0789597  0.01687817 0.8641147 ]\n",
      " ...\n",
      " [0.02556832 0.8852243  0.01561999 0.07358741]\n",
      " [0.02189944 0.00473089 0.96166086 0.01170879]\n",
      " [0.02197035 0.00476002 0.9615059  0.01176369]]\n",
      "Iteration 1728, Accuracy 0.33962\n",
      "95.49271%change in label assignment\n",
      "0.08237959\n",
      "[[0.0083255  0.95592266 0.00489891 0.03085296]\n",
      " [0.05565505 0.49312097 0.02849679 0.42272714]\n",
      " [0.04358546 0.0316596  0.01650307 0.9082519 ]\n",
      " ...\n",
      " [0.01613768 0.9018654  0.00910329 0.0728936 ]\n",
      " [0.02184447 0.00473563 0.9617247  0.01169517]\n",
      " [0.0219482  0.00477417 0.96150726 0.01177036]]\n",
      "Iteration 1729, Accuracy 0.34566\n",
      "95.99352%change in label assignment\n",
      "0.059847165\n",
      "[[0.02229914 0.89834046 0.01354086 0.06581947]\n",
      " [0.01733908 0.8949132  0.00945664 0.07829114]\n",
      " [0.02964826 0.03914548 0.01184111 0.91936517]\n",
      " ...\n",
      " [0.00887453 0.95472765 0.00514175 0.03125604]\n",
      " [0.02179958 0.00469315 0.96186864 0.01163863]\n",
      " [0.02183082 0.00471275 0.96178526 0.01167114]]\n",
      "Iteration 1730, Accuracy 0.35071\n",
      "91.98704%change in label assignment\n",
      "0.062727906\n",
      "[[0.01161209 0.93285835 0.00662505 0.04890455]\n",
      " [0.0598956  0.33488798 0.02950004 0.5757163 ]\n",
      " [0.04772667 0.03276878 0.01781768 0.9016868 ]\n",
      " ...\n",
      " [0.03119552 0.7847172  0.0169837  0.16710359]\n",
      " [0.02174527 0.00468554 0.96195686 0.01161236]\n",
      " [0.02177786 0.00470525 0.96187156 0.01164534]]\n",
      "Iteration 1731, Accuracy 0.33554\n",
      "97.77581%change in label assignment\n",
      "0.06875408\n",
      "[[0.03856406 0.83526576 0.02413243 0.10203777]\n",
      " [0.01026475 0.94281334 0.00572809 0.04119381]\n",
      " [0.03817562 0.07077997 0.01594849 0.8750959 ]\n",
      " ...\n",
      " [0.01681165 0.9210549  0.01004731 0.05208615]\n",
      " [0.02180601 0.00469777 0.9618404  0.01165584]\n",
      " [0.02183662 0.00471636 0.9617601  0.01168697]]\n",
      "Iteration 1732, Accuracy 0.33549\n",
      "95.84622%change in label assignment\n",
      "0.07954823\n",
      "[[0.0098708  0.9444715  0.00569962 0.03995805]\n",
      " [0.0591511  0.39754707 0.02972514 0.51357657]\n",
      " [0.05077557 0.03380396 0.01898425 0.8964363 ]\n",
      " ...\n",
      " [0.02690398 0.8206145  0.01482826 0.13765328]\n",
      " [0.02172394 0.00469385 0.9619568  0.01162535]\n",
      " [0.02176881 0.00471616 0.9618502  0.01166483]]\n",
      "Iteration 1733, Accuracy 0.327\n",
      "95.12447%change in label assignment\n",
      "0.06514883\n",
      "[[0.02571705 0.88464403 0.01572262 0.07391628]\n",
      " [0.02076665 0.8705286  0.01121425 0.09749048]\n",
      " [0.03014013 0.0407751  0.01205913 0.9170257 ]\n",
      " ...\n",
      " [0.01026671 0.9489158  0.00599994 0.03481759]\n",
      " [0.02180194 0.0047076  0.96182    0.01167045]\n",
      " [0.02184936 0.00473026 0.96170926 0.0117111 ]]\n",
      "Iteration 1734, Accuracy 0.33294\n",
      "98.23243%change in label assignment\n",
      "0.06555952\n",
      "[[0.00988294 0.944431   0.00571698 0.03996909]\n",
      " [0.05826899 0.429987   0.02956605 0.4821779 ]\n",
      " [0.04167521 0.03110046 0.01594309 0.9112812 ]\n",
      " ...\n",
      " [0.02666465 0.8225913  0.01472967 0.13601443]\n",
      " [0.02170979 0.0046825  0.96199065 0.01161704]\n",
      " [0.0217385  0.00470077 0.9619135  0.01164725]]\n",
      "Iteration 1735, Accuracy 0.33652\n",
      "95.39942%change in label assignment\n",
      "0.060940534\n",
      "[[0.0512323  0.7896418  0.03256901 0.12655692]\n",
      " [0.00998414 0.9498318  0.00574216 0.03444183]\n",
      " [0.04348793 0.0921265  0.01831006 0.8460755 ]\n",
      " ...\n",
      " [0.02745818 0.8779498  0.01671562 0.07787634]\n",
      " [0.02176921 0.00469881 0.9618736  0.01165841]\n",
      " [0.02180486 0.00471904 0.9617832  0.01169291]]\n",
      "Iteration 1736, Accuracy 0.32508\n",
      "97.05897%change in label assignment\n",
      "0.09407782\n",
      "[[0.01112468 0.9361322  0.00638639 0.04635678]\n",
      " [0.0597444  0.3571763  0.0297779  0.55330133]\n",
      " [0.05315931 0.03450166 0.0198426  0.8924964 ]\n",
      " ...\n",
      " [0.02941873 0.7998184  0.01614222 0.15462065]\n",
      " [0.0217382  0.00471472 0.961877   0.01167004]\n",
      " [0.02181299 0.00474466 0.96171534 0.011727  ]]\n",
      "Iteration 1737, Accuracy 0.33324\n",
      "93.69077%change in label assignment\n",
      "0.06839348\n",
      "[[0.03909116 0.8332965  0.02451025 0.10310209]\n",
      " [0.01011465 0.9437155  0.00565611 0.04051374]\n",
      " [0.03520329 0.05941983 0.01455606 0.8908208 ]\n",
      " ...\n",
      " [0.01754643 0.9179888  0.01051945 0.05394536]\n",
      " [0.0218371  0.00473899 0.9616873  0.01173658]\n",
      " [0.0219231  0.00477153 0.9615056  0.01179984]]\n",
      "Iteration 1738, Accuracy 0.34448\n",
      "97.08352%change in label assignment\n",
      "0.07071291\n",
      "[[0.00808453 0.9567614  0.00471998 0.03043412]\n",
      " [0.05858814 0.42334303 0.02940897 0.4886598 ]\n",
      " [0.03966492 0.03024833 0.01506207 0.91502464]\n",
      " ...\n",
      " [0.01903675 0.881164   0.01061111 0.08918814]\n",
      " [0.02176679 0.00473025 0.96180016 0.01170278]\n",
      " [0.02186735 0.0047675  0.9615895  0.01177569]]\n",
      "Iteration 1739, Accuracy 0.35258\n",
      "96.84784%change in label assignment\n",
      "0.05609661\n",
      "[[0.0219313  0.89984685 0.01334729 0.06487462]\n",
      " [0.02770527 0.8167678  0.01478594 0.14074099]\n",
      " [0.02906075 0.03774098 0.01164111 0.9215571 ]\n",
      " ...\n",
      " [0.00897441 0.954453   0.00522646 0.03134618]\n",
      " [0.02173811 0.00470998 0.9618751  0.01167679]\n",
      " [0.02180238 0.00473749 0.9617321  0.01172798]]\n",
      "Iteration 1740, Accuracy 0.35513\n",
      "96.93622%change in label assignment\n",
      "0.05750812\n",
      "[[0.00807102 0.95770216 0.00472874 0.02949804]\n",
      " [0.05348239 0.53588814 0.02739882 0.38323057]\n",
      " [0.03280834 0.02846543 0.01266806 0.92605823]\n",
      " ...\n",
      " [0.0156102  0.9056798  0.00874799 0.06996199]\n",
      " [0.02166033 0.00468893 0.9620199  0.0116309 ]\n",
      " [0.02170835 0.00471231 0.96190673 0.01167261]]\n",
      "Iteration 1741, Accuracy 0.35145\n",
      "98.6105%change in label assignment\n",
      "0.052163057\n",
      "[[0.01522062 0.9277135  0.00912466 0.04794122]\n",
      " [0.03474835 0.7563262  0.01832268 0.19060273]\n",
      " [0.02804946 0.03301249 0.01111179 0.9278262 ]\n",
      " ...\n",
      " [0.00801725 0.9574196  0.0046044  0.02995879]\n",
      " [0.02166414 0.00468863 0.96201164 0.01163563]\n",
      " [0.02170949 0.00471145 0.96190315 0.01167594]]\n",
      "Iteration 1742, Accuracy 0.35013\n",
      "99.08676%change in label assignment\n",
      "0.052426636\n",
      "[[0.00792845 0.95804214 0.00461703 0.02941245]\n",
      " [0.0559072  0.4941107  0.02830361 0.42167845]\n",
      " [0.03365016 0.02854986 0.01289566 0.92490435]\n",
      " ...\n",
      " [0.01642967 0.90003103 0.00916371 0.07437557]\n",
      " [0.02166232 0.0047048  0.9619762  0.01165679]\n",
      " [0.02173393 0.00473386 0.96182036 0.01171184]]\n",
      "Iteration 1743, Accuracy 0.34605\n",
      "95.37978%change in label assignment\n",
      "0.0523093\n",
      "[[0.0154055  0.9268965  0.00926181 0.04843622]\n",
      " [0.03769564 0.7285639  0.01979676 0.21394363]\n",
      " [0.02786745 0.03278912 0.01108366 0.9282598 ]\n",
      " ...\n",
      " [0.00793702 0.95807266 0.00457951 0.0294108 ]\n",
      " [0.02168002 0.00471523 0.96192586 0.01167883]\n",
      " [0.02176175 0.00474689 0.9617515  0.01173983]]\n",
      "Iteration 1744, Accuracy 0.34536\n",
      "98.80198%change in label assignment\n",
      "0.05289243\n",
      "[[0.00791879 0.9581179  0.00461278 0.02935054]\n",
      " [0.05628233 0.4864186  0.02843286 0.4288662 ]\n",
      " [0.03280322 0.02838443 0.01261352 0.92619884]\n",
      " ...\n",
      " [0.01526719 0.90809697 0.00854286 0.06809296]\n",
      " [0.02158368 0.00468682 0.96211153 0.01161802]\n",
      " [0.02164372 0.00471354 0.96197575 0.01166705]]\n",
      "Iteration 1745, Accuracy 0.34767\n",
      "98.90509%change in label assignment\n",
      "0.052380305\n",
      "[[0.02259594 0.8970784  0.01382229 0.06650347]\n",
      " [0.01921077 0.8810729  0.01048824 0.08922813]\n",
      " [0.03069069 0.04476863 0.01257663 0.911964  ]\n",
      " ...\n",
      " [0.01041333 0.9483514  0.00614543 0.0350899 ]\n",
      " [0.02155145 0.00465669 0.9622154  0.01157639]\n",
      " [0.02156281 0.00467225 0.96216637 0.01159856]]\n",
      "Iteration 1746, Accuracy 0.34472\n",
      "92.49276%change in label assignment\n",
      "0.059255566\n",
      "[[0.00947171 0.9469175  0.0054382  0.03817256]\n",
      " [0.05998217 0.34305766 0.02947513 0.56748503]\n",
      " [0.04319711 0.03116652 0.01620333 0.909433  ]\n",
      " ...\n",
      " [0.02450674 0.839877   0.01347957 0.12213666]\n",
      " [0.02152606 0.00467938 0.9621994  0.01159515]\n",
      " [0.02158256 0.00470476 0.9620711  0.01164157]]\n",
      "Iteration 1747, Accuracy 0.33613\n",
      "93.30289%change in label assignment\n",
      "0.060950637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0341463  0.85176164 0.02131739 0.09277473]\n",
      " [0.0115153  0.9341928  0.00641797 0.04787394]\n",
      " [0.03407126 0.05620969 0.0141269  0.89559215]\n",
      " ...\n",
      " [0.01705994 0.91997474 0.01026929 0.05269611]\n",
      " [0.02154293 0.00467726 0.9621766  0.01160328]\n",
      " [0.02158695 0.00469978 0.96207047 0.01164284]]\n",
      "Iteration 1748, Accuracy 0.34315\n",
      "97.29464%change in label assignment\n",
      "0.06582739\n",
      "[[0.00816135 0.9559297  0.00475553 0.03115348]\n",
      " [0.0579603  0.4416168  0.0292254  0.4711975 ]\n",
      " [0.04069759 0.03048133 0.01541827 0.9134028 ]\n",
      " ...\n",
      " [0.0183094  0.886351   0.01023751 0.08510211]\n",
      " [0.02148115 0.00467576 0.96226096 0.0115821 ]\n",
      " [0.02153687 0.00470085 0.9621343  0.01162794]]\n",
      "Iteration 1749, Accuracy 0.34443\n",
      "96.59253%change in label assignment\n",
      "0.05845653\n",
      "[[0.02888023 0.8721042  0.01784058 0.08117495]\n",
      " [0.0181133  0.88926846 0.00988004 0.08273828]\n",
      " [0.03003684 0.0414568  0.01212227 0.91638404]\n",
      " ...\n",
      " [0.01300858 0.9371633  0.00772933 0.04209877]\n",
      " [0.02149832 0.00467737 0.9622303  0.01159401]\n",
      " [0.02154859 0.00470115 0.96211344 0.01163679]]\n",
      "Iteration 1750, Accuracy 0.34585\n",
      "97.30446%change in label assignment\n",
      "0.053295985\n",
      "[[0.00996701 0.95006865 0.00593163 0.0340327 ]\n",
      " [0.05068083 0.57810766 0.0261121  0.34509942]\n",
      " [0.03266233 0.02835006 0.0125762  0.9264114 ]\n",
      " ...\n",
      " [0.01021319 0.9419936  0.00582553 0.04196768]\n",
      " [0.02143724 0.00467514 0.96231174 0.01157588]\n",
      " [0.02149571 0.00470092 0.96218    0.0116233 ]]\n",
      "Iteration 1751, Accuracy 0.34826\n",
      "96.45505%change in label assignment\n",
      "0.04830696\n",
      "[[0.01682072 0.9208188  0.01017947 0.05218095]\n",
      " [0.03401795 0.7617588  0.01804011 0.18618312]\n",
      " [0.02774111 0.03072235 0.01095091 0.9305857 ]\n",
      " ...\n",
      " [0.00809605 0.95793897 0.00471492 0.02925002]\n",
      " [0.02138801 0.00464349 0.9624438  0.01152462]\n",
      " [0.0214064  0.0046602  0.96238333 0.01155004]]\n",
      "Iteration 1752, Accuracy 0.35189\n",
      "96.14082%change in label assignment\n",
      "0.046756603\n",
      "[[0.00943171 0.952411   0.00559125 0.03256603]\n",
      " [0.05016841 0.58553594 0.02585123 0.33844444]\n",
      " [0.031229   0.02816152 0.01207391 0.9285356 ]\n",
      " ...\n",
      " [0.01033357 0.9412047  0.00588333 0.04257845]\n",
      " [0.02134093 0.00463614 0.9625172  0.01150583]\n",
      " [0.02135538 0.00465143 0.9624648  0.01152842]]\n",
      "Iteration 1753, Accuracy 0.34546\n",
      "99.22915%change in label assignment\n",
      "0.04889951\n",
      "[[0.02050111 0.9054127  0.01255697 0.06152925]\n",
      " [0.02502938 0.8364432  0.01358313 0.12494435]\n",
      " [0.02762871 0.03375099 0.01114453 0.9274757 ]\n",
      " ...\n",
      " [0.00889906 0.9547451  0.00524155 0.03111435]\n",
      " [0.02132795 0.00464055 0.9625194  0.011512  ]\n",
      " [0.0213489  0.00465725 0.9624558  0.01153811]]\n",
      "Iteration 1754, Accuracy 0.3461\n",
      "95.84622%change in label assignment\n",
      "0.04790963\n",
      "[[0.01151323 0.9434376  0.00688162 0.0381676 ]\n",
      " [0.04522745 0.64760655 0.02355532 0.28361067]\n",
      " [0.02942348 0.0282692  0.0114661  0.9308412 ]\n",
      " ...\n",
      " [0.00914358 0.94904536 0.00522859 0.03658251]\n",
      " [0.02129434 0.00464586 0.9625499  0.01150985]\n",
      " [0.02133044 0.00466596 0.9624594  0.01154422]]\n",
      "Iteration 1755, Accuracy 0.35273\n",
      "97.92311%change in label assignment\n",
      "0.047256947\n",
      "[[0.01480162 0.9293433  0.00892002 0.04693504]\n",
      " [0.03875508 0.71689093 0.02039581 0.22395821]\n",
      " [0.02784195 0.02967256 0.01096922 0.93151623]\n",
      " ...\n",
      " [0.00792511 0.9575592  0.00457494 0.02994065]\n",
      " [0.02126491 0.0046377  0.96260417 0.01149323]\n",
      " [0.02129291 0.00465597 0.96252805 0.01152313]]\n",
      "Iteration 1756, Accuracy 0.35307\n",
      "99.62194%change in label assignment\n",
      "0.045111883\n",
      "[[0.01329204 0.9357629  0.00798848 0.04295656]\n",
      " [0.0424507  0.6785721  0.02221391 0.25676322]\n",
      " [0.02818553 0.02895158 0.01108035 0.9317826 ]\n",
      " ...\n",
      " [0.00819361 0.9554496  0.00471771 0.03163913]\n",
      " [0.02123526 0.00463736 0.9626414  0.01148596]\n",
      " [0.02126813 0.00465694 0.9625562  0.01151879]]\n",
      "Iteration 1757, Accuracy 0.35292\n",
      "99.53847%change in label assignment\n",
      "0.047354642\n",
      "[[0.01614238 0.92357796 0.00978686 0.05049279]\n",
      " [0.03785002 0.7250678  0.0200103  0.21707189]\n",
      " [0.02748263 0.03029262 0.01093069 0.931294  ]\n",
      " ...\n",
      " [0.00785323 0.9583584  0.00456148 0.02922688]\n",
      " [0.0211958  0.00462404 0.962719   0.01146109]\n",
      " [0.02121262 0.00463961 0.9626631  0.01148462]]\n",
      "Iteration 1758, Accuracy 0.35194\n",
      "98.4632%change in label assignment\n",
      "0.045864202\n",
      "[[0.0126692  0.9384676  0.00759024 0.041273  ]\n",
      " [0.04637749 0.63490856 0.02399996 0.29471406]\n",
      " [0.02804098 0.02918342 0.01101879 0.9317568 ]\n",
      " ...\n",
      " [0.00809441 0.9561699  0.00466148 0.03107414]\n",
      " [0.02119419 0.00460237 0.962769   0.01143448]\n",
      " [0.02117088 0.0046082  0.9627858  0.01143513]]\n",
      "Iteration 1759, Accuracy 0.35361\n",
      "95.03609%change in label assignment\n",
      "[[0.01021254 0.9491305  0.00605338 0.0346036 ]\n",
      " [0.05335947 0.54090774 0.0270389  0.37869385]\n",
      " [0.02949096 0.02831852 0.01141818 0.9307724 ]\n",
      " ...\n",
      " [0.00927996 0.9481684  0.00528601 0.03726568]\n",
      " [0.02117223 0.00459794 0.9628061  0.01142371]\n",
      " [0.0211441  0.00460242 0.9628321  0.01142137]]\n",
      "Iteration 1760, Accuracy 0.34242\n",
      "96.04262%change in label assignment\n",
      "0.054321565\n",
      "[[0.008657   0.95222443 0.00497453 0.03414399]\n",
      " [0.05945426 0.28601217 0.02858304 0.6259506 ]\n",
      " [0.03782136 0.02949733 0.01428314 0.9183982 ]\n",
      " ...\n",
      " [0.01996975 0.87428737 0.01104517 0.09469771]\n",
      " [0.02113789 0.00459664 0.9628495  0.01141599]\n",
      " [0.02111829 0.00460316 0.96286017 0.01141847]]\n",
      "Iteration 1761, Accuracy 0.33309\n",
      "98.24716%change in label assignment\n",
      "0.054823987\n",
      "[[0.01559742 0.9260198  0.00940083 0.04898193]\n",
      " [0.04286465 0.67560333 0.02228699 0.25924498]\n",
      " [0.02784218 0.03465871 0.01121392 0.9262852 ]\n",
      " ...\n",
      " [0.00798754 0.9583222  0.00464091 0.0290493 ]\n",
      " [0.02113552 0.00460404 0.962828   0.01143256]\n",
      " [0.02112667 0.00461342 0.96281844 0.01144148]]\n",
      "Iteration 1762, Accuracy 0.33721\n",
      "97.46649%change in label assignment\n",
      "0.05969172\n",
      "[[0.01722493 0.89398706 0.00964349 0.07914446]\n",
      " [0.04369208 0.10470377 0.01969308 0.831911  ]\n",
      " [0.05467612 0.03467612 0.02017387 0.8904739 ]\n",
      " ...\n",
      " [0.0330345  0.76797736 0.01790193 0.1810862 ]\n",
      " [0.02108308 0.00461137 0.9628809  0.01142463]\n",
      " [0.02110336 0.00462817 0.9628179  0.01145055]]\n",
      "Iteration 1763, Accuracy 0.33805\n",
      "94.89861%change in label assignment\n",
      "0.07052932\n",
      "[[0.03441851 0.85076845 0.02145975 0.09335332]\n",
      " [0.03728155 0.7345041  0.0194259  0.20878845]\n",
      " [0.0372015  0.06865846 0.01567251 0.8784676 ]\n",
      " ...\n",
      " [0.0190689  0.9116132  0.01151744 0.05780043]\n",
      " [0.02114817 0.00463471 0.9627363  0.01148083]\n",
      " [0.02118628 0.0046558  0.96264094 0.01151694]]\n",
      "Iteration 1764, Accuracy 0.34684\n",
      "96.85275%change in label assignment\n",
      "0.0804071\n",
      "[[0.01644657 0.89937425 0.00927141 0.07490776]\n",
      " [0.03166946 0.0514557  0.01364053 0.9032343 ]\n",
      " [0.06738721 0.03831745 0.0244613  0.869834  ]\n",
      " ...\n",
      " [0.03096743 0.78576195 0.01693039 0.16634019]\n",
      " [0.02111145 0.00464277 0.9627755  0.01147024]\n",
      " [0.02117633 0.00467072 0.962631   0.01152199]]\n",
      "Iteration 1765, Accuracy 0.35302\n",
      "95.70874%change in label assignment\n",
      "0.069348425\n",
      "[[0.03460398 0.8498522  0.02166404 0.09387977]\n",
      " [0.03455535 0.7580496  0.01819818 0.18919685]\n",
      " [0.03457379 0.0593089  0.0144967  0.8916206 ]\n",
      " ...\n",
      " [0.01839685 0.9142876  0.01113641 0.05617911]\n",
      " [0.02112097 0.00462048 0.9628008  0.01145782]\n",
      " [0.02114175 0.00463754 0.9627364  0.01148425]]\n",
      "Iteration 1766, Accuracy 0.35337\n",
      "93.70059%change in label assignment\n",
      "0.07713584\n",
      "[[0.01017997 0.9421242  0.0058828  0.041813  ]\n",
      " [0.0458364  0.11885472 0.02106027 0.8142486 ]\n",
      " [0.05255923 0.03428454 0.01972146 0.89343476]\n",
      " ...\n",
      " [0.02182616 0.85984087 0.01219792 0.10613512]\n",
      " [0.02106779 0.00462863 0.9628557  0.01144792]\n",
      " [0.02111497 0.00465195 0.9627439  0.0114892 ]]\n",
      "Iteration 1767, Accuracy 0.35125\n",
      "96.89203%change in label assignment\n",
      "0.062096722\n",
      "[[0.03041855 0.8660011  0.01886835 0.08471196]\n",
      " [0.03895133 0.7170915  0.02031408 0.22364314]\n",
      " [0.03029632 0.0435705  0.01237541 0.9137578 ]\n",
      " ...\n",
      " [0.0161323  0.9237976  0.00969132 0.05037877]\n",
      " [0.02118941 0.00467081 0.9625962  0.01154356]\n",
      " [0.02127285 0.00470301 0.96241844 0.01160568]]\n",
      "Iteration 1768, Accuracy 0.3541\n",
      "95.57618%change in label assignment\n",
      "0.0649489\n",
      "[[0.00804228 0.9564742  0.00467868 0.03080486]\n",
      " [0.05251    0.17040649 0.0244818  0.75260174]\n",
      " [0.04215649 0.03085956 0.01593611 0.9110478 ]\n",
      " ...\n",
      " [0.01444075 0.91340566 0.00814543 0.06400817]\n",
      " [0.02117997 0.00468048 0.96258986 0.0115497 ]\n",
      " [0.02128308 0.0047177  0.96237594 0.0116233 ]]\n",
      "Iteration 1769, Accuracy 0.3681\n",
      "97.28482%change in label assignment\n",
      "0.053823356\n",
      "[[0.0166893  0.92141956 0.01007461 0.05181652]\n",
      " [0.0579903  0.4544096  0.02871967 0.4588804 ]\n",
      " [0.0275145  0.0319183  0.01094832 0.9296189 ]\n",
      " ...\n",
      " [0.0087098  0.9555833  0.00509414 0.03061279]\n",
      " [0.0211257  0.00465519 0.96270823 0.01151092]\n",
      " [0.02119927 0.00468497 0.9625484  0.01156738]]\n",
      "Iteration 1770, Accuracy 0.36451\n",
      "97.31428%change in label assignment\n",
      "0.055045333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00840629 0.95384085 0.00486348 0.03288932]\n",
      " [0.05384792 0.18417151 0.02522272 0.7367578 ]\n",
      " [0.03750611 0.02944174 0.0143373  0.9187149 ]\n",
      " ...\n",
      " [0.01634798 0.9001584  0.00916681 0.07432681]\n",
      " [0.02104938 0.00463196 0.9628572  0.01146153]\n",
      " [0.02110662 0.00465801 0.9627265  0.01150891]]\n",
      "Iteration 1771, Accuracy 0.3626\n",
      "97.26027%change in label assignment\n",
      "0.054749444\n",
      "[[0.01796403 0.91604394 0.01090128 0.05509083]\n",
      " [0.05251484 0.5549534  0.02664461 0.36588722]\n",
      " [0.02786012 0.03537874 0.01127867 0.92548245]\n",
      " ...\n",
      " [0.0091881  0.9535884  0.005405   0.03181845]\n",
      " [0.02106474 0.0046391  0.9628145  0.0114816 ]\n",
      " [0.0211252  0.00466584 0.96267825 0.01153073]]\n",
      "Iteration 1772, Accuracy 0.35224\n",
      "97.06388%change in label assignment\n",
      "0.056483604\n",
      "[[0.00842193 0.9537285  0.00487025 0.03297936]\n",
      " [0.0548088  0.19591555 0.02580806 0.7234675 ]\n",
      " [0.03761336 0.02946287 0.01438685 0.9185369 ]\n",
      " ...\n",
      " [0.0167179  0.89755744 0.00936319 0.07636146]\n",
      " [0.02102873 0.00463778 0.96286345 0.01147008]\n",
      " [0.02109511 0.0046658  0.9627167  0.01152238]]\n",
      "Iteration 1773, Accuracy 0.35945\n",
      "97.36338%change in label assignment\n",
      "0.054041907\n",
      "[[0.01401373 0.93279856 0.00839811 0.04478959]\n",
      " [0.05846829 0.44119918 0.02890583 0.47142673]\n",
      " [0.02747341 0.03277689 0.01100301 0.9287467 ]\n",
      " ...\n",
      " [0.00805442 0.9581368  0.00468124 0.0291275 ]\n",
      " [0.02104365 0.00464161 0.9628301  0.01148455]\n",
      " [0.02111302 0.00467069 0.96267736 0.011539  ]]\n",
      "Iteration 1774, Accuracy 0.35813\n",
      "99.0082%change in label assignment\n",
      "0.0532218\n",
      "[[0.00912744 0.94901025 0.00525849 0.03660381]\n",
      " [0.05360774 0.18188056 0.02515613 0.7393556 ]\n",
      " [0.03672279 0.0292224  0.01411947 0.91993535]\n",
      " ...\n",
      " [0.01874927 0.88296986 0.01046526 0.08781557]\n",
      " [0.02100267 0.00463537 0.9628954  0.01146657]\n",
      " [0.02107209 0.00466439 0.96274257 0.01152093]]\n",
      "Iteration 1775, Accuracy 0.36014\n",
      "98.99347%change in label assignment\n",
      "0.05241494\n",
      "[[0.03517726 0.8479219  0.02195572 0.09494509]\n",
      " [0.02932103 0.8040451  0.01555614 0.15107769]\n",
      " [0.03459796 0.05857868 0.01440875 0.8924146 ]\n",
      " ...\n",
      " [0.02098435 0.90375066 0.01272903 0.06253598]\n",
      " [0.02103404 0.00464959 0.96281916 0.01149728]\n",
      " [0.02111398 0.00468148 0.9626465  0.01155807]]\n",
      "Iteration 1776, Accuracy 0.35926\n",
      "96.95586%change in label assignment\n",
      "0.07481697\n",
      "[[0.013168   0.92213374 0.00748239 0.05721592]\n",
      " [0.04873564 0.13827686 0.02254272 0.7904448 ]\n",
      " [0.04731739 0.03247255 0.01784074 0.9023693 ]\n",
      " ...\n",
      " [0.02581224 0.82899326 0.014238   0.13095653]\n",
      " [0.02095134 0.00462842 0.9629698  0.01145044]\n",
      " [0.02102114 0.00465788 0.9628156  0.01150543]]\n",
      "Iteration 1777, Accuracy 0.36598\n",
      "98.18334%change in label assignment\n",
      "0.061854042\n",
      "[[0.0304254  0.8660853  0.0188727  0.08461653]\n",
      " [0.03361791 0.7668823  0.01772537 0.18177445]\n",
      " [0.03297038 0.05311676 0.01368718 0.9002257 ]\n",
      " ...\n",
      " [0.01742666 0.9184417  0.01050741 0.05362422]\n",
      " [0.02098427 0.00463624 0.9629039  0.01147563]\n",
      " [0.02105137 0.00466472 0.9627552  0.01152874]]\n",
      "Iteration 1778, Accuracy 0.36132\n",
      "97.79545%change in label assignment\n",
      "0.066099614\n",
      "[[0.00845438 0.9535014  0.00490287 0.03314137]\n",
      " [0.05667809 0.22434841 0.02702915 0.69194436]\n",
      " [0.03912118 0.02988969 0.01493404 0.91605514]\n",
      " ...\n",
      " [0.01593222 0.90301067 0.00896847 0.07208866]\n",
      " [0.0208789  0.00460504 0.9631118  0.01140421]\n",
      " [0.02092327 0.0046283  0.9630039  0.01144463]]\n",
      "Iteration 1779, Accuracy 0.36643\n",
      "96.68582%change in label assignment\n",
      "0.052410435\n",
      "[[0.01732753 0.9187838  0.01047349 0.05341523]\n",
      " [0.0549247  0.5185596  0.02754656 0.39896908]\n",
      " [0.02756215 0.03215853 0.01095455 0.92932475]\n",
      " ...\n",
      " [0.00927183 0.95326144 0.00544281 0.03202393]\n",
      " [0.02093783 0.00463217 0.9629723  0.01145773]\n",
      " [0.02100974 0.00466203 0.96281445 0.01151382]]\n",
      "Iteration 1780, Accuracy 0.3599\n",
      "96.82329%change in label assignment\n",
      "0.05323197\n",
      "[[0.00784393 0.9579819  0.00457235 0.02960186]\n",
      " [0.05819502 0.25294718 0.02790288 0.66095495]\n",
      " [0.03527085 0.02873367 0.01355124 0.9224442 ]\n",
      " ...\n",
      " [0.01278806 0.9247063  0.00724084 0.05526485]\n",
      " [0.02089106 0.00462433 0.9630481  0.01143657]\n",
      " [0.02096237 0.00465412 0.9628911  0.01149238]]\n",
      "Iteration 1781, Accuracy 0.36628\n",
      "98.20298%change in label assignment\n",
      "0.049159084\n",
      "[[0.01429416 0.9316169  0.00857624 0.04551272]\n",
      " [0.05723938 0.47191852 0.02848423 0.44235787]\n",
      " [0.02739318 0.03105611 0.0108728  0.93067795]\n",
      " ...\n",
      " [0.00829343 0.9573185  0.00483808 0.02954997]\n",
      " [0.02086934 0.00461483 0.96309346 0.01142237]\n",
      " [0.02092781 0.00464142 0.96296    0.01147075]]\n",
      "Iteration 1782, Accuracy 0.3649\n",
      "98.72834%change in label assignment\n",
      "0.049442858\n",
      "[[0.00780004 0.95842826 0.00455285 0.02921889]\n",
      " [0.05799793 0.24794959 0.02774397 0.6663085 ]\n",
      " [0.03324842 0.02824448 0.01286456 0.9256426 ]\n",
      " ...\n",
      " [0.01203093 0.9297831  0.00682724 0.0513587 ]\n",
      " [0.02083693 0.00461437 0.9631343  0.01141441]\n",
      " [0.0209027  0.00464294 0.96298724 0.01146721]]\n",
      "Iteration 1783, Accuracy 0.36466\n",
      "98.73815%change in label assignment\n",
      "0.049393103\n",
      "[[0.01330566 0.9358093  0.00797925 0.04290577]\n",
      " [0.05747613 0.46344835 0.02864519 0.45043036]\n",
      " [0.02725427 0.0303973  0.01083796 0.9315105 ]\n",
      " ...\n",
      " [0.00807698 0.958153   0.00471522 0.02905478]\n",
      " [0.02083656 0.00461794 0.9631244  0.01142117]\n",
      " [0.0209065  0.00464766 0.9629693  0.01147652]]\n",
      "Iteration 1784, Accuracy 0.36427\n",
      "98.66942%change in label assignment\n",
      "0.04879284\n",
      "[[0.00779861 0.9582653  0.00453903 0.02939711]\n",
      " [0.05717608 0.2303641  0.0271304  0.68532944]\n",
      " [0.03385916 0.02834639 0.01303244 0.924762  ]\n",
      " ...\n",
      " [0.01228918 0.9280549  0.00695704 0.05269879]\n",
      " [0.02077363 0.00460031 0.963244   0.01138204]\n",
      " [0.02083015 0.00462681 0.96311337 0.01142972]]\n",
      "Iteration 1785, Accuracy 0.3676\n",
      "98.76761%change in label assignment\n",
      "0.04991138\n",
      "[[0.02031729 0.90631014 0.01240377 0.0609688 ]\n",
      " [0.04432321 0.65971905 0.02296322 0.2729945 ]\n",
      " [0.02824475 0.03713152 0.01147474 0.923149  ]\n",
      " ...\n",
      " [0.01183201 0.94218886 0.0070522  0.03892701]\n",
      " [0.02074419 0.00457931 0.9633248  0.01135175]\n",
      " [0.02076834 0.00459803 0.9632526  0.01138101]]\n",
      "Iteration 1786, Accuracy 0.36623\n",
      "95.50744%change in label assignment\n",
      "0.05386444\n",
      "[[0.00979181 0.9445609  0.00561162 0.04003565]\n",
      " [0.05402206 0.18650426 0.0253181  0.73415565]\n",
      " [0.04192847 0.03064811 0.01582959 0.9115938 ]\n",
      " ...\n",
      " [0.01807286 0.8877908  0.01009172 0.08404459]\n",
      " [0.0207491  0.00460612 0.9632605  0.0113843 ]\n",
      " [0.02082165 0.00463694 0.9630998  0.01144165]]\n",
      "Iteration 1787, Accuracy 0.36697\n",
      "94.52055%change in label assignment\n",
      "0.055063523\n",
      "[[0.02689899 0.8797202  0.01664945 0.07673137]\n",
      " [0.03300122 0.7709976  0.01750591 0.17849527]\n",
      " [0.03036089 0.04489791 0.01253238 0.91220886]\n",
      " ...\n",
      " [0.01726757 0.91898316 0.01046222 0.05328703]\n",
      " [0.02071382 0.00458501 0.9633465  0.01135467]\n",
      " [0.02075224 0.00460744 0.963248   0.01139239]]\n",
      "Iteration 1788, Accuracy 0.36338\n",
      "95.80203%change in label assignment\n",
      "0.06039671\n",
      "[[0.00853226 0.95289516 0.0049458  0.03362683]\n",
      " [0.05735473 0.23836131 0.02748534 0.6767986 ]\n",
      " [0.04110469 0.03045774 0.01561861 0.9128189 ]\n",
      " ...\n",
      " [0.01437206 0.91375685 0.00813053 0.06374057]\n",
      " [0.02066052 0.00458083 0.9634259  0.0113328 ]\n",
      " [0.02070717 0.00460551 0.96331185 0.01137548]]\n",
      "Iteration 1789, Accuracy 0.36628\n",
      "97.11298%change in label assignment\n",
      "0.054234073\n",
      "[[0.02441018 0.8897055  0.01500079 0.07088352]\n",
      " [0.03903118 0.71567917 0.02038754 0.2249021 ]\n",
      " [0.02896257 0.03935815 0.01176324 0.91991603]\n",
      " ...\n",
      " [0.01532231 0.9272257  0.00921366 0.04823839]\n",
      " [0.02065375 0.00456482 0.9634653  0.01131609]\n",
      " [0.0206674  0.00458114 0.96341205 0.01133948]]\n",
      "Iteration 1790, Accuracy 0.36328\n",
      "97.75126%change in label assignment\n",
      "0.051990747\n",
      "[[0.00890036 0.9546     0.00529172 0.03120789]\n",
      " [0.05918814 0.39149106 0.02946729 0.5198535 ]\n",
      " [0.03144053 0.02791467 0.01227257 0.9283722 ]\n",
      " ...\n",
      " [0.00808006 0.956017   0.00470072 0.03120226]\n",
      " [0.02059641 0.00455132 0.9635677  0.01128467]\n",
      " [0.02059736 0.00456494 0.96353656 0.0113012 ]]\n",
      "Iteration 1791, Accuracy 0.36353\n",
      "97.61379%change in label assignment\n",
      "0.047097694\n",
      "[[0.01383927 0.9334541  0.00832799 0.04437863]\n",
      " [0.05127862 0.5709095  0.02618215 0.35162976]\n",
      " [0.02731322 0.02954691 0.01081741 0.9323225 ]\n",
      " ...\n",
      " [0.00894955 0.9545986  0.00527698 0.03117482]\n",
      " [0.02059478 0.00454743 0.9635765  0.01128125]\n",
      " [0.02058712 0.00455907 0.96356076 0.01129298]]\n",
      "Iteration 1792, Accuracy 0.36343\n",
      "98.08023%change in label assignment\n",
      "0.047178477\n",
      "[[0.00788247 0.9586595  0.00462954 0.02882843]\n",
      " [0.05972963 0.351408   0.02940153 0.5594608 ]\n",
      " [0.0340961  0.02839442 0.01313682 0.9243727 ]\n",
      " ...\n",
      " [0.00974904 0.94479424 0.00558731 0.03986937]\n",
      " [0.02057722 0.00457655 0.96353245 0.01131373]\n",
      " [0.02063108 0.00460339 0.9634045  0.01136102]]\n",
      "Iteration 1793, Accuracy 0.35921\n",
      "95.91987%change in label assignment\n",
      "0.04750333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01911734 0.9112004  0.01166006 0.05802217]\n",
      " [0.03915978 0.7125861  0.02058015 0.22767389]\n",
      " [0.02733916 0.0336592  0.01103513 0.9279665 ]\n",
      " ...\n",
      " [0.01167592 0.9428227  0.00696985 0.0385315 ]\n",
      " [0.02054887 0.00455932 0.9636026  0.01128916]\n",
      " [0.02057542 0.00457967 0.963524   0.01132096]]\n",
      "Iteration 1794, Accuracy 0.3679\n",
      "96.94113%change in label assignment\n",
      "0.04712887\n",
      "[[0.00881084 0.95513713 0.0052111  0.03084096]\n",
      " [0.05708178 0.4642936  0.02870652 0.44991812]\n",
      " [0.03024179 0.02774701 0.01177092 0.9302403 ]\n",
      " ...\n",
      " [0.00804988 0.95617485 0.00465436 0.0311209 ]\n",
      " [0.02050801 0.00455473 0.9636654  0.01127178]\n",
      " [0.02053697 0.00457585 0.96358204 0.01130515]]\n",
      "Iteration 1795, Accuracy 0.37055\n",
      "98.37482%change in label assignment\n",
      "0.04624895\n",
      "[[0.01036164 0.9485131  0.00615865 0.03496663]\n",
      " [0.05615198 0.48852617 0.02821494 0.42710692]\n",
      " [0.02816384 0.02825995 0.01104047 0.9325357 ]\n",
      " ...\n",
      " [0.00776519 0.9591111  0.00452088 0.02860289]\n",
      " [0.02048803 0.0045421  0.9637179  0.01125203]\n",
      " [0.02049499 0.00455783 0.9636745  0.01127272]]\n",
      "Iteration 1796, Accuracy 0.36706\n",
      "98.55158%change in label assignment\n",
      "0.043919794\n",
      "[[0.00969481 0.95136064 0.00575886 0.03318563]\n",
      " [0.05738378 0.4581706  0.02875608 0.45568952]\n",
      " [0.02878842 0.02788344 0.01127852 0.93204963]\n",
      " ...\n",
      " [0.00773673 0.95869374 0.00449718 0.02907237]\n",
      " [0.02045876 0.00454731 0.96374243 0.01125148]\n",
      " [0.02048173 0.00456677 0.9636704  0.01128113]]\n",
      "Iteration 1797, Accuracy 0.36397\n",
      "98.87072%change in label assignment\n",
      "0.045892835\n",
      "[[0.01114138 0.94508517 0.00665418 0.0371193 ]\n",
      " [0.05599869 0.48984087 0.02820007 0.42596033]\n",
      " [0.02773578 0.02844639 0.01094123 0.93287665]\n",
      " ...\n",
      " [0.00786499 0.95888346 0.00459861 0.02865295]\n",
      " [0.02044623 0.00453007 0.9637948  0.01122885]\n",
      " [0.0204336  0.00454012 0.96378917 0.01123717]]\n",
      "Iteration 1798, Accuracy 0.36392\n",
      "98.16861%change in label assignment\n",
      "0.0442868\n",
      "[[0.00990327 0.9504392  0.00589071 0.03376683]\n",
      " [0.05791209 0.44370413 0.02892348 0.4694603 ]\n",
      " [0.02799916 0.02817257 0.01103862 0.9327897 ]\n",
      " ...\n",
      " [0.00776791 0.95908475 0.00453836 0.02860901]\n",
      " [0.02046734 0.00452218 0.9637835  0.011227  ]\n",
      " [0.0204269  0.0045252  0.9638289  0.01121903]]\n",
      "Iteration 1799, Accuracy 0.36044\n",
      "96.05735%change in label assignment\n",
      "[[0.00776798 0.9591253  0.00452842 0.02857825]\n",
      " [0.05951681 0.29204947 0.0285628  0.61987096]\n",
      " [0.03210485 0.02795294 0.01232796 0.9276143 ]\n",
      " ...\n",
      " [0.00942703 0.9469498  0.00537788 0.03824528]\n",
      " [0.02041225 0.00452035 0.96385825 0.01120917]\n",
      " [0.02038436 0.0045262  0.9638814  0.01120812]]\n",
      "Iteration 1800, Accuracy 0.35229\n",
      "96.59253%change in label assignment\n",
      "0.051189616\n",
      "[[0.00866032 0.9557418  0.0051225  0.03047534]\n",
      " [0.05976138 0.34993878 0.02930529 0.56099457]\n",
      " [0.02841802 0.02787333 0.01122584 0.93248284]\n",
      " ...\n",
      " [0.00787874 0.95737493 0.00457168 0.03017465]\n",
      " [0.02037315 0.00452814 0.963885   0.01121379]\n",
      " [0.0203711  0.0045405  0.9638605  0.01122785]]\n",
      "Iteration 1801, Accuracy 0.35283\n",
      "96.03771%change in label assignment\n",
      "0.050491344\n",
      "[[0.00777198 0.9583723  0.00449722 0.02935846]\n",
      " [0.05990292 0.31690672 0.02895065 0.5942397 ]\n",
      " [0.0304664  0.02773039 0.0117905  0.9300127 ]\n",
      " ...\n",
      " [0.01095008 0.93699384 0.00619599 0.04586016]\n",
      " [0.02037647 0.00452145 0.9638928  0.0112093 ]\n",
      " [0.02036292 0.00453084 0.96388966 0.01121654]]\n",
      "Iteration 1802, Accuracy 0.36039\n",
      "96.56307%change in label assignment\n",
      "0.050539836\n",
      "[[0.01124514 0.9446104  0.00673344 0.03741094]\n",
      " [0.04709497 0.62346125 0.02447509 0.30496866]\n",
      " [0.02671078 0.03173641 0.01084395 0.9307088 ]\n",
      " ...\n",
      " [0.00774266 0.9591819  0.00452466 0.02855079]\n",
      " [0.02037153 0.00451995 0.96389675 0.01121177]\n",
      " [0.02035185 0.00452798 0.96390456 0.01121563]]\n",
      "Iteration 1803, Accuracy 0.35204\n",
      "97.46649%change in label assignment\n",
      "0.054337732\n",
      "[[0.01084358 0.9375224  0.00617457 0.04545937]\n",
      " [0.05869124 0.272849   0.02831144 0.6401483 ]\n",
      " [0.03902657 0.02973852 0.01483664 0.9163983 ]\n",
      " ...\n",
      " [0.01855574 0.8841127  0.01033426 0.08699725]\n",
      " [0.02032744 0.00452201 0.96395093 0.01119963]\n",
      " [0.02032724 0.0045346  0.9639237  0.01121447]]\n",
      "Iteration 1804, Accuracy 0.35003\n",
      "98.02622%change in label assignment\n",
      "0.059023187\n",
      "[[0.02630906 0.8820639  0.01626801 0.07535902]\n",
      " [0.02049373 0.8710914  0.01118986 0.09722499]\n",
      " [0.03402135 0.05891117 0.01440717 0.8926603 ]\n",
      " ...\n",
      " [0.01586502 0.9249033  0.00957824 0.04965343]\n",
      " [0.02034122 0.00452738 0.96391064 0.01122082]\n",
      " [0.02034239 0.00454061 0.96388024 0.01123683]]\n",
      "Iteration 1805, Accuracy 0.35366\n",
      "96.41086%change in label assignment\n",
      "0.07034928\n",
      "[[0.01908658 0.88000727 0.01067832 0.09022781]\n",
      " [0.04199998 0.0975413  0.01896231 0.8414964 ]\n",
      " [0.06118473 0.03655417 0.02248847 0.87977266]\n",
      " ...\n",
      " [0.03112962 0.78365797 0.01700015 0.16821223]\n",
      " [0.02029338 0.00453099 0.9639748  0.01120092]\n",
      " [0.02031748 0.00454938 0.9639036  0.01122957]]\n",
      "Iteration 1806, Accuracy 0.35616\n",
      "94.84951%change in label assignment\n",
      "0.07291207\n",
      "[[0.03634819 0.8433183  0.02280582 0.09752771]\n",
      " [0.02899755 0.8057971  0.01543643 0.14976892]\n",
      " [0.03718193 0.07014477 0.0157925  0.87688076]\n",
      " ...\n",
      " [0.02580408 0.8841758  0.01587288 0.07414724]\n",
      " [0.02036254 0.00456154 0.9638064  0.01126953]\n",
      " [0.02041797 0.00458819 0.96367687 0.011317  ]]\n",
      "Iteration 1807, Accuracy 0.34954\n",
      "94.78568%change in label assignment\n",
      "0.07836168\n",
      "[[0.01423135 0.9144808  0.00810368 0.06318419]\n",
      " [0.0339513  0.06156491 0.01496036 0.88952345]\n",
      " [0.05818699 0.03589041 0.02173738 0.88418514]\n",
      " ...\n",
      " [0.02235394 0.8552889  0.0124883  0.10986884]\n",
      " [0.02034932 0.00457767 0.96379334 0.01127967]\n",
      " [0.02043509 0.00461188 0.96360826 0.01134473]]\n",
      "Iteration 1808, Accuracy 0.3567\n",
      "94.42726%change in label assignment\n",
      "0.062478926\n",
      "[[0.02504472 0.8871037  0.01540726 0.07244432]\n",
      " [0.04795519 0.6178033  0.02452255 0.309719  ]\n",
      " [0.02899137 0.04005599 0.01181509 0.9191375 ]\n",
      " ...\n",
      " [0.01654436 0.9220127  0.00997881 0.05146417]\n",
      " [0.02032065 0.00454743 0.96389085 0.01124108]\n",
      " [0.02036043 0.00456985 0.9637908  0.01127905]]\n",
      "Iteration 1809, Accuracy 0.35936\n",
      "94.4469%change in label assignment\n",
      "0.06381061\n",
      "[[0.00971283 0.94490707 0.00561836 0.0397618 ]\n",
      " [0.04945954 0.14711845 0.02307671 0.7803453 ]\n",
      " [0.04564583 0.0320126  0.01737925 0.9049623 ]\n",
      " ...\n",
      " [0.01535143 0.90666306 0.0087089  0.06927659]\n",
      " [0.02026362 0.00454116 0.9639783  0.01121697]\n",
      " [0.02030579 0.00456421 0.9638737  0.01125634]]\n",
      "Iteration 1810, Accuracy 0.35636\n",
      "98.16861%change in label assignment\n",
      "0.057342973\n",
      "[[0.02412249 0.890801   0.01482438 0.07025208]\n",
      " [0.03986895 0.70630085 0.02083161 0.23299862]\n",
      " [0.02905675 0.04086931 0.01192186 0.9181521 ]\n",
      " ...\n",
      " [0.01637742 0.9227149  0.00988361 0.05102402]\n",
      " [0.02030587 0.00455851 0.9638749  0.01126076]\n",
      " [0.02036414 0.00458571 0.9637404  0.01130972]]\n",
      "Iteration 1811, Accuracy 0.35469\n",
      "97.89856%change in label assignment\n",
      "0.0634823\n",
      "[[0.00838705 0.9538013  0.00489311 0.03291855]\n",
      " [0.05900166 0.30052134 0.02903962 0.6114373 ]\n",
      " [0.03932726 0.03000464 0.01518713 0.915481  ]\n",
      " ...\n",
      " [0.01255251 0.9259506  0.00717759 0.05431926]\n",
      " [0.02022478 0.00453022 0.96404314 0.01120191]\n",
      " [0.02025512 0.00455032 0.9639602  0.0112344 ]]\n",
      "Iteration 1812, Accuracy 0.36029\n",
      "95.23248%change in label assignment\n",
      "0.05384039\n",
      "[[0.01520654 0.92767406 0.00915063 0.04796879]\n",
      " [0.0516918  0.56594235 0.0262709  0.35609502]\n",
      " [0.02705564 0.03044276 0.01075445 0.9317471 ]\n",
      " ...\n",
      " [0.00987691 0.9506423  0.00582911 0.03365169]\n",
      " [0.02029052 0.00456411 0.9638782  0.01126716]\n",
      " [0.02036357 0.00459538 0.9637159  0.01132518]]\n",
      "Iteration 1813, Accuracy 0.35469\n",
      "95.85113%change in label assignment\n",
      "0.05372543\n",
      "[[0.00882217 0.9507765  0.00508556 0.03531577]\n",
      " [0.05008078 0.15040359 0.0231702  0.77634543]\n",
      " [0.04027798 0.03011864 0.01534369 0.9142597 ]\n",
      " ...\n",
      " [0.01388722 0.9169441  0.00784577 0.06132287]\n",
      " [0.02030227 0.00457858 0.9638325  0.01128672]\n",
      " [0.02039745 0.00461536 0.9636296  0.01135761]]\n",
      "Iteration 1814, Accuracy 0.36814\n",
      "96.85275%change in label assignment\n",
      "0.053830206\n",
      "[[0.02788083 0.8759041  0.01725138 0.07896373]\n",
      " [0.04015844 0.7039324  0.02094047 0.23496863]\n",
      " [0.03030198 0.0451736  0.01252021 0.91200423]\n",
      " ...\n",
      " [0.01841481 0.91423047 0.01116241 0.05619226]\n",
      " [0.0202611  0.00455894 0.96392107 0.01125891]\n",
      " [0.02032674 0.00458791 0.9637733  0.01131208]]\n",
      "Iteration 1815, Accuracy 0.368\n",
      "97.30937%change in label assignment\n",
      "0.07031965\n",
      "[[0.02322318 0.84876096 0.01292518 0.1150907 ]\n",
      " [0.03464672 0.06426711 0.01531271 0.8857735 ]\n",
      " [0.05922729 0.03606298 0.02207492 0.88263476]\n",
      " ...\n",
      " [0.03555604 0.7434341  0.01933001 0.20167981]\n",
      " [0.02019395 0.00453568 0.964057   0.01121337]\n",
      " [0.0202451  0.00456131 0.96393526 0.01125839]]\n",
      "Iteration 1816, Accuracy 0.3677\n",
      "97.65798%change in label assignment\n",
      "0.083720796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07159559 0.7203706  0.04732285 0.16071096]\n",
      " [0.00856584 0.95527226 0.00488427 0.03127766]\n",
      " [0.05573837 0.18724473 0.02508793 0.73192894]\n",
      " ...\n",
      " [0.05909389 0.7621225  0.03830023 0.1404833 ]\n",
      " [0.02024316 0.00455435 0.9639402  0.01126221]\n",
      " [0.02029813 0.00458074 0.9638119  0.01130929]]\n",
      "Iteration 1817, Accuracy 0.36122\n",
      "95.41906%change in label assignment\n",
      "0.09424971\n",
      "[[0.01237326 0.93955153 0.0074802  0.04059502]\n",
      " [0.05949831 0.31769276 0.02908771 0.5937212 ]\n",
      " [0.03316766 0.02806958 0.01287207 0.92589074]\n",
      " ...\n",
      " [0.00809184 0.9579448  0.00478314 0.02918025]\n",
      " [0.02019227 0.00455784 0.96400464 0.01124522]\n",
      " [0.02026591 0.00458887 0.9638421  0.01130312]]\n",
      "Iteration 1818, Accuracy 0.36898\n",
      "95.46325%change in label assignment\n",
      "0.049855374\n",
      "[[0.00983701 0.950776   0.00584331 0.03354372]\n",
      " [0.05921185 0.28245398 0.02839521 0.62993896]\n",
      " [0.03389793 0.02817351 0.01294859 0.92498   ]\n",
      " ...\n",
      " [0.00771157 0.95873255 0.00447405 0.0290819 ]\n",
      " [0.02019691 0.00455571 0.9640009  0.01124645]\n",
      " [0.02027069 0.00458688 0.9638379  0.01130457]]\n",
      "Iteration 1819, Accuracy 0.36687\n",
      "98.42883%change in label assignment\n",
      "0.047688782\n",
      "[[0.00790447 0.95886266 0.00464207 0.02859082]\n",
      " [0.05150291 0.16099994 0.02377056 0.7637266 ]\n",
      " [0.03581482 0.02865167 0.01367939 0.9218541 ]\n",
      " ...\n",
      " [0.00910576 0.9489487  0.00521964 0.03672593]\n",
      " [0.02026115 0.0045861  0.9638483  0.01130449]\n",
      " [0.0203656  0.00462471 0.96362936 0.0113803 ]]\n",
      "Iteration 1820, Accuracy 0.37006\n",
      "97.19154%change in label assignment\n",
      "0.047850844\n",
      "[[0.00903805 0.9543044  0.00533576 0.03132178]\n",
      " [0.04903227 0.13830425 0.02221541 0.7904481 ]\n",
      " [0.02953792 0.02749959 0.01149023 0.93147224]\n",
      " ...\n",
      " [0.00804177 0.9561961  0.00462741 0.03113471]\n",
      " [0.02022124 0.00457328 0.9639236  0.01128183]\n",
      " [0.02030975 0.00460732 0.9637353  0.01134764]]\n",
      "Iteration 1821, Accuracy 0.37369\n",
      "97.31428%change in label assignment\n",
      "0.04466069\n",
      "[[0.00778353 0.9592802  0.00456821 0.02836812]\n",
      " [0.04354608 0.10460987 0.01952804 0.832316  ]\n",
      " [0.03112687 0.02756078 0.01213625 0.92917615]\n",
      " ...\n",
      " [0.00955783 0.9459664  0.0054716  0.03900415]\n",
      " [0.02015386 0.00455297 0.96405166 0.01124147]\n",
      " [0.02022662 0.00458321 0.9638919  0.01129821]]\n",
      "Iteration 1822, Accuracy 0.37345\n",
      "99.32243%change in label assignment\n",
      "0.04577646\n",
      "[[0.01275322 0.93816155 0.00766561 0.04141958]\n",
      " [0.05924004 0.2805499  0.02837933 0.63183063]\n",
      " [0.02662842 0.02953782 0.01067877 0.93315494]\n",
      " ...\n",
      " [0.00808434 0.9582566  0.00474713 0.02891193]\n",
      " [0.02008842 0.00452612 0.9641914  0.01119405]\n",
      " [0.02013263 0.00454939 0.9640837  0.01123429]]\n",
      "Iteration 1823, Accuracy 0.37301\n",
      "97.30937%change in label assignment\n",
      "0.047768615\n",
      "[[0.0079856  0.95641917 0.00463208 0.03096312]\n",
      " [0.03850541 0.07978826 0.01700768 0.8646987 ]\n",
      " [0.03528393 0.02852117 0.01359754 0.92259735]\n",
      " ...\n",
      " [0.01324862 0.92132896 0.00749401 0.05792836]\n",
      " [0.02005923 0.00452358 0.96423554 0.01118162]\n",
      " [0.02010428 0.00454657 0.9641274  0.01122176]]\n",
      "Iteration 1824, Accuracy 0.37369\n",
      "98.81671%change in label assignment\n",
      "0.050103948\n",
      "[[0.01256218 0.9390054  0.00753848 0.04089392]\n",
      " [0.05637496 0.21407583 0.02635591 0.7031933 ]\n",
      " [0.0265883  0.03043649 0.01069008 0.93228513]\n",
      " ...\n",
      " [0.00808252 0.95827144 0.00474105 0.02890499]\n",
      " [0.02002325 0.0045013  0.9643253  0.01115013]\n",
      " [0.02003676 0.00451614 0.9642756  0.0111715 ]]\n",
      "Iteration 1825, Accuracy 0.37139\n",
      "96.86748%change in label assignment\n",
      "0.050209865\n",
      "[[0.00891371 0.9501545  0.00513759 0.03579419]\n",
      " [0.03465425 0.06370757 0.01509713 0.886541  ]\n",
      " [0.03585424 0.02869257 0.01383116 0.92162204]\n",
      " ...\n",
      " [0.01551062 0.9057194  0.00872891 0.07004107]\n",
      " [0.0199918  0.00449969 0.964369   0.01113947]\n",
      " [0.02001034 0.00451555 0.96431065 0.01116348]]\n",
      "Iteration 1826, Accuracy 0.37163\n",
      "97.99185%change in label assignment\n",
      "0.04897194\n",
      "[[0.01657552 0.9218988  0.01004661 0.05147913]\n",
      " [0.05951603 0.28173015 0.02834399 0.63040984]\n",
      " [0.02718433 0.03461355 0.01107341 0.92712873]\n",
      " ...\n",
      " [0.01025258 0.9490708  0.00608595 0.03459064]\n",
      " [0.01999071 0.0045072  0.964349   0.01115311]\n",
      " [0.02001662 0.00452485 0.96427715 0.01118139]]\n",
      "Iteration 1827, Accuracy 0.36623\n",
      "98.74306%change in label assignment\n",
      "0.050679944\n",
      "[[0.00791671 0.9569088  0.00460596 0.03056853]\n",
      " [0.03850597 0.08011723 0.01706051 0.86431634]\n",
      " [0.03458848 0.02836266 0.0134177  0.92363113]\n",
      " ...\n",
      " [0.01233232 0.9274691  0.0070112  0.05318742]\n",
      " [0.01995465 0.00450726 0.9643961  0.01114194]\n",
      " [0.01998816 0.00452683 0.96431035 0.01117465]]\n",
      "Iteration 1828, Accuracy 0.36908\n",
      "98.40919%change in label assignment\n",
      "0.048521377\n",
      "[[0.01407775 0.93252534 0.00846383 0.04493311]\n",
      " [0.05679813 0.21848792 0.02648433 0.6982297 ]\n",
      " [0.02669076 0.03100083 0.01071251 0.9315959 ]\n",
      " ...\n",
      " [0.00875128 0.95553744 0.00514574 0.03056552]\n",
      " [0.01994972 0.00450669 0.9644006  0.01114298]\n",
      " [0.019981   0.00452557 0.9643192  0.01117424]]\n",
      "Iteration 1829, Accuracy 0.36638\n",
      "98.92964%change in label assignment\n",
      "0.050009076\n",
      "[[0.00781616 0.95762056 0.00454759 0.03001574]\n",
      " [0.03531923 0.06641211 0.01540835 0.88286024]\n",
      " [0.03416791 0.02822858 0.01323722 0.92436624]\n",
      " ...\n",
      " [0.01237189 0.9271965  0.00702338 0.0534082 ]\n",
      " [0.01990252 0.00448129 0.96451837 0.01109782]\n",
      " [0.01990431 0.00449236 0.9644921  0.01111127]]\n",
      "Iteration 1830, Accuracy 0.36927\n",
      "98.1637%change in label assignment\n",
      "0.046615735\n",
      "[[0.01208845 0.94104415 0.00723918 0.03962813]\n",
      " [0.055566   0.20249954 0.02582143 0.71611303]\n",
      " [0.0265596  0.03073048 0.01067993 0.93203   ]\n",
      " ...\n",
      " [0.00836937 0.9571146  0.00492131 0.02959479]\n",
      " [0.0199796  0.00446963 0.96444243 0.01110838]\n",
      " [0.01993599 0.00447    0.96449804 0.01109599]]\n",
      "Iteration 1831, Accuracy 0.36289\n",
      "93.74969%change in label assignment\n",
      "0.048546456\n",
      "[[0.00791666 0.95865756 0.00467617 0.02874962]\n",
      " [0.04731871 0.12994914 0.02168237 0.8010498 ]\n",
      " [0.03050172 0.02751041 0.01202861 0.92995924]\n",
      " ...\n",
      " [0.00850268 0.9528189  0.0049257  0.03375279]\n",
      " [0.01987222 0.00446789 0.96458364 0.01107634]\n",
      " [0.01985088 0.0044741  0.96459776 0.01107729]]\n",
      "Iteration 1832, Accuracy 0.35071\n",
      "98.2668%change in label assignment\n",
      "0.04472421\n",
      "[[0.01093753 0.94601303 0.00652446 0.03652501]\n",
      " [0.05692361 0.22524683 0.02672682 0.6911027 ]\n",
      " [0.02697418 0.02850123 0.01071646 0.9338081 ]\n",
      " ...\n",
      " [0.00805247 0.9583537  0.00472327 0.02887059]\n",
      " [0.01991669 0.00446503 0.96453065 0.01108759]\n",
      " [0.01987385 0.00446629 0.96458346 0.01107644]]\n",
      "Iteration 1833, Accuracy 0.35405\n",
      "95.69892%change in label assignment\n",
      "0.047123924\n",
      "[[0.0076094  0.95944506 0.00444289 0.02850262]\n",
      " [0.04645601 0.12299276 0.0210507  0.8095005 ]\n",
      " [0.03062345 0.02747368 0.01196479 0.929938  ]\n",
      " ...\n",
      " [0.00866297 0.95174533 0.00498886 0.03460286]\n",
      " [0.01996625 0.00446509 0.9644673  0.01110142]\n",
      " [0.01989795 0.00446142 0.96456367 0.01107702]]\n",
      "Iteration 1834, Accuracy 0.34846\n",
      "95.3896%change in label assignment\n",
      "0.04641776\n",
      "[[0.00843843 0.9567455  0.00498736 0.02982879]\n",
      " [0.05303268 0.17751008 0.02464521 0.7448121 ]\n",
      " [0.02968983 0.02739027 0.01164664 0.9312733 ]\n",
      " ...\n",
      " [0.00775152 0.95803535 0.00449868 0.02971452]\n",
      " [0.01979703 0.00446769 0.96467716 0.01105808]\n",
      " [0.01978895 0.00447772 0.964666   0.01106736]]\n",
      "Iteration 1835, Accuracy 0.33358\n",
      "93.406%change in label assignment\n",
      "0.04636684\n",
      "[[0.00816729 0.9578996  0.00479789 0.02913527]\n",
      " [0.05075797 0.15401556 0.0232049  0.7720216 ]\n",
      " [0.02974495 0.02743362 0.01158227 0.9312391 ]\n",
      " ...\n",
      " [0.00796414 0.9565488  0.00459109 0.03089593]\n",
      " [0.01978578 0.00446538 0.964698   0.01105086]\n",
      " [0.01977682 0.00447467 0.96468955 0.011059  ]]\n",
      "Iteration 1836, Accuracy 0.35022\n",
      "97.81509%change in label assignment\n",
      "0.04461576\n",
      "[[0.00987099 0.9505876  0.00588072 0.03366067]\n",
      " [0.05490943 0.1994499  0.02569314 0.7199476 ]\n",
      " [0.02758937 0.02769418 0.01094497 0.93377143]\n",
      " ...\n",
      " [0.00762725 0.9597028  0.00445942 0.02821052]\n",
      " [0.01976022 0.00446576 0.9647275  0.0110465 ]\n",
      " [0.01975475 0.00447569 0.96471316 0.01105641]]\n",
      "Iteration 1837, Accuracy 0.35391\n",
      "99.38135%change in label assignment\n",
      "0.04633503\n",
      "[[0.00862305 0.95590615 0.00511396 0.03035689]\n",
      " [0.05182592 0.1671281  0.02406965 0.75697637]\n",
      " [0.03141395 0.0276242  0.01227594 0.9286859 ]\n",
      " ...\n",
      " [0.00801484 0.95605505 0.0046445  0.03128562]\n",
      " [0.01973647 0.00448277 0.9647206  0.01106016]\n",
      " [0.01976155 0.00450008 0.9646507  0.01108767]]\n",
      "Iteration 1838, Accuracy 0.35489\n",
      "95.75784%change in label assignment\n",
      "0.044661053\n",
      "[[0.00928233 0.9532002  0.00548378 0.03203369]\n",
      " [0.05086891 0.15436497 0.02317593 0.77159023]\n",
      " [0.02978692 0.02748798 0.01154982 0.9311753 ]\n",
      " ...\n",
      " [0.00783245 0.9575562  0.00451349 0.03009783]\n",
      " [0.01972415 0.00447821 0.96474826 0.01104937]\n",
      " [0.01974592 0.00449472 0.96468437 0.01107496]]\n",
      "Iteration 1839, Accuracy 0.36049\n",
      "97.64325%change in label assignment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01086674 0.94620436 0.00650801 0.03642083]\n",
      " [0.05418822 0.19151334 0.02530881 0.7289897 ]\n",
      " [0.02834538 0.02744646 0.01119198 0.9330162 ]\n",
      " ...\n",
      " [0.00759423 0.9596648  0.00443681 0.02830408]\n",
      " [0.01969294 0.00446714 0.9648104  0.01102959]\n",
      " [0.01970142 0.00448028 0.96477103 0.01104732]]\n",
      "Iteration 1840, Accuracy 0.36279\n",
      "98.57613%change in label assignment\n",
      "0.05117341\n",
      "[[0.00877151 0.9509853  0.00503482 0.03520835]\n",
      " [0.0300485  0.04702196 0.01272924 0.9102003 ]\n",
      " [0.03765623 0.02923019 0.01436954 0.9187441 ]\n",
      " ...\n",
      " [0.01436232 0.91352874 0.00806961 0.06403933]\n",
      " [0.01969353 0.00447772 0.96478456 0.01104418]\n",
      " [0.01972069 0.0044955  0.96471095 0.01107283]]\n",
      "Iteration 1841, Accuracy 0.36201\n",
      "97.1228%change in label assignment\n",
      "0.050305422\n",
      "[[0.01318737 0.9362698  0.00791369 0.04262908]\n",
      " [0.05133135 0.15813246 0.02341589 0.7671203 ]\n",
      " [0.02646738 0.03164719 0.01069639 0.93118906]\n",
      " ...\n",
      " [0.00860849 0.9561041  0.00506332 0.03022413]\n",
      " [0.01970391 0.0044849  0.96475023 0.01106099]\n",
      " [0.01973811 0.00450461 0.96466327 0.01109398]]\n",
      "Iteration 1842, Accuracy 0.36623\n",
      "97.67762%change in label assignment\n",
      "0.052130483\n",
      "[[0.00840692 0.9533481  0.00487181 0.03337318]\n",
      " [0.03359762 0.06082727 0.01467679 0.89089835]\n",
      " [0.03343652 0.02812378 0.01308731 0.9253524 ]\n",
      " ...\n",
      " [0.01326046 0.92091864 0.00752736 0.05829357]\n",
      " [0.01965958 0.00447353 0.96483004 0.01103684]\n",
      " [0.01968511 0.00449103 0.9647592  0.01106469]]\n",
      "Iteration 1843, Accuracy 0.37325\n",
      "98.37973%change in label assignment\n",
      "0.05084036\n",
      "[[0.01105459 0.94550717 0.00657665 0.03686164]\n",
      " [0.04562308 0.11584052 0.02033005 0.8182063 ]\n",
      " [0.02643497 0.03074736 0.0106304  0.9321873 ]\n",
      " ...\n",
      " [0.0077644  0.9593335  0.00451978 0.02838232]\n",
      " [0.01965996 0.00447218 0.9648293  0.01103852]\n",
      " [0.01968218 0.00448895 0.9647643  0.01106455]]\n",
      "Iteration 1844, Accuracy 0.37463\n",
      "98.34536%change in label assignment\n",
      "0.05023605\n",
      "[[0.00975403 0.9443924  0.00561782 0.04023576]\n",
      " [0.02597046 0.03137201 0.01074886 0.9319087 ]\n",
      " [0.03508241 0.02863801 0.01372211 0.9225575 ]\n",
      " ...\n",
      " [0.01626532 0.89993113 0.00917545 0.07462806]\n",
      " [0.0196271  0.00447538 0.9648638  0.01103369]\n",
      " [0.01965876 0.0044944  0.9647817  0.01106511]]\n",
      "Iteration 1845, Accuracy 0.37615\n",
      "98.07532%change in label assignment\n",
      "0.050667677\n",
      "[[0.01516861 0.9277241  0.0091627  0.04794456]\n",
      " [0.05146033 0.15998788 0.02351144 0.76504034]\n",
      " [0.02699041 0.03521709 0.01106968 0.9267228 ]\n",
      " ...\n",
      " [0.00913258 0.9538327  0.00539316 0.0316416 ]\n",
      " [0.01962989 0.00447122 0.9648648  0.01103405]\n",
      " [0.01965356 0.00448836 0.96479714 0.01106093]]\n",
      "Iteration 1846, Accuracy 0.37998\n",
      "97.09334%change in label assignment\n",
      "0.055073656\n",
      "[[0.01009524 0.94208944 0.00581257 0.04200273]\n",
      " [0.02717208 0.03772093 0.01149328 0.9236137 ]\n",
      " [0.03629021 0.02901666 0.01416795 0.92052513]\n",
      " ...\n",
      " [0.01795963 0.8876439  0.01010222 0.0842943 ]\n",
      " [0.01957938 0.00445329 0.96497    0.01099733]\n",
      " [0.01958449 0.0044654  0.96493727 0.01101284]]\n",
      "Iteration 1847, Accuracy 0.37325\n",
      "96.44523%change in label assignment\n",
      "0.054573424\n",
      "[[0.01652306 0.92197305 0.01000715 0.05149677]\n",
      " [0.05501012 0.19740751 0.02546783 0.7221145 ]\n",
      " [0.02738514 0.03668154 0.01125095 0.9246824 ]\n",
      " ...\n",
      " [0.00983415 0.9507835  0.00582268 0.03355972]\n",
      " [0.01958986 0.00445794 0.964941   0.01101118]\n",
      " [0.01959735 0.00447099 0.9649032  0.01102852]]\n",
      "Iteration 1848, Accuracy 0.36859\n",
      "98.24225%change in label assignment\n",
      "0.056080587\n",
      "[[0.0085883  0.9521397  0.00501076 0.0342613 ]\n",
      " [0.03010965 0.04869695 0.01310183 0.90809155]\n",
      " [0.03602884 0.02911955 0.01422685 0.92062473]\n",
      " ...\n",
      " [0.01419145 0.91432095 0.00809658 0.06339099]\n",
      " [0.01955772 0.00447676 0.96493953 0.01102592]\n",
      " [0.0195972  0.00449788 0.9648428  0.01106208]]\n",
      "Iteration 1849, Accuracy 0.3653\n",
      "95.42397%change in label assignment\n",
      "0.05177677\n",
      "[[0.01330327 0.93576163 0.00793952 0.04299553]\n",
      " [0.04914968 0.13830625 0.02202793 0.7905162 ]\n",
      " [0.0267751  0.03250141 0.01077441 0.9299491 ]\n",
      " ...\n",
      " [0.00874153 0.95543325 0.00511148 0.03071371]\n",
      " [0.0195877  0.00448206 0.96488863 0.01104154]\n",
      " [0.01963347 0.00450522 0.9647793  0.01108195]]\n",
      "Iteration 1850, Accuracy 0.36215\n",
      "97.97221%change in label assignment\n",
      "0.052415084\n",
      "[[0.00786363 0.957111   0.00459923 0.03042609]\n",
      " [0.03854143 0.08241645 0.01726585 0.8617763 ]\n",
      " [0.03252818 0.02798399 0.01285811 0.9266298 ]\n",
      " ...\n",
      " [0.01201542 0.92921674 0.00687113 0.05189666]\n",
      " [0.01953162 0.00446141 0.9650069  0.01100009]\n",
      " [0.01955765 0.00447953 0.96493405 0.01102876]]\n",
      "Iteration 1851, Accuracy 0.36594\n",
      "96.5778%change in label assignment\n",
      "0.05083653\n",
      "[[0.01135771 0.94413924 0.00676534 0.03773776]\n",
      " [0.05179381 0.16339958 0.02367399 0.76113266]\n",
      " [0.02635532 0.02981665 0.01056581 0.9332623 ]\n",
      " ...\n",
      " [0.00772461 0.95939296 0.00449248 0.02838992]\n",
      " [0.01954214 0.00446414 0.9649861  0.01100761]\n",
      " [0.01957023 0.00448258 0.96490985 0.01103727]]\n",
      "Iteration 1852, Accuracy 0.35783\n",
      "99.3077%change in label assignment\n",
      "0.050511293\n",
      "[[0.01002899 0.9424705  0.00576436 0.04173614]\n",
      " [0.02719924 0.03792498 0.01147489 0.9234009 ]\n",
      " [0.03607749 0.02891252 0.01405645 0.9209536 ]\n",
      " ...\n",
      " [0.01790721 0.88795257 0.01005369 0.08408654]\n",
      " [0.0194993  0.00445184 0.96506804 0.01098081]\n",
      " [0.01951723 0.00446769 0.9650106  0.01100444]]\n",
      "Iteration 1853, Accuracy 0.35837\n",
      "98.30608%change in label assignment\n",
      "0.05290228\n",
      "[[0.01480567 0.92925745 0.00892162 0.04701522]\n",
      " [0.05581744 0.2089163  0.0259383  0.709328  ]\n",
      " [0.02738547 0.03684515 0.01125268 0.92451674]\n",
      " ...\n",
      " [0.00902762 0.9542654  0.00531991 0.03138701]\n",
      " [0.01950981 0.00444709 0.9650609  0.0109823 ]\n",
      " [0.019517   0.00446086 0.9650218  0.01100033]]\n",
      "Iteration 1854, Accuracy 0.35612\n",
      "96.94113%change in label assignment\n",
      "0.05598685\n",
      "[[0.01435142 0.9131446  0.00816608 0.06433798]\n",
      " [0.02610392 0.03355521 0.01096795 0.9293729 ]\n",
      " [0.04185181 0.03086574 0.01621182 0.9110706 ]\n",
      " ...\n",
      " [0.02417297 0.8401465  0.01345506 0.12222547]\n",
      " [0.01946578 0.00444466 0.9651241  0.0109655 ]\n",
      " [0.01947977 0.00446007 0.96507275 0.01098741]]\n",
      "Iteration 1855, Accuracy 0.35081\n",
      "97.65307%change in label assignment\n",
      "0.0634654\n",
      "[[0.05164541 0.7874331  0.03316445 0.12775707]\n",
      " [0.04262681 0.68098354 0.0218735  0.2545161 ]\n",
      " [0.04526261 0.10979552 0.01974355 0.8251983 ]\n",
      " ...\n",
      " [0.03923026 0.8323856  0.02467887 0.10370521]\n",
      " [0.01948693 0.00445294 0.9650729  0.01098727]\n",
      " [0.01950045 0.00446812 0.9650227  0.0110088 ]]\n",
      "Iteration 1856, Accuracy 0.35297\n",
      "97.81509%change in label assignment\n",
      "0.091941915\n",
      "[[0.0078835  0.95696265 0.00461068 0.03054318]\n",
      " [0.0313926  0.05335791 0.01364752 0.901602  ]\n",
      " [0.0362729  0.02901793 0.01415285 0.92055637]\n",
      " ...\n",
      " [0.0120419  0.9289912  0.00688668 0.05208021]\n",
      " [0.01941935 0.00444364 0.9651781  0.01095891]\n",
      " [0.01943355 0.0044588  0.96512705 0.01098059]]\n",
      "Iteration 1857, Accuracy 0.35695\n",
      "96.53852%change in label assignment\n",
      "0.050315976\n",
      "[[0.00900438 0.9543167  0.00529474 0.03138413]\n",
      " [0.03469971 0.06289532 0.01476215 0.8876428 ]\n",
      " [0.02762107 0.02793721 0.01083195 0.9336097 ]\n",
      " ...\n",
      " [0.00773789 0.9584991  0.00446338 0.02929963]\n",
      " [0.01947991 0.00446981 0.96504664 0.0110036 ]\n",
      " [0.01952941 0.00449463 0.96492904 0.01104699]]\n",
      "Iteration 1858, Accuracy 0.36103\n",
      "96.86748%change in label assignment\n",
      "0.04688393\n",
      "[[0.0076217  0.9597277  0.00447927 0.02817134]\n",
      " [0.02910435 0.04462181 0.01234303 0.9139308 ]\n",
      " [0.02978525 0.02730389 0.01175262 0.9311582 ]\n",
      " ...\n",
      " [0.00933639 0.9470171  0.0053644  0.03828212]\n",
      " [0.01943667 0.00445869 0.96512425 0.01098037]\n",
      " [0.01947627 0.00448105 0.9650247  0.01101796]]\n",
      "Iteration 1859, Accuracy 0.36702\n",
      "99.38626%change in label assignment\n",
      "0.04552804\n",
      "[[0.00837824 0.9569821  0.00494016 0.02969957]\n",
      " [0.02864843 0.0422576  0.0119732  0.91712075]\n",
      " [0.02803076 0.02734157 0.01105867 0.93356895]\n",
      " ...\n",
      " [0.00806085 0.9556286  0.00464346 0.03166712]\n",
      " [0.01949112 0.00449019 0.9649848  0.01103385]\n",
      " [0.01956418 0.00452064 0.9648244  0.01109078]]\n",
      "Iteration 1860, Accuracy 0.36544\n",
      "96.19973%change in label assignment\n",
      "0.04527195\n",
      "[[0.00823478 0.9543715  0.00474503 0.03264868]\n",
      " [0.02608575 0.02933487 0.0105066  0.9340728 ]\n",
      " [0.02998671 0.02730491 0.01175636 0.93095195]\n",
      " ...\n",
      " [0.01105944 0.9356441  0.00628295 0.0470135 ]\n",
      " [0.01942271 0.00446197 0.9651347  0.01098057]\n",
      " [0.01947182 0.00448694 0.9650172  0.011024  ]]\n",
      "Iteration 1861, Accuracy 0.37634\n",
      "97.91329%change in label assignment\n",
      "0.04586763\n",
      "[[0.00881871 0.9550727  0.00523255 0.03087609]\n",
      " [0.03080268 0.05031423 0.01310779 0.90577525]\n",
      " [0.02593537 0.02907339 0.0105092  0.93448204]\n",
      " ...\n",
      " [0.00756153 0.9599934  0.00443007 0.02801507]\n",
      " [0.01939193 0.00445305 0.9651906  0.0109644 ]\n",
      " [0.01943106 0.00447578 0.9650909  0.01100227]]\n",
      "Iteration 1862, Accuracy 0.37443\n",
      "98.42883%change in label assignment\n",
      "0.04643788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00821299 0.9544315  0.00474902 0.03260655]\n",
      " [0.02577895 0.03034028 0.01052423 0.9333566 ]\n",
      " [0.03015454 0.02733742 0.01189408 0.93061393]\n",
      " ...\n",
      " [0.01068973 0.93798125 0.00610274 0.04522629]\n",
      " [0.0193569  0.00444441 0.96525425 0.01094441]\n",
      " [0.01938881 0.00446519 0.9651681  0.01097784]]\n",
      "Iteration 1863, Accuracy 0.3705\n",
      "99.39608%change in label assignment\n",
      "0.045998827\n",
      "[[0.0106512  0.94708127 0.00637337 0.03589414]\n",
      " [0.03311143 0.05876602 0.01422774 0.89389485]\n",
      " [0.0258438  0.02958738 0.01048962 0.93407923]\n",
      " ...\n",
      " [0.00772845 0.95952576 0.00453873 0.02820705]\n",
      " [0.01935336 0.00444864 0.96524775 0.01095031]\n",
      " [0.01939066 0.00447063 0.96515197 0.01098675]]\n",
      "Iteration 1864, Accuracy 0.36883\n",
      "98.64487%change in label assignment\n",
      "0.047587726\n",
      "[[0.00753428 0.95945406 0.0044045  0.02860718]\n",
      " [0.02806088 0.04125652 0.01183732 0.91884524]\n",
      " [0.02921945 0.02722424 0.01157561 0.9319808 ]\n",
      " ...\n",
      " [0.00932856 0.9469706  0.00536264 0.03833824]\n",
      " [0.01931935 0.0044179  0.9653587  0.01090411]\n",
      " [0.01931025 0.00442802 0.9653487  0.01091294]]\n",
      "Iteration 1865, Accuracy 0.36736\n",
      "95.75784%change in label assignment\n",
      "0.04630898\n",
      "[[0.00887506 0.95484245 0.00525609 0.03102636]\n",
      " [0.03572003 0.06888121 0.01547969 0.87991905]\n",
      " [0.02607802 0.02877235 0.01049856 0.934651  ]\n",
      " ...\n",
      " [0.00759677 0.9599349  0.00444503 0.02802341]\n",
      " [0.01931103 0.00441677 0.9653706  0.0109016 ]\n",
      " [0.01929981 0.00442651 0.96536434 0.01090935]]\n",
      "Iteration 1866, Accuracy 0.35754\n",
      "99.14077%change in label assignment\n",
      "0.045611292\n",
      "[[0.00751674 0.9597015  0.00440252 0.02837931]\n",
      " [0.02985903 0.04784254 0.01276779 0.90953064]\n",
      " [0.02900259 0.02721809 0.01152609 0.93225324]\n",
      " ...\n",
      " [0.00879891 0.9504509  0.00507634 0.0356739 ]\n",
      " [0.01927303 0.00441721 0.965417   0.01089268]\n",
      " [0.01927109 0.00442921 0.96539396 0.0109058 ]]\n",
      "Iteration 1867, Accuracy 0.3567\n",
      "99.59739%change in label assignment\n",
      "0.046691813\n",
      "[[0.01096724 0.94562304 0.00658484 0.03682484]\n",
      " [0.04321823 0.10630602 0.01942442 0.83105135]\n",
      " [0.02622621 0.02805578 0.01058236 0.93513566]\n",
      " ...\n",
      " [0.00770913 0.9595439  0.00453636 0.02821061]\n",
      " [0.01926085 0.0044251  0.9654141  0.01089995]\n",
      " [0.01927631 0.0044422  0.9653574  0.01092415]]\n",
      "Iteration 1868, Accuracy 0.35739\n",
      "94.82987%change in label assignment\n",
      "0.04627305\n",
      "[[0.00751773 0.9600684  0.00439992 0.02801397]\n",
      " [0.03105848 0.0518379  0.01328711 0.9038166 ]\n",
      " [0.03159371 0.02759677 0.01233896 0.9284706 ]\n",
      " ...\n",
      " [0.00938729 0.9465899  0.00537675 0.03864606]\n",
      " [0.01924954 0.00443224 0.9654151  0.0109031 ]\n",
      " [0.01928202 0.00445339 0.9653276  0.01093706]]\n",
      "Iteration 1869, Accuracy 0.35327\n",
      "97.85928%change in label assignment\n",
      "0.04686285\n",
      "[[0.01206721 0.9408994  0.00724674 0.0397867 ]\n",
      " [0.04573701 0.12030865 0.02058271 0.81337166]\n",
      " [0.02592768 0.02926643 0.01046161 0.9343443 ]\n",
      " ...\n",
      " [0.00813546 0.9579661  0.0047909  0.02910754]\n",
      " [0.01923527 0.00441717 0.9654649  0.01088274]\n",
      " [0.01924309 0.00443216 0.9654226  0.0109022 ]]\n",
      "Iteration 1870, Accuracy 0.35842\n",
      "98.50739%change in label assignment\n",
      "0.04522724\n",
      "[[0.00770805 0.959499   0.00454382 0.02824914]\n",
      " [0.03226915 0.05682376 0.0139716  0.8969355 ]\n",
      " [0.03074358 0.02745153 0.01211392 0.92969096]\n",
      " ...\n",
      " [0.00878314 0.9505086  0.00506372 0.03564451]\n",
      " [0.01919794 0.00441738 0.96551186 0.01087288]\n",
      " [0.01921142 0.00443295 0.96546096 0.0108947 ]]\n",
      "Iteration 1871, Accuracy 0.35764\n",
      "97.83473%change in label assignment\n",
      "0.045711115\n",
      "[[0.00963024 0.9515125  0.00573344 0.03312381]\n",
      " [0.04163793 0.09735934 0.01853525 0.8424674 ]\n",
      " [0.02731843 0.02738015 0.01086243 0.93443894]\n",
      " ...\n",
      " [0.00748668 0.95989454 0.00436265 0.02825617]\n",
      " [0.01921091 0.00443623 0.965455   0.01089784]\n",
      " [0.01925283 0.00445914 0.96535146 0.01093664]]\n",
      "Iteration 1872, Accuracy 0.36181\n",
      "97.64325%change in label assignment\n",
      "0.042842627\n",
      "[[0.00869627 0.95552504 0.00516431 0.03061439]\n",
      " [0.03861171 0.08299655 0.01708527 0.8613065 ]\n",
      " [0.02842806 0.02717593 0.011275   0.93312097]\n",
      " ...\n",
      " [0.00772782 0.957707   0.00448632 0.03007889]\n",
      " [0.01918913 0.00443358 0.9654877  0.01088948]\n",
      " [0.01923195 0.00445698 0.965382   0.0109291 ]]\n",
      "Iteration 1873, Accuracy 0.36088\n",
      "98.38464%change in label assignment\n",
      "0.043504987\n",
      "[[0.01149583 0.943292   0.00691397 0.0382981 ]\n",
      " [0.04913369 0.1469386  0.0225588  0.7813689 ]\n",
      " [0.02632664 0.02787112 0.0105868  0.9352154 ]\n",
      " ...\n",
      " [0.00781126 0.95918626 0.0046022  0.02840028]\n",
      " [0.01915821 0.00442049 0.96555465 0.01086665]\n",
      " [0.01918658 0.00444053 0.96547484 0.01089805]]\n",
      "Iteration 1874, Accuracy 0.35906\n",
      "97.57451%change in label assignment\n",
      "0.043263953\n",
      "[[0.00875678 0.95523155 0.00520836 0.03080332]\n",
      " [0.04386437 0.11126924 0.01984335 0.82502306]\n",
      " [0.02988609 0.02728777 0.01179021 0.93103594]\n",
      " ...\n",
      " [0.00775719 0.9574633  0.00450737 0.03027214]\n",
      " [0.01919773 0.00446126 0.9654091  0.0109319 ]\n",
      " [0.01926443 0.00449436 0.9652443  0.01099692]]\n",
      "Iteration 1875, Accuracy 0.35975\n",
      "96.88712%change in label assignment\n",
      "0.043542366\n",
      "[[0.00993594 0.95018923 0.00591776 0.03395702]\n",
      " [0.04644804 0.12605035 0.02100402 0.8064976 ]\n",
      " [0.02790577 0.02725327 0.01101996 0.93382096]\n",
      " ...\n",
      " [0.00749006 0.9597788  0.0043554  0.02837583]\n",
      " [0.01916617 0.0044503  0.965474   0.01090946]\n",
      " [0.01922417 0.00448121 0.9653253  0.01096937]]\n",
      "Iteration 1876, Accuracy 0.36829\n",
      "98.73815%change in label assignment\n",
      "0.04112649\n",
      "[[0.01069003 0.94683087 0.00640605 0.03607309]\n",
      " [0.04950171 0.15021394 0.02275908 0.77752525]\n",
      " [0.02734954 0.02729228 0.01088682 0.9344713 ]\n",
      " ...\n",
      " [0.00747511 0.9601361  0.00437092 0.02801782]\n",
      " [0.01915085 0.00445608 0.96547425 0.01091884]\n",
      " [0.01921782 0.00448934 0.9653091  0.01098373]]\n",
      "Iteration 1877, Accuracy 0.36834\n",
      "99.02784%change in label assignment\n",
      "0.043791622\n",
      "[[0.01135824 0.9439064  0.00682102 0.03791434]\n",
      " [0.05309539 0.18489513 0.02473622 0.7372733 ]\n",
      " [0.02658281 0.02765751 0.01062825 0.9351315 ]\n",
      " ...\n",
      " [0.00770818 0.959584   0.00453095 0.02817686]\n",
      " [0.01906952 0.0044076  0.9656905  0.01083231]\n",
      " [0.0190833  0.00442741 0.96562326 0.01086606]]\n",
      "Iteration 1878, Accuracy 0.37045\n",
      "97.13262%change in label assignment\n",
      "0.042116676\n",
      "[[0.00852937 0.9562403  0.00505744 0.03017288]\n",
      " [0.04565295 0.12198051 0.02069554 0.811671  ]\n",
      " [0.02902745 0.02717031 0.01143704 0.9323652 ]\n",
      " ...\n",
      " [0.00765171 0.9582348  0.00444445 0.029669  ]\n",
      " [0.01905191 0.00439997 0.96573555 0.01081256]\n",
      " [0.01906103 0.00441796 0.9656798  0.01084119]]\n",
      "Iteration 1879, Accuracy 0.36922\n",
      "98.18825%change in label assignment\n",
      "[[0.00974307 0.951008   0.00580585 0.03344315]\n",
      " [0.05036717 0.15722166 0.02317147 0.7692396 ]\n",
      " [0.0274426  0.02727124 0.01088652 0.93439966]\n",
      " ...\n",
      " [0.00746264 0.96013826 0.00435486 0.02804428]\n",
      " [0.01903525 0.0044161  0.96571153 0.01083713]\n",
      " [0.01906786 0.00444147 0.9656075  0.01088316]]\n",
      "Iteration 1880, Accuracy 0.36765\n",
      "96.5778%change in label assignment\n",
      "0.051100172\n",
      "[[0.01039862 0.93978494 0.00592967 0.0438868 ]\n",
      " [0.02686919 0.03737365 0.01122227 0.9245349 ]\n",
      " [0.03946791 0.02990367 0.01507366 0.91555476]\n",
      " ...\n",
      " [0.01671747 0.89617556 0.00935288 0.07775414]\n",
      " [0.01906295 0.00443304 0.9656392  0.01086481]\n",
      " [0.01911798 0.00446384 0.9654946  0.01092363]]\n",
      "Iteration 1881, Accuracy 0.36903\n",
      "98.53194%change in label assignment\n",
      "0.052205227\n",
      "[[0.01743649 0.91790915 0.01061977 0.05403456]\n",
      " [0.05845487 0.27373993 0.02785108 0.6399541 ]\n",
      " [0.02766405 0.03958202 0.01152257 0.9212313 ]\n",
      " ...\n",
      " [0.01093093 0.94588524 0.00652743 0.03665642]\n",
      " [0.01907346 0.00443513 0.9656169  0.01087455]\n",
      " [0.01912848 0.00446579 0.9654724  0.01093333]]\n",
      "Iteration 1882, Accuracy 0.36908\n",
      "98.44356%change in label assignment\n",
      "0.059366442\n",
      "[[0.01132547 0.933473   0.00647181 0.0487297 ]\n",
      " [0.0334466  0.06219675 0.0146688  0.88968784]\n",
      " [0.04107401 0.03050964 0.01577245 0.9126439 ]\n",
      " ...\n",
      " [0.01973975 0.8737372  0.01102813 0.09549489]\n",
      " [0.01902198 0.00441381 0.9657376  0.01082653]\n",
      " [0.01907148 0.0044394  0.96561855 0.01087054]]\n",
      "Iteration 1883, Accuracy 0.37011\n",
      "96.69564%change in label assignment\n",
      "0.056444634\n",
      "[[0.02264931 0.8963061  0.01395448 0.0670901 ]\n",
      " [0.05390919 0.52068967 0.02723667 0.39816448]\n",
      " [0.02964767 0.04642254 0.01248098 0.91144884]\n",
      " ...\n",
      " [0.01285529 0.9374932  0.00772363 0.04192785]\n",
      " [0.01902623 0.00441739 0.96571517 0.01084119]\n",
      " [0.01907447 0.00444228 0.96559924 0.01088405]]\n",
      "Iteration 1884, Accuracy 0.36137\n",
      "96.64163%change in label assignment\n",
      "0.06624208\n",
      "[[0.01319383 0.92065954 0.00753153 0.05861515]\n",
      " [0.03479391 0.06826105 0.01548985 0.8814552 ]\n",
      " [0.05179567 0.03412703 0.01965899 0.8944183 ]\n",
      " ...\n",
      " [0.02441316 0.8371439  0.01358265 0.12486035]\n",
      " [0.01900462 0.00442045 0.96574444 0.01083046]\n",
      " [0.01906573 0.00444886 0.9656043  0.01088119]]\n",
      "Iteration 1885, Accuracy 0.36132\n",
      "98.22752%change in label assignment\n",
      "0.060578443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02703114 0.87862015 0.0168166  0.07753208]\n",
      " [0.03753536 0.72495586 0.01977728 0.2177315 ]\n",
      " [0.03127423 0.05206022 0.01325147 0.90341413]\n",
      " ...\n",
      " [0.01485977 0.9288243  0.00898898 0.04732691]\n",
      " [0.01898989 0.00440873 0.9657755  0.01082589]\n",
      " [0.01903038 0.00443159 0.965674   0.01086409]]\n",
      "Iteration 1886, Accuracy 0.3598\n",
      "95.30122%change in label assignment\n",
      "0.07083371\n",
      "[[0.0137812  0.91654193 0.00786409 0.06181276]\n",
      " [0.04334659 0.11092885 0.01992531 0.8257993 ]\n",
      " [0.05277012 0.03441279 0.01996282 0.8928543 ]\n",
      " ...\n",
      " [0.02710517 0.81493425 0.01501179 0.14294876]\n",
      " [0.01894116 0.00439297 0.9658821  0.0107837 ]\n",
      " [0.01896859 0.00441219 0.9658054  0.01081374]]\n",
      "Iteration 1887, Accuracy 0.35886\n",
      "95.48289%change in label assignment\n",
      "0.06545658\n",
      "[[0.0404259  0.8272015  0.02573759 0.106635  ]\n",
      " [0.01364096 0.9185591  0.00759959 0.06020036]\n",
      " [0.0375618  0.0767574  0.0163261  0.8693547 ]\n",
      " ...\n",
      " [0.02283189 0.8955401  0.01406708 0.06756086]\n",
      " [0.01895372 0.00439382 0.9658497  0.01080274]\n",
      " [0.01897351 0.00441077 0.9657878  0.01082791]]\n",
      "Iteration 1888, Accuracy 0.35612\n",
      "96.26847%change in label assignment\n",
      "0.079876736\n",
      "[[0.00961632 0.94481564 0.00557942 0.0399886 ]\n",
      " [0.05267765 0.18935774 0.02524464 0.73272   ]\n",
      " [0.05204987 0.03430754 0.01985734 0.8937853 ]\n",
      " ...\n",
      " [0.02089366 0.86455125 0.01174251 0.10281264]\n",
      " [0.01892064 0.00440388 0.96588016 0.01079538]\n",
      " [0.01896325 0.00442637 0.9657766  0.01083375]]\n",
      "Iteration 1889, Accuracy 0.34929\n",
      "95.6842%change in label assignment\n",
      "0.060527474\n",
      "[[0.02461136 0.8883421  0.01520061 0.07184588]\n",
      " [0.04234749 0.6756983  0.02204111 0.25991312]\n",
      " [0.02684488 0.0356123  0.0109803  0.9265625 ]\n",
      " ...\n",
      " [0.01264439 0.9383859  0.00757645 0.04139324]\n",
      " [0.01901612 0.00444491 0.9656579  0.01088103]\n",
      " [0.01909373 0.00447612 0.96549016 0.01094003]]\n",
      "Iteration 1890, Accuracy 0.3458\n",
      "95.2423%change in label assignment\n",
      "0.059861764\n",
      "[[0.00827504 0.95378727 0.00482747 0.03311015]\n",
      " [0.05107943 0.17049685 0.02416834 0.7542553 ]\n",
      " [0.04406174 0.03159094 0.01694541 0.9074019 ]\n",
      " ...\n",
      " [0.01623133 0.8992156  0.00919091 0.07536221]\n",
      " [0.01904078 0.00446245 0.9655911  0.01090571]\n",
      " [0.01914122 0.00449996 0.96538    0.01097879]]\n",
      "Iteration 1891, Accuracy 0.3567\n",
      "97.79054%change in label assignment\n",
      "0.05524497\n",
      "[[0.0196723  0.9085232  0.01203671 0.05976787]\n",
      " [0.05293258 0.5361773  0.02684914 0.38404104]\n",
      " [0.02606846 0.03265889 0.01061862 0.93065405]\n",
      " ...\n",
      " [0.00998444 0.9499991  0.00592983 0.0340866 ]\n",
      " [0.01912342 0.00449154 0.96541303 0.01097201]\n",
      " [0.01924506 0.00453458 0.96516246 0.01105792]]\n",
      "Iteration 1892, Accuracy 0.35886\n",
      "97.80527%change in label assignment\n",
      "0.055291563\n",
      "[[0.00758867 0.9586836  0.00445167 0.02927608]\n",
      " [0.05461483 0.21223798 0.02615316 0.706994  ]\n",
      " [0.03479143 0.02848729 0.01362811 0.9230932 ]\n",
      " ...\n",
      " [0.01361592 0.9177194  0.00774664 0.06091806]\n",
      " [0.01900056 0.0044513  0.9656591  0.01088902]\n",
      " [0.01909429 0.00448712 0.9654604  0.01095817]]\n",
      "Iteration 1893, Accuracy 0.36451\n",
      "97.1719%change in label assignment\n",
      "0.049979094\n",
      "[[0.01308347 0.93646526 0.00788134 0.04256987]\n",
      " [0.05834671 0.40780672 0.02888383 0.5049628 ]\n",
      " [0.02566835 0.02991606 0.01038668 0.93402886]\n",
      " ...\n",
      " [0.00754168 0.960159   0.0044049  0.02789441]\n",
      " [0.01901263 0.00445567 0.9656279  0.01090381]\n",
      " [0.0191082  0.00449184 0.965426   0.01097395]]\n",
      "Iteration 1894, Accuracy 0.36363\n",
      "99.36171%change in label assignment\n",
      "0.04950659\n",
      "[[0.00869743 0.9509366  0.00503217 0.03533382]\n",
      " [0.04796733 0.14122143 0.02225987 0.78855133]\n",
      " [0.03516309 0.02853168 0.01371296 0.9225922 ]\n",
      " ...\n",
      " [0.01753089 0.890024   0.00984701 0.08259805]\n",
      " [0.01901219 0.00446006 0.96561885 0.01090885]\n",
      " [0.01911317 0.00449745 0.9654073  0.01098203]]\n",
      "Iteration 1895, Accuracy 0.36589\n",
      "97.68744%change in label assignment\n",
      "0.05029482\n",
      "[[0.03778734 0.83724326 0.02388881 0.1010806 ]\n",
      " [0.02444409 0.8398099  0.01320579 0.12254024]\n",
      " [0.0338244  0.06097291 0.01442484 0.8907779 ]\n",
      " ...\n",
      " [0.0226533  0.8963856  0.01391444 0.06704662]\n",
      " [0.01896895 0.0044471  0.96569824 0.01088562]\n",
      " [0.01905564 0.00448071 0.9655135  0.01095013]]\n",
      "Iteration 1896, Accuracy 0.36186\n",
      "97.7218%change in label assignment\n",
      "0.07617505\n",
      "[[0.01183072 0.9299868  0.00677021 0.05141233]\n",
      " [0.04502993 0.12129679 0.02081724 0.812856  ]\n",
      " [0.04639296 0.03226763 0.01774787 0.9035916 ]\n",
      " ...\n",
      " [0.02443233 0.83713865 0.01355884 0.12487017]\n",
      " [0.01890739 0.00442844 0.9658203  0.01084383]\n",
      " [0.01898179 0.00445837 0.9656594  0.01090039]]\n",
      "Iteration 1897, Accuracy 0.36702\n",
      "95.74311%change in label assignment\n",
      "0.05906247\n",
      "[[0.02977977 0.86787254 0.01859599 0.08375169]\n",
      " [0.0322127  0.774501   0.01716373 0.17612253]\n",
      " [0.03000845 0.04748746 0.01261613 0.9098879 ]\n",
      " ...\n",
      " [0.01614561 0.92340165 0.00978552 0.05066722]\n",
      " [0.01886191 0.00440693 0.9659192  0.0108119 ]\n",
      " [0.01891333 0.00443091 0.96580106 0.01085469]]\n",
      "Iteration 1898, Accuracy 0.35724\n",
      "95.83149%change in label assignment\n",
      "0.06339524\n",
      "[[0.00753865 0.95937485 0.00444872 0.02863777]\n",
      " [0.05830576 0.32130313 0.02896363 0.5914275 ]\n",
      " [0.03603037 0.02890613 0.01414566 0.92091787]\n",
      " ...\n",
      " [0.01240924 0.92596996 0.00710479 0.05451604]\n",
      " [0.01879917 0.00439141 0.9660374  0.010772  ]\n",
      " [0.01884307 0.00441371 0.9659327  0.01081053]]\n",
      "Iteration 1899, Accuracy 0.35803\n",
      "97.77581%change in label assignment\n",
      "0.048864648\n",
      "[[0.01552176 0.926008   0.00940166 0.04906861]\n",
      " [0.0461299  0.6306537  0.0238983  0.2993181 ]\n",
      " [0.02575897 0.02966968 0.01037554 0.9341959 ]\n",
      " ...\n",
      " [0.00828047 0.9573787  0.00487064 0.02947016]\n",
      " [0.0187922  0.00438424 0.96605796 0.01076562]\n",
      " [0.01882846 0.00440486 0.96596676 0.01079991]]\n",
      "Iteration 1900, Accuracy 0.35636\n",
      "97.98694%change in label assignment\n",
      "0.0495916\n",
      "[[0.00744725 0.9599324  0.00437915 0.02824121]\n",
      " [0.05843725 0.3626818  0.02917414 0.54970676]\n",
      " [0.03305659 0.0278968  0.01299928 0.9260473 ]\n",
      " ...\n",
      " [0.01196987 0.92901146 0.0068356  0.05218311]\n",
      " [0.01876816 0.00438351 0.96608984 0.0107585 ]\n",
      " [0.01880809 0.00440505 0.96599185 0.01079495]]\n",
      "Iteration 1901, Accuracy 0.35287\n",
      "98.81671%change in label assignment\n",
      "0.046989605\n",
      "[[0.01590097 0.92439663 0.00964568 0.05005674]\n",
      " [0.04183198 0.67961395 0.0219077  0.25664634]\n",
      " [0.02569356 0.03106532 0.01043845 0.93280274]\n",
      " ...\n",
      " [0.00849    0.9565171  0.00500502 0.0299879 ]\n",
      " [0.01875146 0.00437349 0.96612936 0.0107457 ]\n",
      " [0.01877913 0.00439203 0.9660539  0.01077499]]\n",
      "Iteration 1902, Accuracy 0.35081\n",
      "97.54996%change in label assignment\n",
      "0.04899059\n",
      "[[0.00745897 0.9600124  0.00439604 0.02813265]\n",
      " [0.05780078 0.4020155  0.02916246 0.5110212 ]\n",
      " [0.03191792 0.02760099 0.01263072 0.92785037]\n",
      " ...\n",
      " [0.011707   0.9307644  0.00669859 0.05083004]\n",
      " [0.01871288 0.00436403 0.96619827 0.01072473]\n",
      " [0.01873355 0.00438084 0.9661357  0.0107499 ]]\n",
      "Iteration 1903, Accuracy 0.34698\n",
      "98.23734%change in label assignment\n",
      "0.047803376\n",
      "[[0.01441634 0.93071234 0.00872166 0.04614967]\n",
      " [0.04601042 0.6308809  0.0239069  0.29920182]\n",
      " [0.0255047  0.02998767 0.01035418 0.93415344]\n",
      " ...\n",
      " [0.007908   0.9589251  0.00464714 0.02851967]\n",
      " [0.01871879 0.00437287 0.9661683  0.01074009]\n",
      " [0.01875066 0.00439263 0.96608466 0.01077205]]\n",
      "Iteration 1904, Accuracy 0.34438\n",
      "99.05239%change in label assignment\n",
      "0.048977576\n",
      "[[0.00747751 0.95938075 0.00437635 0.02876542]\n",
      " [0.05838314 0.31535557 0.02876516 0.5974962 ]\n",
      " [0.03336054 0.02795723 0.01308656 0.92559564]\n",
      " ...\n",
      " [0.01296092 0.92223334 0.00736569 0.05744009]\n",
      " [0.01867975 0.00435857 0.9662511  0.01071068]\n",
      " [0.01869812 0.00437488 0.9661924  0.01073455]]\n",
      "Iteration 1905, Accuracy 0.34492\n",
      "97.54014%change in label assignment\n",
      "0.049719445\n",
      "[[0.02190686 0.8992804  0.01349264 0.06532005]\n",
      " [0.03406971 0.7565448  0.0181507  0.19123474]\n",
      " [0.02643038 0.03551276 0.01092435 0.9271325 ]\n",
      " ...\n",
      " [0.01160361 0.9429197  0.00695084 0.03852588]\n",
      " [0.01868705 0.00436918 0.9662147  0.01072905]\n",
      " [0.01871699 0.00438838 0.96613485 0.01075976]]\n",
      "Iteration 1906, Accuracy 0.34251\n",
      "98.52212%change in label assignment\n",
      "0.052694045\n",
      "[[0.00773989 0.95736724 0.00452217 0.03037074]\n",
      " [0.05753403 0.27826196 0.02817255 0.6360315 ]\n",
      " [0.03753398 0.02932355 0.01462764 0.9185148 ]\n",
      " ...\n",
      " [0.01458771 0.9108409  0.00827189 0.06629951]\n",
      " [0.01865732 0.00436726 0.96625876 0.01071665]\n",
      " [0.0186909  0.00438722 0.9661725  0.01074933]]\n",
      "Iteration 1907, Accuracy 0.34423\n",
      "98.99838%change in label assignment\n",
      "0.050752718\n",
      "[[0.02179589 0.8997261  0.01341811 0.06505986]\n",
      " [0.03728399 0.7260218  0.01971872 0.21697545]\n",
      " [0.02626507 0.03478013 0.01082285 0.928132  ]\n",
      " ...\n",
      " [0.01139355 0.94383186 0.0068178  0.03795678]\n",
      " [0.0186613  0.00436775 0.9662492  0.01072171]\n",
      " [0.01869321 0.0043873  0.9661661  0.01075341]]\n",
      "Iteration 1908, Accuracy 0.3435\n",
      "99.06221%change in label assignment\n",
      "0.05256586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0074922  0.95931524 0.00440119 0.02879138]\n",
      " [0.05788795 0.2949026  0.02849761 0.6187118 ]\n",
      " [0.03613101 0.02888623 0.01414857 0.9208342 ]\n",
      " ...\n",
      " [0.01297842 0.92198753 0.00739955 0.05763461]\n",
      " [0.01862196 0.00436091 0.96631616 0.01070095]\n",
      " [0.01865251 0.00438019 0.9662354  0.01073191]]\n",
      "Iteration 1909, Accuracy 0.34561\n",
      "98.90018%change in label assignment\n",
      "0.050687626\n",
      "[[0.02196221 0.89905536 0.01351516 0.06546733]\n",
      " [0.03582957 0.7401564  0.01899291 0.20502113]\n",
      " [0.02627111 0.0347333  0.01081089 0.92818475]\n",
      " ...\n",
      " [0.01177749 0.94215554 0.00705199 0.03901498]\n",
      " [0.01861431 0.00435237 0.96634036 0.01069294]\n",
      " [0.01863163 0.00436836 0.9662839  0.0107161 ]]\n",
      "Iteration 1910, Accuracy 0.34541\n",
      "97.57451%change in label assignment\n",
      "0.048950687\n",
      "[[0.00764596 0.9598101  0.00452215 0.0280218 ]\n",
      " [0.05841657 0.32867575 0.02885853 0.5840491 ]\n",
      " [0.03162154 0.02747603 0.01247437 0.9284281 ]\n",
      " ...\n",
      " [0.01004183 0.9418718  0.00576267 0.04232368]\n",
      " [0.01857006 0.00434477 0.96641386 0.01067136]\n",
      " [0.01858285 0.00435948 0.966366   0.01069165]]\n",
      "Iteration 1911, Accuracy 0.34909\n",
      "96.83311%change in label assignment\n",
      "0.046009757\n",
      "[[0.01259589 0.938524   0.0075856  0.04129446]\n",
      " [0.05497374 0.49242043 0.0277991  0.42480674]\n",
      " [0.02555628 0.02845572 0.01030198 0.93568605]\n",
      " ...\n",
      " [0.0075427  0.960281   0.00441852 0.0277577 ]\n",
      " [0.0185741  0.00433465 0.96642935 0.0106619 ]\n",
      " [0.01856762 0.00434407 0.9664181  0.01067028]]\n",
      "Iteration 1912, Accuracy 0.35499\n",
      "96.7791%change in label assignment\n",
      "0.044142887\n",
      "[[0.00813404 0.95776504 0.00484751 0.02925342]\n",
      " [0.058286   0.3542523  0.02907824 0.5583834 ]\n",
      " [0.02937488 0.0270307  0.0117553  0.93183905]\n",
      " ...\n",
      " [0.00904742 0.9484268  0.00523424 0.03729159]\n",
      " [0.01853001 0.00433718 0.9664768  0.01065593]\n",
      " [0.01853355 0.00434889 0.96644753 0.01066999]]\n",
      "Iteration 1913, Accuracy 0.3516\n",
      "97.84455%change in label assignment\n",
      "0.043983698\n",
      "[[0.01559161 0.9255568  0.00948519 0.04936644]\n",
      " [0.04837237 0.5980891  0.02503737 0.3285012 ]\n",
      " [0.02528296 0.02901687 0.01026907 0.93543106]\n",
      " ...\n",
      " [0.00818941 0.9577408  0.00483799 0.02923182]\n",
      " [0.01852652 0.00434104 0.9664729  0.01065949]\n",
      " [0.01853729 0.00435487 0.96642965 0.01067817]]\n",
      "Iteration 1914, Accuracy 0.35538\n",
      "98.48284%change in label assignment\n",
      "0.042765155\n",
      "[[0.00921031 0.9532563  0.00550164 0.03203175]\n",
      " [0.05784987 0.4008003  0.02895001 0.51239985]\n",
      " [0.02820067 0.0268269  0.01123371 0.93373865]\n",
      " ...\n",
      " [0.00808214 0.9548988  0.00467764 0.03234142]\n",
      " [0.01849816 0.00434259 0.96650594 0.01065335]\n",
      " [0.01851613 0.00435838 0.96644914 0.01067644]]\n",
      "Iteration 1915, Accuracy 0.35602\n",
      "98.74306%change in label assignment\n",
      "0.042603567\n",
      "[[0.01268869 0.9380317  0.00766705 0.04161256]\n",
      " [0.0530623  0.52448195 0.02714362 0.39531216]\n",
      " [0.02594445 0.02735802 0.01044452 0.9362531 ]\n",
      " ...\n",
      " [0.00737014 0.96073055 0.00431624 0.02758311]\n",
      " [0.01848816 0.00434412 0.96651524 0.01065245]\n",
      " [0.01850992 0.00436095 0.96645117 0.01067789]]\n",
      "Iteration 1916, Accuracy 0.35852\n",
      "98.73815%change in label assignment\n",
      "0.04220512\n",
      "[[0.01126355 0.9442334  0.00678139 0.03772164]\n",
      " [0.05534995 0.4775001  0.02809893 0.43905106]\n",
      " [0.02664564 0.02695685 0.01068237 0.93571514]\n",
      " ...\n",
      " [0.00737035 0.9600936  0.00430095 0.02823517]\n",
      " [0.01846359 0.00433496 0.9665661  0.01063535]\n",
      " [0.01847434 0.00434845 0.9665236  0.01065361]]\n",
      "Iteration 1917, Accuracy 0.3596\n",
      "98.79707%change in label assignment\n",
      "0.0436017\n",
      "[[0.01267534 0.93801785 0.00767571 0.04163107]\n",
      " [0.05306392 0.5223281  0.02721038 0.39739764]\n",
      " [0.02573899 0.02739559 0.01041374 0.9364517 ]\n",
      " ...\n",
      " [0.00747071 0.9604709  0.00439785 0.02766052]\n",
      " [0.01845818 0.00432015 0.96660274 0.01061894]\n",
      " [0.01844371 0.00432721 0.9666069  0.01062212]]\n",
      "Iteration 1918, Accuracy 0.36132\n",
      "97.15717%change in label assignment\n",
      "0.042035848\n",
      "[[0.00883317 0.9548954  0.00526159 0.03100987]\n",
      " [0.05844281 0.346386   0.02884297 0.56632817]\n",
      " [0.0284701  0.02684904 0.01130839 0.9333725 ]\n",
      " ...\n",
      " [0.00803099 0.9552081  0.00464605 0.03211487]\n",
      " [0.01843511 0.00431792 0.9666371  0.01060983]\n",
      " [0.01842131 0.00432509 0.9666403  0.01061334]]\n",
      "Iteration 1919, Accuracy 0.35297\n",
      "98.14897%change in label assignment\n",
      "[[0.01420532 0.9314128  0.00862654 0.04575536]\n",
      " [0.05332342 0.5184797  0.02725628 0.4009406 ]\n",
      " [0.02561749 0.02756973 0.01035715 0.9364556 ]\n",
      " ...\n",
      " [0.00762188 0.9599887  0.00448883 0.02790064]\n",
      " [0.01840128 0.00432588 0.9666622  0.01061063]\n",
      " [0.01840631 0.00433823 0.96662974 0.01062579]]\n",
      "Iteration 1920, Accuracy 0.354\n",
      "96.66127%change in label assignment\n",
      "0.053402167\n",
      "[[0.00867819 0.95081085 0.00501368 0.03549728]\n",
      " [0.05582139 0.23891638 0.02695136 0.67831093]\n",
      " [0.03900661 0.0298329  0.01510338 0.91605705]\n",
      " ...\n",
      " [0.01794008 0.8864935  0.01005482 0.08551167]\n",
      " [0.01839257 0.0043212  0.9666827  0.01060359]\n",
      " [0.01839591 0.00433327 0.9666529  0.0106179 ]]\n",
      "Iteration 1921, Accuracy 0.36201\n",
      "97.82982%change in label assignment\n",
      "0.051904067\n",
      "[[0.02086356 0.90343755 0.01283536 0.06286353]\n",
      " [0.02402641 0.84123015 0.01309329 0.12165019]\n",
      " [0.02650638 0.0368128  0.01102838 0.92565244]\n",
      " ...\n",
      " [0.0092996  0.9529631  0.00552169 0.0322156 ]\n",
      " [0.01840787 0.00431851 0.9666614  0.01061221]\n",
      " [0.01840174 0.00432732 0.9666509  0.01061995]]\n",
      "Iteration 1922, Accuracy 0.35612\n",
      "94.81023%change in label assignment\n",
      "0.062096708\n",
      "[[0.01487383 0.90844643 0.00847367 0.06820609]\n",
      " [0.05386206 0.21023251 0.02609472 0.7098106 ]\n",
      " [0.05219027 0.03442024 0.02001651 0.893373  ]\n",
      " ...\n",
      " [0.03176998 0.7730105  0.01747198 0.1777475 ]\n",
      " [0.0183814  0.00430842 0.9667206  0.01058957]\n",
      " [0.01836569 0.00431479 0.9667279  0.01059156]]\n",
      "Iteration 1923, Accuracy 0.34566\n",
      "96.7791%change in label assignment\n",
      "0.06750597\n",
      "[[0.0343672  0.8500074  0.02159974 0.0940256 ]\n",
      " [0.01830082 0.8855409  0.01004487 0.08611339]\n",
      " [0.03254671 0.05752226 0.01388904 0.896042  ]\n",
      " ...\n",
      " [0.0171959  0.91886497 0.01042806 0.05351106]\n",
      " [0.01838386 0.00432792 0.9666667  0.0106215 ]\n",
      " [0.0183965  0.004342   0.96662056 0.01064099]]\n",
      "Iteration 1924, Accuracy 0.33957\n",
      "95.88059%change in label assignment\n",
      "0.07707396\n",
      "[[0.01612862 0.8993974  0.00916621 0.07530781]\n",
      " [0.03451487 0.06923398 0.01556586 0.88068527]\n",
      " [0.07691899 0.04110073 0.02773765 0.8542427 ]\n",
      " ...\n",
      " [0.03214344 0.76936495 0.01767991 0.1808117 ]\n",
      " [0.01836737 0.00434255 0.9666689  0.01062116]\n",
      " [0.01840744 0.00436364 0.9665719  0.010657  ]]\n",
      "Iteration 1925, Accuracy 0.34639\n",
      "94.56965%change in label assignment\n",
      "0.069889024\n",
      "[[0.03820223 0.8349951  0.02436707 0.10243563]\n",
      " [0.0204758  0.868245   0.01128985 0.09998932]\n",
      " [0.03547459 0.07134467 0.01562118 0.8775595 ]\n",
      " ...\n",
      " [0.02066858 0.904065   0.01275903 0.06250741]\n",
      " [0.01838525 0.00434002 0.96663827 0.01063652]\n",
      " [0.01841735 0.00435941 0.96655524 0.01066801]]\n",
      "Iteration 1926, Accuracy 0.35504\n",
      "96.37649%change in label assignment\n",
      "0.080535725\n",
      "[[0.00903025 0.94842523 0.00527988 0.03726468]\n",
      " [0.05266966 0.19673133 0.02555472 0.72504425]\n",
      " [0.04648344 0.03275101 0.0181956  0.90256995]\n",
      " ...\n",
      " [0.0199719  0.8707182  0.01129271 0.09801719]\n",
      " [0.01838304 0.00435599 0.966616   0.01064495]\n",
      " [0.01843654 0.00438033 0.96649444 0.01068863]]\n",
      "Iteration 1927, Accuracy 0.35317\n",
      "96.25374%change in label assignment\n",
      "0.058483243\n",
      "[[0.02897025 0.8708288  0.01805033 0.08215067]\n",
      " [0.01820404 0.88594234 0.01001599 0.08583762]\n",
      " [0.02824504 0.04248892 0.01181718 0.9174489 ]\n",
      " ...\n",
      " [0.01393636 0.93270415 0.00838478 0.04497465]\n",
      " [0.01841218 0.0043605  0.96656    0.01066733]\n",
      " [0.01846662 0.00438513 0.9664366  0.01071167]]\n",
      "Iteration 1928, Accuracy 0.34939\n",
      "96.40104%change in label assignment\n",
      "0.06303373\n",
      "[[0.00745002 0.95979804 0.00440811 0.02834389]\n",
      " [0.05568938 0.45612976 0.02864018 0.45954067]\n",
      " [0.03415545 0.02838353 0.01358193 0.9238791 ]\n",
      " ...\n",
      " [0.01459289 0.910338   0.00831214 0.06675698]\n",
      " [0.01833544 0.00433267 0.96672267 0.01060922]\n",
      " [0.01836924 0.00435204 0.96663755 0.0106412 ]]\n",
      "Iteration 1929, Accuracy 0.35415\n",
      "95.55163%change in label assignment\n",
      "0.050901186\n",
      "[[0.01397017 0.9325294  0.00843044 0.04506997]\n",
      " [0.04281612 0.6663198  0.02239867 0.26846537]\n",
      " [0.0252797  0.02903462 0.01022943 0.9354563 ]\n",
      " ...\n",
      " [0.0074404  0.96002483 0.00431758 0.02821713]\n",
      " [0.01835535 0.00434076 0.966674   0.01062989]\n",
      " [0.01839561 0.00436159 0.96657723 0.01066553]]\n",
      "Iteration 1930, Accuracy 0.34202\n",
      "99.28315%change in label assignment\n",
      "0.050002493\n",
      "[[0.00749422 0.9589988  0.00440468 0.02910227]\n",
      " [0.0579921  0.35042542 0.02907435 0.5625081 ]\n",
      " [0.03285372 0.02790348 0.01307167 0.9261711 ]\n",
      " ...\n",
      " [0.01651932 0.89663696 0.00934223 0.07750149]\n",
      " [0.0183489  0.00434682 0.9666688  0.01063557]\n",
      " [0.01839778 0.00436974 0.9665562  0.01067624]]\n",
      "Iteration 1931, Accuracy 0.34237\n",
      "98.74306%change in label assignment\n",
      "0.05153854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01937377 0.9096321  0.01185943 0.05913471]\n",
      " [0.02799965 0.80899495 0.01511103 0.14789435]\n",
      " [0.02654093 0.0369615  0.01103844 0.92545915]\n",
      " ...\n",
      " [0.00818065 0.95775735 0.0048117  0.02925024]\n",
      " [0.01831391 0.00432158 0.966762   0.01060251]\n",
      " [0.0183343  0.00433677 0.9667033  0.01062563]]\n",
      "Iteration 1932, Accuracy 0.34089\n",
      "95.11956%change in label assignment\n",
      "0.056568693\n",
      "[[0.00890729 0.9492057  0.00517886 0.03670814]\n",
      " [0.05767677 0.3079966  0.02871261 0.60561395]\n",
      " [0.0379615  0.02960563 0.01495667 0.9174762 ]\n",
      " ...\n",
      " [0.0228042  0.8489964  0.01273976 0.11545963]\n",
      " [0.01827923 0.0043185  0.9668161  0.01058616]\n",
      " [0.01830292 0.00433453 0.9667513  0.01061124]]\n",
      "Iteration 1933, Accuracy 0.33368\n",
      "95.72838%change in label assignment\n",
      "0.05545553\n",
      "[[0.01961749 0.9086575  0.01199731 0.05972769]\n",
      " [0.03841183 0.71393985 0.02025069 0.22739767]\n",
      " [0.02654472 0.03679033 0.0110157  0.9256493 ]\n",
      " ...\n",
      " [0.00833942 0.95708525 0.00490272 0.02967258]\n",
      " [0.01830555 0.00433196 0.9667466  0.01061584]\n",
      " [0.01834039 0.00435092 0.96666104 0.0106477 ]]\n",
      "Iteration 1934, Accuracy 0.33608\n",
      "97.38793%change in label assignment\n",
      "0.05687303\n",
      "[[0.00981632 0.9431604  0.00571055 0.04131269]\n",
      " [0.05401157 0.21463569 0.02636934 0.7049834 ]\n",
      " [0.03958772 0.03031991 0.01570638 0.914386  ]\n",
      " ...\n",
      " [0.02501052 0.8310846  0.01398144 0.12992342]\n",
      " [0.01832774 0.00435785 0.96666175 0.01065257]\n",
      " [0.01838744 0.00438294 0.96653056 0.01069908]]\n",
      "Iteration 1935, Accuracy 0.34242\n",
      "95.85113%change in label assignment\n",
      "0.057990745\n",
      "[[0.05336385 0.78076065 0.03443587 0.13143961]\n",
      " [0.00817178 0.95594543 0.00466596 0.03121681]\n",
      " [0.0403923  0.0893734  0.01759123 0.8526431 ]\n",
      " ...\n",
      " [0.0308993  0.863551   0.01918322 0.08636652]\n",
      " [0.01832815 0.00435674 0.9666597  0.0106553 ]\n",
      " [0.01838284 0.00438057 0.9665378  0.01069883]]\n",
      "Iteration 1936, Accuracy 0.34998\n",
      "97.28973%change in label assignment\n",
      "0.09001014\n",
      "[[0.00782364 0.9565748  0.00459314 0.03100837]\n",
      " [0.05784072 0.3372008  0.02909386 0.5758647 ]\n",
      " [0.03995852 0.03034105 0.01575534 0.913945  ]\n",
      " ...\n",
      " [0.01890259 0.8789488  0.01066848 0.09148018]\n",
      " [0.01827993 0.00434787 0.96674216 0.01062998]\n",
      " [0.01833619 0.00437225 0.9666169  0.0106746 ]]\n",
      "Iteration 1937, Accuracy 0.34865\n",
      "96.83311%change in label assignment\n",
      "0.05403931\n",
      "[[0.02381865 0.8914084  0.01470342 0.07006954]\n",
      " [0.02282305 0.8509416  0.01244243 0.11379292]\n",
      " [0.02611247 0.03504093 0.01078181 0.92806476]\n",
      " ...\n",
      " [0.01000575 0.94986176 0.00594    0.03419245]\n",
      " [0.01830457 0.00435193 0.9666982  0.01064522]\n",
      " [0.01836326 0.0043772  0.9665679  0.0106916 ]]\n",
      "Iteration 1938, Accuracy 0.35199\n",
      "96.3814%change in label assignment\n",
      "0.05437252\n",
      "[[0.00733451 0.96058804 0.00433238 0.02774504]\n",
      " [0.05672788 0.4262134  0.02891983 0.48813888]\n",
      " [0.03407779 0.02821398 0.01349746 0.9242108 ]\n",
      " ...\n",
      " [0.01452821 0.91081774 0.008251   0.06640312]\n",
      " [0.01827549 0.0043491  0.96674365 0.01063175]\n",
      " [0.01833534 0.00437452 0.96661144 0.01067864]]\n",
      "Iteration 1939, Accuracy 0.3514\n",
      "98.19807%change in label assignment\n",
      "0.047963373\n",
      "[[0.0164856  0.92176044 0.01003267 0.05172123]\n",
      " [0.04083974 0.6873625  0.02150117 0.25029665]\n",
      " [0.02504547 0.02941978 0.01019594 0.93533885]\n",
      " ...\n",
      " [0.00755979 0.96024007 0.00442508 0.02777504]\n",
      " [0.01829539 0.00435697 0.96669745 0.01065025]\n",
      " [0.01835977 0.00438349 0.96655697 0.01069982]]\n",
      "Iteration 1940, Accuracy 0.35047\n",
      "98.33554%change in label assignment\n",
      "0.048861027\n",
      "[[0.00730164 0.96089226 0.00430713 0.02749899]\n",
      " [0.0578943  0.31534597 0.02869553 0.5980642 ]\n",
      " [0.03299123 0.02780831 0.01306024 0.9261402 ]\n",
      " ...\n",
      " [0.01461245 0.910296   0.0082778  0.06681374]\n",
      " [0.01838998 0.00440161 0.96647686 0.01073151]\n",
      " [0.01847905 0.00443631 0.96628344 0.01080118]]\n",
      "Iteration 1941, Accuracy 0.35386\n",
      "94.14249%change in label assignment\n",
      "0.04701271\n",
      "[[0.01772481 0.9164971  0.01083634 0.05494175]\n",
      " [0.04569829 0.63116795 0.02382646 0.2993073 ]\n",
      " [0.02493195 0.03007182 0.01021583 0.93478036]\n",
      " ...\n",
      " [0.00775468 0.9595613  0.00456018 0.02812387]\n",
      " [0.01833075 0.00438606 0.9665766  0.01070658]\n",
      " [0.01840734 0.00441869 0.96640193 0.01077203]]\n",
      "Iteration 1942, Accuracy 0.36147\n",
      "98.30608%change in label assignment\n",
      "0.047252677\n",
      "[[0.00753741 0.9603252  0.00446364 0.02767379]\n",
      " [0.0580919  0.3561294  0.02898354 0.5567952 ]\n",
      " [0.02946488 0.02681464 0.01175614 0.93196434]\n",
      " ...\n",
      " [0.01209478 0.92778826 0.00689046 0.05322651]\n",
      " [0.01823352 0.00434666 0.9667952  0.0106247 ]\n",
      " [0.01828826 0.0043721  0.9666666  0.01067308]]\n",
      "Iteration 1943, Accuracy 0.36652\n",
      "97.28482%change in label assignment\n",
      "0.045006678\n",
      "[[0.0163436  0.922262   0.009991   0.05140348]\n",
      " [0.04277313 0.66435355 0.02254107 0.2703322 ]\n",
      " [0.02479129 0.03061475 0.01024517 0.93434876]\n",
      " ...\n",
      " [0.00758561 0.9602334  0.00447114 0.02770983]\n",
      " [0.01817485 0.00432319 0.96691805 0.01058384]\n",
      " [0.01821748 0.00434393 0.96681845 0.01062011]]\n",
      "Iteration 1944, Accuracy 0.36382\n",
      "97.11789%change in label assignment\n",
      "0.047013514\n",
      "[[0.0074775  0.9604328  0.00443568 0.02765404]\n",
      " [0.05728088 0.4045302  0.02901558 0.5091734 ]\n",
      " [0.03001595 0.02697091 0.01201615 0.930997  ]\n",
      " ...\n",
      " [0.01192906 0.9288088  0.00682188 0.05244024]\n",
      " [0.01814302 0.00432074 0.9669619  0.01057435]\n",
      " [0.01818428 0.00434578 0.9668489  0.01062106]]\n",
      "Iteration 1945, Accuracy 0.36004\n",
      "97.46649%change in label assignment\n",
      "0.045473408\n",
      "[[0.01946117 0.90920556 0.01194893 0.05938444]\n",
      " [0.03593674 0.7366996  0.01914578 0.2082179 ]\n",
      " [0.02512277 0.03188625 0.01036635 0.9326246 ]\n",
      " ...\n",
      " [0.00860215 0.95600116 0.00509424 0.03030246]\n",
      " [0.01815516 0.00433199 0.9669152  0.01059766]\n",
      " [0.01820515 0.00435912 0.9667864  0.01064935]]\n",
      "Iteration 1946, Accuracy 0.36225\n",
      "98.67924%change in label assignment\n",
      "0.04691134\n",
      "[[0.0074828  0.9604253  0.00443944 0.02765246]\n",
      " [0.05650009 0.43434447 0.02879678 0.48035866]\n",
      " [0.03003864 0.02696342 0.01201799 0.93097997]\n",
      " ...\n",
      " [0.01183838 0.9294289  0.00677191 0.05196082]\n",
      " [0.01812329 0.00432447 0.9669739  0.01057832]\n",
      " [0.01817106 0.00435112 0.966849   0.01062877]]\n",
      "Iteration 1947, Accuracy 0.36692\n",
      "98.48775%change in label assignment\n",
      "0.045525417\n",
      "[[0.01633736 0.92234415 0.00996362 0.05135495]\n",
      " [0.04053855 0.6896928  0.02140746 0.24836121]\n",
      " [0.0248462  0.03016908 0.0102012  0.9347835 ]\n",
      " ...\n",
      " [0.00768296 0.9598632  0.0045214  0.02793242]\n",
      " [0.01813294 0.00433121 0.96694547 0.01059037]\n",
      " [0.0181862  0.00435933 0.9668102  0.01064425]]\n",
      "Iteration 1948, Accuracy 0.3652\n",
      "98.57122%change in label assignment\n",
      "0.04619029\n",
      "[[0.00762531 0.9598944  0.00453431 0.02794598]\n",
      " [0.05680136 0.42367584 0.02888525 0.4906376 ]\n",
      " [0.0289766  0.02672621 0.01164736 0.9326498 ]\n",
      " ...\n",
      " [0.01128554 0.933162   0.00647044 0.04908206]\n",
      " [0.01808612 0.00432469 0.96701247 0.01057674]\n",
      " [0.01813716 0.0043524  0.96688086 0.01062962]]\n",
      "Iteration 1949, Accuracy 0.36554\n",
      "97.45667%change in label assignment\n",
      "0.0465307\n",
      "[[0.01762535 0.9169193  0.01076781 0.05468753]\n",
      " [0.04235896 0.6704585  0.0222272  0.26495546]\n",
      " [0.02502892 0.03104861 0.01027463 0.9336478 ]\n",
      " ...\n",
      " [0.00793381 0.95884615 0.00467184 0.02854826]\n",
      " [0.01805066 0.00430039 0.96711636 0.01053254]\n",
      " [0.01807872 0.00432179 0.9670286  0.01057091]]\n",
      "Iteration 1950, Accuracy 0.3681\n",
      "97.82491%change in label assignment\n",
      "0.042224865\n",
      "[[0.01011283 0.9492154  0.00609264 0.03457915]\n",
      " [0.0518645  0.53805184 0.02687205 0.3832116 ]\n",
      " [0.02611312 0.02657652 0.01059172 0.9367187 ]\n",
      " ...\n",
      " [0.00798277 0.9552961  0.00463688 0.03208424]\n",
      " [0.01800952 0.00428803 0.96719736 0.01050511]\n",
      " [0.01802869 0.00430723 0.967126   0.0105381 ]]\n",
      "Iteration 1951, Accuracy 0.36652\n",
      "99.34207%change in label assignment\n",
      "0.043266144\n",
      "[[0.01486024 0.9285537  0.00906006 0.04752596]\n",
      " [0.03759548 0.71884924 0.02006247 0.22349286]\n",
      " [0.02461035 0.02911948 0.01011859 0.9361515 ]\n",
      " ...\n",
      " [0.00748211 0.96061707 0.00441188 0.02748897]\n",
      " [0.01799602 0.00427238 0.9672525  0.01047911]\n",
      " [0.01800388 0.00428447 0.967217   0.01049466]]\n",
      "Iteration 1952, Accuracy 0.36643\n",
      "97.00987%change in label assignment\n",
      "0.04239066\n",
      "[[0.00925078 0.95304185 0.00554539 0.03216201]\n",
      " [0.04804385 0.59622306 0.02513918 0.33059388]\n",
      " [0.02599427 0.02659765 0.01052402 0.936884  ]\n",
      " ...\n",
      " [0.00805241 0.9548199  0.00466932 0.03245831]\n",
      " [0.01797832 0.00426282 0.9672944  0.0104644 ]\n",
      " [0.01797501 0.00427206 0.9672798  0.01047322]]\n",
      "Iteration 1953, Accuracy 0.36201\n",
      "97.73653%change in label assignment\n",
      "0.042660005\n",
      "[[0.01352772 0.9343125  0.00821576 0.04394403]\n",
      " [0.03630492 0.73154056 0.0194304  0.2127242 ]\n",
      " [0.02462913 0.02855104 0.01010048 0.93671936]\n",
      " ...\n",
      " [0.00733666 0.96110886 0.00431613 0.0272384 ]\n",
      " [0.01796399 0.00425996 0.9673171  0.01045887]\n",
      " [0.01795707 0.00426816 0.96730936 0.01046539]]\n",
      "Iteration 1954, Accuracy 0.35327\n",
      "98.27662%change in label assignment\n",
      "0.041695215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00927094 0.95293546 0.00555865 0.0322349 ]\n",
      " [0.0472464  0.60641426 0.02478962 0.32154968]\n",
      " [0.02670218 0.02647389 0.01078142 0.9360425 ]\n",
      " ...\n",
      " [0.00826599 0.9533584  0.00478614 0.03358945]\n",
      " [0.01793064 0.00427188 0.96733487 0.01046267]\n",
      " [0.01794545 0.00428539 0.9672871  0.01048199]]\n",
      "Iteration 1955, Accuracy 0.34954\n",
      "97.39775%change in label assignment\n",
      "0.04279012\n",
      "[[0.01229241 0.93969274 0.00743083 0.04058397]\n",
      " [0.0418346  0.6736052  0.0221127  0.26244757]\n",
      " [0.02517464 0.02720312 0.01021629 0.93740594]\n",
      " ...\n",
      " [0.00723087 0.9608879  0.00422255 0.02765867]\n",
      " [0.01791756 0.00427038 0.9673547  0.01045731]\n",
      " [0.01793159 0.0042836  0.96730876 0.01047605]]\n",
      "Iteration 1956, Accuracy 0.35592\n",
      "99.48937%change in label assignment\n",
      "0.04110135\n",
      "[[0.01109467 0.94490147 0.00669339 0.03731049]\n",
      " [0.04267577 0.6631955  0.02257887 0.27154985]\n",
      " [0.02564294 0.02673836 0.01039492 0.9372238 ]\n",
      " ...\n",
      " [0.00735625 0.959689   0.00428982 0.02866495]\n",
      " [0.01789665 0.00427048 0.9673806  0.01045226]\n",
      " [0.01791319 0.00428441 0.9673298  0.01047257]]\n",
      "Iteration 1957, Accuracy 0.35739\n",
      "99.18496%change in label assignment\n",
      "0.042298585\n",
      "[[0.01296056 0.9367214  0.00786269 0.04245536]\n",
      " [0.03859987 0.7076579  0.02058021 0.23316203]\n",
      " [0.0250127  0.02724549 0.01018802 0.9375539 ]\n",
      " ...\n",
      " [0.00721315 0.9612924  0.00423084 0.0272636 ]\n",
      " [0.01787581 0.00426157 0.96742564 0.01043701]\n",
      " [0.01788284 0.00427298 0.9673926  0.0104515 ]]\n",
      "Iteration 1958, Accuracy 0.3596\n",
      "99.72505%change in label assignment\n",
      "0.041425753\n",
      "[[0.01140666 0.94349694 0.00689271 0.03820372]\n",
      " [0.04564907 0.62711805 0.02398472 0.3032481 ]\n",
      " [0.02563507 0.02671827 0.01039297 0.93725365]\n",
      " ...\n",
      " [0.00725226 0.96052545 0.0042412  0.02798106]\n",
      " [0.01786363 0.00425082 0.96746296 0.01042269]\n",
      " [0.01785597 0.00425851 0.9674571  0.01042839]]\n",
      "Iteration 1959, Accuracy 0.35985\n",
      "97.48613%change in label assignment\n",
      "[[0.01041005 0.94792473 0.00625674 0.03540848]\n",
      " [0.04962108 0.5734582  0.02575541 0.35116535]\n",
      " [0.02670561 0.02649467 0.01072486 0.9360749 ]\n",
      " ...\n",
      " [0.0076699  0.9573973  0.00444865 0.03048415]\n",
      " [0.01783778 0.00425514 0.96748763 0.01041939]\n",
      " [0.01784106 0.00426542 0.96746206 0.01043144]]\n",
      "Iteration 1960, Accuracy 0.35391\n",
      "97.04424%change in label assignment\n",
      "0.04901553\n",
      "[[0.00809727 0.95448625 0.00468104 0.03273542]\n",
      " [0.05553209 0.23731677 0.0267282  0.68042296]\n",
      " [0.03400822 0.02805632 0.01329746 0.92463803]\n",
      " ...\n",
      " [0.01785325 0.88680375 0.00997936 0.08536365]\n",
      " [0.01783199 0.00425616 0.96749055 0.0104213 ]\n",
      " [0.01784046 0.00426789 0.9674551  0.0104366 ]]\n",
      "Iteration 1961, Accuracy 0.3597\n",
      "97.00005%change in label assignment\n",
      "0.049373627\n",
      "[[0.01810279 0.914728   0.0111216  0.05604762]\n",
      " [0.03027131 0.7875297  0.01637562 0.16582336]\n",
      " [0.02539537 0.03471702 0.01065328 0.9292343 ]\n",
      " ...\n",
      " [0.00815663 0.957879   0.00483807 0.02912636]\n",
      " [0.01782689 0.00425763 0.9674857  0.01042971]\n",
      " [0.01783532 0.00426936 0.9674504  0.010445  ]]\n",
      "Iteration 1962, Accuracy 0.36417\n",
      "96.73%change in label assignment\n",
      "0.056949288\n",
      "[[0.01447596 0.91081804 0.00824127 0.06646466]\n",
      " [0.04539702 0.12993497 0.02129194 0.803376  ]\n",
      " [0.04748431 0.03285426 0.0183864  0.9012751 ]\n",
      " ...\n",
      " [0.02937839 0.7932571  0.01620783 0.16115662]\n",
      " [0.01781293 0.0042685  0.9674844  0.01043416]\n",
      " [0.01783893 0.00428463 0.9674165  0.01045991]]\n",
      "Iteration 1963, Accuracy 0.35754\n",
      "97.05406%change in label assignment\n",
      "0.0640404\n",
      "[[0.03472529 0.84841293 0.02189492 0.0949669 ]\n",
      " [0.01735272 0.8918011  0.0095665  0.08127972]\n",
      " [0.03333491 0.06200302 0.01437577 0.89028627]\n",
      " ...\n",
      " [0.01721958 0.91864896 0.01047305 0.05365845]\n",
      " [0.01781316 0.00425895 0.96749467 0.01043319]\n",
      " [0.01782447 0.00427139 0.96745396 0.01045017]]\n",
      "Iteration 1964, Accuracy 0.35744\n",
      "94.74149%change in label assignment\n",
      "0.0779773\n",
      "[[0.01269819 0.92319274 0.00730977 0.05679922]\n",
      " [0.04540197 0.13083628 0.02144711 0.80231464]\n",
      " [0.05494225 0.03544111 0.02117775 0.8884388 ]\n",
      " ...\n",
      " [0.02898826 0.7962718  0.01609846 0.15864144]\n",
      " [0.01778566 0.00426188 0.967536   0.01041649]\n",
      " [0.01780695 0.00427635 0.96747786 0.01043891]]\n",
      "Iteration 1965, Accuracy 0.35047\n",
      "97.27009%change in label assignment\n",
      "0.06390087\n",
      "[[0.02958782 0.8681487  0.01853518 0.08372831]\n",
      " [0.03205448 0.77321815 0.01713267 0.17759466]\n",
      " [0.02842365 0.04453594 0.01201466 0.9150257 ]\n",
      " ...\n",
      " [0.01371469 0.9335581  0.00828618 0.04444104]\n",
      " [0.01783568 0.00428807 0.96740264 0.01047364]\n",
      " [0.01787598 0.00430749 0.96730894 0.01050762]]\n",
      "Iteration 1966, Accuracy 0.35371\n",
      "96.47469%change in label assignment\n",
      "0.06485562\n",
      "[[0.00846152 0.95193446 0.00495075 0.03465322]\n",
      " [0.05008064 0.17001116 0.02396563 0.75594264]\n",
      " [0.04273652 0.03128377 0.0167437  0.909236  ]\n",
      " ...\n",
      " [0.01935456 0.8748883  0.01093742 0.09481973]\n",
      " [0.01784367 0.00430259 0.96736664 0.01048709]\n",
      " [0.01789783 0.0043255  0.9672473  0.01052934]]\n",
      "Iteration 1967, Accuracy 0.35666\n",
      "98.02131%change in label assignment\n",
      "0.055563264\n",
      "[[0.02686397 0.87897193 0.01672236 0.07744173]\n",
      " [0.02908347 0.7992884  0.01564126 0.15598692]\n",
      " [0.02697365 0.03938038 0.01128691 0.92235905]\n",
      " ...\n",
      " [0.01245604 0.9390442  0.00748601 0.04101382]\n",
      " [0.01786935 0.00431056 0.9673078  0.01051231]\n",
      " [0.01792836 0.00433496 0.9671789  0.01055777]]\n",
      "Iteration 1968, Accuracy 0.3572\n",
      "98.72343%change in label assignment\n",
      "0.0588054\n",
      "[[0.00737365 0.9604988  0.00438479 0.02774272]\n",
      " [0.05747261 0.3667326  0.02904212 0.5467527 ]\n",
      " [0.03319769 0.02795458 0.01327131 0.9255764 ]\n",
      " ...\n",
      " [0.01268818 0.9232459  0.0072791  0.05678677]\n",
      " [0.01782436 0.00430263 0.96738464 0.01048839]\n",
      " [0.01787918 0.00432561 0.96726424 0.01053095]]\n",
      "Iteration 1969, Accuracy 0.35911\n",
      "98.89527%change in label assignment\n",
      "0.0485154\n",
      "[[0.01463632 0.9295532  0.00887472 0.04693578]\n",
      " [0.04590661 0.6272525  0.02387879 0.3029621 ]\n",
      " [0.02473776 0.02890117 0.01007452 0.93628657]\n",
      " ...\n",
      " [0.00732157 0.96097416 0.00427595 0.02742828]\n",
      " [0.01779551 0.0042866  0.9674521  0.01046577]\n",
      " [0.01784012 0.0043071  0.9673504  0.01050232]]\n",
      "Iteration 1970, Accuracy 0.357\n",
      "97.8691%change in label assignment\n",
      "0.048511237\n",
      "[[0.00728323 0.960482   0.00430869 0.02792614]\n",
      " [0.05736885 0.38035735 0.02904827 0.53322554]\n",
      " [0.03125691 0.02731245 0.01255093 0.92887974]\n",
      " ...\n",
      " [0.01419072 0.9127694  0.00809727 0.06494264]\n",
      " [0.01777082 0.00428385 0.9674892  0.0104562 ]\n",
      " [0.0178164  0.00430467 0.96738553 0.01049341]]\n",
      "Iteration 1971, Accuracy 0.35454\n",
      "99.29297%change in label assignment\n",
      "0.04911482\n",
      "[[0.01585188 0.9243356  0.00965087 0.05016166]\n",
      " [0.03854467 0.70984024 0.02040953 0.23120561]\n",
      " [0.02474925 0.03104134 0.01020189 0.9340076 ]\n",
      " ...\n",
      " [0.00752256 0.96044064 0.00441701 0.02761983]\n",
      " [0.01776207 0.00427891 0.9675072  0.01045183]\n",
      " [0.01780403 0.00429887 0.96741015 0.01048695]]\n",
      "Iteration 1972, Accuracy 0.35307\n",
      "98.60559%change in label assignment\n",
      "0.050194412\n",
      "[[0.00726437 0.9604677  0.00428828 0.02797967]\n",
      " [0.05591011 0.44318908 0.02865997 0.4722408 ]\n",
      " [0.03055522 0.02707419 0.01227793 0.9300927 ]\n",
      " ...\n",
      " [0.01431372 0.91191405 0.00815496 0.06561732]\n",
      " [0.01771936 0.00426692 0.9675885  0.01042525]\n",
      " [0.01775527 0.0042853  0.9675028  0.01045666]]\n",
      "Iteration 1973, Accuracy 0.35125\n",
      "98.92964%change in label assignment\n",
      "0.04876916\n",
      "[[0.01440182 0.9305402  0.00874537 0.04631259]\n",
      " [0.0399841  0.69420874 0.02114744 0.24465969]\n",
      " [0.02455091 0.03033002 0.01012693 0.9349922 ]\n",
      " ...\n",
      " [0.00726948 0.96127594 0.00425704 0.0271975 ]\n",
      " [0.0177458  0.00428034 0.9675215  0.01045237]\n",
      " [0.01778995 0.00430061 0.9674209  0.01048855]]\n",
      "Iteration 1974, Accuracy 0.35101\n",
      "97.84946%change in label assignment\n",
      "0.04841736\n",
      "[[0.00732305 0.9599132  0.00431836 0.02844533]\n",
      " [0.05472723 0.47405288 0.02826665 0.44295323]\n",
      " [0.03013231 0.026958   0.01214668 0.93076295]\n",
      " ...\n",
      " [0.0148151  0.90834606 0.00843609 0.06840277]\n",
      " [0.01776304 0.00429412 0.9674694  0.01047343]\n",
      " [0.01781858 0.0043173  0.9673477  0.01051645]]\n",
      "Iteration 1975, Accuracy 0.35381\n",
      "97.73162%change in label assignment\n",
      "0.047033623\n",
      "[[0.03408401 0.8508902  0.02144045 0.09358534]\n",
      " [0.01023855 0.94095606 0.00577725 0.0430281 ]\n",
      " [0.03002842 0.04985096 0.01276013 0.9073605 ]\n",
      " ...\n",
      " [0.01708567 0.9192115  0.01037464 0.0533281 ]\n",
      " [0.01777355 0.00430147 0.96743673 0.01048825]\n",
      " [0.01783128 0.00432509 0.96731114 0.01053248]]\n",
      "Iteration 1976, Accuracy 0.35926\n",
      "98.11951%change in label assignment\n",
      "0.06744664\n",
      "[[0.01188814 0.9287894  0.00684043 0.05248205]\n",
      " [0.05697209 0.29139537 0.02836313 0.62326944]\n",
      " [0.04355251 0.03150079 0.01705686 0.9078898 ]\n",
      " ...\n",
      " [0.02732355 0.81094426 0.01516954 0.14656264]\n",
      " [0.01771511 0.00428052 0.96755916 0.01044521]\n",
      " [0.0177652  0.00430228 0.9674476  0.0104849 ]]\n",
      "Iteration 1977, Accuracy 0.36535\n",
      "97.4714%change in label assignment\n",
      "0.06265557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03783827 0.83657914 0.02400916 0.10157343]\n",
      " [0.0080977  0.95501924 0.00463507 0.03224798]\n",
      " [0.03302048 0.06119627 0.01426738 0.8915159 ]\n",
      " ...\n",
      " [0.02013945 0.90641236 0.01234822 0.06109995]\n",
      " [0.0177302  0.00428781 0.967516   0.0104659 ]\n",
      " [0.01778538 0.00431137 0.96739405 0.01050921]]\n",
      "Iteration 1978, Accuracy 0.3594\n",
      "98.00167%change in label assignment\n",
      "0.06767371\n",
      "[[0.00766968 0.9572682  0.00450334 0.03055878]\n",
      " [0.05748008 0.35676593 0.02903964 0.5567144 ]\n",
      " [0.04031283 0.03037425 0.01588113 0.9134318 ]\n",
      " ...\n",
      " [0.01690485 0.8931559  0.00958686 0.08035245]\n",
      " [0.01777615 0.00431613 0.96740305 0.01050465]\n",
      " [0.01784647 0.00434604 0.96724385 0.01056359]]\n",
      "Iteration 1979, Accuracy 0.36269\n",
      "94.63348%change in label assignment\n",
      "0.05116825\n",
      "[[0.02301307 0.8945093  0.01422029 0.06825729]\n",
      " [0.02918161 0.7979501  0.01572402 0.15714428]\n",
      " [0.02505731 0.03239081 0.01034252 0.9322093 ]\n",
      " ...\n",
      " [0.01070216 0.9467519  0.00639483 0.03615108]\n",
      " [0.01772581 0.0042945  0.9675105  0.01046915]\n",
      " [0.01777862 0.00431976 0.9673837  0.01051793]]\n",
      "Iteration 1980, Accuracy 0.365\n",
      "98.63996%change in label assignment\n",
      "0.051687468\n",
      "[[0.00781998 0.9590456  0.00467912 0.02845529]\n",
      " [0.05179449 0.53259605 0.02706419 0.3885453 ]\n",
      " [0.0296418  0.02679044 0.01197066 0.93159705]\n",
      " ...\n",
      " [0.01007624 0.9410291  0.00582611 0.04306854]\n",
      " [0.01763268 0.00426759 0.96768075 0.01041891]\n",
      " [0.01767301 0.00429064 0.96757424 0.01046216]]\n",
      "Iteration 1981, Accuracy 0.36495\n",
      "97.34865%change in label assignment\n",
      "0.04464631\n",
      "[[0.01472298 0.92918986 0.00894025 0.04714694]\n",
      " [0.03567236 0.7380534  0.01902808 0.20724623]\n",
      " [0.02456146 0.02834603 0.01001576 0.9370767 ]\n",
      " ...\n",
      " [0.00742268 0.9608593  0.00435645 0.02736154]\n",
      " [0.01760681 0.00424564 0.967766   0.01038157]\n",
      " [0.01762853 0.0042626  0.96769786 0.01041107]]\n",
      "Iteration 1982, Accuracy 0.36741\n",
      "98.26189%change in label assignment\n",
      "0.045856282\n",
      "[[0.00875956 0.95509076 0.00526613 0.03088355]\n",
      " [0.04515311 0.63067776 0.02395435 0.30021474]\n",
      " [0.02598196 0.02624594 0.01061728 0.9371548 ]\n",
      " ...\n",
      " [0.00857667 0.9510748  0.00498225 0.03536628]\n",
      " [0.01756809 0.00422907 0.9678498  0.01035306]\n",
      " [0.01758414 0.00424216 0.9678015  0.01037221]]\n",
      "Iteration 1983, Accuracy 0.36461\n",
      "96.85766%change in label assignment\n",
      "0.044111397\n",
      "[[0.01203999 0.9407958  0.00727638 0.03988788]\n",
      " [0.0402942  0.6898666  0.02136923 0.24846998]\n",
      " [0.024439   0.02772108 0.01000194 0.93783796]\n",
      " ...\n",
      " [0.00718002 0.9610208  0.0041908  0.02760831]\n",
      " [0.01755969 0.00422811 0.9678605  0.01035173]\n",
      " [0.01757515 0.00424088 0.9678137  0.01037034]]\n",
      "Iteration 1984, Accuracy 0.36284\n",
      "98.64487%change in label assignment\n",
      "0.04421436\n",
      "[[0.0081002  0.9580455  0.00483662 0.0290177 ]\n",
      " [0.05113569 0.5464599  0.02659991 0.3758045 ]\n",
      " [0.02722024 0.02621689 0.01099798 0.93556494]\n",
      " ...\n",
      " [0.00950994 0.94487286 0.00547975 0.04013741]\n",
      " [0.01756364 0.00424287 0.9678246  0.01036888]\n",
      " [0.01759505 0.00425973 0.96774805 0.01039713]]\n",
      "Iteration 1985, Accuracy 0.36078\n",
      "96.42559%change in label assignment\n",
      "0.044579215\n",
      "[[0.01732568 0.9180507  0.01061028 0.05401333]\n",
      " [0.03001923 0.78969765 0.01622801 0.1640551 ]\n",
      " [0.0244373  0.03042932 0.01010333 0.9350301 ]\n",
      " ...\n",
      " [0.00792453 0.9589083  0.00468624 0.02848093]\n",
      " [0.01753717 0.00422832 0.96788704 0.01034745]\n",
      " [0.01755554 0.00424174 0.967835   0.01036771]]\n",
      "Iteration 1986, Accuracy 0.36201\n",
      "97.85437%change in label assignment\n",
      "0.04535076\n",
      "[[0.00800931 0.9583542  0.00479154 0.02884498]\n",
      " [0.04902843 0.57771164 0.02575457 0.34750536]\n",
      " [0.02729912 0.02623429 0.01107732 0.9353892 ]\n",
      " ...\n",
      " [0.00964927 0.9438834  0.00557383 0.04089351]\n",
      " [0.01750559 0.00421277 0.9679601  0.01032149]\n",
      " [0.01751002 0.00422233 0.96793467 0.010333  ]]\n",
      "Iteration 1987, Accuracy 0.3596\n",
      "96.81838%change in label assignment\n",
      "0.045339838\n",
      "[[0.01510774 0.927499   0.00920081 0.04819245]\n",
      " [0.03750843 0.719245   0.0199704  0.22327615]\n",
      " [0.02432722 0.02878056 0.009989   0.93690324]\n",
      " ...\n",
      " [0.00732935 0.9612534  0.00430703 0.02711018]\n",
      " [0.01750094 0.00422004 0.96794957 0.01032945]\n",
      " [0.0175139  0.00423173 0.96790826 0.01034607]]\n",
      "Iteration 1988, Accuracy 0.35504\n",
      "97.31919%change in label assignment\n",
      "0.04430421\n",
      "[[0.00723247 0.961468   0.00428021 0.0270193 ]\n",
      " [0.05720699 0.3961075  0.02888281 0.5178028 ]\n",
      " [0.02956647 0.02666236 0.0118405  0.93193066]\n",
      " ...\n",
      " [0.01201754 0.92789525 0.00686161 0.05322563]\n",
      " [0.01747438 0.00421284 0.9680001  0.01031267]\n",
      " [0.01748269 0.00422299 0.9679682  0.01032608]]\n",
      "Iteration 1989, Accuracy 0.35445\n",
      "97.26518%change in label assignment\n",
      "0.046895765\n",
      "[[0.02241794 0.8967187  0.01392206 0.06694128]\n",
      " [0.02849379 0.8024131  0.01548418 0.15360898]\n",
      " [0.02482041 0.03331742 0.01041412 0.9314481 ]\n",
      " ...\n",
      " [0.01012308 0.94923115 0.00607676 0.03456905]\n",
      " [0.01746223 0.00421035 0.96801656 0.01031086]\n",
      " [0.01746723 0.00422004 0.96799004 0.01032268]]\n",
      "Iteration 1990, Accuracy 0.35494\n",
      "96.43541%change in label assignment\n",
      "0.045587912\n",
      "[[0.0103619  0.9480085  0.00626905 0.03536049]\n",
      " [0.04550017 0.6258113  0.02407    0.3046185 ]\n",
      " [0.02615494 0.02616937 0.01065713 0.9370185 ]\n",
      " ...\n",
      " [0.00744412 0.95871633 0.00435532 0.02948424]\n",
      " [0.01744075 0.00419938 0.9680682  0.01029172]\n",
      " [0.01743509 0.00420632 0.96806157 0.01029707]]\n",
      "Iteration 1991, Accuracy 0.35327\n",
      "98.83635%change in label assignment\n",
      "0.04302739\n",
      "[[0.01518295 0.9270595  0.00927717 0.04848045]\n",
      " [0.03274844 0.7640039  0.01767446 0.18557318]\n",
      " [0.02439376 0.02731996 0.00999122 0.93829507]\n",
      " ...\n",
      " [0.0073558  0.9611575  0.00434135 0.02714538]\n",
      " [0.01742723 0.00420926 0.9680656  0.01029793]\n",
      " [0.01743641 0.00421999 0.96803135 0.01031223]]\n",
      "Iteration 1992, Accuracy 0.35219\n",
      "98.01149%change in label assignment\n",
      "0.04129322\n",
      "[[0.01128241 0.943985   0.00683617 0.03789636]\n",
      " [0.03681197 0.7238696  0.01980392 0.21951449]\n",
      " [0.02573867 0.02619816 0.0104713  0.9375918 ]\n",
      " ...\n",
      " [0.00732867 0.9595499  0.00428297 0.02883846]\n",
      " [0.01741679 0.00421795 0.9680591  0.01030618]\n",
      " [0.01743669 0.00423147 0.9680048  0.01032701]]\n",
      "Iteration 1993, Accuracy 0.35577\n",
      "98.17843%change in label assignment\n",
      "0.04136799\n",
      "[[0.01705937 0.9190502  0.01046933 0.05342106]\n",
      " [0.02232518 0.852701   0.01230691 0.11266695]\n",
      " [0.02413177 0.02829501 0.00993752 0.9376357 ]\n",
      " ...\n",
      " [0.00776943 0.9595251  0.00460362 0.02810188]\n",
      " [0.01739876 0.00420963 0.96809894 0.01029271]\n",
      " [0.01741192 0.00422143 0.9680572  0.01030946]]\n",
      "Iteration 1994, Accuracy 0.36142\n",
      "99.33225%change in label assignment\n",
      "0.040250808\n",
      "[[0.01097014 0.9453868  0.00663088 0.03701216]\n",
      " [0.03452577 0.7464957  0.01862936 0.20034915]\n",
      " [0.02578455 0.02619925 0.01045682 0.9375594 ]\n",
      " ...\n",
      " [0.00733879 0.9594686  0.0042817  0.02891086]\n",
      " [0.01737594 0.00420475 0.9681389  0.01028043]\n",
      " [0.0173868  0.00421609 0.9681012  0.01029592]]\n",
      "Iteration 1995, Accuracy 0.36103\n",
      "99.46973%change in label assignment\n",
      "0.041198015\n",
      "[[0.01290445 0.93694586 0.00782715 0.04232256]\n",
      " [0.02995275 0.7891916  0.01624461 0.164611  ]\n",
      " [0.0247389  0.02681394 0.01006665 0.9383805 ]\n",
      " ...\n",
      " [0.00712277 0.9618434  0.00418063 0.02685326]\n",
      " [0.01736208 0.00419766 0.96817166 0.01026863]\n",
      " [0.01736651 0.00420758 0.9681455  0.01028045]]\n",
      "Iteration 1996, Accuracy 0.35999\n",
      "97.45176%change in label assignment\n",
      "0.04052334\n",
      "[[0.01188537 0.9413255  0.00720954 0.03957954]\n",
      " [0.03448075 0.74667937 0.01861537 0.20022449]\n",
      " [0.02575479 0.02619302 0.01046188 0.9375903 ]\n",
      " ...\n",
      " [0.00719478 0.9605454  0.0042088  0.028051  ]\n",
      " [0.01737591 0.00422919 0.96808594 0.01030896]\n",
      " [0.01741074 0.0042467  0.9680036  0.01033898]]\n",
      "Iteration 1997, Accuracy 0.36029\n",
      "95.34541%change in label assignment\n",
      "0.04237902\n",
      "[[0.01578758 0.9244261  0.00966029 0.05012613]\n",
      " [0.02404623 0.8387571  0.01321118 0.12398551]\n",
      " [0.02431591 0.02728738 0.00995902 0.93843776]\n",
      " ...\n",
      " [0.00738728 0.9610402  0.00436176 0.02721079]\n",
      " [0.0173329  0.00420801 0.96818626 0.01027289]\n",
      " [0.01734472 0.00422123 0.96813995 0.0102941 ]]\n",
      "Iteration 1998, Accuracy 0.36829\n",
      "99.38626%change in label assignment\n",
      "0.038847335\n",
      "[[0.01215998 0.94011676 0.00737827 0.04034496]\n",
      " [0.03311849 0.75962126 0.01790703 0.18935323]\n",
      " [0.02535978 0.02628227 0.01030589 0.938052  ]\n",
      " ...\n",
      " [0.00714909 0.96090686 0.00418297 0.02776104]\n",
      " [0.01730581 0.00419657 0.96824485 0.0102528 ]\n",
      " [0.01731054 0.00420744 0.9682139  0.01026808]]\n",
      "Iteration 1999, Accuracy 0.36819\n",
      "99.73978%change in label assignment\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "y = ae_data_y\n",
    "update_interval = X.shape[0]/batch_size\n",
    "iteration = 0\n",
    "index = 0\n",
    "train_dec_epochs = 2000\n",
    "master_acc = []\n",
    "# training loop\n",
    "for i in range(train_dec_epochs):\n",
    "    \n",
    "    #if i % update_interval == 0:\n",
    "    if True:\n",
    "        q = model.predict(X, verbose=0)\n",
    "        print(q)\n",
    "        p = p_mat(q)\n",
    "        y_pred = q.argmax(1)\n",
    "        delta_label = ((y_pred == y_prediction).sum().astype(np.float32) / y_pred.shape[0])\n",
    "        if y is not None:\n",
    "            acc = cluster_acc(y, y_prediction)[0]\n",
    "            master_acc.append(acc)\n",
    "            print('Iteration '+str(i)+', Accuracy '+str(np.round(acc, 5)))\n",
    "        else:\n",
    "            print(str(np.round(delta_label*100, 5))+'%change in label assignment')\n",
    "        print(str(np.round(delta_label*100, 5))+'%change in label assignment')            \n",
    "        \n",
    "        y_prediction = y_pred\n",
    "        \n",
    "    for i, layer in enumerate(encoder.layers):\n",
    "        layer.set_weights(model.layers[0].layers[i].get_weights())\n",
    "    cluster_centers = model.layers[-1].get_weights()[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(index+1)*batch_size > X.shape[0]:\n",
    "        loss = model.train_on_batch(X[index*batch_size::], p[index*batch_size::],\n",
    "                                    p[index*batch_size::])\n",
    "        index = 0\n",
    "    else:\n",
    "        loss = model.train_on_batch(X[index*batch_size:(index+1) * batch_size],\n",
    "                                    p[index*batch_size:(index+1) * batch_size])\n",
    "        print(loss)\n",
    "        index += 1\n",
    "    \n",
    "    iteration +=1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-3a264709f570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "plt(master_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
